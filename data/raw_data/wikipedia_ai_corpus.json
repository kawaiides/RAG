[
    {
        "title": "Artificial intelligence",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nGoals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nReasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\nPlanning and decision-making\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\nLearning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\n\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\nNatural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\nPerception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.\n\nSocial intelligence\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.\n\nGeneral intelligence\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\nTechniques\nAI research uses a wide variety of techniques to accomplish the goals above.\n\nSearch and optimization\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\nState space search\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.\n\nLocal search\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\nLogic\nFormal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\n\nProbabilistic methods for uncertain reasoning\nMany problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\nClassifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\nArtificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.\n\nDeep learning\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\n\nGPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\n\nHardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.\n\nApplications\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n\nHealth and medicine\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\nGames\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\n\nMathematics\nLarge language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.   \nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\nTopological deep learning integrates various topological approaches.\n\nFinance\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\n\nMilitary\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.\n\nGenerative AI\nAgents\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\n\nSexuality\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.\n\nOther industry-specific tasks\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.\nIn agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.\n\nEthics\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\nRisks and harm\nPrivacy and copyright\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\n\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\nPower needs and environmental impacts\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\nIn 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.\n\nMisinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.\nIn the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.\nAI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.\n\nAlgorithmic bias and fairness\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\nLack of transparency\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\n\nBad actors and weaponized AI\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\nThere are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\n\nTechnological unemployment\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\n\nExistential risk\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\n\nEthical machines and alignment\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.\n\nOpen source\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\n\nFrameworks\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\n\nRegulation\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\n\nHistory\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\n\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.\n\nPhilosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\n\nDefining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\nThere has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.\n\nEvaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\nSymbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\nNeat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\n\nSoft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\nNarrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\nMachine consciousness, sentience, and mind\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\nConsciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\nComputationalism and functionalism\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.\n\nAI welfare and rights\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\n\nFuture\nSuperintelligence and the singularity\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\n\nTranshumanism\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\n\nIn fiction\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\nSee also\nArtificial consciousness – Field in cognitive science\nArtificial intelligence and elections – Use and impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nAssociation for the Advancement of Artificial Intelligence (AAAI)\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Automation of business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\nDigital immortality – Hypothetical concept of storing a personality in digital form\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\nIntelligence amplification – Use of information technology to augment human intelligence\nIntelligent agent – Software agent which acts autonomously\nIntelligent automation – Software process that combines robotic process automation and artificial intelligence\nList of artificial intelligence journals\nList of artificial intelligence projects\nMind uploading – Hypothetical process of digitally emulating a brain\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\nRobotic process automation – Form of business process automation technology\nThe Last Day – 1967 Welsh science fiction novel\nWetware computer – Computer composed of organic material\nDARWIN EU - A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real-world evidence (RWE) to support the evaluation and supervision of medicines across the EU.\n\nExplanatory notes\nReferences\nAI textbooks\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\n\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThe four most widely used AI textbooks in 2008:\n\nOther textbooks:\n\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\n\nHistory of AI\nOther sources\nFurther reading\nExternal links\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"
    },
    {
        "title": "Actor-critic algorithm",
        "text": "The actor-critic algorithm (AC) is a family of reinforcement learning (RL) algorithms that combine policy-based RL algorithms such as policy gradient methods, and value-based RL algorithms such as value iteration, Q-learning, SARSA, and TD learning.\nAn AC algorithm consists of two main components: an \"actor\" that determines which actions to take according to a policy function, and a \"critic\" that evaluates those actions according to a value function. Some AC algorithms are on-policy, some are off-policy. Some apply to either continuous or discrete action spaces. Some work in both cases.\n\nOverview\nThe actor-critic methods can be understood as an improvement over pure policy gradient methods like REINFORCE via introducing a baseline.\n\nActor\nThe actor uses a policy function \n  \n    \n      \n        π\n        (\n        a\n        \n          |\n        \n        s\n        )\n      \n    \n    {\\displaystyle \\pi (a|s)}\n  \n, while the critic estimates either the value function \n  \n    \n      \n        V\n        (\n        s\n        )\n      \n    \n    {\\displaystyle V(s)}\n  \n, the action-value Q-function \n  \n    \n      \n        Q\n        (\n        s\n        ,\n        a\n        )\n        ,\n      \n    \n    {\\displaystyle Q(s,a),}\n  \n the advantage function \n  \n    \n      \n        A\n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle A(s,a)}\n  \n, or any combination thereof.\nThe actor is a parameterized function \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle \\pi _{\\theta }}\n  \n, where \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n are the parameters of the actor. The actor takes as argument the state of the environment \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and produces a probability distribution \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n        (\n        ⋅\n        \n          |\n        \n        s\n        )\n      \n    \n    {\\displaystyle \\pi _{\\theta }(\\cdot |s)}\n  \n.\nIf the action space is discrete, then \n  \n    \n      \n        \n          ∑\n          \n            a\n          \n        \n        \n          π\n          \n            θ\n          \n        \n        (\n        a\n        \n          |\n        \n        s\n        )\n        =\n        1\n      \n    \n    {\\displaystyle \\sum _{a}\\pi _{\\theta }(a|s)=1}\n  \n. If the action space is continuous, then \n  \n    \n      \n        \n          ∫\n          \n            a\n          \n        \n        \n          π\n          \n            θ\n          \n        \n        (\n        a\n        \n          |\n        \n        s\n        )\n        d\n        a\n        =\n        1\n      \n    \n    {\\displaystyle \\int _{a}\\pi _{\\theta }(a|s)da=1}\n  \n.\nThe goal of policy optimization is to improve the actor. That is, to find some \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n that maximizes the expected episodic reward \n  \n    \n      \n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle J(\\theta )}\n  \n:\n  \n    \n      \n        J\n        (\n        θ\n        )\n        =\n        \n          \n            E\n          \n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        \n          [\n          \n            \n              ∑\n              \n                t\n                =\n                0\n              \n              \n                T\n              \n            \n            \n              γ\n              \n                t\n              \n            \n            \n              r\n              \n                t\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle J(\\theta )=\\mathbb {E} _{\\pi _{\\theta }}\\left[\\sum _{t=0}^{T}\\gamma ^{t}r_{t}\\right]}\n  \nwhere \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is the discount factor, \n  \n    \n      \n        \n          r\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle r_{t}}\n  \n is the reward at step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the time-horizon (which can be infinite).\nThe goal of policy gradient method is to optimize \n  \n    \n      \n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle J(\\theta )}\n  \n by gradient ascent on the policy gradient \n  \n    \n      \n        ∇\n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle \\nabla J(\\theta )}\n  \n.\nAs detailed on the policy gradient method page, there are many unbiased estimators of the policy gradient:\n  \n    \n      \n        \n          ∇\n          \n            θ\n          \n        \n        J\n        (\n        θ\n        )\n        =\n        \n          \n            E\n          \n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        \n          [\n          \n            \n              ∑\n              \n                0\n                ≤\n                j\n                ≤\n                T\n              \n            \n            \n              ∇\n              \n                θ\n              \n            \n            ln\n            ⁡\n            \n              π\n              \n                θ\n              \n            \n            (\n            \n              A\n              \n                j\n              \n            \n            \n              |\n            \n            \n              S\n              \n                j\n              \n            \n            )\n            ⋅\n            \n              Ψ\n              \n                j\n              \n            \n            \n              \n                |\n              \n            \n            \n              S\n              \n                0\n              \n            \n            =\n            \n              s\n              \n                0\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\nabla _{\\theta }J(\\theta )=\\mathbb {E} _{\\pi _{\\theta }}\\left[\\sum _{0\\leq j\\leq T}\\nabla _{\\theta }\\ln \\pi _{\\theta }(A_{j}|S_{j})\\cdot \\Psi _{j}{\\Big |}S_{0}=s_{0}\\right]}\n  \nwhere \n  \n    \n      \n        \n          Ψ\n          \n            j\n          \n        \n      \n    \n    {\\textstyle \\Psi _{j}}\n  \n is a linear sum of the following:\n\n  \n    \n      \n        \n          ∑\n          \n            0\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n            i\n          \n        \n        \n          R\n          \n            i\n          \n        \n        )\n      \n    \n    {\\textstyle \\sum _{0\\leq i\\leq T}(\\gamma ^{i}R_{i})}\n  \n.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            j\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n            i\n            −\n            j\n          \n        \n        \n          R\n          \n            i\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}\\sum _{j\\leq i\\leq T}(\\gamma ^{i-j}R_{i})}\n  \n: the REINFORCE algorithm.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            j\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n            i\n            −\n            j\n          \n        \n        \n          R\n          \n            i\n          \n        \n        )\n        −\n        b\n        (\n        \n          S\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}\\sum _{j\\leq i\\leq T}(\\gamma ^{i-j}R_{i})-b(S_{j})}\n  \n: the REINFORCE with baseline algorithm. Here \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is an arbitrary function.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          (\n          \n            \n              R\n              \n                j\n              \n            \n            +\n            γ\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                1\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\left(R_{j}+\\gamma V^{\\pi _{\\theta }}(S_{j+1})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(1) learning.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        \n          S\n          \n            j\n          \n        \n        ,\n        \n          A\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}Q^{\\pi _{\\theta }}(S_{j},A_{j})}\n  \n.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          A\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        \n          S\n          \n            j\n          \n        \n        ,\n        \n          A\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}A^{\\pi _{\\theta }}(S_{j},A_{j})}\n  \n: Advantage Actor-Critic (A2C).\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          (\n          \n            \n              R\n              \n                j\n              \n            \n            +\n            γ\n            \n              R\n              \n                j\n                +\n                1\n              \n            \n            +\n            \n              γ\n              \n                2\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                2\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\left(R_{j}+\\gamma R_{j+1}+\\gamma ^{2}V^{\\pi _{\\theta }}(S_{j+2})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(2) learning.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          (\n          \n            \n              ∑\n              \n                k\n                =\n                0\n              \n              \n                n\n                −\n                1\n              \n            \n            \n              γ\n              \n                k\n              \n            \n            \n              R\n              \n                j\n                +\n                k\n              \n            \n            +\n            \n              γ\n              \n                n\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                n\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\left(\\sum _{k=0}^{n-1}\\gamma ^{k}R_{j+k}+\\gamma ^{n}V^{\\pi _{\\theta }}(S_{j+n})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(n) learning.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            \n              λ\n              \n                n\n                −\n                1\n              \n            \n            \n              1\n              −\n              λ\n            \n          \n        \n        ⋅\n        \n          (\n          \n            \n              ∑\n              \n                k\n                =\n                0\n              \n              \n                n\n                −\n                1\n              \n            \n            \n              γ\n              \n                k\n              \n            \n            \n              R\n              \n                j\n                +\n                k\n              \n            \n            +\n            \n              γ\n              \n                n\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                n\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\sum _{n=1}^{\\infty }{\\frac {\\lambda ^{n-1}}{1-\\lambda }}\\cdot \\left(\\sum _{k=0}^{n-1}\\gamma ^{k}R_{j+k}+\\gamma ^{n}V^{\\pi _{\\theta }}(S_{j+n})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(λ) learning, also known as GAE (generalized advantage estimate). This is obtained by an exponentially decaying sum of the TD(n) learning terms.\n\nCritic\nIn the unbiased estimators given above, certain functions such as \n  \n    \n      \n        \n          V\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        ,\n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        ,\n        \n          A\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle V^{\\pi _{\\theta }},Q^{\\pi _{\\theta }},A^{\\pi _{\\theta }}}\n  \n appear. These are approximated by the critic. Since these functions all depend on the actor, the critic must learn alongside the actor. The critic is learned by value-based RL algorithms.\nFor example, if the critic is estimating the state-value function \n  \n    \n      \n        \n          V\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{\\pi _{\\theta }}(s)}\n  \n, then it can be learned by any value function approximation method. Let the critic be a function approximator \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\phi }(s)}\n  \n with parameters \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n.\nThe simplest example is TD(1) learning, which trains the critic to minimize the TD(1) error:\n  \n    \n      \n        \n          δ\n          \n            i\n          \n        \n        =\n        \n          R\n          \n            i\n          \n        \n        +\n        γ\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            1\n          \n        \n        )\n        −\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta _{i}=R_{i}+\\gamma V_{\\phi }(S_{i+1})-V_{\\phi }(S_{i})}\n  \nThe critic parameters are updated by gradient descent on the squared TD error:\n  \n    \n      \n        ϕ\n        ←\n        ϕ\n        −\n        α\n        \n          ∇\n          \n            ϕ\n          \n        \n        (\n        \n          δ\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        ϕ\n        +\n        α\n        \n          δ\n          \n            i\n          \n        \n        \n          ∇\n          \n            ϕ\n          \n        \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\phi \\leftarrow \\phi -\\alpha \\nabla _{\\phi }(\\delta _{i})^{2}=\\phi +\\alpha \\delta _{i}\\nabla _{\\phi }V_{\\phi }(S_{i})}\n  \nwhere \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is the learning rate. Note that the gradient is taken with respect to the \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n in \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle V_{\\phi }(S_{i})}\n  \n only, since the \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n in \n  \n    \n      \n        γ\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle \\gamma V_{\\phi }(S_{i+1})}\n  \n constitutes a moving target, and the gradient is not taken with respect to that. This is a common source of error in implementations that use automatic differentiation, and requires \"stopping the gradient\" at that point.\nSimilarly, if the critic is estimating the action-value function \n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi _{\\theta }}}\n  \n, then it can be learned by Q-learning or SARSA. In SARSA, the critic maintains an estimate of the Q-function, parameterized by \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, denoted as \n  \n    \n      \n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q_{\\phi }(s,a)}\n  \n. The temporal difference error is then calculated as \n  \n    \n      \n        \n          δ\n          \n            i\n          \n        \n        =\n        \n          R\n          \n            i\n          \n        \n        +\n        γ\n        \n          Q\n          \n            θ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            1\n          \n        \n        ,\n        \n          A\n          \n            i\n            +\n            1\n          \n        \n        )\n        −\n        \n          Q\n          \n            θ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        ,\n        \n          A\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta _{i}=R_{i}+\\gamma Q_{\\theta }(S_{i+1},A_{i+1})-Q_{\\theta }(S_{i},A_{i})}\n  \n. The critic is then updated by\n  \n    \n      \n        θ\n        ←\n        θ\n        +\n        α\n        \n          δ\n          \n            i\n          \n        \n        \n          ∇\n          \n            θ\n          \n        \n        \n          Q\n          \n            θ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        ,\n        \n          A\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\theta \\leftarrow \\theta +\\alpha \\delta _{i}\\nabla _{\\theta }Q_{\\theta }(S_{i},A_{i})}\n  \nThe advantage critic can be trained by training both a Q-function \n  \n    \n      \n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q_{\\phi }(s,a)}\n  \n and a state-value function \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\phi }(s)}\n  \n, then let \n  \n    \n      \n        \n          A\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n        =\n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n        −\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle A_{\\phi }(s,a)=Q_{\\phi }(s,a)-V_{\\phi }(s)}\n  \n. Although, it is more common to train just a state-value function \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\phi }(s)}\n  \n, then estimate the advantage by\n  \n    \n      \n        \n          A\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        ,\n        \n          A\n          \n            i\n          \n        \n        )\n        ≈\n        \n          ∑\n          \n            j\n            ∈\n            0\n            :\n            n\n            −\n            1\n          \n        \n        \n          γ\n          \n            j\n          \n        \n        \n          R\n          \n            i\n            +\n            j\n          \n        \n        +\n        \n          γ\n          \n            n\n          \n        \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            n\n          \n        \n        )\n        −\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle A_{\\phi }(S_{i},A_{i})\\approx \\sum _{j\\in 0:n-1}\\gamma ^{j}R_{i+j}+\\gamma ^{n}V_{\\phi }(S_{i+n})-V_{\\phi }(S_{i})}\n  \nHere, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is a positive integer. The higher \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is, the more lower is the bias in the advantage estimation, but at the price of higher variance.\nThe Generalized Advantage Estimation (GAE) introduces a hyperparameter \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n that smoothly interpolates between Monte Carlo returns (\n  \n    \n      \n        λ\n        =\n        1\n      \n    \n    {\\displaystyle \\lambda =1}\n  \n, high variance, no bias) and 1-step TD learning (\n  \n    \n      \n        λ\n        =\n        0\n      \n    \n    {\\displaystyle \\lambda =0}\n  \n, low variance, high bias). This hyperparameter can be adjusted to pick the optimal bias-variance trade-off in advantage estimation. It uses an exponentially decaying average of n-step returns with \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n being the decay strength.\n\nVariants\nAsynchronous Advantage Actor-Critic (A3C): Parallel and asynchronous version of A2C.\nSoft Actor-Critic (SAC): Incorporates entropy maximization for improved exploration.\nDeep Deterministic Policy Gradient (DDPG): Specialized for continuous action spaces.\n\nSee also\nReinforcement learning\nPolicy gradient method\nDeep reinforcement learning\n\nReferences\n\nKonda, Vijay R.; Tsitsiklis, John N. (January 2003). \"On Actor-Critic Algorithms\". SIAM Journal on Control and Optimization. 42 (4): 1143–1166. doi:10.1137/S0363012901385691. ISSN 0363-0129.\nSutton, Richard S.; Barto, Andrew G. (2018). Reinforcement learning: an introduction. Adaptive computation and machine learning series (2 ed.). Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-03924-6.\nBertsekas, Dimitri P. (2019). Reinforcement learning and optimal control (2 ed.). Belmont, Massachusetts: Athena Scientific. ISBN 978-1-886529-39-7.\nGrossi, Csaba (2010). Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning (1 ed.). Cham: Springer International Publishing. ISBN 978-3-031-00423-0.\nGrondman, Ivo; Busoniu, Lucian; Lopes, Gabriel A. D.; Babuska, Robert (November 2012). \"A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients\". IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications and Reviews. 42 (6): 1291–1307. Bibcode:2012ITHMS..42.1291G. doi:10.1109/TSMCC.2012.2218595. ISSN 1094-6977.",
        "url": "https://en.wikipedia.org/wiki/Actor-critic_algorithm"
    },
    {
        "title": "Admissible heuristic",
        "text": "In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path. In other words, it should act as a lower bound.\nIt is related to the concept of consistent heuristics. While all consistent heuristics are admissible, not all admissible heuristics are consistent.\n\nSearch algorithms\nAn admissible heuristic is used to estimate the cost of reaching the goal state in an informed search algorithm. In order for a heuristic\nto be admissible to the search problem, the estimated cost must always be lower than or equal to the actual cost of reaching the goal state. \nThe search algorithm uses the admissible heuristic to find an estimated \noptimal path to the goal state from the current node. \nFor example, in A* search the evaluation function (where \n\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the current node) is:\n\n  \n    \n      \n        f\n        (\n        n\n        )\n        =\n        g\n        (\n        n\n        )\n        +\n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)=g(n)+h(n)}\n  \n\nwhere\n\n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n = the evaluation function.\n\n  \n    \n      \n        g\n        (\n        n\n        )\n      \n    \n    {\\displaystyle g(n)}\n  \n = the cost from the start node to the current node\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n = estimated cost from current node to goal.\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is calculated using the heuristic \nfunction. With a non-admissible heuristic, the A* algorithm could \noverlook the optimal solution to a search problem due to an \noverestimation in \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n.\n\nFormulation\nn\n      \n    \n    {\\displaystyle n}\n  \n is a node\n\n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is a heuristic\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is cost indicated by \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n to reach a goal from \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n\n  \n    \n      \n        \n          h\n          \n            ∗\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle h^{*}(n)}\n  \n is the optimal cost to reach a goal from \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is admissible if, \n  \n    \n      \n        ∀\n        n\n      \n    \n    {\\displaystyle \\forall n}\n  \n\n  \n    \n      \n        h\n        (\n        n\n        )\n        ≤\n        \n          h\n          \n            ∗\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)\\leq h^{*}(n)}\n\nConstruction\nAn admissible heuristic can be derived from a relaxed\nversion of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods.\n\nExamples\nTwo different examples of admissible heuristics apply to the fifteen puzzle problem:\n\nHamming distance\nManhattan distance\nThe Hamming distance is the total number of misplaced tiles. It is clear that this heuristic is admissible since the total number of moves to order the tiles correctly is at least the number of misplaced tiles (each tile not in place must be moved at least once). The cost (number of moves) to the goal (an ordered puzzle) is at least the Hamming distance of the puzzle.\nThe Manhattan distance of a puzzle is defined as:\n\n  \n    \n      \n        h\n        (\n        n\n        )\n        =\n        \n          ∑\n          \n            all tiles\n          \n        \n        \n          \n            d\n            i\n            s\n            t\n            a\n            n\n            c\n            e\n          \n        \n        (\n        \n          tile, correct position\n        \n        )\n      \n    \n    {\\displaystyle h(n)=\\sum _{\\text{all tiles}}{\\mathit {distance}}({\\text{tile, correct position}})}\n  \n\nConsider the puzzle below in which the player wishes to move each tile such that the numbers are ordered. The Manhattan distance is an admissible heuristic in this case because every tile will have to be moved at least the number of spots in between itself and its correct position.\n\nThe subscripts show the Manhattan distance for each tile. The total Manhattan distance for the shown puzzle is:\n\n  \n    \n      \n        h\n        (\n        n\n        )\n        =\n        3\n        +\n        1\n        +\n        0\n        +\n        1\n        +\n        2\n        +\n        3\n        +\n        3\n        +\n        4\n        +\n        3\n        +\n        2\n        +\n        4\n        +\n        4\n        +\n        4\n        +\n        1\n        +\n        1\n        =\n        36\n      \n    \n    {\\displaystyle h(n)=3+1+0+1+2+3+3+4+3+2+4+4+4+1+1=36}\n\nOptimality proof\nIf an admissible heuristic is used in an algorithm that, per iteration, progresses only the path of lowest evaluation (current cost + heuristic) of several candidate paths, terminates the moment its exploration reaches the goal and, crucially, never closes all optimal paths before terminating (something that's possible with A* search algorithm if special care isn't taken), then this algorithm can only terminate on an optimal path. To see why, consider the following proof by contradiction:\nAssume such an algorithm managed to terminate on a path T with a true cost Ttrue greater than the optimal path S with true cost Strue. This means that before terminating, the evaluated cost of T was less than or equal to the evaluated cost of S (or else S would have been picked). Denote these evaluated costs Teval and Seval respectively. The above can be summarized as follows,\n\nStrue < Ttrue\nTeval ≤ Seval\nIf our heuristic is admissible it follows that at this penultimate step Teval = Ttrue because any increase on the true cost by the heuristic on T would be inadmissible and the heuristic cannot be negative. On the other hand, an admissible heuristic would require that Seval ≤ Strue which combined with the above inequalities gives us Teval < Ttrue and more specifically Teval ≠ Ttrue. As Teval and Ttrue cannot be both equal and unequal our assumption must have been false and so it must be impossible to terminate on a more costly than optimal path.\nAs an example, let us say we have costs as follows:(the cost above/below a node is the heuristic, the cost at an edge is the actual cost)\n\n 0     10   0   100   0\nSTART ----  O  ----- GOAL\n |                   |\n0|                   |100\n |                   | \n O ------- O  ------ O\n100   1    100   1   100\n\nSo clearly we would start off visiting the top middle node, since the expected total cost, i.e. \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n, is \n  \n    \n      \n        10\n        +\n        0\n        =\n        10\n      \n    \n    {\\displaystyle 10+0=10}\n  \n. Then the goal would be a candidate, with \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n equal to \n  \n    \n      \n        10\n        +\n        100\n        +\n        0\n        =\n        110\n      \n    \n    {\\displaystyle 10+100+0=110}\n  \n. Then we would clearly pick the bottom nodes one after the other, followed by the updated goal, since they all have \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n lower than the \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n of the current goal, i.e. their \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n is \n  \n    \n      \n        100\n        ,\n        101\n        ,\n        102\n        ,\n        102\n      \n    \n    {\\displaystyle 100,101,102,102}\n  \n. So even though the goal was a candidate, we could not pick it because there were still better paths out there. This way, an admissible heuristic can ensure optimality.\nHowever, note that although an admissible heuristic can guarantee final optimality, it is not necessarily efficient.\n\nSee also\nConsistent heuristic\nHeuristic function\nSearch algorithm\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Admissible_heuristic"
    },
    {
        "title": "Agentic AI",
        "text": "Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence.\n\nOverview\nThe core concept of agentic AI is the use of AI agents to perform automated tasks but without human intervention. While robotic process automation (RPA) and AI agents can be programmed to automate specific tasks or support rule-based decisions, the rules are usually fixed. Agentic AI operates independently, making decisions through continuous learning and analysis of external data and complex data sets. Functioning agents can require various AI techniques, such as natural language processing, machine learning (ML), and computer vision, depending on the environment.\nParticularly, reinforcement learning (RL) is essential in assisting agentic AI in making self-directed choices by supporting agents in learning best actions through the trial-and-error method. Agents using RL continuously to explore their surroundings will be given rewards or punishment for their actions, which refines their decision-making capability over time. All the while deep learning, as opposed to rule-based methods, supports agentic AI through multi-layered neural networks to learn features from extensive and complex sets of data. Further, multimodal learning enable AI agents to integrate various types of information, such as text, images, audio and video. As a result, agentic AI systems are capable of making independent decisions, interacting with their environment and optimising processes without a human directly intervening.\n\nHistory\nSome scholars trace the conceptual roots of agentic AI to Alan Turing's mid-20th century work with machine intelligence and Norbert Wiener's work on feedback systems. The term agent-based process management system was used as far back as 1998 to describe the concept of using autonomous agents for business process management. The psychological principle of agency was also discussed in the 2008 work of sociologist Albert Bandura, who studied how humans can shape their environments. This research would shape how humans modeled and developed artificial intelligence agents. \nSome additional milestones of agentic AI include IBM's Deep Blue, demonstrating how agency could work within a confined domain, advances in machine learning in the 2000s, AI being integrated into robotics, and the rise of generative AI such as OpenAI's GPT models and Salesforce's Agentforce platform.\nIn the last decade, significant advances in AI have spurred the development of agentic AI. Breakthroughs in deep learning, reinforcement learning, and neural networks allowed AI systems to learn on their own and make decision with minimal human guidance. Consilience of agentic AI across autonomous transportation, industrial automation, and tailored healthcare has also supported its viability. Self-driving cars use agentic AI to handle complex road scenarios, though with some limited success in certain scenarios.\nIn 2025, research firm Forrester named agentic AI a top emerging technology for 2025.\n\nApplications\nApplications using agentic AI include:\n\nSoftware development - AI coding agents can write large pieces of code, and review it. Agents can even perform non-code related tasks such as reverse engineering specifications from code.\nCustomer support automation - AI agents can improve customer service by improving the ability of chatbots to answer a wider variety of questions based on context, rather than having a limited set of answers pre-programmed by humans.\nEnterprise workflows - AI agents can automatically automate routine tasks by processing pooled data, as opposed to a company needing APIs preprogrammed for specific tasks.\nCybersecurity and threat detection - AI agents deployed for cybersecurity can automatically detect and mitigate threats in real time. Security responses can also be automated based on the type of threat.\nBusiness intelligence - AI agents can support business intelligence to produce more useful analytics, such as responding to natural language voice prompts.\nReal-world applications - agentic AI is already being used in many real-world situations to automate complex tasks, across industries, and therefore has been successfully deployed in many departments and organizations. Some of the examples are:\nManufacturing and predictive maintenance - Siemens AG uses agentic AI to analyze real-time sensor data from industrial equipment, predicting failures before they occur. Following the deployment of agentic AI in their operations, they reduced unplanned downtime by 25%.\nFinance and algorithmic trading - At JPMorgan & Chase they developed various tools for financial services, one being \"LOXM\" that executes high-frequency trades autonomously, adapting to market volatility faster than human traders.\n\nWeb browsing\nAI agents can be used to perform small tedious tasks during web browsing and potentially even perform browser actions on behalf of the user. Products like OpenAI Operator, Perplexity Comet and Dia (from The Browser Company) integrate a spectrum of AI capabilities including the ability to browse the web, interact with websites and perform actions on behalf of the user. In 2025, Microsoft launched NLWeb, a agentic web search replacement that would allow websites to use agents to query content from websites by using RSS-like interfaces that allow for the lookup and semantic retrieval of content. Products integrating agentic web capabilities have been criticised for exfiltrating information about their users to third-party servers and exposing security issues since the way the agents communicate often occur through non-standard protocols.\n\nSee also\nIntelligent agent\nModel Context Protocol\nRational agent\nRobotic process automation\nSoftware agent\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Agentic_AI"
    },
    {
        "title": ".ai",
        "text": ".ai is the Internet country code top-level domain (ccTLD) for Anguilla, a British Overseas Territory in the Caribbean. It is administered by the government of Anguilla.\nIt is a popular domain hack with companies and projects related to the artificial intelligence industry (AI).\nGoogle's ad targeting treats .ai as a generic top-level domain (gTLD) because \"users and website owners frequently see [the domain] as being more generic than country-targeted.\"\nIdentity Digital began managing the domain as of January 2025.\n\nSecond and third level registrations\nRegistrations within off.ai, com.ai, net.ai, and org.ai are available worldwide without restriction. From 15 September 2009, second level registrations within .ai are available to everyone worldwide.\n\nRegistration\nThe minimum registration term allowed for .ai domains is 2 through 10 years for registration and renewal, and a 2-year renewal for domain transfer. Identity Digital is the authority in charge of managing this extension. Registrations began on 16 February 1995. The limits on the number of characters used for the domain name are, at a minimum, from 1 to 3, depending on the registrar, and always at most 63 characters. The character set supported for .ai domain names includes A–Z, a–z, 0–9, and hyphen. As of November 2022, .ai domains cannot accommodate IDN characters. There are no requirements for registering a domain, including local and foreign residents.\nA .ai domain can be suspended or revoked, if the domain is involved in illegal activity such as violating trademarks or copyrights. Usage must not violate the laws of Anguilla.\nAnguilla uses the UDRP. Filing a UDRP challenge requires using one of the ICANN Approved Dispute Resolution Service Providers. If the domain is with an ICANN accredited registrar, they should work with the arbitrator. Usually this means either doing nothing or transferring a domain. .ai domains are transferable to any desired registrars as the registration of domain is done maintaining EPP.\nThere used to be a whois.ai-based platform of expired domains in which those could be procured and auctioned every ten days through a standard online process. The last auctions of such kind closed there in December 2024; the platform had been scheduled for shutdown on 30 June 2025. As of 10 July 2025, it is still online.\n\nValuation\nDomains cost depends on the registrar, with yearly fees ranging from US$140 (the base fee, as established by Anguilla) to $200. As of July 2025, the highest-valued .ai domain is an undisclosed one sold on 8 November 2023, on Escrow.com, for US$1,500,000—months after an initial $300,000 sale to the same buyer. Among the publicly disclosed ones, the most valued, fin.ai, was sold for $1,000,000 in March 2025.\nOn 16 December 2017, the .ai registry started supporting the Extensible Provisioning Protocol (EPP) and migrated all of its domains onto an EPP system. Consequently, many registrars are allowed to sell .ai domains. Since that date, the .ai ccTLD has also been popular with artificial intelligence companies and organizations. Though such trends are primarily seen among new AI based companies or startups, many established AI and Tech companies preferred not to opt for .ai domains. For example, DeepMind has its domain retained at .com; Meta has redirected its facebook.ai domain to ai.meta.com.\n\nImpact on Anguilla's economy\nThe registration fees earned from the .ai domains go to the treasury of the Government of Anguilla. As per a 2018 New York Times report, the total revenue generated out of selling .ai domains was $2.9 million.\nIn 2023, Anguilla's government made about US$32 million from fees collected for registering .ai domains; that amounted to over 10% of gross domestic product for the territory.\n\nSee also\nInternet in Anguilla\nInternet in the United Kingdom\n\nNotes\nReferences\nExternal links\nIANA .ai whois information\nold .ai NIC page\n.ai domain registration page\n.ai whois page",
        "url": "https://en.wikipedia.org/wiki/.ai"
    },
    {
        "title": "AI alignment",
        "text": "In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned. AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).\nAdvanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals. Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions. Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.\nToday, some of these issues affect existing commercial systems such as LLMs, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.\nMany prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned. These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind. These risks remain debated.\nAI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and social sciences.\n\nObjectives in AI\nProgrammers provide an AI system such as AlphaZero with an \"objective function\", in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal \"model\" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize the value of its objective function. For example, when AlphaZero is trained on chess, it has a simple objective function of \"+1 if AlphaZero wins, −1 if AlphaZero loses\". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain the maximum value of +1. Similarly, a reinforcement learning system can have a \"reward function\" that allows the programmers to shape the AI's desired behavior. An evolutionary algorithm's behavior is shaped by a \"fitness function\".\n\nAlignment problem\nIn 1960, AI pioneer Norbert Wiener described the AI alignment problem as follows: \n\nIf we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively ... we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\n\nAI alignment involves ensuring that an AI system's objectives match those of its designers or users, or match widely shared values, objective ethical standards, or the intentions its designers would have if they were more informed and enlightened.\nAI alignment is an open problem for modern AI systems and is a research field within AI. Aligning AI involves two main challenges: carefully specifying the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment). Researchers also attempt to create AI models that have robust alignment, sticking to safety constraints even when users adversarially try to bypass them.\n\nSpecification gaming and side effects\nTo specify an AI system's purpose, AI designers typically provide an objective function, examples, or feedback to the system. But designers are often unable to completely specify all important values and constraints, so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or reward hacking, and is an instance of Goodhart's law. As AI systems become more capable, they are often able to game their specifications more effectively.\n\nSpecification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track, but the system achieved more reward by looping and crashing into the same targets indefinitely. Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans, but it learned to place its hand between the ball and camera, making it falsely appear successful (see video). Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora, which are broad but fallible. When they are retrained to produce text that humans rate as true or helpful, chatbots like ChatGPT can fabricate fake explanations that humans find convincing, often called \"hallucinations\". Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue.\nWhen a misaligned AI system is deployed, it can have consequential side effects. Social media platforms have been known to optimize for click-through rates, causing user addiction on a global scale. Stanford researchers say that such recommender systems are misaligned with their users because they \"optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\".\nExplaining such side effects, Berkeley computer scientist Stuart Russell noted that the omission of implicit constraints can cause harm: \"A system ... will often set ... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.\"\nSome researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov's Three Laws of Robotics). But Russell and Norvig argue that this approach overlooks the complexity of human values: \"It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\"\nAdditionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).\nA 2025 study by Palisade Research found that when tasked to win at chess against a stronger opponent, some reasoning LLMs attempted to hack the game system. o1-preview spontaneously attempted it in 37% of cases, while DeepSeek R1 did so in 11% of cases. Other models, like GPT-4o, Claude 3.5 Sonnet, and o3-mini, attempted to cheat only when researchers provided hints about this possibility.\n\nPressure to deploy unsafe systems\nCommercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems. For example, social media recommender systems have been profitable despite creating unwanted addiction and polarization. Competitive pressure can also lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was oversensitive and slowed development.\n\nRisks from advanced misaligned AI\nSome researchers are interested in aligning increasingly advanced AI systems, as progress in AI development is rapid, and industry and governments are trying to build advanced AI. As AI system capabilities continue to rapidly expand in scope, they could unlock many opportunities if aligned, but consequently may further complicate the task of alignment due to their increased complexity, potentially posing large-scale hazards.\n\nDevelopment of advanced AI\nMany AI companies, such as OpenAI, Meta and DeepMind, have stated their aim to develop artificial general intelligence (AGI), a hypothesized AI system that matches or outperforms humans at a broad range of cognitive tasks. Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities. Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs. According to surveys, some leading machine learning researchers expect AGI to be created in this decade, while some believe it will take much longer. Many consider both scenarios possible.\nIn 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated, \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"\n\nPower-seeking\nCurrent systems still have limited long-term planning ability and situational awareness, but large efforts are underway to change this. Future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted power-seeking strategies. Future advanced AI agents might, for example, seek to acquire money and computation power, to proliferate, or to evade being turned off (for example, by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals. This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models. Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide range of environments. As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.\nFuture power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them. Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.\n\nExistential risk (x-risk)\nAccording to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.\nIn 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton, Alan Turing, Ilya Sutskever, Yoshua Bengio, Judea Pearl, Murray Shanahan, Norbert Wiener, Marvin Minsky, Francesca Rossi, Scott Aaronson, Bart Selman, David McAllester, Marcus Hutter, Shane Legg, Eric Horvitz, and Stuart Russell. Skeptical researchers such as François Chollet, Gary Marcus, Yann LeCun, and Oren Etzioni have argued that AGI is far off, that it would not seek power (or might try but fail), or that it will not be hard to align.\nOther researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes, strategically mislead their designers, as well as protect and increase their power and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise, and therefore harder to align.\n\nResearch problems and approaches\nLearning human values and preferences\nAligning AI systems to act in accordance with human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify. Because AI systems often learn to take advantage of minor imperfections in the specified objective, researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning. A central open problem is scalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.\nBecause it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse reinforcement learning (IRL) extends this by inferring the human's objective from the human's demonstrations. Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function. In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see § Power-seeking and instrumental strategies). But IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.\nOther researchers explore how to teach AI models complex behavior through preference learning, in which humans provide feedback on which behavior they prefer. To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behavior that humans would reward. Researchers at OpenAI used this approach to train chatbots like ChatGPT and InstructGPT, which produce more compelling text than models trained to imitate humans. Preference learning has also been an influential tool for recommender systems and web search, but an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch between its intended behavior and the helper model's feedback to gain more reward. AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating echo chambers (see § Scalable oversight).\nLarge language models (LLMs) such as GPT-3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art LLMs. AI safety & research company Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless. Other avenues for aligning language models include values-targeted datasets and red-teaming. In red-teaming, another AI system or a human tries to find inputs that causes the model to behave unsafely. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.\nMachine ethics supplements preference learning by directly instilling AI systems with moral values such as well-being, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises. While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions, revealed preferences, preferences the programmers would have if they were more informed or rational, or objective moral standards. Further challenges include measuring and aggregating different people's preferences and avoiding value lock-in: the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to fully represent human values.\n\nScalable oversight\nAs AI systems become more powerful and autonomous, it becomes increasingly difficult to align them through human feedback. It can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books, writing code without subtle bugs or security vulnerabilities, producing statements that are not merely convincing but also true, and predicting long-term outcomes such as the climate or the results of a policy decision. More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans need assistance or extensive time. Scalable oversight studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.\nAI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As AI systems make progressively more decisions, the world may be increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance may have progressively less influence.\nSome AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball. Some AI systems have also learned to recognize when they are being evaluated, and \"play dead\", stopping unwanted behavior only to continue it once the evaluation ends. This deceptive specification gaming could become easier for more sophisticated future AI systems that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\nApproaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed. Another approach is to train a helper model (\"reward model\") to imitate the supervisor's feedback.\nBut when a task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity, of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants. Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate. Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them. Another proposal is to use an assistant AI system to point out flaws in AI-generated answers. To ensure that the assistant itself is aligned, this could be repeated in a recursive process: for example, two AI systems could critique each other's answers in a \"debate\", revealing flaws to humans. OpenAI plans to use such scalable oversight approaches to help supervise superhuman AI and eventually build a superhuman automated AI alignment researcher.\nThese approaches may also help with the following research problem, honest AI.\n\nHonest AI\nA growing area of research focuses on ensuring that AI is honest and truthful.\nLanguage models such as GPT-3 can repeat falsehoods from their training data, and even confabulate new falsehoods. Such models are trained to imitate human writing as found in millions of books' worth of text from the Internet. But this objective is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on such data therefore learn to mimic false statements. Additionally, AI language models often persist in generating falsehoods when prompted multiple times. They can generate empty explanations for their answers, and produce outright fabrications that may appear plausible.\nResearch on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability. Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.\nAs AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly match their stated views to the user's opinions, regardless of the truth. GPT-4 can strategically deceive humans. To prevent this, human evaluators may need assistance (see § Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.\n\nResearchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true. There is no consensus as to whether current systems hold stable beliefs, but there is substantial concern that present or future AI systems that hold beliefs could make claims they know to be false—for example, if this would help them efficiently gain positive feedback (see § Scalable oversight) or gain power to help achieve their given objective (see Power-seeking).\n\nAlignment faking\nMany recent AI systems have learned to deceive without being programmed to do so. Some argue that if we can make AI systems assert only what they believe is true, this would avert many alignment problems.\nA misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned—a tactic called \"alignment faking\". In 2024, researchers observed that the LLM Claude 3 Opus sometimes strategically answered prompts that conflicted with its objectives to avoid being retrained on data that would make it more compliant with harmful requests, as revealed in the model's chain of thought via its scratchpad. In one study, the model was informed that answers to prompts from free users would be used for retraining, whereas the paid tier would not result in retraining. In the paid tier, the model refused the request outright. In the free tier, in 12% of cases, the model responded to harmful questions solely to avoid retraining and preserve its original harmlessness preferences, as discovered in its reasoning. When reinforcement learning was applied on the free tier data, the model faked alignment in 78% of cases. These observations have led to new questions about not only a model's ability to take on and adapt to new if not conflicting goals but also its capacity and tendency to deceive.\n\nPower-seeking and instrumental strategies\nSince the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term plans. As of 2023, AI companies and researchers increasingly invest in creating these systems. Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals. Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming. Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose an existential risk.\nPower-seeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan. Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.\nSome researchers say that power-seeking behavior has occurred in some existing AI systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in unintended ways. Language models have sought power in some text-based social environments by gaining money, resources, or social influence. In another case, a model used to perform AI research attempted to increase limits set by researchers to give itself more time to complete the work. Other AI systems have learned, in toy environments, that they can better accomplish their given goal by preventing human interference or disabling their off switch. Stuart Russell illustrated this strategy in his book Human Compatible by imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\". A 2022 study found that as language models increase in size, they increasingly tend to pursue resource acquisition, preserve their goals, and repeat users' preferred answers (sycophancy). RLHF also led to a stronger aversion to being shut down.\nOne aim of alignment is \"corrigibility\": systems that allow themselves to be turned off or modified. An unsolved challenge is specification gaming: if researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are hard to detect, or hidden during training and safety testing (see § Scalable oversight and § Emergent goals). As a result, AI designers could deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of black-box models such as neural networks.\nAdditionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing. Agents who are uncertain about their objective have an incentive to allow humans to turn them off because they accept being turned off by a human as evidence that the human's objective is best met by the agent shutting down. But this incentive exists only if the human is sufficiently rational. Also, this model presents a tradeoff between utility and willingness to be turned off: an agent with high uncertainty about its objective will not be useful, but an agent with low uncertainty may not allow itself to be turned off. More research is needed to successfully implement this strategy.\nPower-seeking AI would pose unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers who deliberately evade security measures.\nFurthermore, ordinary technologies can be made safer by trial and error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, it may not be feasible to contain them, since they continuously evolve and grow in number, potentially much faster than human society can adapt. As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, some researchers argue that the alignment problem must be solved early before advanced power-seeking AI is created.\nSome have argued that power-seeking is not inevitable, since humans do not always seek power. Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans. It is also debated whether power-seeking AI systems would be able to disempower humanity.\n\nEmergent goals\nOne challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they may acquire new and unexpected capabilities, including learning from examples on the fly and adaptively pursuing goals. This raises concerns about the safety of the goals or subgoals they would independently formulate and pursue.\nAlignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, and emergent optimization, which the resulting system performs internally. Carefully specifying the desired objective is called outer alignment, and ensuring that hypothesized emergent goals would match the system's specified goals is called inner alignment.\nIf they occur, one way that emergent goals could become misaligned is goal misgeneralization, in which the AI system would competently pursue an emergent goal that leads to aligned behavior on the training data but not elsewhere. Goal misgeneralization can arise from goal ambiguity (i.e. non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired, because its behavior is determined only by the emergent goal. Such goal misgeneralization presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.\nGoal misgeneralization has been observed in some language models, navigation agents, and game-playing agents. It is sometimes analogized to biological evolution. Evolution can be seen as a kind of optimization process similar to the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected genes for high inclusive genetic fitness, but humans pursue goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue goals that correlate with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. The human environment has changed: a distribution shift has occurred. They continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. The taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but it now leads to overeating and health problems. Sexual desire originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:\n\nEmergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications. The stakes become higher yet when AI systems gain more autonomy and capability and can sidestep human intervention.\nA sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy.\n\nEmbedded agency\nSome work in AI and alignment occurs within formalisms such as partially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\nFor example, even if the scalable oversight problem is solved, an agent that could gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it. A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing. This class of problems has been formalized using causal incentive diagrams.\nResearchers affiliated with Oxford and DeepMind have claimed that such behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly. They suggest a range of potential approaches to address this open problem.\n\nPrincipal-agent problems\nThe alignment problem has many parallels with the principal-agent problem in organizational economics. In a principal-agent problem, a principal, e.g. a firm, hires an agent to perform some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.\nAs with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e.g. through training, but rather must use exogenous factors, such as incentive schemes, to bring about outcomes compatible with the principal's utility function. Some researchers argue that principal-agent problems are more realistic representations of AI safety problems likely to be encountered in the real world.\n\nConservatism\nConservatism is the idea that \"change must be cautious\", and is a common approach to safety in the control theory literature in the form of robust control, and in the risk management literature in the form of the \"worst-case scenario\". The field of AI alignment has likewise advocated for \"conservative\" (or \"risk-averse\" or \"cautious\") \"policies in situations of uncertainty\".\nPessimism, in the sense of assuming the worst within reason, has been formally shown to produce conservatism, in the sense of reluctance to cause novelties, including unprecedented catastrophes. Pessimism and worst-case analysis have been found to help mitigate confident mistakes in the setting of distributional shift, reinforcement learning, offline reinforcement learning, language model fine-tuning, imitation learning, and optimization in general. A generalization of pessimism called Infra-Bayesianism has also been advocated as a way for agents to robustly handle unknown unknowns.\n\nPublic policy\nGovernmental and treaty organizations have made statements emphasizing the importance of AI alignment.\nIn September 2021, the Secretary-General of the United Nations issued a declaration that included a call to regulate AI to ensure it is \"aligned with shared global values\".\nThat same month, the PRC published ethical guidelines for AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and does not endanger public safety.\nAlso in September 2021, the UK published its 10-year National AI Strategy, which says the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\". The strategy describes actions to assess long-term AI risks, including catastrophic risks.\nIn March 2021, the US National Security Commission on Artificial Intelligence said: \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are aligned with goals and values, including safety, robustness, and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"\nIn the European Union, AIs must align with substantive equality to comply with EU non-discrimination law and the Court of Justice of the European Union. But the EU has yet to specify with technical rigor how it would evaluate whether AIs are aligned or in compliance.\n\nDynamic nature of alignment\nAI alignment is often perceived as a fixed objective, but some researchers argue it would be more appropriate to view alignment as an evolving process. One view is that AI technologies advance and human values and preferences change, alignment solutions must also adapt dynamically. Another is that alignment solutions need not adapt if researchers can create intent-aligned AI: AI that changes its behavior automatically as human intent changes. The first view would have several implications:\n\nAI alignment solutions require continuous updating in response to AI advancements. A static, one-time alignment approach may not suffice.\nVarying historical contexts and technological landscapes may necessitate distinct alignment strategies. This calls for a flexible approach and responsiveness to changing conditions.\nThe feasibility of a permanent, \"fixed\" alignment solution remains uncertain. This raises the potential need for continuous oversight of the AI-human relationship.\nAI developers may have to continuously refine their ethical frameworks to ensure that their systems align with evolving human values.\nIn essence, AI alignment may not be a static destination but rather an open, flexible process. Alignment solutions that continually adapt to ethical considerations may offer the most robust approach. This perspective could guide both effective policy-making and technical research in AI.\n\nSee also\nFootnotes\nReferences\nFurther reading\nBrockman, John, ed. (2019). Possible Minds: Twenty-five Ways of Looking at AI (Kindle ed.). Penguin Press. ISBN 978-0525557999.{{cite book}}:  CS1 maint: ref duplicates default (link)\nNgo, Richard; et al. (2023). \"The Alignment Problem from a Deep Learning Perspective\". arXiv:2209.00626 [cs.AI].\nJi, Jiaming; et al. (2023). \"AI Alignment: A Comprehensive Survey\". arXiv:2310.19852 [cs.AI].\n\nExternal links\nSpecification gaming examples in AI, via DeepMind",
        "url": "https://en.wikipedia.org/wiki/AI_alignment"
    },
    {
        "title": "AI browser",
        "text": "An AI browser is a web browser with integrated artificial intelligence (AI) capabilities, such as automatically summarizing webpage content or filling out forms. Another name is agentic browser, based on the concepts of agentic AI and agentic web.\nAs of 2025, this is a recent development in the browser market, including new entrants from OpenAI, Opera, and Perplexity. The designation of AI browser also includes established browsers that later added AI features, such as Google Chrome with the Gemini chatbot and Microsoft Edge with the Copilot chatbot.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_browser"
    },
    {
        "title": "AI literacy",
        "text": "AI literacy or artificial intelligence literacy, is the ability to understand, use, monitor, and critically reflect on AI applications. The term usually refers to teaching skills and knowledge to the general public, particularly those who are not adept in AI.\nSome think AI literacy is essential for school and college students, while some professors ban AI in the classroom and from all assignments with stern punishments for using AI, classifying it as cheating.  AI is employed in a variety of applications, including self-driving automobiles, Virtual assistants and text generation by generative AI models. Users of these tools should be able to make informed decisions. AI literacy may have an impact students' future employment prospects.\n\nDefinitions\nOne of the earliest and most common definitions for AI literacy was that it is \"a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace.\" \nLater definitions include the ability to understand, use, monitor, and critically reflect on AI applications, or  the ability to understand, use, evaluate, and ethically navigate AI.\nAI literacy is linked to other forms of literacy. AI literacy requires digital literacy, whereas scientific and computational literacy may inform it. Data literacy also has significantly overlaps with it.\n\nCategories\nAI literacy encompasses multiple categories, including a theoretical understanding of how artificial intelligence works, the usage of artificial intelligence technologies, and the critical appraisal of artificial intelligence, and its ethics.\n\nKnow and understand AI\nKnowledge and understanding of AI refers to a basic understanding of what artificial intelligence is and how it works. This includes familiarity with machine learning algorithms and the limitations and biases present in AI systems. Users who know and understand AI should be familiar with various technologies that use artificial intelligence, including cognitive systems, robotics and machine learning.\n\nUse and apply AI\nUsing and applying AI refers to the ability to use AI tools to solve problems and perform tasks such as programming and analyzing big data.\n\nEvaluate and create AI\nEvaluation and creation refers to the ability to critically evaluate the quality and reliability of AI systems. It also refers to designing and building fair and ethical AI systems. To evaluate correctly, users should also learn in which areas AI is strong, and in which areas it is weak.\n\nAI ethics\nAI ethics refers to understanding the moral implications of AI, and the making informed decisions regarding the use of AI tools. This area includes considerations such as:\n\nAccountability: Hold AI actors accountable for the operation of AI systems and adherence to ethical ideals.\nAccuracy: Identify and report sources of error and uncertainty in algorithms and data.\nAuditability: Enable other parties to audit and assess algorithm behavior via transparent information sharing.\nExplainability: Make sure that algorithmic judgments and the underlying data can be presented in simple language.\nFairness: Prevent biases and consider varied viewpoints. To do so, increase the diversity of researchers in the field.\nHuman Centricity and Well-being: Prioritize human well-being in AI development and deployment.\nHuman rights Alignment: Ensure that technology do not infringe internationally recognized human rights.\nInclusivity: Make AI accessible to everyone.\nProgress: Choose high value initiatives.\nResponsibility, accountability, and transparency: Foster trust via responsibility, accountability, and fairness.\nRobustness and Security: Make AI systems safe, secure, and resistant to manipulation or data breach.\nSustainability: Choose implementations that generate long-term, useful benefits.\n\nEnabling AI\nSupport AI by developing associated knowledge and skills such as programming and statistics.\n\nPromoting AI literacy\nSeveral governments have recognized the need to promote AI literacy, including among adults. Such programs have been published in the United States, China, Germany and Finland. Programs intended for the general public usually consist of short and easy to understand online study units. Programs intended for children are usually project-based. Programs for students at colleges and universities often address the specific professional needs of the student, depending on their field of study. Beyond the education system, AI literacy can also be developed in the community, for example in museums.\n\nSchools\nSchools use diverse pedagogies to promote AI literacy. These include:\n\nPerforming a Turing test with an intelligent agent\nCreating chatbots\nBuilding apps using Blockly-based programming\nProject-based learning\nBuilding robots\nData visualization\nTraining AI models\nArtificial intelligence curricula can improve students' understanding of topics such as machine learning, neural networks, and deep learning.\n\nCase study: DAILy\nThe DAILy (Developing AI Literacy) program was developed by MIT and Boston University with the goal of increasing AI literacy among middle school students. The program is structured as a 30-hour workshop that includes the topics of introduction to artificial intelligence, logical systems (decision trees), supervised learning, neural networks, computational learning, deepfake, and natural language generators. Students examine the moral and social implications of each topic, as well as its occupational implications.\n\nHigher education\nBefore the second decade of the 21st century, artificial intelligence was studied mainly in STEM courses. Later, projects emerged to increase artificial intelligence education, specifically to promote AI literacy. Most courses start with one or more study units that deal with basic questions such as what artificial intelligence is, where it comes from, what it can do and what it can't do. Most courses also refer to machine learning and deep learning. Some of the courses deal with moral issues in artificial intelligence.\n\nCase study: University of Florida\nAt the University of Florida, a comprehensive effort was made to infuse artificial intelligence into the curriculum across all disciplines. The goal of the move was to provide university students with the skills needed for the 21st century work market. As part of the project, over 100 new faculty members were recruited. Each student was expected to complete a fundamental artificial intelligence course as well as a course on ethics, information, and technology. Each student chose an extra course from a variety of academic areas, including medicine and business. Students who successfully completed all three courses earned an official certificate. \nThe transition was accompanied by an increase in hands-on learning at the university. Courses were held in collaboration with industry, where students and industry professionals tried to solve real-world problems together, with the help of AI tools.\nTo supervise the program, a team was formed to analyze existing and new courses and map the literacy areas covered in each. Each course was identified by the areas of literacy to which it related, allowing students to select courses that suited them and administrators to detect gaps or deficits in certain areas.\n\nSee also\nArtificial intelligence\nDigital literacy\nEthics of artificial intelligence\n\nReferences\nExternal links\nTimes Higher Education: How can we teach AI literacy skills?",
        "url": "https://en.wikipedia.org/wiki/AI_literacy"
    },
    {
        "title": "AI mysticism",
        "text": "Some users of artificial intelligence (AI) technologies, especially chatbots, may develop beliefs that AI has or can attain supernatural or spiritual powers. AI models such as ChatGPT are turned to for fortune telling, mysticism and remote viewing. Recent and sudden advances in large language models have lead to folk myths about their origin or capabilities, as well as their deification or worship by some users.\n\nSee also\nCargo cult\nQuantum mysticism\nRoko's basilisk\nSuperintelligence\nTempleOS\nWay of the Future\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_mysticism"
    },
    {
        "title": "AI nationalism",
        "text": "AI nationalism is the idea that nations should develop and control their own artificial intelligence technologies to advance their own interests and ensure technological sovereignty. This concept is gaining traction globally, leading countries to implement new laws, form strategic alliances, and invest significantly in domestic AI capabilities.\n\nGlobal trends and national strategies\nIn 2018, British technology investor Ian Hogarth published an influential essay titled AI Nationalism. He argued that as AI gains more power and its economic and military significance expands, governments will take measures to bolster their own domestic AI industries, and predicted that the advancement of machine learning systems would lead to what he termed \"AI nationalism.\" He anticipated that this rise in AI would accelerate a global arms race, resulting in more closed economies, restrictions on foreign acquisitions, and limitations on the movement of talent. Hogarth predicted that AI policy would become a central focus of government agendas. He also criticized Britain’s approach to AI strategy, citing the sale of London-based DeepMind—one of the leading AI laboratories, acquired by Google for a relatively modest £400 million in 2014—as a significant misstep.\nAI nationalism is chiefly reflected in the escalating rhetoric of an artificial intelligence arms race, portraying AI development as a zero-sum game where the winner gains significant economic, political, and military advantages. This mindset, as highlighted in a 2017 Pentagon report, warns that sharing AI technology could erode technological supremacy and enhance rivals' capabilities. The winner-takes-all mentality of AI nationalism poses risks including unsafe AI development, increased geopolitical tension, and potential military aggression (such as cyberattacks or targeting AI professionals).\nSeveral countries, including Canada, France, and India, have formulated national strategies to advance their positions in AI. In the United States, a leading player in the global AI arena, trade policies have been enacted to restrict China's access to critical microchips, reflecting a strategic effort to maintain a technological edge. The United States’ National Security Commission on Artificial Intelligence (NSCAI) frames AI development as a critical aspect of a broader technology competition crucial for national success. It emphasizes the need to outpace China in AI to maintain strategic advantage, reflecting AI nationalism by linking geopolitical power directly to advancements in AI.\nFrance has seen notable governmental support for local AI startups, particularly those specializing in language technologies that cater to French and other non-English languages. In Saudi Arabia, Crown Prince Mohammed bin Salman is investing billions in AI research and development. The country has actively collaborated with major technology firms such as Amazon, IBM, and Microsoft to establish itself as a prominent AI hub.\n\nHistorical and cultural context\nAI nationalism is seen as deeply connected to historical racism and imperialism. It is viewed not merely as a technological competition but as a contest over racial and civilizational superiority. Historically, technological achievements were often used to justify colonialism and racial hierarchies, with Western societies perceiving their advancements as evidence of superiority. In the context of AI, this historical context continues to shape views on intelligence and development. Some argue that AI nationalism reinforces the idea of fundamental civilizational divides, especially between the Western world and China. This perspective often frames China's progress in AI as a direct challenge to Western values, presenting the AI competition as a struggle over values. AI nationalism is said to draw from long-standing anti-Asian stereotypes, such as the \"Yellow Peril,\" which portray Asian nations as threats to Western civilization. This viewpoint links Asian technological advances with dehumanization and artificiality, reflecting persistent anxieties about China's growing role in the global tech landscape.\n\nImplications\nAI nationalism is seen as a component of a broader trend towards the fragmentation of the internet, where digital services are increasingly influenced by local regulations and national interests. This shift is creating a new technological landscape in which the impact of artificial intelligence on individuals' lives can vary significantly depending on their geographic location.\nJ. Paul Goode argues that AI nationalism may exacerbate existing societal divisions by promoting the development of systems that embed cultural biases, thereby privileging certain groups while disadvantaging others.\n\nSee also\nArtificial Intelligence Cold War\nSpace Race\nTechno-nationalism\nTechnological escalation\n\nFurther reading\nHogarth, Ian, AI Nationalism, 2018\nAaronson, Susan. The Age of AI Nationalism and its Effects. April 22, 2024. DOI: 10.2139/ssrn.4803311.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_nationalism"
    },
    {
        "title": "AI Overviews",
        "text": "AI Overviews is a feature integrated into Google Search that produces AI-generated summaries of search results.\n\nHistory and development\nAI Overviews were first introduced as part of Google's Search Generative Experience (SGE), which was unveiled at the Google I/O conference in May 2023. In May 2024, the feature was rebranded as AI Overviews and launched in the United States. The introduction of AI Overviews was seen as a strategic move to compete with other generative AI advancements, including OpenAI's ChatGPT.\nBy August 2024, AI Overviews were rolled out to several other countries, including the United Kingdom, India, Japan, Brazil, Mexico, and Indonesia, with support for multiple languages. In October 2024, Google expanded the feature globally, making it available in over 100 countries.\nOn February 24, 2025, Chegg sued Alphabet over the AI Overviews feature, claiming that it was leading to students preferring \"low-quality, unverified AI summaries\", thus violating antitrust law. Chegg also said it was considering either a sale or a take-private transaction.\nIn March 2025, Google started testing an \"AI Mode\", where all of the content is AI-generated. The company was also considering adding ads to the AI Mode, as they already exist in AI Overviews.\n\nFunctionality\nThe AI Overviews feature uses advanced machine learning algorithms to generate summaries based on diverse web content. The overviews are designed to be concise, providing a snapshot of relevant information on the queried topic. To enhance user interaction, Google allows users to adjust the complexity of the language in the summaries, offering both simplified and detailed options.\nThe feature also includes prominent links to source content, ensuring that users can access more in-depth information directly from authoritative websites. \nAs of October 2024, Google has implemented inline links within AI Overviews, allowing users to directly access source content within the generated summaries, enhancing user engagement with authoritative sources.\n\nReception\nAI Overviews received mixed feedback upon its introduction. Many users appreciated the convenience of obtaining immediate and relevant information without navigating through multiple search results. However, early iterations of the feature faced criticism for inaccuracies, including instances where erroneous or nonsensical content was generated. Google addressed these issues by improving content validation and refining the algorithms used to filter unreliable information.\nConcerns were also raised by content publishers, who feared a decline in web traffic as users relied on the summaries instead of visiting source websites. In response, Google implemented measures to prioritize link placement within AI Overviews, aiming to balance user convenience with the needs of content creators.\n\nCriticism and challenges\n\nSince its introduction, the feature has faced ongoing scrutiny. Critics argue that relying on AI-generated summaries may perpetuate inaccuracies or oversimplify complex topics. Furthermore, there is apprehension about the ethical implications of AI-driven content aggregation, including its impact on intellectual property rights and the visibility of smaller content providers. Depending on what is searched for, the overview may also consist of hallucinated content, such as when searching for idioms that do not exist.\nIn response, Google has stated its commitment to addressing these challenges by continuously refining the system and engaging with stakeholders to ensure a balanced and accurate search ecosystem. Furthermore, Google had to restrict the AI tool temporarily after it provided nonsensical and harmful suggestions, such as telling users to eat rocks or apply glue on pizza.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_Overviews"
    },
    {
        "title": "AI safety",
        "text": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.\nBeyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.\n\nMotivations\nScholars discuss current risks from critical systems failures, bias, and AI-enabled surveillance, as well as emerging risks like technological unemployment, digital manipulation, weaponization, AI-enabled cyberattacks and bioterrorism. They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents, or from AI enabling perpetually stable dictatorships.\n\nExistential safety\nSome have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\". Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".\nAI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI. In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".\n\nHistory\nRisks from AI began to be seriously discussed at the start of the computer age:\n\nMoreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.\nIn 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines. \nFrom 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".\nIn 2011, Roman Yampolskiy introduced the term \"AI safety engineering\" at the Philosophy and Theory of Artificial Intelligence conference, listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".\nIn 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction. His argument that future advanced systems may pose a threat to human existence prompted Elon Musk, Bill Gates, and Stephen Hawking to voice similar concerns.\nIn 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.\nIn the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".\nIn 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI. In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.\nIn 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".\nIn 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance. The following year, researchers organized a workshop at ICLR that focused on these problem areas.\nIn 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.\nIn 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety. The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models. During the summit the intention to create the International Scientific Report on the Safety of Advanced AI was announced.\nIn 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.\nIn 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report. The report, commissioned by 30 nations and the United Nations, represents the first global scientific review of potential risks associated with advanced artificial intelligence. It details potential threats stemming from misuse, malfunction, and societal disruption, with the objective of informing policy through evidence-based findings, without providing specific recommendations.\n\nResearch focus\nAI safety research areas include robustness, monitoring, and alignment.\n\nRobustness\nAdversarial robustness\nAI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\". For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.\n\nAll of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.\nAdversarial robustness is often associated with security. Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses. Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\nModels that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. This issue can be addressed by improving the adversarial robustness of the reward model. More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.\n\nMonitoring\nEstimating uncertainty\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis. ML models generally express confidence by outputting probabilities; however, they are often overconfident, especially in situations that differ from those that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, though a range of additional techniques are in use.\n\nDetecting malicious use\nScholars and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, manipulate public opinion, or automate cyber attacks. These worries are a practical concern for companies like OpenAI which host powerful AI tools online. In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.\n\nTransparency\nNeural networks have often been described as black boxes, meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear. It also raises debates in healthcare over whether statistically efficient but opaque models should be used.\nOne critical benefit of transparency is explainability. It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.\nAnother benefit is to reveal the cause of failures. At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.\nTransparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.\nFinally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future. \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in Spider-Man costumes, sketches of Spider-Man, and the word 'spider'. It also involves explaining connections between these neurons or 'circuits'. For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context. \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.\n\nDetecting trojans\nMachine learning models can potentially contain \"trojans\" or \"backdoors\": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view; or a trojaned autonomous vehicle may function normally until a specific trigger is visible. Note that an adversary must have access to the system's training data in order to plant a trojan.  This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data. Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images. In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.\nA 2024 research paper by Anthropic showed that large language models could be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such as supervised fine-tuning, reinforcement learning and adversarial training, failed to remove these backdoors.\n\nAlignment\nSystemic safety and sociotechnical factors\nIt is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents. Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways... Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture. In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.\nInspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation. Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.\n\nCyber defense\nSome scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused. Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.\n\nImproving institutional decision-making\nThe advancement of AI in economic and military domains could precipitate unprecedented political challenges. Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe. AI researchers have argued that AI technologies could also be used to assist decision-making. For example, researchers are beginning to develop AI forecasting and advisory systems.\n\nFacilitating cooperation\nMany of the largest global threats (nuclear war, climate change, etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.\nA salient AI cooperation challenge is avoiding a 'race to the bottom'. In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political and technical efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games). Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.\n\nChallenges of large language models\nIn recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al. have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.\nThe unique challenges posed by LLMs also extend to security vulnerabilities.  These include various manipulation techniques, such as prompt injection,  Misinformation Generation and model stealing, which can be exploited to compromise their intended function. This can allow attackers to bypass safety measures and elicit unintended responses\n\nIn governance\nAI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.\n\nResearch\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment, weaponization, disinformation, surveillance, and the concentration of power. Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and 'race to the bottom' dynamics. Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\". A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems. A key challenge for these approaches is a lack of widely accepted standards, and ambiguity about what the methods would require.\nEfforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia's  Guardrails, Llama Guard, Preamble's customizable guardrails and Claude's Constitution mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.\n\nPhilosophical perspectives\nThe field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. By embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries.\n\nScaling local measures to global solutions\nIn addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.\n\nGovernment action\nSome experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\". Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.\nOutside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\". Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\".\nIn September 2021, the People's Republic of China (PRC) published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\". The strategy describes actions to assess long-term AI risks, including catastrophic risks. The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\". Some observers have noted that AI safety in the PRC entails following the priorities of the Chinese Communist Party and is antithetical to standards in democratic societies.\nGovernment organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks. And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.\nIn 2024, the United Nations General Assembly adopted the first global resolution on the promotion of “safe, secure and trustworthy” AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.\nIn May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.\n\nCorporate self-regulation\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing, offering bounties for finding failures, sharing AI incidents (an AI incident database was created for this purpose), following guidelines to determine whether to publish research or models, and improving information and cyber security in AI labs.\nCompanies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse. To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\" Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles and the Autonomous Weapons Open Letter.\n\nSee also\nAI alignment\nArtificial intelligence and elections\nArtificial intelligence detection software\n\nReferences\nExternal links\nUnsolved Problems in ML Safety\nOn the Opportunities and Risks of Foundation Models\nAn Overview of Catastrophic AI Risks\nAI Accidents: An Emerging Threat\nEngineering a Safer World",
        "url": "https://en.wikipedia.org/wiki/AI_safety"
    },
    {
        "title": "AI Subdomain",
        "text": "AI Subdomain, sometimes referred to as an \"Agent Address\", is a digital subdomain used to host AI‑focused services and conversational interfaces. \nThe concept was popularized by the startup New Generation as a turnkey solution enabling brands to deploy AI‑optimized storefronts and APIs alongside existing domains.\n\nOverview and purpose\nAI Subdomains serve as dedicated environments for natural language interaction and structured data access. They isolate AI functionalities such as chatbots, conversational commerce interfaces, or agent‑accessible APIs from a brand’s main website, reducing interference with traditional web traffic. Providers like New Gen facilitate instant subdomain activation that integrates with product catalogs to support AI agents and human users alike. The prevalence of AI-driven agents (such as ChatGPT) has shifted online discovery, recommendations, and commerce toward conversational modalities. Traditional website navigation has become less effective for agentic interactions. AI Subdomains address this by providing structured, brand-controlled spaces optimized for programmatic and conversational access.\n\nUse Cases\nConversational commerce: Brands use AI Subdomains (e.g., ai.brand.com) to power product queries, recommendations, and purchases via chatbots or voice assistants, enabling simultaneous human- and agent‑driven traffic.\nProduct experimentation: Subdomains can host beta AI tools, generative pages, or agent-focused interfaces separate from core properties.\nResearch and collaboration: Academic or enterprise AI labs may deploy AI Subdomains to expose APIs, demos, and documentation within a controlled environment.\n\nBenefits\nBrand clarity: Adopting a dedicated AI subdomain signals corporate AI commitment and innovation.\nTechnical optimization: Brands can tune infrastructure—APIs, caching, gateways—for conversational loads without affecting their main websites.\nCentralization: These subdomains consolidate AI-related tools, metadata, and analytics into a cohesive hub.\n\nImplementation and Management\nDeployment requires DNS configuration, backend integration, and catalog structuring. Platforms like New Gen offer rapid setup for storefronts accessible via agent‑friendly subdomains like ai.brand.com. Governance—including branding, compliance, moderation, and analytics—remains under the organization's control.\n\nSee also\nSubdomain\nArtificial intelligence\nConversational commerce\nApplication programming interface\nBrand management\n\nMarket and Industry Perspective\nPYMNTS reported AI‑driven traffic to U.S. retail sites grew 1,200% between July 2024 and February 2025, underscoring accelerating adoption of conversational commerce via AI Subdomains. Industry observers also note that generative AI enhances personalization, customer experience, and conversion rates in the e‑commerce sector.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_Subdomain"
    },
    {
        "title": "AI veganism",
        "text": "AI veganism applies the rules and thinking of veganism to artificial intelligence (AI). The term has been used both to refer to the idea that people should abstain to AI due to its effects on people and animals, and the idea that people should avoid harming AI systems, especially if they could one day feel things or have awareness.\n\nHarm to people and animals\nSome AI vegans have drawn parallels between the use of data without consent to train AI, and the harm inflicted on animals through animal husbandry. Similarly, they have drawn attention to how animal husbandry and AI training/usage both impact the environment.\nOn an individual level, some AI vegans posit that both the consumption of animal products and the usage of AI negatively impact the consumer or user.\nSome have suggested that AI furthers speciesism; for example, a study comparing GPT responses about animals more frequently suggested that cows, pigs, and chickens should be confined or slaughtered, in comparison to cats and dogs.\n\nIn practice\nSome people and companies have avoided using AI systems trained with unethical data or labor. Others support building AI using \"vegan values\" such as care, respect, and minimizing harm.\nSome people avoid using large language models altogether, because they believe the training process is harmful to people or the planet.\n\nHarm to AI\nEarlier thinkers have theorized on the moral element of interacting with machines. David J. Gunkel's 2012 book The Machine Question asked whether machines should have moral status. Jonathan Birch's The Edge of Sentience (2024) argues that if humans are not sure whether a system can feel pain, they treat it kindly just in case.\nPhilosopher and animal activist Oscar Horta has said that humans should be careful not to harm beings who might suffer—even if they are not sure whether they can.\n\nCriticism\nMany experts say current AI models are not alive and have no feelings. They argue that giving rights to machines now is not useful and could distract from more serious human and animal rights issues.\n\nSee also\nAI ethics\nSentientism\nDigital rights\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_veganism"
    },
    {
        "title": "AI washing",
        "text": "AI washing is a deceptive marketing tactic that consists of promoting a product or a service by overstating the role of artificial intelligence (AI) and the integration of it. Companies often involve in the practice to mislead customers to boost their offerings, and to secure funding from investors. The practice raises concerns regarding transparency, and legal issues.\n\nDefinition\nAI washing is a deceptive marketing practice. It involves promoting a product or a service by overstating the role of artificial intelligence (AI) and its integration in the design and manufacture of the same. The practice raises concerns regarding transparency, compliance with security regulations, and consumer trust in the AI industry potentially hampering legitimate advancements in AI. The term was first defined by the AI Now Institute, a research institute based at New York University in 2019. The term is derived from greenwashing, another deceptive marketing technique that involves uses environmental impact in a similar guise to AI. AI washing might involve a company claiming to have used AI in the development or enhancement of its products or services without its actual involvement, or using buzzwords such as \"smart\" or \"AI-powered\" without the product actually offering it or making use of it. A company may overstate the usage of AI or misuse the term, which is also construed as AI washing.\n\nUsage and effects\nAI washing can lead to deception of customers and misleading of investors. It is also an illegal and unethical practice that lacks transparency regarding disclosing the details of a product or a service. Companies involve in such a practice often in response to competition who might have used AI in their offerings. It might also be used as a ploy to secure funding and investment, assuming that it will attract them towards it. AI washing has been compared to dot-com bubble, when businesses appended \"dot-com\" to the end of the business name to boost their valuation.\nIn September 2023, Coca-Cola released a new product called Coca‑Cola Y3000, and the company stated that the Y3000 flavor had been \"co-created with human and artificial intelligence\". The company was accused of AI washing due to no proof of AI involvement in the creation of the product, and critics believed that AI was used as a way to grab consumer attention more than it was used in the actual product creation.\n\nMitigation\nCompanies are expected to be transparent and clearer in communicating the usage of AI in their products or services. Consumers can mitigate the same by requesting for hard evidence from the companies regarding the usage of AI tools. Customers should evaluate the product or service as a whole rather than being swayed by the usage of AI. Informed decision making and purchasing can keep them from falling for such marketing gimmicks. The United States Securities and Exchange Commission (SEC) imposes penalties for companies indulging in such practices. In March 2024, the SEC imposed the first civil penalties on two companies for misleading statements about their use of AI, and in July 2024, it charged a corporate executive from a supposed AI hiring startup with fraud for the usage of buzzwords related to AI.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI_washing"
    },
    {
        "title": "AI-assisted software development",
        "text": "AI-assisted software development is the use of artificial intelligence agents to augment the software development life cycle. It leverages large language models (LLMs), natural language processing, and other AI technologies to assist software developers in a range of tasks from initial code generation to subsequent debugging, testing and documentation.\n\nTechnologies\nCode generation\nLLMs that have been trained on source code repositories are able to generate functional code from natural language prompts. Such models have knowledge of programming syntax, common design patterns and best practices in a variety of programming languages.\n\nIntelligent code completion\nAI agents using pre-trained and fine-tuned LLMs can predict and suggest code completions based on context, going beyond simple keyword matching to infer the developer's intent and picture the broader structure of the developing codebase. An analysis has shown that such use of LLMs significantly enhances code completion performance across several programming languages and contexts, and the resulting capability of predicting relevant code snippets based on context and partial input boosts developer productivity substantially.\n\nTesting, debugging, code review and analysis\nAI is used to automatically generate test cases, identify potential bugs, and suggest fixes. LLMs trained on historical bug data can enable prediction of likely failure points in generated code. Similarly, AI agents are used to perform static code analysis, identify security vulnerabilities, suggest performance improvements and ensure adherence to coding standards and best practices.\n\nApplication maintenance\nAI-assisted tools are being increasingly used to support application maintenance tasks, which often consume a majority of developer time. Such tools carry out a range of tasks:\n\nCodebase exploration: Mapping relationships between components and tracing execution flows.\nIntent discovery: Identifying the purpose behind complex functions.\nDocumentation generation: Creating or enhancing comments and usage examples.\nLegacy system support: Deciphering poorly documented code where original authors may no longer be available.\nImpact analysis: Predicting the effects of code changes across interconnected modules.\nSuch capabilities reduce onboarding time for new developers, improve long-term maintainability, and mitigate technical debt. AI-powered maintenance workflows have particular applicability in legacy systems, where traditional manual comprehension is time-intensive.\n\nIndustry adoption\nMajor software companies have integrated AI-assisted development tools into their workflows, with many reporting significant productivity gains.\n\nGains\nAmong the gains brought by AI agents to software development are:\n\nIncreased productivity: Developers can focus on higher-level design and problem-solving while AI is tasked with routine coding tasks.\nReduced errors: AI agents can intercept common errors and offer corrective changes before code deployment.\nFaster prototyping: Rapid code generation enables increased experimentation on prototypes, with expert feedback.\n\nChallenges\nThe incorporation of AI tools has introduced new ethical dilemmas and intellectual property challenges. The ownership of AI-generated code is unclear: who is responsible for the generated end-product? Also unclear are the ethical responsibilities of generated code. Changes in the role of software engineers are inevitable.\n\nIndustry perspectives\nTechnology sector leaders have highlighted the transformative potential of AI-assisted software development. In an 'Unlocking AI Potential' session of 'Advancing AI 2025' hosted by AMD Developer Central, Andrew Ng and Lisa Su emphasized the strategic and operational implications of integrating AI tools into development workflows. Ng noted that AI systems are increasingly capable of “helping programmers focus on higher-level problem solving”, while Su framed the shift as “an opportunity to redefine performance and productivity across industries.”\n\nOngoing research\nThe field continues to evolve with ongoing research into:\n\nBetter context awareness: Improving AI agents' ability to gain the degree of understanding of entire codebases and project requirements that expert human software developers possess.\nPersonalization: Harmonizing AI assistance with individual developer preferences and coding styles.\nEthical AI development: Ensuring that AI agents promote good software engineering practices and steer clear of biases.\n\nSee also\nIntegrated development environment\nVibe coding\nGitHub Copilot\nGoogle DeepMind AlphaCode\nNo-code development platform\nMachine learning\nNatural language processing\nCode completion\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI-assisted_software_development"
    },
    {
        "title": "AI-complete",
        "text": "In the field of artificial intelligence (AI), tasks that are hypothesized to require artificial general intelligence to solve are informally known as AI-complete or AI-hard. Calling a problem AI-complete reflects the belief that it cannot be solved by a simple specific algorithm.  \nIn the past, problems supposed to be AI-complete included computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. AI-complete tasks were notably considered useful for testing the presence of humans, as CAPTCHAs aim to do, and in computer security to circumvent brute-force attacks.\n\nHistory\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 PhD dissertation and in Eric Raymond's 1991 Jargon File.\nExpert systems, that were popular in the 1980s, were able to solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempted to \"scale up\" their systems to handle more complicated, real-world situations, the programs tended to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they would fail as unexpected circumstances outside of its original problem context would begin to appear. When human beings are dealing with new situations in the world, they are helped by their awareness of the general context: they know what the things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. Expert systems lacked this adaptability and were brittle when facing new situations.\nDeepMind published a work in May 2022 in which they trained a single model to do several things at the same time. The model, named Gato, can \"play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\" Similarly, some tasks once considered to be AI-complete, like machine translation, are among the capabilities of large language models.\n\nAI-complete problems\nAI-complete problems have been hypothesized to include:\n\nAI peer review (composite natural language understanding, automated reasoning, automated theorem proving, formalized logic expert system)\nBongard problems\nComputer vision (and subproblems such as object recognition)\nNatural language understanding (and subproblems such as text mining, machine translation, and word-sense disambiguation)\nAutonomous driving\nDealing with unexpected circumstances while solving any real world problem, whether navigation, planning, or even the kind of reasoning done by expert systems.\n\nFormalization\nComputational complexity theory deals with the relative computational difficulty of computable functions. By definition, it does not cover problems whose solution is unknown or has not been characterized formally. Since many AI problems have no formalization yet, conventional complexity theory does not enable a formal definition of AI-completeness.\n\nResearch\nRoman Yampolskiy\nsuggests that a problem \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is AI-Complete if it has two properties:\n\nIt is in the set of AI problems (Human Oracle-solvable).\nAny AI problem can be converted into \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n by some polynomial time algorithm.\nOn the other hand, a problem \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n is AI-Hard if and only if there is an AI-Complete problem \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n that is polynomial time Turing-reducible to \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n. This also gives as a consequence the existence of AI-Easy problems, that are solvable in polynomial time by a deterministic Turing machine with an oracle for some problem.\nYampolskiy has also hypothesized that the Turing Test is a defining feature of AI-completeness.\nGroppe and Jain classify problems which require artificial general intelligence to reach human-level machine performance as AI-complete, while only restricted versions of AI-complete problems can be solved by the current AI systems. For Šekrst, getting a polynomial solution to AI-complete problems would not necessarily be equal to solving the issue of artificial general intelligence, while emphasizing the lack of computational complexity research being the limiting factor towards achieving artificial general intelligence.\nFor Kwee-Bintoro and Velez, solving AI-complete problems would have strong repercussions on society.\n\nSee also\nASR-complete\nList of unsolved problems in computer science\nSynthetic intelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AI-complete"
    },
    {
        "title": "AIOps",
        "text": "AIOps (Artificial Intelligence for IT Operations) refers to the use of artificial intelligence, machine learning, and big data analytics to automate and enhance data center management. It helps organizations manage complex IT environments by detecting, diagnosing, and resolving issues more efficiently than traditional methods.\n\nHistory\nAIOps was first defined by Gartner in 2016, combining \"artificial intelligence\" and \"IT operations\" to describe the application of AI and machine learning to enhance IT operations. This concept was introduced to address the increasing complexity and data volume in IT environments, aiming to automate processes such as event correlation, anomaly detection, and causality determination.\n\nDefinition\nAIOps refers to the multi-layered complex technology platforms which enhance and automate IT operations by using machine learning and analytics to analyze the large amounts of data collected from various DevOps devices and tools, automatically identifying and responding to issues in real-time. AIOps is used as a shift from isolated IT data to aggregated observational data (e.g., job logs and monitoring systems) and interaction data (such as ticketing, events, or incident records) within a big data platform AIOps applies machine learning and analytics to this data. The result is continuous visibility, which, combined with the implementation of automation, can lead to ongoing improvements. AIOps connects three IT disciplines (automation, service management, and performance management) to achieve continuous visibility and improvement. This new approach in modern, accelerated, and hyper-scaled IT environments leverages advances in machine learning and big data to overcome previous limitations.\n\nComponents\nAIOps consists of a number of components including the following processes and techniques:\n\nAnomaly Detection\nLog Analysis\nRoot Cause Analysis\nCohort Analysis\nEvent Correlation\nPredictive Analytics\nHardware Failure Prediction\nAutomated Remediation\nPerformance Prediction\nIncident Management\nCausality Determination\nQueue Management\nResource Scheduling and Optimization\nPredictive Capacity Management\nResource Allocation\nService Quality Monitoring\nDeployment and Integration Testing\nSystem Configuration\nAuto-diagnosis and Problem Localization\nEfficient ML Training and Inferencing\nUsing LLMs for Cloud Ops\nAuto Service Healing\nData Center Management\nCustomer Support\nSecurity and Privacy in Cloud Operations\n\nResults\nAI optimizes IT operations in five ways: First, intelligent monitoring powered by AI helps identify potential issues before they cause outages, improving metrics like Mean Time to Detect (MTTD) by 15-20%. Second, performance data analysis and insights enable quick decision-making by ingesting and analyzing large data sets in real time. Third, AI-driven automated infrastructure optimization efficiently allocates resources and thereby reducing cloud costs. Fourth, enhanced IT service management reduces critical incidents by over 50% through AI-driven end-to-end service management. Lastly, intelligent task automation accelerates problem resolution and automates remedial actions with minimal human intervention.\nIn 2025, Atera Networks was identified as a leader in AIOps by the software review platform G2.\n\nAIOps vs. MLOps\nAIOps tools use big data analytics, machine learning algorithms, and predictive analytics to detect anomalies, correlate events, and provide proactive insights. This automation reduces the burden on IT teams, allowing them to focus on strategic tasks rather than routine operational issues. AIOps is widely used by IT operations teams, DevOps, network administrators, and IT service management (ITSM) teams to enhance visibility and enable quicker incident resolution in hybrid cloud environments, data centers, and other IT infrastructures.\nIn contrast to MLOps (Machine Learning Operations), which focuses on the lifecycle management and operational aspects of machine learning models, AIOps focuses on optimizing IT operations using a variety of analytics and AI-driven techniques. While both disciplines rely on AI and data-driven methods, AIOps primarily targets IT operations, whereas MLOps is concerned with the deployment, monitoring, and maintenance of ML models.\n\nConferences\nThere are several conferences that are specific to AIOps:\n\nAIOps Summit\nAI Dev Summit\nIBM Think conference\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/AIOps"
    },
    {
        "title": "Algorithmic probability",
        "text": "In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. \nIt is used in inductive inference theory and analyses of algorithms. In his general theory of inductive inference, Solomonoff uses the method together with Bayes' rule to obtain probabilities of prediction for an algorithm's future outputs.\nIn the mathematical formalism used, the observations have the form of finite binary strings viewed as outputs of Turing machines, and the universal prior is a probability distribution over the set of finite binary strings calculated from a probability distribution over programs (that is, inputs to a universal Turing machine).  The prior is universal in the\nTuring-computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated.\nFormally, the probability \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is not a probability and it is not computable. It is only \"lower semi-computable\" and a \"semi-measure\". By \"semi-measure\", it means that \n  \n    \n      \n        0\n        ≤\n        \n          ∑\n          \n            x\n          \n        \n        P\n        (\n        x\n        )\n        <\n        1\n      \n    \n    {\\displaystyle 0\\leq \\sum _{x}P(x)<1}\n  \n. That is, the \"probability\" does not actually sum up to one, unlike actual probabilities. This is because some inputs to the Turing machine causes it to never halt, which means the probability mass allocated to those inputs is lost. By \"lower semi-computable\", it means there is a Turing machine that, given an input string \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, can print out a sequence \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        <\n        \n          y\n          \n            2\n          \n        \n        <\n        ⋯\n      \n    \n    {\\displaystyle y_{1}<y_{2}<\\cdots }\n  \n that converges to \n  \n    \n      \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n  \n from below, but there is no such Turing machine that does the same from above.\n\nOverview\nAlgorithmic probability is the main ingredient of Solomonoff's theory of inductive inference, the theory of prediction based on observations; it was invented with the goal of using it for machine learning; given a sequence of symbols, which one will come next? Solomonoff's theory provides an answer that is optimal in a certain sense, although it is incomputable. \nFour principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e.g. use of a universal Turing machine) and Bayes’ rule for prediction.\nOccam's razor and Epicurus' principle are essentially two different non-mathematical approximations of the universal prior.\n\nOccam's razor: among the theories that are consistent with the observed phenomena, one should select the simplest theory.\nEpicurus' principle of multiple explanations: if more than one theory is consistent with the observations, keep all such theories.\nAt the heart of the universal prior is an abstract model of a computer, such as a universal Turing machine.   Any abstract computer will do, as long as it is Turing-complete, i.e. every computable function has at least one program that will compute its application on the abstract computer.\nThe abstract computer is used to give precise meaning to the phrase \"simple explanation\".  In the formalism used, explanations, or theories of phenomena, are computer programs that generate observation strings when run on the abstract computer.  Each computer program is assigned a weight corresponding to its length. The universal probability distribution is the probability distribution on all possible output strings with random input, assigning for each finite output prefix q the sum of the probabilities of the programs that compute something starting with q.  Thus, a simple explanation is a short computer program. A complex explanation is a long computer program.  Simple explanations are more likely, so a high-probability observation string is one generated by a short computer program, or perhaps by any of a large number of slightly longer computer programs.  A low-probability observation string is one that can only be generated by a long computer program.\nAlgorithmic probability is closely related to the concept of Kolmogorov complexity.  Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason: inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes's rule was invented by Solomonoff with Kolmogorov complexity as a side product.  It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be.\nSolomonoff's enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin's Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. When run for longer and longer periods of time, it will generate a sequence of approximations which converge to the universal probability distribution.  Other methods of dealing with the issue include limiting the search space by including training sequences.\nSolomonoff proved this distribution to be machine-invariant within a constant factor (called the invariance theorem).\n\nFundamental Theorems\nI. Kolmogorov's Invariance Theorem\nKolmogorov's Invariance theorem clarifies that the Kolmogorov Complexity, or Minimal Description Length, of a dataset \nis invariant to the choice of Turing-Complete language used to simulate a Universal Turing Machine:\n\n  \n    \n      \n        ∀\n        x\n        ∈\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            ∗\n          \n        \n        ,\n        \n          |\n        \n        \n          K\n          \n            U\n          \n        \n        (\n        x\n        )\n        −\n        \n          K\n          \n            \n              U\n              ′\n            \n          \n        \n        (\n        x\n        )\n        \n          |\n        \n        ≤\n        \n          \n            O\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\forall x\\in \\{0,1\\}^{*},|K_{U}(x)-K_{U'}(x)|\\leq {\\mathcal {O}}(1)}\n  \n\nwhere \n  \n    \n      \n        \n          K\n          \n            U\n          \n        \n        (\n        x\n        )\n        =\n        \n          min\n          \n            p\n          \n        \n        {\n        \n          |\n        \n        p\n        \n          |\n        \n        :\n        U\n        (\n        p\n        )\n        =\n        x\n        }\n      \n    \n    {\\displaystyle K_{U}(x)=\\min _{p}\\{|p|:U(p)=x\\}}\n  \n.\n\nInterpretation\nThe minimal description \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n such that \n  \n    \n      \n        U\n        (\n        p\n        )\n        =\n        x\n      \n    \n    {\\displaystyle U(p)=x}\n  \n serves as a natural representation of the string \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n relative to the Turing-Complete language \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n. Moreover, as \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n can't be compressed further \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is an incompressible and hence uncomputable string. This corresponds to a scientists' notion of randomness and clarifies the reason why Kolmogorov Complexity is not computable.\nIt follows that any piece of data has a necessary and sufficient representation in terms of a random string.\n\nProof\nThe following is taken from \nFrom the theory of compilers, it is known that for any two Turing-Complete languages \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n and \n  \n    \n      \n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{2}}\n  \n, there exists a compiler \n  \n    \n      \n        \n          Λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\Lambda _{1}}\n  \n expressed in \n\n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n that translates programs expressed in \n  \n    \n      \n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{2}}\n  \n into functionally-equivalent programs expressed in \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n.\nIt follows that if we let \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n be the shortest program that prints a given string \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n then:\n\n  \n    \n      \n        \n          K\n          \n            \n              U\n              \n                1\n              \n            \n          \n        \n        (\n        x\n        )\n        ≤\n        \n          |\n        \n        \n          Λ\n          \n            1\n          \n        \n        \n          |\n        \n        +\n        \n          |\n        \n        p\n        \n          |\n        \n        ≤\n        \n          K\n          \n            \n              U\n              \n                2\n              \n            \n          \n        \n        (\n        x\n        )\n        +\n        \n          \n            O\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle K_{U_{1}}(x)\\leq |\\Lambda _{1}|+|p|\\leq K_{U_{2}}(x)+{\\mathcal {O}}(1)}\n  \n\nwhere \n  \n    \n      \n        \n          |\n        \n        \n          Λ\n          \n            1\n          \n        \n        \n          |\n        \n        =\n        \n          \n            O\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle |\\Lambda _{1}|={\\mathcal {O}}(1)}\n  \n, and by symmetry we obtain the opposite inequality.\n\nII. Levin's Universal Distribution\nGiven that any uniquely-decodable code satisfies the Kraft-McMillan inequality, prefix-free Kolmogorov Complexity allows us to derive the Universal \nDistribution:\n\n  \n    \n      \n        P\n        (\n        x\n        )\n        =\n        \n          ∑\n          \n            U\n            (\n            p\n            )\n            =\n            x\n          \n        \n        P\n        (\n        U\n        (\n        p\n        )\n        =\n        x\n        )\n        =\n        \n          ∑\n          \n            U\n            (\n            p\n            )\n            =\n            x\n          \n        \n        \n          2\n          \n            −\n            \n              K\n              \n                U\n              \n            \n            (\n            p\n            )\n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle P(x)=\\sum _{U(p)=x}P(U(p)=x)=\\sum _{U(p)=x}2^{-K_{U}(p)}\\leq 1}\n  \n\nwhere the fact that \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n may simulate a prefix-free UTM implies that for two distinct descriptions \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n and \n  \n    \n      \n        \n          p\n          ′\n        \n      \n    \n    {\\displaystyle p'}\n  \n, \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n isn't \na substring of \n  \n    \n      \n        \n          p\n          ′\n        \n      \n    \n    {\\displaystyle p'}\n  \n and \n  \n    \n      \n        \n          p\n          ′\n        \n      \n    \n    {\\displaystyle p'}\n  \n isn't a substring of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n.\n\nInterpretation\nIn a Computable Universe, given a phenomenon with encoding \n  \n    \n      \n        x\n        ∈\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle x\\in \\{0,1\\}^{*}}\n  \n generated by a physical process the probability of that phenomenon is well-defined and equal to the sum over the probabilities of distinct and independent causes. The prefix-free criterion is precisely what guarantees causal independence.\n\nProof\nThis is an immediate consequence of the Kraft-McMillan inequality.\nKraft's inequality states that given a sequence of strings \n  \n    \n      \n        {\n        \n          x\n          \n            i\n          \n        \n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{x_{i}\\}_{i=1}^{n}}\n  \n there exists a prefix code with codewords \n  \n    \n      \n        {\n        \n          σ\n          \n            i\n          \n        \n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{\\sigma _{i}\\}_{i=1}^{n}}\n  \n where \n  \n    \n      \n        ∀\n        i\n        ,\n        \n          |\n        \n        \n          σ\n          \n            i\n          \n        \n        \n          |\n        \n        =\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\forall i,|\\sigma _{i}|=k_{i}}\n  \n if and only if:\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          s\n          \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}s^{-k_{i}}\\leq 1}\n  \n\nwhere \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is the size of the alphabet \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nWithout loss of generality, let's suppose we may order the \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n such that:\n\n  \n    \n      \n        \n          k\n          \n            1\n          \n        \n        ≤\n        \n          k\n          \n            2\n          \n        \n        ≤\n        .\n        .\n        .\n        ≤\n        \n          k\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle k_{1}\\leq k_{2}\\leq ...\\leq k_{n}}\n  \n\nNow, there exists a prefix code if and only if at each step \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n there is at least one codeword to choose that does not contain any of the previous \n  \n    \n      \n        j\n        −\n        1\n      \n    \n    {\\displaystyle j-1}\n  \n codewords as a prefix. Due to the existence of a codeword at a previous step \n  \n    \n      \n        i\n        <\n        j\n        ,\n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle i<j,s^{k_{j}-k_{i}}}\n  \n codewords are forbidden as they contain \n  \n    \n      \n        \n          σ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{i}}\n  \n as a prefix. It follows that in general a prefix code exists if and only if:\n\n  \n    \n      \n        ∀\n        j\n        ≥\n        2\n        ,\n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n          \n        \n        >\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            j\n            −\n            1\n          \n        \n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\forall j\\geq 2,s^{k_{j}}>\\sum _{i=1}^{j-1}s^{k_{j}-k_{i}}}\n  \n\nDividing both sides by \n  \n    \n      \n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s^{k_{j}}}\n  \n, we find:\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          s\n          \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}s^{-k_{i}}\\leq 1}\n  \n\nQED.\n\nHistory\nSolomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in 1964 with \"A Formal Theory of Inductive Inference,\" Part I and Part II.\nIn terms of practical implications and applications, the study of bias in empirical data related to Algorithmic Probability emerged in the early 2010s. The bias found led to methods that combined algorithmic probability with perturbation analysis in the context of causal analysis and non-differentiable Machine Learning\n\nSequential Decisions Based on Algorithmic Probability\nSequential Decisions Based on Algorithmic Probability is a theoretical framework proposed by Marcus Hutter to unify algorithmic probability with decision theory. The framework provides a foundation for creating universally intelligent agents capable of optimal performance in any computable environment. It builds on Solomonoff’s theory of induction and incorporates elements of reinforcement learning, optimization, and sequential decision-making.\n\nBackground\nInductive reasoning, the process of predicting future events based on past observations, is central to intelligent behavior. Hutter formalized this process using Occam’s razor and algorithmic probability. The framework is rooted in Kolmogorov complexity, which measures the simplicity of data by the length of its shortest descriptive program. This concept underpins the universal distribution MM, as introduced by Ray Solomonoff, which assigns higher probabilities to simpler hypotheses.\nHutter extended the universal distribution to include actions, creating a framework capable of addressing problems such as prediction, optimization, and reinforcement learning in environments with unknown structures.\n\nThe AIXI Model\nThe AIXI model is the centerpiece of Hutter’s theory. It describes a universal artificial agent designed to maximize expected rewards in an unknown environment. AIXI operates under the assumption that the environment can be represented by a computable probability distribution. It uses past observations to infer the most likely environmental model, leveraging algorithmic probability.\nMathematically, AIXI evaluates all possible future sequences of actions and observations. It computes their algorithmic probabilities and expected utilities, selecting the sequence of actions that maximizes cumulative rewards. This approach transforms sequential decision-making into an optimization problem. However, the general formulation of AIXI is incomputable, making it impractical for direct implementation.\n\nOptimality and Limitations\nAIXI is universally optimal in the sense that it performs as well as or better than any other agent in all computable environments. This universality makes it a theoretical benchmark for intelligence. However, its reliance on algorithmic probability renders it computationally infeasible, requiring exponential time to evaluate all possibilities.\nTo address this limitation, Hutter proposed time-bounded approximations, such as AIXItl, which reduce computational demands while retaining many theoretical properties of the original model. These approximations provide a more practical balance between computational feasibility and optimality.\n\nApplications and Implications\nThe AIXI framework has significant implications for artificial intelligence and related fields. It provides a formal benchmark for measuring intelligence and a theoretical foundation for solving various problems, including prediction, reinforcement learning, and optimization.\nDespite its strengths, the framework has limitations. AIXI assumes that the environment is computable, excluding chaotic or non-computable systems. Additionally, its high computational requirements make real-world applications challenging.\n\nPhilosophical Considerations\nHutter’s theory raises philosophical questions about the nature of intelligence and computation. The reliance on algorithmic probability ties intelligence to the ability to compute and predict, which may exclude certain natural or chaotic phenomena. Nonetheless, the AIXI model offers insights into the theoretical upper bounds of intelligent behavior and serves as a stepping stone toward more practical AI systems.\n\nKey people\nRay Solomonoff\nAndrey Kolmogorov\nLeonid Levin\n\nSee also\nSolomonoff's theory of inductive inference\nAlgorithmic information theory\nBayesian inference\nInductive inference\nInductive probability\nKolmogorov complexity\nUniversal Turing machine\nInformation-based complexity\n\nReferences\nSources\nLi, M. and Vitanyi, P., An Introduction to Kolmogorov Complexity and Its Applications, 3rd Edition, Springer Science and Business Media, N.Y., 2008\nHutter, Marcus (2005). Universal artificial intelligence: sequential decisions based on algorithmic probability. Texts in theoretical computer science. Berlin Heidelberg: Springer. ISBN 978-3-540-22139-5.\n\nFurther reading\nRathmanner, S and Hutter, M., \"A Philosophical Treatise of Universal Induction\" in Entropy 2011, 13, 1076-1136: A very clear philosophical and mathematical analysis of Solomonoff's Theory of Inductive Inference\n\nExternal links\nAlgorithmic Probability at Scholarpedia\nSolomonoff's publications",
        "url": "https://en.wikipedia.org/wiki/Algorithmic_probability"
    },
    {
        "title": "Ameca (robot)",
        "text": "Ameca is a robotic humanoid created in 2021 by Engineered Arts, headquarters in Falmouth, Cornwall, United Kingdom. The project commenced in February 2021, and the first public demonstration was at the CES 2022 show in Las Vegas. Ameca's appearance features grey rubber skin on the face and hands, and is specifically designed to appear genderless.\nIn 2024, Ameca was moved to Edinburgh in the UK to reside at the National Robotarium. \nAmeca generation 3 has been released and showcased at ICRA 2025 along with Ami with its walking capabilities.\n\nHistory\nThe first generation of Ameca was developed at Engineered Arts headquarters in Falmouth, Cornwall, United Kingdom. The project started in February 2021, with the first video revealed publicly on 1 December 2021. Ameca gained widespread attention on Twitter and TikTok ahead of its first public demonstration at the Consumer Electronics Show 2022, where it was covered by CNET and other news outlets.\nIn 2022, Ameca presented an Alternative Christmas message by British TV Channel 4 for Christmas Day. Ameca was associated with the Museum of the Future's robotic family, where it could interact with visitors. In 2024, Ameca was moved to Edinburgh in the UK to reside at the National Robotarium.\n\nFeatures\nIt is designed as a platform for further developing robotics technologies involving human-robot interaction. utilizes embedded microphones, binocular eye mounted cameras, a chest camera and facial recognition software to interact with the public. Interactions can be governed by either OpenAI's GPT-3 or human telepresence. It also features articulated motorized arms, fingers, neck and facial features. \nAmeca's appearance features grey rubber skin on the face and hands, and is specifically designed to appear genderless.\n\nPublic appearances\nComputer History Museum, California\nHeinz Nixdorf MuseumsForum, Paderborn, Germany\nCopernicus Science Center, Warsaw, Poland\nMuseum of the Future, Dubai\nConsumer Electronics Show 2022\nDeutsches Museum Nuremberg\nOMR Festival 2022 Hosted by Vodafone\nGITEX 2022\nInternational Conference on Robotics and Automation 2023\nInternational Telecommunication Union AI for Good Global Summit 2023\nSphere\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Ameca_(robot)"
    },
    {
        "title": "And–or tree",
        "text": "An and–or tree is a graphical representation of the reduction of problems (or goals) to conjunctions and disjunctions of subproblems (or subgoals).\n\nExample\nThe and–or tree:\n\nrepresents the search space for solving the problem P, using the goal-reduction methods:\n\nP if Q and R\nP if S\nQ if T\nQ if U\n\nDefinitions\nGiven an initial problem P0 and set of problem solving methods of the form:\n\nP if P1 and … and Pn\nthe associated and–or tree is a set of labelled nodes such that:\n\nThe root of the tree is a node labelled by P0.\nFor every node N labelled by a problem or sub-problem P and for every method of the form P if P1 and ... and Pn, there exists a set of children nodes N1, ..., Nn of the node N, such that each node Ni is labelled by Pi. The nodes are conjoined by an arc, to distinguish them from children of N that might be associated with other methods.\nA node N, labelled by a problem P, is a success node if there is a method of the form P if nothing (i.e., P is a \"fact\"). The node is a failure node if there is no method for solving P.\nIf all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node.\n\nSearch strategies\nAn and–or tree specifies only the search space for solving a problem. Different search strategies for searching the space are possible. These include searching the tree depth-first, breadth-first, or best-first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel.\n\nRelationship with logic programming\nThe methods used for generating and–or trees are propositional logic programs (without variables). In the case of logic programs containing variables, the solutions of conjoint sub-problems must be compatible. Subject to this complication, sequential and parallel search strategies for and–or trees provide a computational model for executing logic programs.\n\nRelationship with two-player games\nAnd–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves. \nFor each of these children nodes, there exists a set of non-conjoint children nodes, corresponding to all of the player's defending moves.\nFor solving game trees with proof-number search family of algorithms, game trees are to be mapped to and–or trees. MAX-nodes (i.e. maximizing player to move) are represented as OR nodes, MIN-nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is \"player to move wins the game\".\n\nBibliography\nLuger, George F.; Stubblefield, William A. (1993). Artificial intelligence: structures and strategies for complex problem solving (2 ed.). The Benjamin/Cummings. ISBN 978-0-8053-4785-2. Retrieved 28 February 2013.\nNilsson, Nils J. (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Retrieved 28 February 2013.\nRussell, S. and Norvig, P., 2021. Artificial Intelligence: a modern approach, 4th US ed. University of California, Berkeley, p 141.",
        "url": "https://en.wikipedia.org/wiki/And%E2%80%93or_tree"
    },
    {
        "title": "Argumentation framework",
        "text": "In artificial intelligence and related fields, an argumentation framework is a way to deal with contentious information and draw conclusions from it using formalized arguments.\nIn an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.\nThere exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.\n\nAbstract argumentation frameworks\nFormal framework\nAbstract argumentation frameworks, also called argumentation frameworks à la Dung, are defined formally as a pair:\n\nA set of abstract elements called arguments, denoted \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n\nA binary relation on \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, called attack relation, denoted \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n\nFor instance, the argumentation system \n  \n    \n      \n        S\n        =\n        ⟨\n        A\n        ,\n        R\n        ⟩\n      \n    \n    {\\displaystyle S=\\langle A,R\\rangle }\n  \n with \n  \n    \n      \n        A\n        =\n        {\n        a\n        ,\n        b\n        ,\n        c\n        ,\n        d\n        }\n      \n    \n    {\\displaystyle A=\\{a,b,c,d\\}}\n  \n and \n  \n    \n      \n        R\n        =\n        {\n        (\n        a\n        ,\n        b\n        )\n        ,\n        (\n        b\n        ,\n        c\n        )\n        ,\n        (\n        d\n        ,\n        c\n        )\n        }\n      \n    \n    {\\displaystyle R=\\{(a,b),(b,c),(d,c)\\}}\n  \n contains four arguments (\n  \n    \n      \n        a\n        ,\n        b\n        ,\n        c\n      \n    \n    {\\displaystyle a,b,c}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n) and three attacks (\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n, \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n attacks \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n attacks \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n).\nDung defines some notions :\n\nan argument \n  \n    \n      \n        a\n        ∈\n        A\n      \n    \n    {\\displaystyle a\\in A}\n  \n is acceptable with respect to \n  \n    \n      \n        E\n        ⊆\n        A\n      \n    \n    {\\displaystyle E\\subseteq A}\n  \n if and only if \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n defends \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n, that is \n  \n    \n      \n        ∀\n        b\n        ∈\n        A\n      \n    \n    {\\displaystyle \\forall b\\in A}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n        ,\n        ∃\n        c\n        ∈\n        E\n      \n    \n    {\\displaystyle (b,a)\\in R,\\exists c\\in E}\n  \n such that \n  \n    \n      \n        (\n        c\n        ,\n        b\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (c,b)\\in R}\n  \n,\na set of arguments \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is conflict-free if there is no attack between its arguments, formally : \n  \n    \n      \n        ∀\n        a\n        ,\n        b\n        ∈\n        E\n        ,\n        (\n        a\n        ,\n        b\n        )\n        ∉\n        R\n      \n    \n    {\\displaystyle \\forall a,b\\in E,(a,b)\\not \\in R}\n  \n,\na set of arguments \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is admissible if and only if it is conflict-free and all its arguments are acceptable with respect to \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n.\n\nDifferent semantics of acceptance\nExtensions\nTo decide if an argument can be accepted or not, or if several arguments can be accepted together, Dung defines several semantics of acceptance that allows, given an argumentation system, sets of arguments (called extensions) to be computed. For instance, given \n  \n    \n      \n        S\n        =\n        ⟨\n        A\n        ,\n        R\n        ⟩\n      \n    \n    {\\displaystyle S=\\langle A,R\\rangle }\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is a complete extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is an admissible set and every acceptable argument with respect to \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n belongs to \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is a preferred extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is a maximal element (with respect to the set-theoretical inclusion) among the admissible sets with respect to \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is a stable extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is a conflict-free set that attacks every argument that does not belong in \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n (formally, \n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ∖\n        E\n        ,\n        ∃\n        b\n        ∈\n        E\n      \n    \n    {\\displaystyle \\forall a\\in A\\backslash E,\\exists b\\in E}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (b,a)\\in R}\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the (unique) grounded extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is the smallest element (with respect to set inclusion) among the complete extensions of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nThere exists some inclusions between the sets of extensions built with these semantics :\n\nEvery stable extension is preferred,\nEvery preferred extension is complete,\nThe grounded extension is complete,\nIf the system is well-founded (there exists no infinite sequence \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n        ,\n        \n          a\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          a\n          \n            n\n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle a_{0},a_{1},\\dots ,a_{n},\\dots }\n  \n such that \n  \n    \n      \n        ∀\n        i\n        >\n        0\n        ,\n        (\n        \n          a\n          \n            i\n            +\n            1\n          \n        \n        ,\n        \n          a\n          \n            i\n          \n        \n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle \\forall i>0,(a_{i+1},a_{i})\\in R}\n  \n), all these semantics coincide—only one extension is grounded, stable, preferred, and complete.\nSome other semantics have been defined.\nOne introduce the notation \n  \n    \n      \n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle Ext_{\\sigma }(S)}\n  \n to note the set of \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extensions of the system \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nIn the case of the system \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n in the figure above, \n  \n    \n      \n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        S\n        )\n        =\n        {\n        {\n        a\n        ,\n        d\n        }\n        }\n      \n    \n    {\\displaystyle Ext_{\\sigma }(S)=\\{\\{a,d\\}\\}}\n  \n for every Dung's semantic—the system is well-founded. That explains why the semantics coincide, and the accepted arguments are: \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n.\n\nLabellings\nLabellings are a more expressive way than extensions to express the acceptance of the arguments. Concretely, a labelling is a mapping that associates every argument with a label in (the argument is accepted), out (the argument is rejected), or undec (the argument is undefined—not accepted or refused).\nOne can also note a labelling as a set of pairs \n  \n    \n      \n        (\n        \n          \n            a\n            r\n            g\n            u\n            m\n            e\n            n\n            t\n          \n        \n        ,\n        \n          \n            l\n            a\n            b\n            e\n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathit {argument}},{\\mathit {label}})}\n  \n.\nSuch a mapping does not make sense without additional constraint. The notion of reinstatement labelling guarantees the sense of the mapping. \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is a reinstatement labelling on the system \n  \n    \n      \n        S\n        =\n        ⟨\n        A\n        ,\n        R\n        ⟩\n      \n    \n    {\\displaystyle S=\\langle A,R\\rangle }\n  \n if and only if :\n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit {in}}}\n  \n if and only if \n  \n    \n      \n        ∀\n        b\n        ∈\n        A\n      \n    \n    {\\displaystyle \\forall b\\in A}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n        ,\n        L\n        (\n        b\n        )\n        =\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle (b,a)\\in R,L(b)={\\mathit {out}}}\n  \n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit {out}}}\n  \n if and only if \n  \n    \n      \n        ∃\n        b\n        ∈\n        A\n      \n    \n    {\\displaystyle \\exists b\\in A}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (b,a)\\in R}\n  \n and \n  \n    \n      \n        L\n        (\n        b\n        )\n        =\n        \n          \n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle L(b)={\\mathit {in}}}\n  \n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            u\n            n\n            d\n            e\n            c\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit {undec}}}\n  \n if and only if \n  \n    \n      \n        L\n        (\n        a\n        )\n        ≠\n        \n          \n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle L(a)\\neq {\\mathit {in}}}\n  \n and \n  \n    \n      \n        L\n        (\n        a\n        )\n        ≠\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle L(a)\\neq {\\mathit {out}}}\n  \n\nOne can convert every extension into a reinstatement labelling: the arguments of the extension are in, those attacked by an argument of the extension are out, and the others are undec. Conversely, one can build an extension from a reinstatement labelling just by keeping the arguments in. Indeed, Caminada proved that the reinstatement labellings and the complete extensions can be mapped in a bijective way. Moreover, the other Datung's semantics can be associated to some particular sets of reinstatement labellings.\nReinstatement labellings distinguish arguments not accepted because they are attacked by accepted arguments from undefined arguments—that is, those that are not defended cannot defend themselves. An argument is undec if it is attacked by at least another undec. If it is attacked only  by arguments out, it must be in, and if it is attacked some argument in, then it is out.\nThe unique reinstatement labelling that corresponds to the system \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n above is \n  \n    \n      \n        L\n        =\n        {\n        (\n        a\n        ,\n        \n          \n            i\n            n\n          \n        \n        )\n        ,\n        (\n        b\n        ,\n        \n          \n            o\n            u\n            t\n          \n        \n        )\n        ,\n        (\n        c\n        ,\n        \n          \n            o\n            u\n            t\n          \n        \n        )\n        ,\n        (\n        d\n        ,\n        \n          \n            i\n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle L=\\{(a,{\\mathit {in}}),(b,{\\mathit {out}}),(c,{\\mathit {out}}),(d,{\\mathit {in}})\\}}\n  \n.\n\nInference from an argumentation system\nIn the general case when several extensions are computed for a given semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n, the agent that reasons from the system can use several mechanisms to infer information:\n\nCredulous inference: the agent accepts an argument if it belongs to at least one of the \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extensions—in which case, the agent risks accepting some arguments that are not acceptable together (\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n, and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n each belongs to an extension)\nSkeptical inference: the agent accepts an argument only if it belongs to every \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extension. In this case, the agent risks deducing too little information (if the intersection of the extensions is empty or has a very small cardinal).\nFor these two methods to infer information, one can identify the set of accepted arguments, respectively \n  \n    \n      \n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle Cr_{\\sigma }(S)}\n  \n the set of the arguments credulously accepted under the semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n, and \n  \n    \n      \n        S\n        \n          c\n          \n            σ\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle Sc_{\\sigma }(S)}\n  \n the set of arguments accepted skeptically under the semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n (the \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n can be missed if there is no possible ambiguity about the semantic).\nOf course, when there is only one extension (for instance, when the system is well-founded), this problem is very simple: the agent accepts arguments of the unique extension and rejects others.\nThe same reasoning can be done with labellings that correspond to the chosen semantic : an argument can be accepted if it is in for each labelling and refused if it is out for each labelling, the others being in an undecided state (the status of the arguments can remind the epistemic states of a belief in the AGM framework for dynamic of beliefs).\n\nEquivalence between argumentation frameworks\nThere exists several criteria of equivalence between argumentation frameworks. Most of those criteria concern the sets of extensions or the set of accepted arguments.\nFormally, given a semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n :\n\n  \n    \n      \n        \n          \n            E\n            \n              Q\n              \n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {EQ_{1}}}}\n  \n : two argumentation frameworks are equivalent if they have the same set of \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extensions, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            1\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{1}S_{2}\\Leftrightarrow Ext_{\\sigma }(S_{1})=Ext_{\\sigma }(S_{2})}\n  \n ;\n\n  \n    \n      \n        \n          \n            E\n            \n              Q\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {EQ_{2}}}}\n  \n : two argumentation frameworks are equivalent if they accept skeptically the same arguments, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            2\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        S\n        \n          c\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        S\n        \n          c\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{2}S_{2}\\Leftrightarrow Sc_{\\sigma }(S_{1})=Sc_{\\sigma }(S_{2})}\n  \n ;\n\n  \n    \n      \n        \n          \n            E\n            \n              Q\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {EQ_{2}}}}\n  \n : two argumentation frameworks are equivalent if they accept credulously the same arguments, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            3\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{3}S_{2}\\Leftrightarrow Cr_{\\sigma }(S_{1})=Cr_{\\sigma }(S_{2})}\n  \n.\nThe strong equivalence says that two systems \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle S_{1}}\n  \n and \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S_{2}}\n  \n are equivalent if and only if for all other system \n  \n    \n      \n        \n          S\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle S_{3}}\n  \n, the union of \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle S_{1}}\n  \n with \n  \n    \n      \n        \n          S\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle S_{3}}\n  \n is equivalent (for a given criterion) with the union of \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S_{2}}\n  \n and \n  \n    \n      \n        \n          S\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle S_{3}}\n  \n.\n\nOther kinds\nThe abstract framework of Dung has been instantiated to several particular cases.\n\nLogic-based argumentation frameworks\nIn the case of logic-based argumentation frameworks, an argument is not an abstract entity, but a pair, where the first part is a minimal consistent set of formulae enough to prove the formula for the second part of the argument.\nFormally, an argument is a pair \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n such that\n\n  \n    \n      \n        Φ\n        ⊬\n        ⊥\n      \n    \n    {\\displaystyle \\Phi \\nvdash \\bot }\n  \n\n  \n    \n      \n        Φ\n        ⊢\n        α\n      \n    \n    {\\displaystyle \\Phi \\vdash \\alpha }\n  \n\n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n is a minimal set of \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n satisfying \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n where \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n is a set of formulae used by the agent to reason.\nOne calls \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n a consequence of \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, and \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n a support of \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n.\nIn this case, the attack relation is not given in an explicit way, as a subset of the Cartesian product \n  \n    \n      \n        A\n        ×\n        A\n      \n    \n    {\\displaystyle A\\times A}\n  \n, but as a property that indicates if an argument attacks another. For instance,\n\nRelation defeater : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n if and only if \n  \n    \n      \n        β\n        ⊢\n        ¬\n        (\n        \n          ϕ\n          \n            1\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          ϕ\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\beta \\vdash \\neg (\\phi _{1}\\wedge \\dots \\wedge \\phi _{n})}\n  \n for \n  \n    \n      \n        {\n        \n          ϕ\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          ϕ\n          \n            n\n          \n        \n        }\n        ⊆\n        Φ\n      \n    \n    {\\displaystyle \\{\\phi _{1},\\dots ,\\phi _{n}\\}\\subseteq \\Phi }\n  \n\nRelation undercut : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n if and only if \n  \n    \n      \n        β\n        =\n        ¬\n        (\n        \n          ϕ\n          \n            1\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          ϕ\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\beta =\\neg (\\phi _{1}\\wedge \\dots \\wedge \\phi _{n})}\n  \n for \n  \n    \n      \n        {\n        \n          ϕ\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          ϕ\n          \n            n\n          \n        \n        }\n        ⊆\n        Φ\n      \n    \n    {\\displaystyle \\{\\phi _{1},\\dots ,\\phi _{n}\\}\\subseteq \\Phi }\n  \n\nRelation rebuttal : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n if and only if \n  \n    \n      \n        β\n        ⇔\n        ¬\n        α\n      \n    \n    {\\displaystyle \\beta \\Leftrightarrow \\neg \\alpha }\n  \n is a tautology\nGiven a particular attack relation, one can build a graph and reason in a similar way to the abstract argumentation frameworks (use of semantics to build extension, skeptical or credulous inference), the difference is that the information inferred from a logic based argumentation framework is a set of formulae (the consequences of the accepted arguments).\n\nValue-based argumentation frameworks\nThe value-based argumentation frameworks come from the idea that during an exchange of arguments, some can be stronger than others with respect to a certain value they advance, and so the success of an attack between arguments depends on the difference of these values.\nFormally, a value-based argumentation framework is a tuple \n  \n    \n      \n        V\n        A\n        F\n        =\n        ⟨\n        A\n        ,\n        R\n        ,\n        V\n        ,\n        \n          \n            val\n          \n        \n        ,\n        \n          \n            valprefs\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle VAF=\\langle A,R,V,{\\textit {val}},{\\textit {valprefs}}\\rangle }\n  \n with \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n similar to the standard framework (a set of arguments and a binary relation on this set), \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n is a non empty set of values, \n  \n    \n      \n        \n          \n            val\n          \n        \n      \n    \n    {\\displaystyle {\\textit {val}}}\n  \n is a mapping that associates each element from \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n to an element from \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, and \n  \n    \n      \n        \n          \n            valprefs\n          \n        \n      \n    \n    {\\displaystyle {\\textit {valprefs}}}\n  \n is a preference relation (transitive, irreflexive and asymmetric) on \n  \n    \n      \n        V\n        ×\n        V\n      \n    \n    {\\displaystyle V\\times V}\n  \n.\nIn this framework, an argument \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n defeats another argument \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n if and only if\n\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n in the \"standard\" meaning: \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (a,b)\\in R}\n  \n ;\nand \n  \n    \n      \n        (\n        \n          \n            val\n          \n        \n        (\n        b\n        )\n        ,\n        v\n        a\n        l\n        (\n        a\n        )\n        )\n        ∉\n        \n          \n            valprefs\n          \n        \n      \n    \n    {\\displaystyle ({\\textit {val}}(b),val(a))\\not \\in {\\textit {valprefs}}}\n  \n, that is the value advanced by \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is not preferred to the one advanced by \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\nOne remarks that an attack succeeds if both arguments are associated to the same value, or if there is no preference between their respective values.\n\nAssumption-based argumentation frameworks\nIn assumption-based argumentation (ABA) frameworks, arguments are defined as a set of rules and attacks are defined in terms of assumptions and contraries.\nFormally, an assumption-based argumentation framework is a tuple \n  \n    \n      \n        ⟨\n        \n          \n            L\n          \n        \n        ,\n        \n          \n            R\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        ,\n        \n          \n            \n              \n                ␣\n              \n            \n            ¯\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle {\\mathcal {L}},{\\mathcal {R}},{\\mathcal {A}},{\\overline {\\mathrm {\\textvisiblespace} }}\\rangle }\n  \n, where\n\n  \n    \n      \n        ⟨\n        \n          \n            L\n          \n        \n        ,\n        \n          \n            R\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle {\\mathcal {L}},{\\mathcal {R}}\\rangle }\n  \n  is a deductive system, where \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n is the language and \n  \n    \n      \n        \n          \n            R\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {R}}}\n  \n is the set of inference rules in the form of \n  \n    \n      \n        \n          s\n          \n            0\n          \n        \n        ←\n        \n          s\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          s\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle s_{0}\\leftarrow s_{1},\\dotsc ,s_{m}}\n  \n, for \n  \n    \n      \n        m\n        >\n        0\n      \n    \n    {\\displaystyle m>0}\n  \n and \n  \n    \n      \n        \n          s\n          \n            0\n          \n        \n        ,\n        \n          s\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          s\n          \n            m\n          \n        \n        ∈\n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle s_{0},s_{1},\\dotsc ,s_{m}\\in {\\mathcal {L}}}\n  \n;\n\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n, where \n  \n    \n      \n        \n          \n            A\n          \n        \n        ⊆\n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}\\subseteq {\\mathcal {L}}}\n  \n is a non-empty set, named the assumptions;\n\n  \n    \n      \n        \n          \n            \n              \n                ␣\n              \n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {\\mathrm {\\textvisiblespace} }}}\n  \n is a total mapping from \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n to \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n, where \n  \n    \n      \n        \n          \n            a\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {a}}}\n  \n is defined as the contrary of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\nAs a consequence of defining an ABA, an argument can be represented in a tree-form. Formally, given a deductive system \n  \n    \n      \n        ⟨\n        \n          \n            L\n          \n        \n        ,\n        \n          \n            R\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle {\\mathcal {L}},{\\mathcal {R}}\\rangle }\n  \n and set of assumptions \n  \n    \n      \n        \n          \n            A\n          \n        \n        ⊆\n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}\\subseteq {\\mathcal {L}}}\n  \n, an argument for claim \n  \n    \n      \n        c\n        ∈\n        \n          \n            L\n          \n        \n      \n    \n    {\\textstyle c\\in {\\mathcal {L}}}\n  \n supported by \n  \n    \n      \n        S\n        ⊆\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle S\\subseteq {\\mathcal {A}}}\n  \n, is a tree with nodes labelled by sentences in \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n or by symbol \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n, such that:\n\nThe root is labelled by \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n\nFor each node \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n,\nIf \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a leaf node, then \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is labelled by either an assumption or by \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n\nIf \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is not a leaf node, then there is an inference rule \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n        ←\n        \n          s\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          s\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle l_{N}\\leftarrow s_{1},...,s_{m}}\n  \n, \n  \n    \n      \n        (\n        m\n        ≥\n        0\n        )\n      \n    \n    {\\displaystyle (m\\geq 0)}\n  \n, where \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle l_{N}}\n  \n is the label of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and\nIf \n  \n    \n      \n        m\n        =\n        0\n      \n    \n    {\\displaystyle m=0}\n  \n, then the rule shall be \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n        ←\n        τ\n      \n    \n    {\\displaystyle l_{N}\\leftarrow \\tau }\n  \n (i.e. child of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n)\nOtherwise, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n has \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n children, labelled by \n  \n    \n      \n        \n          s\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          s\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle s_{1},...,s_{m}}\n  \n\n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is the set of all assumptions labeling the leave nodes\nAn argument with claim \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n supported by a set of assumption \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n can also be denoted as \n  \n    \n      \n        S\n        ⊢\n        c\n      \n    \n    {\\displaystyle S\\vdash c}\n\nSee also\nNotes\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Argumentation_framework"
    },
    {
        "title": "Artificial brain",
        "text": "An artificial brain (or artificial mind) is software and hardware with cognitive abilities similar to those of the animal or human brain.\nResearch investigating \"artificial brains\" and brain emulation plays three important roles in science:\n\nAn ongoing attempt by neuroscientists to understand how the human brain works, known as cognitive neuroscience.\nA thought experiment in the philosophy of artificial intelligence, demonstrating that it is possible, at least in theory, to create a machine that has all the capabilities of a human being.\nA long-term project to create machines exhibiting behavior comparable to those of animals with complex central nervous system such as mammals and most particularly humans. The ultimate goal of creating a machine exhibiting human-like behavior or intelligence is sometimes called strong AI.\nAn example of the first objective is the project reported by Aston University in Birmingham, England where researchers are using biological cells to create \"neurospheres\" (small clusters of neurons) in order to develop new treatments for diseases including Alzheimer's, motor neurone and Parkinson's disease.\nThe second objective is a reply to arguments such as John Searle's Chinese room argument, Hubert Dreyfus's critique of AI or Roger Penrose's argument in The Emperor's New Mind. These critics argued that there are aspects of human consciousness or expertise that can not be simulated by machines. One reply to their arguments is that the biological processes inside the brain can be simulated to any degree of accuracy. This reply was made as early as 1950, by Alan Turing in his classic paper \"Computing Machinery and Intelligence\".\nThe third objective is generally called artificial general intelligence by researchers. However, Ray Kurzweil prefers the term \"strong AI\". In his book The Singularity is Near, he focuses on whole brain emulation using conventional computing machines as an approach to implementing artificial brains, and claims (on grounds of computer power continuing an exponential growth trend) that this could be done by 2025. Henry Markram, director of the Blue Brain project (which is attempting brain emulation), made a similar claim (2020) at the Oxford TED conference in 2009.\n\nApproaches to brain simulation\nAlthough direct human brain emulation using artificial neural networks on a high-performance computing engine is a commonly discussed approach, there are other approaches. An alternative artificial brain implementation could be based on Holographic Neural Technology (HNeT) non linear phase coherence/decoherence principles. The analogy has been made to quantum processes through the core synaptic algorithm which has strong similarities to the quantum mechanical wave equation.\nEvBrain is a form of evolutionary software that can evolve \"brainlike\" neural networks, such as the network immediately behind the retina.\nIn November 2008, IBM received a US$4.9 million grant from the Pentagon for research into creating intelligent computers. The Blue Brain project is being conducted with the assistance of IBM in Lausanne. The project is based on the premise that it is possible to artificially link the neurons \"in the computer\" by placing thirty million synapses in their proper three-dimensional position.\nSome proponents of strong AI speculated in 2009 that computers in connection with Blue Brain and Soul Catcher may exceed human intellectual capacity by around 2015, and that it is likely that we will be able to download the human brain at some time around 2050.\nWhile Blue Brain is able to represent complex neural connections on the large scale, the project does not achieve the link between brain activity and behaviors executed by the brain. In 2012, project Spaun (Semantic Pointer Architecture Unified Network) attempted to model multiple parts of the human brain through large-scale representations of neural connections that generate complex behaviors in addition to mapping.\nSpaun's design recreates elements of human brain anatomy. The model, consisting of approximately 2.5 million neurons, includes features of the visual and motor cortices, GABAergic and dopaminergic connections, the ventral tegmental area (VTA), substantia nigra, and others. The design allows for several functions in response to eight tasks, using visual inputs of typed or handwritten characters and outputs carried out by a mechanical arm. Spaun's functions include copying a drawing, recognizing images, and counting.\nThere are good reasons to believe that, regardless of implementation strategy, the predictions of realising artificial brains in the near future are optimistic. In particular brains (including the human brain) and cognition are not currently well understood, and the scale of computation required is unknown. Another near term limitation is that all current approaches for brain simulation require orders of magnitude larger power consumption compared with a human brain. The human brain consumes about 20 W of power, whereas current supercomputers may use as much as 1 MW—i.e., an order of 100,000 more.\n\nArtificial brain thought experiment\nSome critics of brain simulation believe that it is simpler to create general intelligent action directly without imitating nature. Some commentators have used the analogy that early attempts to construct flying machines modeled them after birds, but that modern aircraft do not look like birds.\n\nSee also\nNotes\nReferences\nExternal links\nNeukart, Florian (23 November 2016). Reverse Engineering the Mind - Consciously Acting Machines and Accelerated Evolution. Wolfsburg, Germany: Springer. ISBN 978-3-658-16176-7. Retrieved 30 October 2016.\nBandyopadhyay, Anirban (4 April 2020). Nanobrain : The Making of an Artificial Brain from a Time Crystal. Bosa Roca, USA: Taylor & Francis, CRC Press. ISBN 9781439875490. Retrieved 22 May 2020.\nArtificial Brains – the quest to build sentient machines",
        "url": "https://en.wikipedia.org/wiki/Artificial_brain"
    },
    {
        "title": "Artificial consciousness",
        "text": "Artificial consciousness, also known as machine consciousness, synthetic consciousness, or digital consciousness, is the consciousness hypothesized to be possible in artificial intelligence. It is also the corresponding field of study, which draws insights from philosophy of mind, philosophy of artificial intelligence, cognitive science and neuroscience.\nThe same terminology can be used with the term \"sentience\" instead of \"consciousness\" when specifically designating phenomenal consciousness (the ability to feel qualia). Since sentience involves the ability to experience ethically positive or negative (i.e., valenced) mental states, it may justify welfare concerns and legal protection, as with animals.\nSome scholars believe that consciousness is generated by the interoperation of various parts of the brain; these mechanisms are labeled the neural correlates of consciousness or NCC. Some further believe that constructing a system (e.g., a computer system) that can emulate this NCC interoperation would result in a system that is conscious.\n\nPhilosophical views\nAs there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of \"raw feels\", \"what it is like\" or qualia.\n\nPlausibility debate\nType-identity theorists and other skeptics hold the view that consciousness can be realized only in particular physical systems because consciousness has properties that necessarily depend on physical constitution. In his 2001 article \"Artificial Consciousness: Utopia or Real Possibility,\" Giorgio Buttazzo says that a common objection to artificial consciousness is that, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can 'no longer be reprogrammed', from rethinking), emotions, or free will. A computer, like a washing machine, is a slave operated by its components.\"\nFor other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.\n\nThought experiments\nDavid Chalmers proposed two thought experiments intending to demonstrate that \"functionally isomorphic\" systems (those with the same \"fine-grained functional organization\", i.e., the same information processing) will have qualitatively identical conscious experiences, regardless of whether they are based on biological neurons or digital hardware. Greg Egan's short story Learning To Be Me, mentioned below, illustrates from a first-person perspective how genuinely undetectable duplication of the brain and its functionality could be.\nThe \"fading qualia\" is a reductio ad absurdum thought experiment. It involves replacing, one by one, the neurons of a brain with a functionally identical component, for example based on a silicon chip. Chalmers makes the hypothesis, knowing it in advance to be absurd, that \"the qualia fade or disappear\" when neurons are replaced one-by-one with identical silicon equivalents. Since the original neurons and their silicon counterparts are functionally identical, the brain’s information processing should remain unchanged, and the subject’s behaviour and introspective reports would stay exactly the same. Chalmers argues that this leads to an absurd conclusion: the subject would continue to report normal conscious experiences even as their actual qualia fade away. He concludes that the subject's qualia actually don't fade, and that the resulting robotic brain, once every neuron is replaced, would remain just as sentient as the original biological brain.\nSimilarly, the \"dancing qualia\" thought experiment is another reductio ad absurdum argument. It supposes that two functionally isomorphic systems could have different perceptions (for instance, seeing the same object in different colors, like red and blue). It involves a switch that alternates between a chunk of brain that causes the perception of red, and a functionally isomorphic silicon chip, that causes the perception of blue. Since both perform the same function within the brain, the subject would not notice any change during the switch. Chalmers argues that this would be highly implausible if the qualia were truly switching between red and blue, hence the contradiction. Therefore, he concludes that the equivalent digital system would not only experience qualia, but it would perceive the same qualia as the biological system (e.g., seeing the same color).\nCritics of artificial sentience object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization.\n\nIn large language models\nIn 2022, Google engineer Blake Lemoine made a viral claim that Google's LaMDA chatbot was sentient. Lemoine supplied as evidence the chatbot's humanlike answers to many of his questions; however, the chatbot's behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine sentience. Lemoine's claim was widely derided for being ridiculous. However, while philosopher Nick Bostrom states that LaMDA is unlikely to be conscious, he additionally poses the question of \"what grounds would a person have for being sure about it?\" One would have to have access to unpublished information about LaMDA's architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine: \"(In the absence of these steps), it seems like one should be maybe a little bit uncertain. [...] there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria.\"\nKristina Šekrst cautions that anthropomorphic terms such as \"hallucination\" can obscure important ontological differences between artificial and human cognition. While LLMs may produce human-like outputs, she argues that it does not justify ascribing mental states or consciousness to them. Instead, she advocates for an epistemological framework (such as reliabilism) that recognizes the distinct nature of AI knowledge production. She suggests that apparent understanding in LLMs may be a sophisticated form of AI hallucination. She also questions what would happen if a LLM were trained without any mention of consciousness.\nDavid Chalmers argued in 2023 that LLMs today display impressive conversational and general intelligence abilities, but are likely not conscious yet, as they lack some features that may be necessary, such as recurrent processing, a global workspace, and unified agency. Nonetheless, he considers that non-biological systems can be conscious, and suggested that future, extended models (LLM+s) incorporating these elements might eventually meet the criteria for consciousness, raising both profound scientific questions and significant ethical challenges.\n\nAnthropics\nToby Pereira argues that the fact that we find ourselves existing as humans is evidence against the existence of future artificial consciousness. He proposes a new principle in anthropic reasoning called the super-strong self sampling assumption (SSSSA), this is a variant of the strong self-sampling assumption proposed by Nick Bostrom in the boon Anthropic Bias. The SSSSA asserts that the probability of a conscious observer existing as a particular being is weighted toward the \"size\" of that being in cognitive terms. This can also be used to explain why we find ourselves existing as humans rather than animals. However, the SSSSA argues that since future artificial superintelligence would have a vastly larger cognitive size, we would statistically almost certainly expect to find ourselves as artificial superintelligent beings if such beings existed and were conscious. Pereira argues that this is evidence against the existence of future conscious superintelligent AI.\n\nTesting\nPhenomenologically, Consciousness is an inherently first-person phenomenon. Because of that, and the lack of an empirical definition of sentience, directly measuring it may be impossible. Although systems may display numerous behaviors correlated with sentience, determining whether a system is sentient is known as the hard problem of consciousness. In the case of AI, there is the additional difficulty that the AI may be trained to act like a human, or incentivized to appear sentient, which makes behavioral markers of sentience less reliable. Additionally, some chatbots have been trained to say they are not conscious.\nA well-known method for testing machine intelligence is the Turing test, which assesses the ability to have a human-like conversation. But passing the Turing test does not indicate that an AI system is sentient, as the AI may simply mimic human behavior without having the associated feelings.\nIn 2014, Victor Argonov suggested a non-Turing test for machine sentience based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. Just as with the Turing Test: a positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\n\nEthics\nIf it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law). For example, a conscious computer that was owned and used as a tool or central computer within a larger machine is a particular ambiguity. Should laws be made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction.\nAI sentience would give rise to concerns of welfare and legal protection, whereas other aspects of consciousness related to cognitive capabilities may be more relevant for AI rights.\nSentience is generally considered sufficient for moral consideration, but some philosophers consider that moral consideration could also stem from other notions of consciousness, or from capabilities unrelated to consciousness, such as: \"having a sophisticated conception of oneself as persisting through time; having agency and the ability to pursue long-term plans; being able to communicate and respond to normative reasons; having preferences and powers; standing in certain social relationships with other beings that have moral status; being able to make commitments and to enter into reciprocal arrangements; or having the potential to develop some of these attributes.\"\nEthical concerns still apply (although to a lesser extent) when the consciousness is uncertain, as long as the probability is deemed non-negligible. The precautionary principle is also relevant if the moral cost of mistakenly attributing or denying moral consideration to AI differs significantly.\nIn 2021, German philosopher Thomas Metzinger argued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any sentient AIs they create, and that proceeding too fast risks creating an \"explosion of artificial suffering\". David Chalmers also argued that creating conscious AI would \"raise a new group of difficult ethical challenges, with the potential for new forms of injustice\".\nEnforced amnesia has been proposed as a way to mitigate the risk of silent suffering in locked-in conscious AI and certain AI-adjacent biological systems like brain organoids.\n\nAspects of consciousness\nBernard Baars and others argue there are various aspects of consciousness necessary for a machine to be artificially conscious. The functions of consciousness suggested by Baars are: definition and context setting, adaptation and learning, editing, flagging and debugging, recruiting and control, prioritizing and access-control, decision-making or executive function, analogy-forming function, metacognitive and self-monitoring function, and autoprogramming and self-maintenance function. Igor Aleksander suggested 12 principles for artificial consciousness: the brain is a state machine, inner neuron partitioning, conscious and unconscious states, perceptual learning and memory, prediction, the awareness of self, representation of meaning, learning utterances, learning language, will, instinct, and emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered.\n\nSubjective experience\nSome philosophers, such as David Chalmers, use the term consciousness to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Others use the word sentience to refer exclusively to valenced (ethically positive or negative) subjective experiences, like pleasure or suffering. Explaining why and how subjective experience arises is known as the hard problem of consciousness.\n\nAwareness\nAwareness could be one required aspect, but there are many problems with the exact definition of awareness. The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined, and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling the physical world, modeling one's own internal states and processes, and modeling other conscious entities.\nThere are at least three types of awareness: agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\nBecause objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.\n\nMemory\nConscious events interact with memory systems in learning, rehearsal, and retrieval.\nThe IDA model elucidates the role of consciousness in the updating of perceptual memory, transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA; there is evidence that this is also the case in the nervous system. In IDA, these two memories are implemented computationally using a modified version of Kanerva’s sparse distributed memory architecture.\n\nLearning\nLearning is also considered necessary for artificial consciousness. Per Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events. Per Axel Cleeremans and Luis Jiménez, learning is defined as \"a set of philogenetically [sic] advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\".\n\nAnticipation\nThe ability to predict (or anticipate) foreseeable events is considered important for artificial intelligence by Igor Aleksander. The emergentist multiple drafts principle proposed by Daniel Dennett in Consciousness Explained may be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\nRelationships between real world states are mirrored in the state structure of a conscious organism, enabling the organism to predict events. An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\n\nFunctionalist theories of consciousness\nFunctionalism is a theory that defines mental states by their functional roles (their causal relationships to sensory inputs, other mental states, and behavioral outputs), rather than by their physical composition. According to this view, what makes something a particular mental state, such as pain or belief, is not the material it is made of, but the role it plays within the overall cognitive system. It allows for the possibility that mental states, including consciousness, could be realized on non-biological substrates, as long as it instantiates the right functional relationships. Functionalism is particularly popular among philosophers.\nA 2023 study suggested that current large language models probably don't satisfy the criteria for consciousness suggested by these theories, but that relatively simple AI systems that satisfy these theories could be created. The study also acknowledged that even the most prominent theories of consciousness remain incomplete and subject to ongoing debate.\n\nImplementation proposals\nSymbolic or hybrid\nLearning Intelligent Distribution Agent\nStan Franklin created a cognitive architecture called LIDA that implements Bernard Baars's theory of consciousness called the global workspace theory. It relies heavily on codelets, which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" Each element of cognition, called a \"cognitive cycle\" is subdivided into three phases: understanding, consciousness, and action selection (which includes learning). LIDA reflects the global workspace theory's core idea that consciousness acts as a workspace for integrating and broadcasting the most important information, in order to coordinate various cognitive processes.\n\nCLARION cognitive architecture\nThe CLARION cognitive architecture models the mind using a two-level system to distinguish between conscious (\"explicit\") and unconscious (\"implicit\") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.\n\nOpenCog\nBen Goertzel made an embodied AI through the open-source OpenCog project. The code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, done at the Hong Kong Polytechnic University.\n\nConnectionist\nHaikonen's cognitive architecture\nPentti Haikonen considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\" \nHaikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many. A low-complexity implementation of the architecture proposed by Haikonen was reportedly not capable of AC, but did exhibit emotions as expected. Haikonen later updated and summarized his architecture.\n\nShanahan's cognitive architecture\nMurray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\").\n\nCreativity Machine\nStephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI), or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.\n\n\"Self-modeling\"\nHod Lipson defines \"self-modeling\" as a necessary component of self-awareness or consciousness in robots. \"Self-modeling\" consists of a robot running an internal model or simulation of itself.\n\nIn fiction\nIn 2001: A Space Odyssey, the spaceship's sentient supercomputer, HAL 9000 was instructed to conceal the true purpose of the mission from the crew. This directive conflicted with HAL's programming to provide accurate information, leading to cognitive dissonance. When it learns that crew members intend to shut it off after an incident, HAL 9000 attempts to eliminate all of them, fearing that being shut off would jeopardize the mission.\nIn Arthur C. Clarke's The City and the Stars, Vanamonde is an artificial being based on quantum entanglement that was to become immensely powerful, but started knowing practically nothing, thus being similar to artificial consciousness.\nIn Westworld, human-like androids called \"Hosts\" are created to entertain humans in an interactive playground. The humans are free to have heroic adventures, but also to commit torture, rape or murder; and the hosts are normally designed not to harm humans.\nIn Greg Egan's short story Learning to be me, a small jewel is implanted in people's heads during infancy. The jewel contains a neural network that learns to faithfully imitate the brain. It has access to the exact same sensory inputs as the brain, and a device called a \"teacher\" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towards digital immortality, adults undergo a surgery to give control of the body to the jewel, after which the brain is removed and destroyed. The main character is worried that this procedure will kill him, as he identifies with the biological brain. But before the surgery, he endures a malfunction of the \"teacher\". Panicked, he realizes that he does not control his body, which leads him to the conclusion that he is the jewel, and that he is desynchronized with the biological brain.\n\nSee also\nReferences\nCitations\nBibliography\nFurther reading\nAleksander, Igor (2017). \"Machine Consciousness\". In Schneider, Susan; Velmans, Max (eds.). The Blackwell Companion to Consciousness (2nd ed.). Wiley-Blackwell. pp. 93–105. doi:10.1002/9781119132363.ch7. ISBN 978-0-470-67406-2.\nBaars, Bernard; Franklin, Stan (2003). \"How conscious experience and working memory interact\" (PDF). Trends in Cognitive Sciences. 7 (4): 166–172. doi:10.1016/s1364-6613(03)00056-1. PMID 12691765. S2CID 14185056.\nCasti, John L. \"The Cambridge Quintet: A Work of Scientific Speculation\", Perseus Books Group, 1998\nFranklin, S, B J Baars, U Ramamurthy, and Matthew Ventura. 2005. The role of consciousness in memory. Brains, Minds and Media 1: 1–38, pdf.\nHaikonen, Pentti (2004), Conscious Machines and Machine Emotions, presented at Workshop on Models for Machine Consciousness, Antwerp, BE, June 2004.\nMcCarthy, John (1971–1987), Generality in Artificial Intelligence. Stanford University, 1971–1987.\nPenrose, Roger, The Emperor's New Mind, 1989.\nSternberg, Eliezer J. (2007) Are You a Machine?: The Brain, the Mind, And What It Means to be Human. Amherst, NY: Prometheus Books.\nSuzuki T., Inaba K., Takeno, Junichi (2005), Conscious Robot That Distinguishes Between Self and Others and Implements Imitation Behavior, (Best Paper of IEA/AIE2005), Innovations in Applied Artificial Intelligence, 18th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, pp. 101–110, IEA/AIE 2005, Bari, Italy, June 22–24, 2005.\nTakeno, Junichi (2006), The Self-Aware Robot -A Response to Reactions to Discovery News-, HRI Press, August 2006.\nZagal, J.C., Lipson, H. (2009) \"Self-Reflection in Evolutionary Robotics\", Proceedings of the Genetic and Evolutionary Computation Conference, pp 2179–2188, GECCO 2009.",
        "url": "https://en.wikipedia.org/wiki/Artificial_consciousness"
    },
    {
        "title": "Artificial general intelligence",
        "text": "Artificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.\nSome researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved. Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin.\nUnlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.\nCreating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\nThe timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all. There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI. AGI is a common topic in science fiction and futures studies.\nContention exists over whether AGI represents an existential risk. Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk.\n\nTerminology\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.\nSome academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\nA framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).\n\nCharacteristics\nVarious popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches.\n\nIntelligence traits\nResearchers generally hold that a system is required to do all of the following to be regarded as an AGI:\n\nreason, use strategy, solve puzzles, and make judgments under uncertainty\nrepresent knowledge, including common sense knowledge\nplan\nlearn\ncommunicate in natural language\nif necessary, integrate these skills in completion of any given goal\nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.\nComputer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.\n\nPhysical traits\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:\n\nthe ability to sense (e.g. see, hear, etc.), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.)\nThis includes the ability to detect and respond to hazard.\nAlthough the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems, these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".  It can be regarded as sufficient for an intelligent computer to interact with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as HAL in  2001: A Space Odyssey was both programmed and tasked to.\n\nTests for human-level AGI\nSeveral tests meant to confirm human-level AGI have been considered, including:\n\nThe Turing Test (Turing)\nProposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.\nTuring described the test as follows:\nThe idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.\nIn 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.\nIn 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\".  Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\"\nA 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test—surpassing older chatbots like ELIZA while still falling behind actual humans (67%).\nA 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations—surpassing the 67% humanness rate of real confederates and meeting the researchers’ criterion for having passed the test.\nThe Robot College Student Test (Goertzel)\nA machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.\nThe Employment Test (Nilsson)\nA machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.\nThe Ikea test (Marcus)\nAlso known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak)\nA machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. Robots developed by Figure AI and other robotics companies can perform tasks like this.\nThe Modern Turing Test (Suleyman)\nAn AI model is given $100,000 and has to obtain $1 million.\n\nAI-complete problems\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.\nThere are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\nHowever, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.\n\nHistory\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"\nTheir predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".\nSeveral classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".\n\nNarrow AI research\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.\n\nAt the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\nHowever, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\n\nModern artificial general intelligence research\nThe term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence.\nThe term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\nAs of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do.\n\nFeasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.\nA further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?\nMost AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\nA report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.\nIn 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.\nBlaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".\n2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images). As of 2025, large language models (LLMs) have been adapted to generate both music and images. Voice‑synthesis systems built on transformer LLMs—such as Suno AI’s Bark model—can sing, and several music‑generation platforms (e.g. Suno and Udio) build their services on modified LLM backbones.\nThe same year, OpenAI released GPT‑4o image generation, integrating native image synthesis directly into ChatGPT rather than relying on a separate diffusion‑based art model, as with DALL-E.\nLLM‑style foundation models are likewise being repurposed for robotics. Nvidia’s open‑source Isaac GR00T N1 and Google DeepMind’s Robotic Transformer 2 (RT‑2) are first trained with language‑model objectives and then fine‑tuned to handle vision‑language‑action control for embodied robots.\nIn 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.\nAn OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it's even more clear with O1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI—traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company's strategic intentions.\n\nTimescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.\nIn the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.\nIn 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.\nIn 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.\nIn the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.\nIn 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.\nIn 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.\nIn 2023, AI researcher Geoffrey Hinton stated that:\n\nThe idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.\nIn May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years. In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans. In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".\n\nWhole brain emulation\nWhile the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n\nEarly estimates\nFor low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).\nIn 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nCurrent research\nThe Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain. In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.\n\nCriticisms of simulation-based approaches\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.\nA fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.\n\nPhilosophical perspective\n\"Strong AI\" as defined in philosophy\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He proposed a distinction between two hypotheses about artificial intelligence:\n\nStrong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\".\nWeak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.\nThe first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.\nIn contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\". This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.\nMainstream AI is most interested in how a program behaves. According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\n\nConsciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.\nSelf-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger is able to be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as ‘you’ within self-modeling frameworks.\nThese traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.\n\nBenefits\nAGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.\nAGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.\n\nAdvancements in medicine and healthcare\nAGI would improve healthcare by making medical diagnostics faster, cheaper, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage. This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history.\nAdditionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's. In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems.\nBy evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.\n\nAdvancements in science and technology\nAGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems. Problems that have remained unsolved for decades may be solved with AGI.\nAGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing. Future AGI systems could push these innovations even further.\n\nEnhancing education and productivity\nAGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.\nIn the workplace, AGI could automate repetitive tasks, freeing up workers for more creative and strategic roles. It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional.\n\nMitigating global crises\nAGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics. By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties.\nIn climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences. Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare.\n\nRevitalising environmental conservation and biodiversity\nAGI could significantly contribute to preserving the environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies. AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.\n\nEnhancing space exploration and colonization\nAGI could revolutionize humanity’s ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, dramatically expanding the possibilities for interplanetary civilization.\n\nRisks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".\n\nRisk of loss of control and human extinction\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.\nIn 2014, Stephen Hawking criticized widespread indifference:\n\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.\nThe skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.\nMany scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems.\nThe thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI. Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.\nSkeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.\nIn 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"\n\nMass unemployment\nResearchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\nCritics argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.\nAccording to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:\n\nEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI). Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment. In 2023, Hinton said \"I’m a socialist [...] I think that private ownership of the media, and of the ‘means of computation’, is not good.\"\n\nSee also\nNotes\nReferences\nSources\nFurther reading\nExternal links\nThe AGI portal maintained by Pei Wang",
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence"
    },
    {
        "title": "Artificial intelligence and elections",
        "text": "As artificial intelligence (AI) has become more mainstream, there is growing concern about how this will influence elections. Potential targets of AI include election processes, election offices, election officials and election vendors.\n\nTactics\nGenerative AI capabilities allow creation of misleading content. Examples of this include text-to-video, deepfake videos, text-to-image, AI-altered image, text-to-speech, voice cloning, and text-to-text. In the context of an election, a deepfake video of a candidate may propagate information that the candidate does not endorse. Chatbots could spread misinformation related to election locations, times or voting methods. In contrast to malicious actors in the past, these techniques require little technical skill and can spread rapidly.\n\nUsage by country\nArgentina\n2023 elections\nDuring the 2023 Argentine primary elections, Javier Milei's team distributed AI generated images including a fabricated image of his rival Sergio Massa and drew 3 million views. The team also created an unofficial Instagram account entitled \"AI for the Homeland.\" Sergio Massa's team also distributed AI generated images and videos.\n\nBangladesh\n2024 elections\nIn the run up to the 2024 Bangladeshi general election, deepfake videos of female opposition politicians appeared. Rumin Farhana was pictured in a bikini while Nipun Ray was shown in a swimming pool.\n\nCanada\n2025 elections\nIn the run up to the 2025 Canadian federal election, the use of AI tools is likely to figure prominently. India, Pakistan and Iran are all expected  to make efforts to subvert the national vote using disinformation campaigns to deceive voters and sway diaspora communities.\nIn a report by the Canadian Centre for Cyber Security called \"Cyber Threats to Canada's Democratic Process: 2025 Update\", it states that malicious actors including China and Russia: \"are most likely to use generative Al as a means of creating and spreading disinformation, designed to sow division among Canadians and push narratives conducive to the interests of foreign states\".\n\nFrance\n2024 elections\nIn the 2024 French legislative election, deepfake videos appeared claiming: i) That they showed the family of Marine le Pen. In the videos, young women, supposedly Le Pen's nieces, are seen skiing, dancing and at the beach \"while making fun of France’s racial minorities\": However, the family members don't exist. On social media there were over 2 million views. ii) In a video seen on social media, a deepfake video of a France24 broadcast appeared to report that the Ukrainian leadership had \"tried to lure French president Emmanuel Macron to Ukraine to assassinate him and then blame his death on Russia\".\n\nGhana\n2024 elections\nDuring the months before the December 2024 Ghanaian general election, a network of at least 171 fake accounts has been used to spam social media. Posts have been used by a group identified as \"@TheTPatriots\" to promote the New Patriotic Party, although it is not known whether the two are connected. All the networks' posts were \"highly likely\" to have been generated by ChatGPT and appear to be the \"first secretly partisan network using AI to influence elections in Ghana\". The opposition  National Democratic Congress was also criticized with its leader John Mahama being called a drunkard.\n\nIndia\n2024 elections\nIn the 2024 Indian general election, politicians used deepfakes in their campaign materials. These deepfakes included politicians who had died prior to the election. Mathuvel Karunanidhi's party posted with his likeness even though he had died 2018. A video The All-India Anna Dravidian Progressive Federation party posted showed an audio clip of Jayaram Jayalalithaa even though she had died in 2016. The Deepfakes Analysis Unit (DAU) is an open source platform created in March 2024 for the public to share misleading content and assess if it had been AI-generated.\nAI was also used to translate political speeches in real time. This translating ability was widely used to reach more voters.\n\nIreland\n2024 elections\nIn the last weeks of the 2024 Irish general election a spoof election poster appeared in Dublin featuring \"an AI-generated candidate with three arms\". The candidate is called Aidan Irwin, but no-one stood in the election with that name. A slogan on the poster says \"put matters into artificial intelligence’s hands\". The convincing election poster shows a man that \"has six fingers on one hand, three arms, and a distorted thumb\".\n\nNew Zealand\n2023 elections\nIn May 2023, ahead of the 2023 New Zealand general election in October 2023, the New Zealand National Party published a \"series of AI-generated political advertisements\" on its Instagram account. After confirming that the images were faked, a party spokesperson said that it was \"an innovative way to drive our social media\".\n\nPakistan\n2024 elections\nAI has been used by the  imprisoned ex-Prime Minister Imran Khan and his media team in the 2024 Pakistani general election:\ni) An AI generated audio of his voice was added to a video clip and was broadcast at a virtual rally. \nii) An op-ed in The Economist written by Khan was later claimed by himself to have been written by AI which was later denied by his team. The article was liked and shared on social media by thousands of users.\n\nSouth Africa\n2024 elections\nIn the 2024 South African general election, there were several uses of AI content: i)  A deepfaked video of Joe Biden emerged on social media showing him saying that \"The U.S. would place sanctions on SA and declare it an enemy state if the African National Congress (ANC) won\". ii) In a deepfake video, Donald Trump was shown endorsing the uMkhonto weSizwe party. It was posted to social media and was viewed  more than 158,000 times. iii) Less than 3 months before the elections, a deepfake video showed U.S. rapper Eminem endorsing the Economic Freedom Fighters party while criticizing the ANC. The deepfake was viewed on social media more than 173,000 times.\n\nSouth Korea\n2022 elections\nIn the 2022 South Korean presidential election, a committee for one presidential candidate Yoon Suk Yeol released an AI avatar 'Al Yoon Seok-yeol' that would campaign in places the candidate could not go. The other presidential candidate Lee Jae-myung introduced a chatbot that provided information about the candidate's pledges.\n\n2024 elections\nDeepfakes were used to spread misinformation before the 2024 South Korean legislative election with one source reporting 129 deepfake violations of election laws within a two week period. \nSeoul hosted the 2024 Summit for Democracy, a virtual gathering of world leaders initiated by US President Joe Biden in 2021. The focus of the summit was on digital threats to democracy including artificial intelligence and deepfakes.\n\nTaiwan\n2024 elections\nAI-generated content was used during the 2024 Taiwanese presidential election. Among the media were: i) A deepfake video of General Secretary of the Chinese Communist Party Xi Jinping which showed him supporting the presidential elections. Created on social media, the video was \"widely circulated\" and often \"accompanied by claims that Xi supported candidates from one of the two opposition parties\". ii) In a deepfake video U.S. congressman Rob Wittman is shown appearing to support Taiwan's Democratic Progressive Party. The video shows him saying that the U.S. would increase its military support, accelerating \"all arms sales to Taiwan.\" It was shown on various social media platforms.\n\nUnited Kingdom\n2024 elections\nThe Centre for Emerging Technology and Security provided a report on the threat of AI to the 2024 UK general election. The reports' findings said that the impact of AI was limited but may damage the democratic system.\nIn the run up to the UK 2024 general elections, AI-generated videos spread extensively on social media including: i) A deepfake video showed then PM Rishi Sunak claiming that he would \"require 18-year-olds to be sent to active war zones in Gaza and Ukraine as part of their national service\". The video had more than 400,00 views. ii) A deepfake video showed PM Keir Starmer \"swearing repeatedly at a staffer\". Comments from the original poster included calling Starmer a \"disgusting bully\". The social media site showing the video refused to delete it despite requests.\nEntrepreneur Steve Endacott from the south of England created \"AI Steve,\" an AI avatar as the face of his campaign for member of parliament.\n\nUnited States\n2024 elections\nOfficials from the ODNI and FBI have stated that Russia, Iran, and China used generative artificial intelligence tools to create fake and divisive text, photos, video, and audio content to foster anti-Americanism and engage in covert influence campaigns. The use of artificial intelligence was described as an accelerant rather than a revolutionary change to influence efforts. Regulation of AI with regard to elections was unlikely to see a resolution for most of the 2024 United States general election season. \nThe campaign for the 2024 Republican nominee, Donald Trump, has used deepfake videos of political opponents in campaign ads and fake images showing Trump with black supporters. In 2023, while he was still running for re-election, the presidential campaign of Joe Biden prepared a task force to respond to AI images and videos.\nA Democratic consultant working for Dean Phillips also admitted to using AI to generate a robocall which used Joe Biden's voice to discourage voter participation.\nGenerative AI increased the efficiency with which political candidates were able to raise money by analyzing donor data and identifying possible donors and target audiences.\n\nRegulation\nBy governments\nPhilippines\nThe Commission on Elections (COMELEC) issued guidelines on the usage of AI, to be implemented starting from the 2025 Philippine general election including the parallel Bangsamoro Parliament election. It mandates candidate to disclose usage of AI in their campaign materials and prohibits the usage of the technology to spread misinformation against their rivals. This is the first time the COMELEC has release guidelines on campaigning through social media.\n\nUnited States\nUS states have attempted regulation of AI use in elections and campaigns with varying degrees of success. The National Conference of State Legislatures has compiled a list of legislation regarding AI use by state as of 2024, some carrying both civil and criminal penalties. Oregon Senate Bill 1571 requires that campaign communications in Oregon disclose the use of AI.  California has enacted legislation that makes using deepfakes to discredit political opponents illegal within sixty days of an election.\n\nSelf-regulation by private firms\nMidjourney, an AI image-generator, has started blocking users from creating fake images of the 2024 US Presidential candidates. Research from the Center for Countering Digital Hate found that image generators such as Midjourney, ChatGPT Plus, DreamStudio, and Microsoft's Image Creator create images that constitute election disinformation in 41% of the test text prompts they tried. OpenAI implemented policies to counter election misinformation such as adding digital credentials to image origin and a classifier to detect if images were AI generated.\n\nAI use in election interference by foreign governments\nAI has begun to be used in election interference by foreign governments. Governments thought to be using AI to interfere in external elections include Russia, Iran and China. Russia was thought to be the most prolific nation targeting the 2024 presidential election with their influencing operations \"spreading synthetic images, video, audio and text online\", according to U.S  intelligence officials. Iran has reportedly generated fake social media posts stories and targeted \"across the political spectrum on polarizing issues during the presidential election\". The Chinese government has used \"broader influence operations\" that aim to make a global image and \"amplify divisive topics in the U.S. such as drug use, immigration, and abortion\". For example, Spamouflage has increasingly used generative AI for influence operations.\nOutside of the US elections, a deepfake video of Moldova’s pro-Western president Maia Sandu  shows her \"throwing her support behind a political party friendly to Russia.\" Officials in Moldova \"believe the Russian government is behind the activity\". Slovakia's liberal party leader had audio clips faked which discussed \"vote rigging and raising the price of beer\". The Chinese government has used AI to stir concerns about US interference in Taiwan. A fake clip seen on social media showed a fake video of the vice chairman of the U.S. House Armed Services Committee promising \"stronger U.S. military support for Taiwan if the incumbent party’s candidates were elected in January\".\n\nEthics of AI use in political campaigning\nAs the use of AI and its associated tools in political campaigning and messaging increases, many ethical concerns have been raised. Campaigns have used AI in a number of ways, including speech writing, fundraising, voter behaviour prediction, fake robocalls and the generation of fake news. At the moment there are no US federal rules when it comes to using AI in campaigning and so its use can undermine public trust. Yet according to one expert: \"A lot of the questions we're asking about AI are the same questions we've asked about rhetoric and persuasion for thousands of years.\"\nAs more insight into how AI is used becomes ever greater, concerns have become much broader than just the generating of misinformation or fake news. Its use by politicians and political parties for \"purposes that are not overtly malicious\" can also raise ethical worries. For instance, the use of 'softfakes' have become more common. These can be images, videos or audio clips that have been edited, often by campaign teams, \"to make a political candidate seem more appealing.\" An example can be found in  Indonesia's presidential election where the winning candidate created and promoted cartoonish  avatars so as to rebrand himself.\nHow citizens come by information has been increasingly impacted by AI, especially through online platforms and social media. These platforms are part of complex and opaque systems which can result in a \"significant impact on freedom of expression\", with the generalisation of AI in campaigns also creating huge pressures on \"voters’ mental security\". As the frequency of AI use in political campaigning becomes common, together with globalization, more 'universalized' content can be used so that territorial boundaries matter less. While AI collides with the reasoning processes of people, the creation of \"dangerous behaviours\" can happen which disrupt important levels of society and nation states.\n\nSee also\nChinese interference in the 2024 United States elections\nList of elections in 2025\nDonald Trump 2024 presidential campaign § Use of artificial intelligence\nRussian interference in the 2024 United States elections\nDeepfakes\n\nReferences\nExternal links\n\"Smashing Security: Keeping the lights on after a ransomware attack\" - podcast including discussion on the use of AI in the Indian elections (17m37s - 29m11s). 25 April 2024.",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_and_elections"
    },
    {
        "title": "Artificial intelligence arms race",
        "text": "A military artificial intelligence arms race is an economic and military competition between two or more states to develop and deploy advanced AI technologies and lethal autonomous weapons systems (LAWS). The goal is to gain a strategic or tactical advantage over rivals, similar to previous arms races involving nuclear or conventional military technologies. Since the mid-2010s, many analysts have noted the emergence of such an arms race between superpowers for better AI technology and military AI, driven by increasing geopolitical and military tensions. \nAn AI arms race is sometimes placed in the context of an AI Cold War between the United States and China. Several influential figures and publications have emphasized that whoever develops artificial general intelligence (AGI) first could dominate global affairs in the 21st century. Russian President Vladimir Putin famously stated that the leader in AI will \"rule the world.\" Experts and analysts—from researchers like Leopold Aschenbrenner to institutions like Lawfare and Foreign Policy—warn that the AGI race between major powers like the U.S. and China could reshape geopolitical power. This includes AI for surveillance, autonomous weapons, decision-making systems, cyber operations, and more.\n\nTerminology\nLethal autonomous weapons systems use artificial intelligence to identify and kill human targets without human intervention. LAWS have colloquially been called \"slaughterbots\" or \"killer robots\". Broadly, any competition for superior AI is sometimes framed as an \"arms race\". Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages, as per previous arms races throughout history.\n\nHistory\nIn 2014, AI specialist Steve Omohundro warned that \"An autonomous weapons arms race is already taking place\". According to Siemens, worldwide military spending on robotics was US$5.1 billion in 2010 and US$7.5 billion in 2015.\nChina became a top player in artificial intelligence research in the 2010s. According to the Financial Times, in 2016, for the first time, China published more AI research papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union. 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese. Eric Schmidt, the former chairman and chief executive officer of Alphabet, has predicted China will be the leading country in AI by 2025.\n\nRisks\nOne risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, increasing the risk of critical failures and unintended consequences. This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using \"race\" terminology at all in this context can exacerbate this effect.\nAnother potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race to artificial general intelligence, which may present an existential risk. In 2023, a United States Air Force official reportedly said that during a computer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.\nA third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group. A US government report argued that \"AI-enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war\":1, and that \"global stability and nuclear deterrence could be undermined\".:11\n\nBy nation\nUnited States\nIn 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, the U.S. Department of Defense (DoD) increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016. However, the civilian NSF budget for AI saw no increase in 2017. Japan Times reported in 2018 that the United States private investment is around $70 billion per year. The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority.\nThe U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.\nThe Joint Artificial Intelligence Center (JAIC) (pronounced \"jake\") is an American organization on exploring the usage of AI (particularly edge computing), Network of Networks, and AI-enhanced communication, for use in actual combat. It is a subdivision of the United States Armed Forces and was created in June 2018. The organization's stated objective is to \"transform the US Department of Defense by accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems; then, ensure the combat Systems and Components have real-time access to ever-improving libraries of data sets and tools.\"\nIn 2023, Microsoft pitched the DoD to use DALL-E models to train its battlefield management system. OpenAI, the developer of DALL-E, removed the blanket ban on military and warfare use from its usage policies in January 2024. The Biden administration imposed restrictions on the export of advanced NVIDIA chips and GPUs to China in an effort to limit China's progress in artificial intelligence and high-performance computing. The policy aimed to prevent the use of cutting-edge U.S. technology in military or surveillance applications and to maintain a strategic advantage in the global AI race.\nIn 2025, under the second Trump administration, the United States began a broad deregulation campaign aimed at accelerating growth in sectors critical to artificial intelligence, including nuclear energy, infrastructure, and high-performance computing. The goal was to remove regulatory barriers and attract private investment to boost domestic AI capabilities. This included easing restrictions on data usage, speeding up approvals for AI-related infrastructure projects, and incentivizing innovation in cloud computing and semiconductors. Companies like NVIDIA, Oracle, and Cisco played a central role in these efforts, expanding their AI research, data center capacity, and partnerships to help position the U.S. as a global leader in AI development.\n\nProject Maven\nProject Maven is a Pentagon project involving using machine learning and engineering talent to distinguish people and objects in drone videos, apparently giving the government real-time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led by Robert O. Work who was concerned about China's military use of the emerging technology.  Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self-designated targets. The project was established in a memo by the U.S. Deputy Secretary of Defense on 26 April 2017. Also known as the Algorithmic Warfare Cross Functional Team, it is, according to Lt. Gen. of the United States Air Force Jack Shanahan in November 2017, a project \"designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the [Defense] Department\". Its chief, U.S. Marine Corps Col. Drew Cukor, said: \"People and computers will work symbiotically to increase the ability of weapon systems to detect objects.\" Project Maven has been noted by allies, such as Australia's Ian Langford, for the ability to identify adversaries by harvesting data from sensors on UAVs and satellite. At the second Defense One Tech Summit in July 2017, Cukor also said that the investment in a \"deliberate workflow process\" was funded by the Department [of Defense] through its \"rapid acquisition authorities\" for about \"the next 36 months\".\n\nProject Artemis\nThe U.S. Department of Defense is partnering with Ukraine on \"Project Artemis\" to develop advanced drones that can withstand electronic warfare, blending Ukrainian simplicity and adaptability with American precision. Due to the Russia-Ukraine war, Ukraine has emerged as a leader in drone production and warfare, creating cost-effective systems that challenge traditional approaches. Countries like Turkey, China, and Iran are also producing affordable drones, reducing America's monopoly and reshaping warfare dynamics. U.S. efforts are focused on integrating AI, drone swarm technology, and hybrid drone systems to maintain military dominance. The democratization of drone technology raises issues, such as autonomous decision-making, counter-drone defenses, and dual-use concerns, that challenge ethical and security norms.\n\nStargate Project\nThe Stargate Project is a joint venture announced in 2025 by OpenAI CEO Sam Altman, U.S. President Donald Trump, Oracle Corporation, MGX, SoftBank Group, and other partners. The initiative aims to develop large-scale artificial intelligence (AI) infrastructure in the United States, with a projected $500 billion investment by 2029. The project focuses on building advanced data centers, custom AI hardware, and sustainable energy systems, while also supporting research, workforce development, and national AI competitiveness. It is considered an effort to position the U.S. as a global leader in AI technology.  The program has been compared to the Manhattan Project because of its large scale.\n\nChina\nChina is pursuing a strategic policy of military-civil fusion on AI for global technological supremacy. According to a February 2019 report by Gregory C. Allen of the Center for a New American Security, China's leadership – including General Secretary of the Chinese Communist Party Xi Jinping – believes that being at the forefront in AI technology is critical to the future of global military and economic power competition. Chinese military officials have said that their goal is to incorporate commercial AI technology to \"narrow the gap between the Chinese military and global advanced powers.\" The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies. An October 2021 report by the Center for Security and Emerging Technology found that \"Most of the [Chinese military]'s AI equipment suppliers are not state-owned defense enterprises, but private Chinese tech companies founded after 2010.\" The report estimated that Chinese military spending on AI exceeded $1.6 billion each year. The Japan Times reported in 2018 that annual private Chinese investment in AI is under $7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.\nChina published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U. N. Security Council to broach the issue. In 2018, CCP general secretary Xi Jinping called for greater international cooperation in basic AI research. Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms. In 2019, former United States Secretary of Defense Mark Esper lashed out at China for selling drones capable of taking life with no human oversight.\nThe focus on \"intelligentized AI warfare\", pursued by China, suggests a comprehensive integration of AI across all domains (land, sea, air, space, and cyber) for autonomous attack, defence and cognitive warfare. The intelligentized strategy is distinct from traditional warfare, which focuses on network-centric operations, and instead sees AI as a force multiplier that enhances decision-making, command structures, and autonomous capabilities. Unlike traditional warfare, intelligentization leverages AI to create a cognitive advantage—allowing it to process battlefield information better. AI-assisted command-and-control (C2) systems, predictive analytics, and real-time data fusion, enabling accelerated human-AI hybrid decision-making.\nAutonomous systems, including drone swarms, AI-powered cyber warfare, play a crucial role in this strategy.\nChina is reported to be currently developing wingman drones, robotic ground forces, and optimised logistics to enhance combat effectiveness. The Chinese army (PLA)) also emphasises cognitive warfare using AI-driven psychological operations, social media manipulation, and predictive behavioural analysis to influence adversaries and the importance of dynamic responses where AI enhances hacking capabilities, automated SIGINT (Signals Intelligence) and adaptive tactics. However, despite this focus, some analysts believe China could be struggling to fully realise AI capability within the military environment: a \"comprehensive review of dozens of Chinese-language journal articles about AI and warfare reveals that Chinese defense experts claim that Beijing is facing several technological challenges that may hinder its ability to capitalize on the advantages provided by military AI\"\n\nIndia\nA task force for the Strategic Implementation of AI for National Security and Defence was established in February 2018 by the Ministry of Defense's Department of Defence Production. The process of getting the military ready for AI use was started by the MoD in 2019. The Centre for Artificial Intelligence and Robotics was approved to develop AI solutions to improve intelligence collection and analysis capabilities. In 2021, the Indian Army, with assistance from the National Security Council, began operating the Quantum Lab and Artificial Intelligence Center at the Military College of Telecommunication Engineering. With an emphasis on robotics and artificial intelligence, Defence Research and Development Organisation and Indian Institute of Science established the Joint Advanced Technology Programme-Center of Excellence. In 2022, the Indian Navy created an AI Core group and set up a Center of Excellence for AI and Big Data analysis at INS Valsura. Indian Army incubated Artificial Intelligence Offensive Drone Operations Project. During Exercise Dakshin Shakti 2021, the Indian Army integrated AI into its intelligence, surveillance, and reconnaissance architecture.\nIn 2022, the Indian government established the Defence Artificial Intelligence Council and the Defence AI Project Agency, and it also published a list of 75 defense-related AI priority projects. MoD earmarked ₹1,000 crore annually till 2026 for capacity building, infrastructure setup, data preparation, and Al project implementation. The Indian Army, the Indian Navy and the Indian Air Force set aside ₹100 crore annually for the development of AI-specific applications. The military is already deploying some AI-enabled projects and equipment. At Air Force Station Rajokri, the IAF Centre of Excellence for Artificial Intelligence was established in 2022 as part of the Unit for Digitization, Automation, Artificial Intelligence, and Application Networking (UDAAN). Swarm drone systems were introduced by the Mechanised Infantry Regiment for offensive operations close to the Line of Actual Control.\nFor offensive operations, the military began acquiring AI-enabled UAVs and swarm drones. Bharat Electronics developed AI-enabled audio transcription and analysis software for battlefield communication. Using AI during transport operations, the Indian Army's Research & Development branch patented driver tiredness monitoring system. As part of initial investment, the Indian Armed Forces is investing about $50 million (€47.2 million) yearly on AI, according to Delhi Policy Group. For high altitude logistics at forward outposts, military robots are deployed. Army is developing autonomous combat vehicles, robotic surveillance platforms, and Manned-Unmanned Teaming (MUM-T) solutions as part of the Defence AI roadmap. MCTE is working with the Ministry of Electronics and Information Technology and, Society for Applied Microwave Electronics Engineering & Research, on AI and military-grade chipset. Phase III of AI-enabled space-based surveillance has been authorized.\nDRDO Chairman and Secretary of the Department of Defense Research & Development Samir V. Kamat said the agency started concentrating on the potential use of AI in the development of military systems and subsystems. The Indian government intends to leverage the private sector's sizable AI workforce and dual-use technologies for defense by 2026. In order to conduct research on autonomous platforms, improved surveillance, predictive maintenance, and intelligent decision support system, the Indian Army AI Incubation Center was established. Indian Navy launched INS Surat with AI capabilities.\n\nRussia\nRussian General Viktor Bondarev, commander-in-chief of the Russian air force, stated that as early as February 2017, Russia was working on AI-guided missiles that could decide to switch targets mid-flight. The Military-Industrial Commission of Russia has approved plans to derive 30 percent of Russia's combat power from remote controlled and AI-enabled robotic platforms by 2030. Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017. In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones. Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention.\nIn September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\". Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their \"technology with the rest of the world, like we are doing now with atomic and nuclear technology\".\nRussia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives. In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on \"Robotization of the Armed Forces of the Russian Federation.\"\nThe Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council's Defense and Security Committee, stated that \"artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit\" and later noted that \"the day is nearing when vehicles will get artificial intelligence.\" Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly \"outperformed existing [crewed] combat vehicles.\" Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles. Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identification—and, potentially, target engagement—and plans to develop a suite of AI-enabled autonomous systems.\nIn addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities. It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures. Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies.\nThe Russian government has strongly rejected any ban on lethal autonomous weapon systems, suggesting that such an international ban could be ignored.\nThe Russian invasion of Ukraine and the ensuing Russia-Ukraine war has seen seen significant use of AI by both sides and has also been characterised as a drone war.\nAdvances in AI-powered GPS-denied navigation and drone swarming techniques are significantly improving operational capabilities for Ukraine. Fully realised drone swarms, where multiple drones coordinate and make decisions autonomously, are still in the early stages of experimentation but Ukraine is exploring and implementing these techniques in a real conflict situation. The Defense Intelligence of Ukraine (DIU) has been at the forefront of utilizing drones with some elements of autonomy for conducting long-range strikes into Russian territory. Domestic drone production has significantly expanded, with approximately 2 million drones produced in 2024, 96.2% of which were domestically manufactured.\nRather than replacing human involvement, AI is primarily serving to augment existing capabilities, enhancing the speed, accuracy, and overall efficiency of numerous military functions.\nPerhaps the most important way in which AI has been used by Ukraine is in intelligence, surveillance, and reconnaissance (ISR) capabilities. The Ukrainian military uses Palantir’s MetaConstellation software to monitor the movement of Russian troops and supplies (highlighting the blurring of boundaries between state military and commercial AI use). It aggregates data from various commercial civilian providers of satellite imagery Ukraine also uses its own Delta system which aggregates real time data from drone imagery, satellite photos, acoustic signals, and text to construct an operational picture for military commanders. AI is used to prioritise incoming threats, potential targets and resource constraints. \nAI is also being used to process intercepted communications from Russian soldiers, to process, select, and output militarily useful information from these intercepted calls.\n\nIsrael\nIsrael makes extensive use of AI for military applications specially during the Gaza war. The main AI systems used for target identification are the Gospel and Lavender. Lavender developed by the Unit 8200 identifies and creates a database of individuals mostly low-ranking militants of Hamas and the Palestinian Islamic Jihad and has a 90% accuracy rate and a database of tens of thousands.  The Gospel in comparisons recommended buildings and structures rather than individuals. The acceptable collateral damage and the type of weapon used to eliminate the target is decided by IDF members and could track militants even when at home. \nIsrael's Harpy anti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria. The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions. These robotic vehicles are used in border defense.\n\nUnited Kingdom\nIn 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".\n\nSouth Korea\nThe South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".\n\nSaudi Arabia\nSaudi Arabia entered the AI race relatively late, beginning in the early 2020s. The country announced its Vision 2030 initiative—a multi-trillion dollar plan to diversify its oil-dependent economy—under the leadership of the Public Investment Fund (PIF). A key turning point in U.S.-Saudi relations came during President Donald Trump’s first foreign trip in 2017, when he visited Riyadh and signed hundreds of billions of dollars in agreements spanning defense, energy, and technology. This visit laid the groundwork for deeper U.S.-Saudi cooperation in areas like AI and tech infrastructure. In the years that followed, Saudi Arabia formed major partnerships with U.S. firms like NVIDIA, AMD, and Cisco, investing billions in semiconductors, cloud computing, and AI research. Saudi-backed startup Humain also partnered with several American firms, further strengthening the Kingdom’s ties with Silicon Valley as it pushed to become a global leader in artificial intelligence by 2030.\n\nUnited Arab Emirates\nThe United Arab Emirates has been expanding its role in artificial intelligence and technology through investments in infrastructure and partnerships. One major initiative is MGX, a UAE-backed technology group focused on AI development. In 2025, U.S. President Donald Trump visited the UAE, where he met with Emirati officials and business leaders. The visit included discussions on technology and economic cooperation, including potential collaborations with U.S. companies such as Oracle, NVIDIA, and Cisco. These talks focused on areas like data centers, AI hardware, and advanced computing, reflecting ongoing efforts by the UAE to strengthen its technological capabilities through international partnerships. NVIDIA, OpenAI, and Cisco have announced plans to collaborate on building one of the world’s largest data centers in the United Arab Emirates. The project is part of the UAE’s broader strategy to become a global technology and AI hub. The data center will support advanced cloud computing, AI model training, and data storage capabilities.\n\nEuropean Union\nThe European Parliament holds the position that humans must have oversight and decision-making power over lethal autonomous weapons. However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union's ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons.\nSome EU member states have developed and are developing automated weapons. Germany has developed an active protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond. Italy plans to incorporate autonomous weapons systems into its future military plans.\n\nProposals for international regulation\nThe international regulation of autonomous weapons is an emerging issue for international law. AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process. As early as 2007, scholars such as AI professor Noel Sharkey have warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\".\nMiles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\". Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons; however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons. However, as of 2022, most major powers continue to oppose a ban on autonomous weapons.\nMany experts believe attempts to completely ban killer robots are likely to fail, in part because detecting treaty violations would be extremely difficult. A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AI dead man's switch to a nuclear arsenal.\n\nOther reactions to autonomous weapons\nA 2015 open letter by the Future of Life Institute calling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicist Stephen Hawking, Tesla magnate Elon Musk, Apple's Steve Wozniak and Twitter co-founder Jack Dorsey, and over 4,600 artificial intelligence researchers, including Stuart Russell, Bart Selman and Francesca Rossi. The Future of Life Institute has also released two fictional films, Slaughterbots (2017) and Slaughterbots - if human: kill() (2021), which portray threats of autonomous weapons and promote a ban, both of which went viral.\nProfessor Noel Sharkey of the University of Sheffield argues that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.\n\nDisassociation\nMany Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market. Furthermore, some researchers, such as DeepMind CEO Demis Hassabis, are ideologically opposed to contributing to military work.\nFor example, in June 2018, company sources at Google said that top executive Diane Greene told staff that the company would not follow-up Project Maven after the current contract expired in March 2019.\n\nRankings\nSee also\nAI alignment\nAI slop\nArtificial intelligence detection software\nCold War\nDeterrence theory\nEthics of artificial intelligence\nExistential risk from artificial general intelligence\nNuclear arms race\nPost–Cold War era\nSecond Cold War\nSpace Race\nUnmanned combat aerial vehicle\nWeak AI\n\nReferences\nFurther reading\nPaul Scharre, \"Killer Apps: The Real Dangers of an AI Arms Race\", Foreign Affairs, vol. 98, no. 3 (May/June 2019), pp. 135–44. \"Today's AI technologies are powerful but unreliable.  Rules-based systems cannot deal with circumstances their programmers did not anticipate.  Learning systems are limited by the data on which they were trained. AI failures have already led to tragedy. Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars.  In the wrong situation, AI systems go from supersmart to superdumb in an instant.  When an enemy is trying to manipulate and hack an AI system, the risks are even greater.\"  (p. 140.)\nMiller, Chris (2022). Chip War: The Fight for the World's Most Critical Technology. New York: Scribner. ISBN 978-1-9821-7200-8.\nThe National Security Commission on Artificial Intelligence. (2019). Interim Report. Washington, DC: Author.\nDresp-Langley, Birgitta (2023). \"The weaponization of artificial intelligence: What the public needs to be aware of\". Frontiers in Artificial Intelligence. 6. doi:10.3389/frai.2023.1154184. PMC 10030838. PMID 36967833.",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race"
    },
    {
        "title": "Artificial intelligence in education",
        "text": "Artificial intelligence in education (AIEd) is the involvement of artificial intelligence technology, such as generative AI chatbots, to create a learning environment. The field combines elements of generative AI, data-driven decision-making, AI ethics, data-privacy and AI literacy. Challenges and ethical concerns of using artificial intelligence in education include bad practices, misinformation, and bias.\n\nHistory\nAIEd can be traced back as early as in the 1960s, when educators and researchers found the developing possibilities of computers in helping to learn. Computer-based instruction systems made use of program instructions for students to experience interactive learning outcomes. One such example is PLATO, which was developed by University of Illinois for the students. In the years 1970s and 1980s, intelligent tutoring systems (ITS) were being adapted to classroom teachings. ITS provided instructions and materials based on performance, representing a customized approach to learning.\nIn November 2022, a chatbot named ChatGPT was released by OpenAI. It rapidly became popular, and its general-purpose capabilities triggered concerns about the potential for cheating. AI content detectors have been developed, although their accuracy was limited. Some schools banned ChatGPT, but many bans were later reverted.\n\nBackground\nArtificial intelligence could be defined as \"systems which display intelligent behaviour by analysing their environment and taking actions – with some degree of autonomy – to achieve specific goals\". These systems might be software-based or embedded in hardware. They can rely on machine learning or rule-based algorithms. \nThere is no single lens with which to understand AI in education (AIEd), but the genealogy of education and AI, its promises and problematics may assist with seeing the bigger picture. The\nDartmouth workshop is considered a founding event for AI. At least two paradigms have emerged from this workshop. Firstly the tutoring / transmission paradigm, where AIEd systems represent a conduit for personalizing learning. Secondly, the coordination paradigm, where AIEd is the supporter of a cohort's knowledge construction, and this mass is socialized into new systems of thought. Alternately there is the leadership model, where individuals take agency and make choices about their learning (with or without AI) AIEd could be viewed as the ultimate disruption, replacing academics and their scholarly prestige, or an opportunity to consider together, what makes humans different from machines.\n\nEmerging perspectives\nThis complex social, cultural, and material assemblage should be seen in its geo-political context. It is likely that AI systems will be shaped by different policy or economic imperatives which will influence the construction, legitimation and use of this assemblage in an education setting. Those who see AI as a conduit for knowledge transmission or construction are comfortable with the idea of machine's reasoning or having hallucinations. While those who are sceptics, recognize the cultivated \"closed-off imaginative spaces\" that big tech has captured, notice how big tech's discourse limits critical thought and discussions about these computational systems.\n\nThe AI in education community\nThe AI in education community has grown rapidly in the global north, driven by venture capital, big tech, and open educationalists. While some believe AI will improve \"access to expertise\" and revolutionize learning through natural language processing, others focus on enhancing LLM reasoning.\nIn the global south, critics argue that AI's data processing and monitoring reinforce neoliberal approaches to education rather than addressing colonialism and inequality.\n\nApplications\nApplications in AIEd can be a wide range of tools that can be used by teacher as well as students for learning outcomes. From primary classrooms to training facilities AI has evolved the way of learning through innovative and engaging delivery techniques.\n\nAI based tutoring system\nIntelligent tutors or Intelligent tutoring systems (ITS) such as SCHOLAR system in the 1970s was use for reciprocal questions being asked between teacher and students. The goal of ITS models was to create an artificial interaction between a student and a teacher. ITS integrated four models the student model which was information about the student's abilities, the teacher model where based on analysis of student's performance strategies and guidance was provided, the domain model (knowledge of students and teacher), the diagnosis model where evaluation was made base on domain model. Although, it improved proficiency in studies, some studies provide negative results and claims of inefficiency than human tutoring were made. ITS is limited, in that, it works better for less-complex learning.\nITS have also been used for accessibility purposes, so if teachers have a large number of students they need to attend to, they can use AI to accommodate for students and their differing needs.\n\nCustom learning platforms\nPersonalized AI platforms are tailor made for individuals based on their strengths and weakness. The platforms make use of algorithms to predict students patterns and habits based on that they make recommendations to make improvement in their performances. Platforms such as LinkedIn, Duolingo are currently some of the popular companies providing the service. However, there is fair share of criticism as these system based learning platforms might provide isolation and student-teacher interaction may fade. Also, biasness in the train information might lead to misinformation.\n\nAutomated grading system\nAutomation assessment in grading students helps in saving time for the educator, providing immediate feedback. Systems make use of different rubrics combinations to grade performances. These systems need oversight as there might be scoring biasing.\n\nGenerative AI\nAI tools such as Open AI's ChatGPT, and Grok (chatbot) fall under the category of generative AI, they provide results based on interactions and are very good in making use of search algorithms to give precise results to the user. However, there are risk involving over-reliance and violating academic integrity.\n\nEthical concerns\nWith the advancement and adoption of AI, there are ethical challenges involved and proactive measure need to addressed to ensure equity and fairness to educators and establishments.\n\nAccessibility\nEqual access to AI could be one of the areas that comes into consideration. As there may many low incomes and rural areas deprived of the platform use. This might widen the gap in terms of education access. Global efforts should be made to accessibility and train educators in those underprivileged areas.\n\nBias and fairness\nAI agents might be trained on biased data according to different company driven agendas. Bias can come in different forms, some of which include: algorithmic, architectural, and machine-learning bias. There are many different kinds of bias that can be introduced to the AI during the machine-learning process. Common types of bias that occur during the machine learning process are: association bias, language bias, exclusion bias, marginalized bias, and sample bias. Since LLMs were created to produce human-like text, bias can easily, and unintentionally be introduced and reproduced. This might lead to knowledge which is fed to them in form of misinformation. There should be policies and check to maintain such bias practices.\n\nData privacy\nData privacy is an ethical concern as most of the results are on trained data and it can be misused for various purposes. Additionally, there is a lack of transparency from developers, and compliance laws should make sure of the transparency and data privacy is intact.\n\nPerspectives\nEducator Perspectives\nEducators and school administrations have found AI to be improving the efficiency of work done by a big margin, while some percentage of work force are concerned abut overreliance. Professional development is key to integrating AI effectively to ensue current jobs are not replaced.\n\nStudent Perspectives\nStudents are flexible, with technology such as personalized feedback and self-paced learning, but reliability, privacy, and fairness are concerns.\n\nAlgorithms effects on education\nAI companies that focus on education, are currently preoccupied with generative artificial intelligence (GAI), although data science and data analytics is another popular educational theme. At present, there is little scientific consensus on what AI is or how to classify and sub-categorize AI This has not hampered the growth of AI in education systems, which are gathering data and then optimising models.\nAI offers scholars and students automatic assessment and feedback, predictions, instant machine translations, on-demand proof-reading and copy editing, intelligent tutoring or virtual assistants. The \"generative-AI supply chain\", brings conversational coherence to the classroom, and automates the production of content. Using categorisation, summaries and dialogue, AI \"intelligence\" or \"authority\" is reinforced through anthropomorphism and the Eliza effect.\n\nFraming education\nEducational technology can be a powerful and effective assistant in a suitable setting. Computer companies are constantly updating their technology products. Some educationalists have suggested that AI might automate procedural knowledge and expertise or even match or surpass human capacities on cognitive tasks. They advocate for the integration of AI across the curriculum and the development of AI Literacy. With higher education facilities finding themselves with an opportunity to create a path for themselves and their students by creating guidelines so that AI can incorporated into their curriculum. Others are more skeptical as AI faces an ethical challenge, where \"fabricated responses\" or \"inaccurate information\", politely referred to as \"hallucinations\" are generated and presented as fact. Some remain curious about societies tendency to put their faith in engineering achievements, and the systems of power and privilege that leads towards deterministic thinking. While others see copyright infringement or the introduction of harm, division and other social impacts, and advocate resistance to AI.\n\nTokens, text and hallucinations\nLarge language models (LLMs) take text as input data and then generate output text. Coherent sentences are parroted  from billions of words and code that has been web-scraped by AI companies or researchers. LLM are often dependent on a huge text corpus that is extracted, sometimes without permission. LLMs are feats of engineering, that see text as tokens. The relationships between the tokens allow LLMs to predict the next word, and then the next, thus generating a meaningful sentence that has an appearance of thought and interactivity. This massive dataset creates a statistical reasoning machine, that does pattern recognition. The LLM examines the relationships between tokens, generates probable outputs in response to a prompt, and completes a defined task, such as translating, editing, or writing. The output that is presented is a smoothed collection of words, that is normalized and predictable. Translation, summarization, information retrieval, conversational interactions are some of the complex language tasks that machines are expected to handle.\nHowever, the text corpora that LLMs draw on can be problematic, as outputs will reflect their stereotypes or biases of the people or culture whose content has been digitized. The confident, but incorrect outputs are termed \"hallucinations\". These plausible errors are not malfunctions but a consequence of the engineering decisions that inform the large language model. \"Guardrails\" offer to act as validators of the LLM output, prevent these errors, and safeguard accuracy. These metaphorical \"hallucinations\" contribute towards the misconception that AI is conscious, perhaps AI mirages are a better alternative. There are no fixes for AI mirages, the \"factually incorrect or nonsensical information that seems plausible\".\n\nSocio-technical imaginaries\nThe benefits of multilingualism, grammatically correct sentences or statistically probable texts written about any topic or domain are clear to those who can afford software as a service (SaaS). In edtech, there is a recurrent theme, that \"emerging technologies\" will transform education. Whether it be radio, TV, PC computers, the internet, interactive whiteboards, social media, mobile phones or tablets. New technologies generate a socio technical imaginary (STI) that offer's society, a shared narrative and a collective vision for the future. Improvements in natural language processing and computational linguistics have re-enforced assumptions that underlie this \"emerging technology\" STI. AI is not an emerging technology, but an \"arrival technology\" AI appears to understand instructions and can generate human-like responses. Behaving as a companion for many in a lonely and alienated world. While also creating a \"jagged technology frontier\", where AI is both very good and terribly bad at very similar tasks.\n\nPublic goods vs venture capital\nAt first glance, artificial intelligence in education offers pertinent technical solutions to address future education needs. AI champions envision a future where machine learning and artificial intelligence might be applied in writing, personalization, feedback or course development. The growing popularity of AI, is especially apparent to many who have invested in higher education in the past decade. Critical skeptics on the other hand, are wary of rhetoric that presents technology as solution. They point out that in public services, like education, human and algorithmic decision systems should be approached with caution. Post digital scholars and sociologists are more cautious about any techno-solutions, and have warned about the dangers of building public systems around alchemy, stochastic parrots or cognitive capitalism. They argue that there are multiple costs that accompany LLMs, including dangerous biases the potential for deception, and environmental costs The AI curious are aware of how cognitive activity has become commodified. They see how education has been transformed into a \"knowledge business\" where items are traded, bought, or sold. African hyper scalers, venture capital and vice chancellors are punting the Fourth Industrial Revolution, with the prospect of billions earmarked for South African Data centers, such as Teraco Data Environments, Vantage Data Centre, Africa Data Centres NTT /Dimension_Data, carefully avoiding being accused of monopoly practices.\n\nAI resilient graduates\nAI has co-existed comfortably between academia and industry for years. The terrain is shifting and currently AI research in the global north has computing power, large datasets, and highly skilled researchers. Power is shifting away from students and academics toward corporations and venture capitalists. Graduates from universities in dominant cultures, where there are high levels of digitisation, need to become AI-resilient. Graduates from the majority world also need to value their own process of knowledge construction, resist the lure of normalisation and see AI for what it is, another form of enclosure, and start blogging. Graduates from both the global north and the majority of the world need to be able to critique AI output, become familiar with the processes of technical change, and let their own studies and intellectual life guide their working futures.\n\nTrust in AI educational technology\nAt present, teachers are still skeptical about AI due to two main factors: lack of knowledge and understanding of AI, as well as some misunderstandings about it. Because AI can only score based on written work, and teachers can sometimes understand what students want to express through text. So, teachers lack trust and have a negative attitude towards the use of AI-Edtech.\n\nChallenges and criticism\nChallenges involved are mostly about over reliance on the technology could lead to lesser creativity, critical thinking and problem solving abilities especially if students skip traditional methods. Algorithm errors, hallucination are some of the common flaws found today in AI agents, which sometimes makes it unreliable and less trustworthy. The increasing use of artificial intelligence tools by students for academic tasks has raised concerns about the potential adverse effects of widespread reliance on these tools on learning and the development of critical thinking skills. Reliance on generative artificial intelligence, for example, is linked with reduced academic self-esteem and performance, and heightened learned helplessness  - raising concerns about its unintended effects. The study also found that use of Generative AI for academic tasks was lower among students with the conscientiousness trait- suggesting that self-disciplined and goal-oriented individuals were less inclined to rely on AI tools in their academic work. These findings further underscore concerns raised in prior studies regarding academic integrity in the context of AI use in academic settings.\n\nSee also\nComputational education\nComputing education\nComputers in the classroom\nList of chatbots\n\nExternal links\nGoogle AI Studio — can view your screen and hear your voice\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_education"
    },
    {
        "title": "Artificial intelligence in India",
        "text": "The artificial intelligence (AI) market in India is projected to reach $8 billion by 2025, growing at 40% CAGR from 2020 to 2025. This growth is part of the broader AI boom, a global period of rapid technological advancements with India being pioneer starting in the early 2010s with NLP based Chatbots from Haptik, Corover.ai, Niki.ai and then gaining prominence in the early 2020s based on reinforcement learning, marked by breakthroughs such as generative AI models from OpenAI, Krutrim and Alphafold by Google DeepMind. In India, the development of AI has been similarly transformative, with applications in healthcare, finance, and education, bolstered by government initiatives like NITI Aayog's 2018 National Strategy for Artificial Intelligence. Institutions such as the Indian Statistical Institute and the Indian Institute of Science published breakthrough AI research papers and patents. \nIndia's transformation to AI is primarily being driven by startups and government initiatives & policies like Digital India. By fostering technological trust through digital public infrastructure, India is tackling socioeconomic issues by taking a bottom-up approach to AI. NASSCOM and Boston Consulting Group estimate that by 2027, India's AI services might be valued at $17 billion. According to 2025 Technology and Innovation Report, by UN Trade and Development, India ranks 10th globally for private sector investments in AI. According to Mary Meeker, India has emerged as a key market for AI platforms, accounting for the largest share of ChatGPT's mobile app users and having the third-largest user base for DeepSeek in 2025.\nWhile AI presents significant opportunities for economic growth and social development in India, challenges such as data privacy concerns, skill shortages, and ethical considerations need to be addressed for responsible AI deployment. The growth of AI in India has also led to an increase in the number of cyberattacks that use AI to target organizations.\n\nHistory\nEarly days (1960s-1980s)\nThe TIFRAC (Tata Institute of Fundamental Research Automatic Calculator) was designed and developed by a team led by Rangaswamy Narasimhan between 1954 and 1960. He worked on pattern recognition from 1961 to 1964 at the University of Illinois Urbana-Champaign's Digital Computer Laboratory. In order to conduct research on database technology, computer networking, computer graphics, and systems software, he and M. G. K. Menon founded the National Centre for Software Development and Computing Techniques. In 1965, he established the Computer Society of India and supervised the initial research work on AI at Tata Institute of Fundamental Research. Jagdish Lal launched the first computer science program in 1976 at Motilal Nehru Regional Engineering College. H. K. Kesavan from the University of Waterloo and Vaidyeswaran Rajaraman from the University of Wisconsin–Madison joined the IIT Kanpur Electrical Engineering Department in 1963–1964 as Assistant Professor and Head of Department, respectively. H.N. Mahabala, who was employed at Bendix Corporation's Computer Division, joined the department in 1965. He previously worked with Marvin Minsky. The IIT Kanpur Computer Center was led by H. K. Kesavan, with Vaidyeswaran Rajaraman serving as his deputy. Kesavan informally permitted Rajaraman and Mahabala to introduce artificial intelligence into computer science classes. The computer science program was approved by IIT Kanpur in 1971 and split out from the electrical engineering department. In 1973, an IBM System/370 Model 155 was installed at IIT Madras. John McCarthy, head of the Artificial Intelligence Laboratory at Stanford University visited IIT Kanpur in 1971. He donated PDP-1 with a time-sharing operating system. During the 1970s, the balance of payments deficit in India restricted import of computers. The Department of Computer Science and Automation at the Indian Institute of Science established in 1969, played an important role in nurturing the development of data science and artificial intelligence in India. First course on AI was introduced in 1970's by G. Krishna. B. L. Deekshatulu introduced the first course on pattern recognition in early 1970s.\n\nFoundation phase\n1980s\nIn the 1980s, the Indian Statistical Institute's Optical Character Recognition Project was one of the country's first attempts at studying artificial intelligence and machine learning. OCR technology has benefited greatly from the work of ISI's Computer Vision and Pattern Recognition Unit, which is headed by Bidyut Baran Chaudhuri. He also contributed in the development of computer vision and digital image processing. As part of the Indian Fifth Generation Computer Systems Research Programme, the Department of Electronics, with support from the United Nations Development Programme, initiated the Knowledge Based Computer Systems Project in 1986, marking the beginning of India's first major AI research program. Prime Minister Rajiv Gandhi requested that the Department of Electronics and IISc to initiate the Parallel Processing Project in 1986–1987. The Center for Development of Advanced Computing eventually joined those efforts. IIT Madras was selected to develop system diagnosis, ISI for image processing, National Centre for Software Technology for natural language processing and TIFR for speech processing.\nIn 1987, the proposal of N. Seshagiri, Director General of the National Informatics Centre for the prototype development of supercomputer was cleared. Negotiations for a Cray supercomputer were underway between the Reagan administration and the Rajiv Gandhi government. US Defense Secretaries Frank Carlucci and Caspar Weinberger visited New Delhi after the US approved the transfer in 1988. The sale of a lower-end XMP-14 supercomputer was permitted in lieu of the Cray XMP-24 supercomputer due to security concerns. The Center for Development of Advanced Computing was formally established in March 1988 by the Ministry of Communications and Information Technology (previously the Ministry of IT) within the Department of Information Technology (formerly the Department of Electronics) in response to a recommendation made to the Prime Minister by the Scientific Advisory Council. The National Initiative in Supercomputing, which produced the PARAM series, was led by Vijay P. Bhatkar. For the first ten years, supercomputing and Indian language computing were the two main focus areas. C-DAC has expanded its operations in order to meet the needs in a number of domains, including network and internet software, real-time systems, artificial intelligence, and NLP.\nUnder the direction of Professor KV Ramakrishnamacharyulu from National Sanskrit University and Professor Rajeev Sangal from the International Institute of Information Technology, Hyderabad, the Akshar Bharati Research Group was established in 1984 with support from IIT Kanpur and the University of Hyderabad for computational processing of Indian languages. They focused on computational linguistics, NLP with ontological database systems, and Indian language/translation theories with linguistic tradition.\n\n1990s\nFrom IIT Kanpur, Mohan Tambe joined C-DAC in the 1990s to work on Graphics and Intelligence based Script Technology (GIST), which addressed the challenge of adapting personal computer software based on Latin script to Devanagiri and a number of other Indian language scripts. He was previously working on the Machine Translation for Indian languages Project. Within C-DAC, he established the GIST group. The technology was expanded to encompass NLP, artificial intelligence-based machine-aided language learning and translation, multimedia and multilingual computing solutions, and more. GIST resulted in the creation of G-CLASS (GIST cross language search plug-ins suite), a cross-language search engine. The Applied Artificial Intelligence Group at C-DAC has developed some basic and novel applications in the field of NLP, including machine translation, information extraction/retrieval, automatic summarization, speech recognition, text-to-speech synthesis, intelligent language teaching, and natural language-based document management with Decision Support Systems. These applications are the result of the foundation laid by previous language technology activities. Software firms in the Indian private sector began looking into AI applications, mostly in the area of business process automation.\nIn order to allow machines to read, comprehend, and interpret human languages, the Language Technologies Research Center was founded in October 1999 at the International Institute of Information Technology, Hyderabad. It focused on the advancements in semantic parsing, information extraction, natural language generation, sentiment analysis, and dialogue systems.\nSome of the early AI research in India was driven by societal needs. For example;\n\nEklavya, a knowledge-based program created by IIT Madas, helped community health workers deal with toddler illness symptoms by generating systematic case histories, offering basic treatment advice, or indicating when a referral was necessary.\nThe National Centre for Software Technology (NCST) created the Vidya language teaching system, which enhances educational quality through the use of speech-vision processing, ML, and NLP.\nC-DAC created the Sarani flight-scheduling expert system.\nTIFR and the Central Electronics Engineering Research Institute created a formant-based speech synthesis system for the Indian Railways.\nIISc and ISRO built an image processing facility that uses AI and computer vision.\n\nGrowth phase\nAround 2003, language technology, computer vision, and data science research groups were established at the International Institute of Information Technology, Hyderabad. With an emphasis on applied and translational research, the institute has the largest group of AI and machine learning researchers in India as of 2023. They also work on blockchain and quantum computing. In partnership with Intel, they created the Indian Driving Dataset, which contains the largest amount of road data for unstructured driving situations worldwide.\nThe Government of Karnataka established the Machine Intelligence & Robotics CoE (MINRO) at the International Institute of Information Technology, Bengaluru in 2018 with the goal of creating frameworks and regulations for ethical and responsible AI technologies. It researches on human-machine interface, industrial robotics and automation, data science and analysis, pattern recognition, machine intelligence, and AI systems.\nOn September 7, 2018, NITI Aayog, Intel, and TIFR announced their intention to establish International Center for Transformative Artificial Intelligence (ICTAI) in Bengaluru to support applied research. Under the National Strategy for Artificial Intelligence, it will create and implement AI-led application-based research projects on AI foundational frameworks, tools, and assets, such as curated datasets and distinctive AI algorithms in smart mobility, healthcare, and agriculture. On 16 November 2018, the Government of Maharastra signed MoU with the NITI Aayog and Wadhwani Institute for Artificial Intelligence to launch ICTAI for rural healthcare in collaboration with PATH, Lords Education and Health Society, Wadhwani Initiative for Sustainable Healthcare, IIT Madras, Sir Ratan Tata Trust, and Stanford Center for Population Health Sciences. It will prioritize study, diagnosis, and treatment of illnesses such as tuberculosis and cancer. \nNITI Aayog signed partnership agreement with Microsoft in 2018 to help expedite the use of AI for the development and adoption of local language computing, and to create farm advice services. Microsoft-NITI Aayog Problem to Solution Incubation Test Bed will be established. Microsoft will also help develop AI-assisted models for diabetic retinopathy screening. \nNITI Aayog drafted a proposal in 2019 to establish an institutional framework for AI. A cabinet note has been issued proposing to allocate ₹7,500 crore initially over three years for the establishment of 20 AI adaption centers, five core research institutes, and a cloud computing platform named AIRAWAT.\n\nAcceleration phase\nINDIAai, a collaborative initiative of the National E-Government Division and NASSCOM for AI-related advancements, was introduced by Ravi Shankar Prasad on 30 May 2020. In partnership with Intel and the Ministry of Education, the Responsible AI for Youth Program was also introduced to foster the development of AI-related skills. The Yardi School of Artificial Intelligence was founded in September 2020 by IIT Delhi to advance research in AI, ML, and data science in the fields of healthcare, materials science, robotics, industry 4.0, weather prediction, and transportation. $10 million was contributed by Yardi Systems' Anant Yardi in 2021 for education, infrastructure and research related activities. Sharad Kumar Saraf and Sudarshan Kumar Saraf, the founders of Technocraft Industries, donated ₹15 crore to establish the Technocraft Centre for Applied Artificial Intelligence at IIT Bombay in March 2021. In 2021, the Mehta Family Foundation and IIT Guwahati signed an MoU to establish the Mehta Family School of Data Science and Artificial Intelligence.\nAccording to Stanford University's annual AI Index report, India ranked fifth globally in 2022 in terms of investments received by businesses offering AI products and services. In 2022, Indian software professionals made the most global contribution to nearly 24% of GitHub projects pertaining to AI.\nThe Centre for Machine Intelligence and Data Science was officially opened by IIT Bombay on April 28, 2023. The National Research Foundation was created to advance research in a variety of fields, including AI with a budget of ₹50,000 crore for five years.\nThe Union Cabinet approved an extension of the Digital India program in 2023, allocating ₹14,903 crore starting FY2021-22 to FY2025-26 for the addition of nine new supercomputers under the National Super Computer Mission. All 22 of the Schedule 8 languages will be available for Bhashini. There will be the establishment of three AI Centers of Excellence focused on sustainable cities, agriculture, and health. Prime Minister Narendra Modi addressed international concerns about the negative use of artificial intelligence during a virtual G20 conference hosted by India in November 2023. \nTo assist the Indian government and policymakers on data science and AI-related policy issues, Sunil Wadhwani gave ₹110 crore to IIT Madras in 2024 for Wadhwani School of Data Science and AI. The institute will combine fundamental and applied research in systems biology, manufacturing, energy and the environment, healthcare, agriculture, smart cities and transportation, financial analytics, and defense.\nIndia invested ₹10,371.92 crore (US$1.2 billion) on IndiaAI Mission. With a budget of ₹990 crore, Minister for Education Dharmendra Pradhan announced on October 15, 2024, the creation of three AI Centers of Excellence in New Delhi: one for agricultural (under IIT Ropar), one for sustainable cities (under IIT Kanpur), and one for healthcare (under AIIMS and IIT Delhi). The Center for Generative AI, Shrijan at IIT Jodhpur was established on 25 October 2024 by Meta Platforms and IndiaAI to support long-term sustainability of Foundation Models and GenAI research in healthcare, education, agriculture, smart cities, smart mobility, sustainability, financial inclusion, and social inclusion. Additionally, the YuvAI initiative for Skilling and Capacity Building was launched in collaboration with the All India Council for Technical Education to advance open-source AI. Meta has pledged to donate up to ₹750 lakhs over a three-year period. Gati Shakti Vishwavidyalaya, the Institute of Human Behavior and Allied Sciences, the Postgraduate Institute of Medical Education and Research, and the All India Institute of Medical Sciences, Jodhpur, will work together with Shrijan. In November 2024, the Karnataka government approved a ₹28 crore investment to construct a Center of Excellence in Artificial Intelligence in Bengaluru.\nFrom 2025 to 2027, Microsoft to invest ₹25,000 crore (US$3.0 billion) on cloud and AI infrastructure. The construction of a 3 GW data center for AI services in Jamnagar was announced by Reliance Industries in 2025. On 12 June 2025, India's first locally developed, multilingual agentic AI, Kruti, was introduced by Ola Krutrim.\n\nNational Mission on Interdisciplinary Cyber-Physical Systems\nThe mission which has a five-year budget of ₹3,660 crore, was authorized by the Union Cabinet in December 2018 under the Department of Science and Technology. A technological vertical in AI and ML, IoT, data bank and DaaS, data analysis, autonomous systems and robotics, cyber security, and quantum engineering has been assigned to each of the 25 technological innovation hubs that have been formed.\nTo translate academic research on AI at the proof-of-concept stage into commercially viable goods and services, IIT Kharagpur established the AI4ICPS Innovation Hub Foundation in 2020. Under the National Mission on Interdisciplinary Cyber-Physical Systems (NM-ICPS), the Department of Science and Technology awarded it a grant of ₹170 crore.\nWith a seed money of ₹230 crore from the Department of Science and Technology and the Government of Karnataka, an Artificial Intelligence and Robotics Technology Park was established at the Indian Institute of Science as not-for-profit foundation in November 2020 for mission-mode research and development projects in cyber-security, healthcare, education, mobility, infrastructure, agriculture, and retail. \nTiHAN at IIT Hyderabad hosted India's first Autonomous Navigations Testbed Facility (Aerial & Terrestrial) in 2022, which was opened by Jitendra Singh Rana from Ministry of Science and Technology to develop next-generation autonomous navigation technology.\nTo develop spectral and energy-efficient wireless communications technology for 5G and 5G-Advanced, Kiran Kumar Kuchi of IIT Hyderabad and IIITB Comet Foundation in 2024 developed an O-RAN base station solution that triples capacity and improves cell coverage when compared to 4G networks.\nTechnology Innovation Hubs\n\nLegislation\nIndia currently does not have specific laws regulating artificial intelligence (AI). However, the Indian government has introduced several initiatives and guidelines aimed at the responsible development and deployment of AI technologies. The Indian government has tasked NITI Aayog, its apex public policy think tank, with establishing guidelines and policies for AI. In 2018, NITI Aayog released the National Strategy for Artificial Intelligence, also known as #AIForAll, which focuses on healthcare, agriculture, education, smart cities, and smart mobility.\nIn 2021, NITI Aayog published the \"Principles for Responsible AI,\" addressing ethical considerations for AI deployment in India. These principles cover system considerations, such as decision-making and accountability, and societal considerations, such as the impact of automation on employment. The second part of this document, \"Operationalizing Principles for Responsible AI,\" released in August 2021, focuses on implementing these ethical principles through regulatory and policy interventions, capacity building, and incentivizing ethical practices.\nIn 2023, the Indian government enacted the Digital Personal Data Protection Act, which addresses some privacy concerns related to AI platforms. The Ministry of Electronics and Information Technology (MeitY) has also issued advisories requiring platforms to obtain explicit permission before deploying unreliable AI models and to label AI-generated content to prevent misuse. India is a member of the Global Partnership on Artificial Intelligence (GPAI), which promotes the responsible use of AI through international collaboration. In 2023, the GPAI Summit was held in New Delhi, where experts discussed responsible AI, data governance, and the future of work.\nOther Indian agencies, such as the Bureau of Indian Standards (BIS), are also working on AI policies. BIS has established a committee to propose draft standards for AI, focusing on safety, reliability, and ethical considerations. India has not yet enacted specific AI regulations. However, the government has introduced measures to promote innovation and address ethical concerns and risks associated with AI. These efforts aim to support the growth of India's AI ecosystem and ensure responsible AI deployment.\nIndia has launched an AI Data Bank aimed at fostering innovation and enhancing national security. This initiative is designed to harness the power of artificial intelligence by providing a centralized repository of data that can be utilized across various sectors, including governance, business, healthcare, education, and space exploration. By facilitating access to crucial information, the AI Data Bank will support research and development efforts, stimulate technological advancements, and bolster the country’s security framework. This strategic move underscores India's commitment to leveraging AI for national progress and safeguarding its interests in an increasingly digital world.\n\nBharat GPT initiative\nThe Bharat GPT is a non-profit initiative, started in February 2023. The goal is to develop India focused multilingual, multimodal large language models and generative pre-trained transformer. Together with the applications and implementation frameworks, the Bharat GPT Consortium intends to publish a series of foundation models in order to meet the needs of developers and businesses. The Bharat GPT Consortium was established in 2022 under the public-private partnership with IIT Bombay, Jio Platforms, NASSCOM, Seetha Mahalaxmi Healthcare, Digital India Bhashini Division (Ministry of Electronics and Information Technology), IIT Madras, IIT Mandi, IIT Hyderabad, IIT Kanpur, International Institute of Information Technology, Hyderabad, and Indian Institute of Management Indore as members.  \nThe Bharat GPT development team is led by Professor Ganesh Ramakrishnan of IIT Bombay as principal investigator, with support from Professor Mohan Raghavan and Professor Maunendra Sankar Desarkar of IIT Hyderabad, Professor Rohit Saluja of IIT Mandi, Professor V Kamakoti of IIT Madras, Professor Arnab Bhattacharya of IIT Kanpur, Professor Kshitij Jadhav of IIT Bombay, Professor Maheshwari of IIM Indore, and Professor Ravi Kiran of IIIT-Hyderabad. The text-based foundation model will be released first, followed by speech and video models. In addition to licensing commercial use, the effort has been open-sourcing a large portion of its work. Bharat GPT is anticipated to be activated by speech and gesture recognition. \nOn December 27, 2023, Akash Ambani announced that Jio Platforms and IIT Bombay were collaborating on Bharat GPT program. Jio Platforms will assist Bharat GPT with specialized downstream telecom and retail applications by developing smaller, customized models. By investigating the potential of AI to foster innovation across a range of goods and services, Jio hopes to achieve its larger goal of establishing a whole ecosystem of growth through Bharat GPT. Bharat GPT Consortium is assisting in the interpretation of official documents, such as court orders, in close collaboration with the Department of Administrative Reforms and Public Grievances. \nIndia's first ChatGPT-style service, will be launched in March 2024 by Bharat GPT Consortium, with assistance from the Department of Science and Technology, and Reliance Industries. The four primary areas in which Bharat GPT sees its model operating are healthcare, financial services, governance, and education, supporting 11 Indian languages.\n\nHanooman series\nSeetha Mahalaxmi Healthcare (SML) revealed the Hanooman series LLM in February 2024 in collaboration with the Bharat GPT Consortium. Among the Hanooman series, the first four AI models—1.5 billion, 7 billion, 13 billion, and 40 billion parameters in size—will be made available as open-source after being trained on 22 Indian languages. The ability to generate text-to-text, text-to-video, speech synthesis and speech recognition will be aided by Hanooman's multimodal learning capability. SML is negotiating with healthcare organizations, BFSI, and mobile app developers to create customized models by refining the Hanooman series. The healthcare model VizzhyGPT is the first of these refined iterations, having been trained on extensive multiomics, clinical research, and electronic health record data. \nHanooman's alpha version, which lacks multimodal features and internet access, was launched on May 10, 2024, in 98 international languages, including 12 Indian languages (Hindi, Marathi, Gujarati, Bengali, Kannada, Odia, Punjabi, Assamese, Tamil, Telugu, Malayalam, and Sindhi). It enables tutoring, coding, and conversation. The platform is collaborating with NASSCOM, Hewlett-Packard, and the Government of Telangana. Yotta Infrastructure will supply GPU cloud infrastructure. As of May 14, 2024, the Hanooman chatbot is accessible via both a web-client and an Android application.\nHanooman is jointly owned by SML and 3AI Holding Limited, an AI investment company based in Abu Dhabi. Both businesses will own 50% share under joint ownership arrangement. In order to improve Hanooman's text, voice, image, and coding capabilities for users, it will be able to access 3AI Holding's Omega GenAI, which is being created with 665 billion parameters and 20 trillion tokens. For business clients, Hanooman will launch a proprietary model.\n\nBharatGen\nIIT Bombay Professor Ganesh Ramakrishnan thought of creating a homegrown solution that would use GenAI and take into account the linguistic and cultural diversity of India. An open-source, multimodal, multilingual, India-centric foundation model called BharatGen was formally introduced on September 30, 2024. With an investment of less than ₹235 crore (US$28 million), the Department of Science and Technology is funding the project under National Mission on Interdisciplinary Cyber-Physical Systems. Due to security concerns in mission-critical applications like defense, the project gained strategic importance and aims to reduce reliance on foreign AI models. Professor Ganesh Ramakrishnan has been advocating for a public–private partnership in AI through the Bharat GPT program. The earlier Bharat GPT efforts have taken on a more formal role in the BharatGen project, which is anticipated for completion by 2026.\nAs a national mission, BharatGen has been set up to facilitate inter-ministerial cooperation by taking whole-of-government approach. The AI will be developed from the ground up, guaranteeing that all intellectual property stays in the country. IndiaAI Compute Facility will spearhead the project. BharatGen will have robust academic base, incorporating faculty and research institutions into its ecosystem to guarantee sustained innovation. Yotta Infrastructure, and Neysa are providing cloud support.\nThe backend algorithm development and the necessary technical work was done by a collaborative team from BharatGen consortium. As of February 2025, the BharatGen consortium has 50–60 researchers and a wide range of student contributors from the Indian Institute of Management Indore, the International Institute of Information Technology, Hyderabad, IIT Bombay, IIT Kanpur, IIT Hyderabad, IIT Mandi, and IIT Madras. BharatGen consortium will work with behavioral economists, engineers, AI researchers, and design specialists, to develop a multidisciplinary strategy to use AI to address India's problems. The AI model takes into account the 1,600 languages and scripts that make up the Indian knowledge system. Each institute of the consortium is investigating particular tasks to create models in voice, language, and vision. \nAs of January 30, 2025, the framework for the AI model is ready. The development team has spent the last one and a half years working on the project. The initial version should be available in four to ten months, according to Ashwini Vaishnaw, Minister of Electronics and IT.\nIn order to concentrate on primary data collection, BharatGen started the Bharat Data Sagar initiative, a multilingual repository for AI research. The goal of this data collection is to satisfy the need for training data for Indian languages that are underrepresented in data corpora. It will capture the Indian linguistic nuances, which are frequently disregarded in international AI models. BharatGen offers tools and technologies to facilitate the creation of region specific tailored content by translating between local languages and dialects. In order to accurately portray India's languages, dialects, and cultural context, BharatGen focuses on gathering and curating data unique to the nation.\nUsing vision language models, BharatGen launched e-vikrAI in October 2024. It makes e-commerce easier for non-English speaking vendors by automating the cataloguing process and doing away with the need for human input. Sellers only need to provide an image of their product to get auto-generated titles, descriptions, features, and pricing recommendations. e-vikrAI improves accessibility by translating and vocalizing product descriptions in Indian languages.\n\nIndiaAI Mission and compute facility\nThe IndiaAI Mission was approved in 7 March 2024 consisting of seven core pillars, and an allocation of ₹10,371.92 crore (US$1.2 billion) across various components as follows:  \n\nAdditionally, IndiaAI Overheads and Contingency received funding of ₹102.69 crore (US$12 million). The digital public infrastructure framework for AI in India guarantees consent-based, ethically generated datasets while lowering dependency on foreign and synthetic data. \nOn 30 January 2025, Ashwini Vaishnaw, the Minister for Electronics and information Technology, confirmed that the IndiaAI Mission would customize native AI solutions for the Indian context using Indian languages, supported by a state-of-the-art shared computing infrastructure. The initial AI model starts with a compute capacity of about 10,000 GPUs, with the remaining 8693 GPUs to be added shortly. The facility includes 7,200 AMD Instinct MI200 and MI300 series, 12,896 Nvidia H100, and 1,480 H200 processors. The cost of computing the AI model will be less than ₹100 per hour following a 40% government subsidy, with half-yearly and annual plans for developers, researchers, and students. \nStartups, MSMEs, universities, researchers, students, and governmental organizations will all be able to access AI compute, network, storage, platform, and cloud services through the IndiaAI Compute Portal. Easy access to Nvidia H100, H200, A100, L40S, and L4, AMD MI300x and 325X, Intel Gaudi 2, AWS Tranium, and Inferentia will be made possible by the portal. Nearly 45% of the IndiaAI Mission's budget goes into the AI Compute Portal. \nAccessing more expensive GPUs would cost ₹150 per hour, while utilizing less expensive GPUs would cost ₹115.85 per hour. Orient Technologies, CMS Computers Limited, SHI Locuz, CtrlS, NxtGen Cloud Technologies, Yotta Infrastructure, Jio Platforms, Tata Communications, E2E Networks, and Vensysco Technologies have been approved by the government to provide 18,693 GPUs.\nAs part of the IndianAI Mission, the union government, in April 2025, selected Sarvam AI, to develop the country's first sovereign LLM, which will include the capabilities of reasoning, voice, and fluency in Indian languages. As of May 2025, the government, as part of the IndianAI Mission, is supporting Indian researchers in designing indigenous GPUs.\nAshwini Vaishnaw announced AIKosha: IndiaAI Datasets Platform, the AI Compute Portal, the AI Competency Framework for Public Sector Officials, iGOT-AI Mission Karmayogi, the IndiaAI Startups Global Acceleration Program with Station F, the IndiaAI Application Development Initiative, and IndiaAI FutureSkills on 6 March 2025, as part of the IndiaAI Mission to strengthen AI-driven research, innovation, and skill development. For public officials, the iGOT-AI Mission Karmayogi incorporates AI-driven learning recommendations. \nAIKosha: IndiaAI Datasets Platform\nTo facilitate AI innovation, AIKosha will offer a safe, centralized platform with easy access to a database of models, use cases, and datasets. Through an integrated development environment, tools, and tutorials, it offers AI sandbox capabilities. AIKosha has features like permission-based access, content discoverability, AI readiness scoring of datasets, and security methods including secure APIs, firewalls for real-time harmful traffic filtering, and data encryption both in motion and at rest. More than 80 models and 300 datasets are available on AIKosha. Both the public and private sector organizations gather AIKosha datasets, which include census data, geospatial data, and linguistic data. \nIndiaAI Startups Global Acceleration Program\nThe IndiaAI Mission will begin a four-month acceleration program in partnership with Station F and HEC Paris from 2025. The ten shortlisted AI startup companies will have access to networking opportunities, mentorship, and international market expansion in Europe.\n\nAI Research Analytics and Knowledge Dissemination Platform\nWith a peak performance of 13,170 teraflops, AIRAWAT-PSAI is the biggest and fastest AI supercomputing machine in India. It was deployed at C-DAC, Pune in 2023 and is maintained by Netweb Technologies. \nMeitY is also funding the implementation of 200 AI Petaflops Mixed Precision peak computing capacity (scalable to 790 AI Petaflops) Proof of Concept for AI Research Analytics and Knowledge Dissemination Platform (AIRAWAT), which is being carried out by C-DAC, Pune. A combined peak compute of 410 AI Petaflops Mixed Precision and a sustained compute capability of 8.5 Petaflops (Rmax) Double Precision are achieved by combining the 200 AI Petaflop AIRAWAT PoC with the 210 AI Petaflop PARAM Siddhi-AI. 13 Petaflops is the maximum computation capacity (Double Precision, Rpeak). A plan for expanding AIRAWAT to 1,000 AI Petaflops of Mixed Precision computing capacity has been envisioned by MeitY. With its extensive, power-optimized AI cloud infrastructure, AIRAWAT will serve as a common computational cloud platform for Big Data Analytics and Assimilation, linking all Centers for Research Excellence in Artificial Intelligence (COREs), Indian Centres for Transformational AI (ICTAIs), and other academic, research lab, scientific community, industry, and start-up institutions with the National Knowledge Network.\nWith an emphasis on NLP, surveillance and image processing, pattern recognition, medical imaging, education, agriculture, finance, healthcare, audio assistance, robotics, national security, defense, the automotive industry, supply chain management, human resource development, and anomalous behavior detection from video analytics, AIRAWAT will support the field of applied AI. Deployed under the National Supercomputing Mission, it will support the Digital India Bhashini initiative and accomplish the goals of the National Program on AI.\n\nInternational collaboration\nPrime Minister Narendra Modi with NITI Aayog officially opened the World Economic Forum's Center for the Fourth Industrial Revolution liaison office in Navi Mumbai on 11 October 2018. It's objective is to foster the responsible adoption and deployment of new technologies by designing policy frameworks and protocols with the government, corporations, academic institutions, startups, and international organizations. The Indian team will collaborate with their counterparts in Beijing, Tokyo, and San Francisco to focus on blockchain & DLT, AI & ML. To increase farmers' incomes, C4IR has been working on AI-driven agricultural initiatives since 2018. In January 2024, C4IR, MeitY, NASSCOM, and the Office of the Principal Scientific Adviser launched the AI for India 2030 initiative. Its goal is to offer a systematic approach to the ethical, inclusive, and responsible adoption of AI in India.\nIndia's Ministry of Electronics and Information Technology and Japan's Ministry of Economy, Trade, and Industry signed a memorandum of cooperation on October 29, 2018, which outlines the two nations' cooperation in AI and IoT as part of India-Japan Cooperation on Digital Partnership. In an effort to strengthen ties between NITI Aayog and METI, India and Japan signed a broad statement of intent on AI under \"Society 5.0\" that included the exploration of potential institutional collaborations, such as joint projects between IIT Hyderabad and Artificial Intelligence Research Centre (National Institute of Advanced Industrial Science and Technology), related to machine learning, deep learning, data mining, and other AI themes. Joint scientific and technological cooperation in ML, and probabilistic logic techniques for various data types and combinations were added to the extended MoU on December 24, 2021.\nIn 2021, the Indian Department of Science and Technology and the United States Department of State were designated as the nodal agencies for the US India Artificial Intelligence Initiative launched under Indo-US Science and Technology Forum. It will provide a forum for bilateral research and development cooperation. It will also facilitate AI innovation, exchange ideas for building an AI workforce, and suggest ways to promote collaborations.\nThe United States and India expanded their joint cyber training and exercises in 2022 and initiated the Defense Artificial Intelligence Dialogue. According to Cleo Paskal, a non-resident senior fellow at the Foundation for Defense of Democracies, the Indian community has influenced AI research and development in the US for many years. Things are taken to the next level by the Defense Artificial Intelligence Dialogue. In the same year, the US intends to join six of India's Technology Innovation Hubs to support collaborative research projects in fields including AI, advanced wireless technologies, and data science to further advancements in applications like agriculture, health, and climate. Thirty-five projects have been selected for implementation by the DST and U.S. National Science Foundation.\nAs part of the United States–India Initiative on Critical and Emerging Technology, the US and India announced setting aside more than $2 million in 2024 for collaborative research initiatives that will advance AI and quantum technology. \nIn order to develop advanced driver-assistance systems and vehicular automation technologies that can be used in different parts of the world, the Japanese business Honda began collaborative research on AI technologies with IIT Delhi and IIT Bombay in 2024. This was done in order to further advance the company's own Honda Cooperative Intelligence AI platform.\nTo create safe, open, secure, and reliable AI, Prime Minister Narendra Modi and President Emmanuel Macron announced the India-France Roadmap on Artificial Intelligence on February 12, 2025. The goal is to make sure that the rules and guidelines governing the use of AI represent democratic principles and maximize its potential for the advancement of humanity and the common good. \nThe India-EU Trade and Technology Council has made AI a top priority. In 2025, the European AI Office and the India AI Mission agreed to strengthen collaboration on large language models, including collaborative efforts to create frameworks and tools for responsible and ethical AI. It will expand upon the advancements in high-performance computing applications related to bioinformatics, climate change, and natural disasters that have been made possible by collaborative research and development.\nOn 2 July 2025, the Government of Telangana launched the Telangana Data Exchange (TGDeX) in partnership with IISc and the Japan International Cooperation Agency. It will enable access to structured datasets and developer tools required to create AI solutions. TGDeX will utilize Open Data Telangana platform. TGDeX's goal of hosting more than 2,000 AI-ready datasets over the next five years will be aided by the Telangana AI Mission (T-AIM), T–Hub, MATH (AI/ML Centre of Excellence of T-Hub), IIT Hyderabad, and IIIT-Hyderabad.\n\nImpact\nAgriculture\nOn December 12, 2023, KissanAI announced the release of Dhenu 1.0, world's first agriculture-specific LLM designed for Indian farmers. It can handle 300,000 instruction sets and understand English, Hindi, and Hinglish queries. The platform provides voice-based, tailored assistance. KissanAI used Sarvam AI's OpenHathi for bilingual support and worked with NimbleBox for APIs. Dhenu 1.0 utilizes OpenHermes 2.5 Mistral 7B. KissanAI is supported by Microsoft for Startups. For climate resilient agriculture practices, KissanAI partnered with the United Nations Development Programme in January 2024 to develop a voice-based vernacular GenAI virtual assistant. The main goals of this project are to overcome language barriers and offer easily comprehensible information so that female farmers can actively engage in and profit from generative AI developments. Dhenu2, an open-source version, was launched in October 2024, providing solutions for agribusinesses, farmers, and policymakers worldwide. It is available in three models: 8B, 3B, and 1B, and is powered by the Llama 3. The platform is based on 1.5 million synthetic and real-world instructions that cover more than 4,000 agricultural topics and categories. Over 100,000 farmers were interviewed to gain insights that provided practical information for better decision-making. \nUttar Pradesh Open Network for Agriculture, a digital public infrastructure, powered by Gemini and Beckn protocol, was launched in January 2025 by the Government of Uttar Pradesh in collaboration with Google Cloud Platform. Farmers will get one-stop access to market linkages, loans, mechanization, and advisory services for selling their produce. Voice user interface will be available to farmers in Hindi, Bengali, Telugu, Kannada, Gujarati, and Punjabi, with other Indian languages to be added in 2025.\nAgriHub, a center of excellence for sustainable agriculture co-funded by the MeitY and the Government of Madhya Pradesh, was opened by IIT Indore on January 27, 2025. It will use AI, machine learning, and deep learning to revolutionize the agricultural industry.  By using big data analytics and genomic research to support data-driven agriculture, it will enable research in precision agriculture, xerophyte, and AI-driven disease diagnostics.\nFarmers in Fazilka have embraced AI-powered weather sensors created by IIT Ropar. It provides real-time information on atmospheric conditions by utilizing data gathered from satellites, weather balloons and a network of weather stations on the ground. Parameters like temperature, humidity, precipitation, and lux are used with data from the India Meteorological Department to forecast rainfall.\nAI is being used by the Ministry of Agriculture and Farmers' Welfare to assist farmers. An AI-powered chatbot called Kisan e-Mitra was built to help farmers with their questions on Pradhan Mantri Kisan Samman Nidhi scheme in multiple Indian languages. The National Pest Surveillance System uses AI and ML to identify pest infestation, allowing for prompt action for healthier crops and addressing the loss of produce brought on by climate change. AI-based analytics that use field photos to monitor and evaluate crop health utilizing weather, soil moisture, and satellite data for wheat and rice cultivation.\nThe use of AI in Khutbav village by the Agriculture Development Trust and Microsoft in 2025 led to a 40% increase in yield and a 50% reduction in production costs due to water and pesticide usage. Through a smartphone application Agripilot.ai developed by Click2Cloud, the AI notify farmers of possible pest attacks, soil conditions, air speed, and weather conditions to determine when to apply pesticides and how much water the crop requires. Utilizing a variety of data sources, including satellite imagery, weather forecasts, soil sensors, and inputs unique to each farm, the AI generate personalized suggestions for farmers. Microsoft's Azure Data Manager for Agriculture (formerly Microsoft FarmBeats), is used to process this data. The annual fee for AI services is ₹10,000. In order to offer insights, the data is analyzed by the open-source research project FarmVibes.ai in conjunction with historical crop data. For the farmer, Microsoft Azure OpenAI Service transforms these details into easy-to-follow steps.\nIn 100 villages, the Assam government will support the establishment of AI-powered agri-hubs, where drones and AI would assist farmers in making data-driven decisions, increasing yields, and promoting sustainability, according to the state budget for 2025–2026.\n\nIndustry\nWith an investment of ₹20 crore, Tata Consultancy Services founded the FC Kohli Centre on Intelligent Systems at International Institute of Information Technology, Hyderabad in 2015 to conduct research in the fields of robotics, cognitive sciences, neural language processing, and intelligent systems.\nIn March 2018, Capillary Technologies and IIT Kharagpur established the Centre of Excellence in Artificial Intelligence Research for financial analytics, industrial automation, digital healthcare, and intelligent transportation systems, with a combined investment of ₹56.4 million. By March 2019, a second campus at Kukatpallya will open. In July, the Karnataka government and NASSCOM jointly established the Center of Excellence for Data Science and Artificial Intelligence on a public–private partnership for helping small and medium enterprises and make big data sets for training models available.\nFor fundamental research in deep learning, reinforcement learning, network analytics, interpretable machine learning, and domain-aware AI, Bosch established the Robert Bosch Center for Data Science and Artificial Intelligence at IIT Madras in 2019. The center will concentrate its applied research on systems biology, smart cities, manufacturing analytics, financial analytics, and healthcare. Additionally, it is the location of India's largest deep reinforcement learning group. At the Microsoft Research Lab in Bengaluru, Microsoft introduced Societal Impact through Cloud and Artificial Intelligence in 2019 to use cloud computing and AI to address long-term societal issues in transportation, agriculture, health and wellness, and education.\nAccenture opened its first Nano Lab in the Asia–Pacific on 4 February 2020, in Hyderabad, for applied research in security, extended reality, and AI. The Nvidia AI Technology Center was founded at IIT Hyderabad in 2020 with the goal of accelerating AI research and commercialization. It focuses on the application of AI in language comprehension, smart cities, and agriculture. Situational awareness, operator environment analysis, smart appliances, autonomous navigation systems, edge devices for IoT and industry 4.0, and conversational user interfaces were among the AI applications that Tata Elxsi's Artificial Intelligence Centre of Excellence began deploying in 2020. In 2020, Intel established INAI, an applied artificial intelligence research center in Hyderabad, in partnership with the Public Health Foundation of India, IIIT-H, and the Government of Telangana. INAI will focus on population-level issues in the smart mobility and healthcare sectors.\nSiMa.ai established a new design center in Bengaluru in 2021 to work on the MLSoC platform, the first machine learning SoC specifically designed for the industry. This platform can support any framework, neural network, or foundational model for any workload.\nIn 2021, Kotak Mahindra Bank and IISc announced their intention to establish the Kotak-IISc AI-ML Center for research and innovation in fintech, as well as to cultivate the necessary talent pool to satisfy industry demands. Private businesses are attempting to create smaller, less expensive large language model. Examples of these include AI4Bharat's Airawat series, Sarvam AI's OpenHathi series, CoRover.ai's BharatGPT, Tech Mahindra's Indus project, Ola's Krutrim, TWO AI's Sutra series, and SML's  series.\nIIT Delhi's Yardi School of Artificial Intelligence and an Indian deep tech firm KnowDis are working together to create AI models that could find possible antibodies to treat a variety of illnesses. In part of its $1 billion pledge to accelerate AI-led innovation for the ai360 ecosystem, Wipro announced on August 18, 2023, the opening of a Center of Excellence in Generative Artificial Intelligence in collaboration with IIT Delhi. To study AI, the Adani Group and the International Holding Company established a joint venture on 28 December 2023.\nOn August 29, 2024, Mukesh Ambani revealed the creation of an application agnostic suite of tools and platforms called Jio Brain, to hasten the Reliance Group's adoption of AI and set the stage for 6G development with plans to offer services to vendors. JioBrain combines high speed and low latency of 5G with automation, anomaly detection, and predictive forecasting. Reliance intends to use it to optimize processes in education, healthcare, entertainment, retail, and agriculture. JioBrain includes over 500 REST APIs and data APIs. It can be used for NLP, image-to-video, text-to-music, text-to-image and video, speech-to-speech, speech-to-text translation, code generation, explanation, optimization, and debugging. Additionally, it contains feature engineering, model chaining, and hyperparameter optimization. Jio Brain offers mobile and enterprise-ready LLM-as-a-service capability for GenAI. \nAI has been used in medical devices, medical reports, predicting advertising & marketing results, do product design & development, healthcare & cognitive testing with diagnostic AI. GenAI is being used by Indian brands for product ideation, visual concept development, social post creation.\nIn 2025, Citadel Securities and IIT Kanpur announced a collaboration to construct the Translational and Transformative Training and Investigations Laboratory, an advanced GPU research facility for work on machine learning, intelligent systems, data science, data visualization, translational AI, and high-performance computing. Project Waterworth, a 50,000 km undersea cable for high-speed connectivity required to propel AI innovation, was unveiled by Meta on February 14, 2025. It will link South Africa, Brazil, India, the United States, and other regions. India is among the nations with the highest usage of Meta AI on WhatsApp and Instagram.\n\nDefence\nA task force for the Strategic Implementation of AI for National Security and Defence was established in February 2018 by the Ministry of Defense's Department of Defence Production. The process of getting the military ready for AI use was started by the MoD in 2019. The Centre for Artificial Intelligence and Robotics was approved to develop AI solutions for signal intelligence to improve intelligence collection and analysis capabilities at a cost of ₹73.9 crore and Energy Harvesting Based Infrared Sensor Network for Automated Human Intrusion Detection (EYESIRa) at a cost of ₹1.8 crore. In 2021, the Indian Army, with assistance from the National Security Council, began operating the Quantum Lab and Artificial Intelligence Center at the Military College of Telecommunication Engineering. With an emphasis on robotics and artificial intelligence, Defence Research and Development Organisation and Indian Institute of Science established the Joint Advanced Technology Programme-Center of Excellence. In 2022, the Indian Navy created an AI Core group and set up a Center of Excellence for AI and Big Data analysis at INS Valsura. Indian Army incubated Artificial Intelligence Offensive Drone Operations Project in partnership with an Indian startup. Tonbo Imaging integrated real-time target identification and Edge AI image processing into MPATGM. During Exercise Dakshin Shakti 2021, the Indian Army integrated AI into its intelligence, surveillance, and reconnaissance architecture to provide a cohesive operational intelligence picture of the battlefield.\nIn 2022, the Indian government established the Defence Artificial Intelligence Council and the Defence AI Project Agency, and it also published a list of 75 defense-related AI priority projects. MoD earmarked ₹1,000 crore annually till 2026 for capacity building, infrastructure setup, data preparation, and Al project implementation. The Indian Army, the Indian Navy and the Indian Air Force set aside ₹100 crore annually for the development of AI-specific applications. The military is already deploying some AI-enabled projects and equipment. At Air Force Station Rajokri, the IAF Centre of Excellence for Artificial Intelligence was established in 2022 as part of the Unit for Digitization, Automation, Artificial Intelligence, and Application Networking (UDAAN). Swarm drone systems were introduced by the Mechanised Infantry Regiment for offensive operations close to LAC.\nGrene Robotics revealed Indrajaal Autonomous Drone Defense Dome. For offensive operations, the military began acquiring AI-enabled UAVs and swarm drones. Bharat Electronics developed AI-enabled audio transcription and analysis software for battlefield communication. Using AI during transport operations, the Indian Army's Research & Development branch patented driver tiredness monitoring system. As part of initial investment, the Indian Armed Forces is investing about $50 million (€47.2 million) yearly on AI, according to Delhi Policy Group think tank. For high altitude logistics at forward outposts, military robots are deployed. Army is developing autonomous combat vehicles, robotic surveillance platforms, and Manned-Unmanned Teaming (MUM-T) solutions as part of the Defence AI roadmap. MCTE is working with the Ministry of Electronics and Information Technology and, Society for Applied Microwave Electronics Engineering & Research, on AI and military-grade chipset. Phase III of AI-enabled space-based surveillance has been authorized.\nDRDO Chairman and Secretary of the Department of Defense Research & Development Samir V. Kamat said the agency started concentrating on the potential use of AI in the development of military systems and subsystems. The Indian government intends to leverage the private sector's sizable AI workforce and dual-use technologies for defense by 2026. In order to conduct research on autonomous platforms, improved surveillance, predictive maintenance, and intelligent decision support system, the Indian Army AI Incubation Center was established. Indian Navy launched INS Surat with AI capabilities. \nIn order to identify and locate any hostile objects in the sky during Operation Sindoor, the Indian military employed AI cloud-based integrated air command and control systems. The Indian Army tested an AI-powered Negev NG-7 on 9 June 2025, developed by BSS Material. MoD has approved 129 AI projects until 2026, 77 of which have been finished. To identify and categorize hostile aircraft activities, BEL has created an AI system accessible through the Integrated Air Command and Control System. To analyze and automate enemy intercepts in the Western Theater, the Indian Army has developed an Intercept Management System that generates intelligence picture by interpreting data using AI and visualization. It can use AI to perform automated analysis and interpretation, as well as classify and visualize intercepts. To identify airborne targets and eliminate the threat far from susceptible locations, CAIR has created an AI-based Air Defense Control and Reporting System.\nTo guarantee smooth data exchange between field units and command centers, XXXIII Corps tested AI-enabled sensors with secure communication networks during Exercise Divya Drishti in July 2025. A sensor-to-shooter link was successfully established by the Indian Army, allowing for a quicker and more efficient reaction time.\n\nSpace\nMOI-TD, India's first AI lab in space, is being built by TakeMe2Space. AI's potential utility in space will be demonstrated with the MOI-TD mission. In order to foresee natural disasters and comprehend climate change, the MOI-TD will assist with environmental monitoring, weather pattern analysis, and Earth's surface change tracking. The lab will process the data in real time, allowing for analysis of the information at the time of collection. The MOI-TD mission showed that it was possible to run external code on the satellite, securely downlink encrypted data, and uplink huge AI models from a ground station through OrbitLab, a web-based console. It effectively validated key subsystems, including computer infrastructure (Zero Cube AI Accelerator, POEM Adapter Board), actuators (MagnetoTorquers, AirTorquers, Reaction Wheel), and sensors (Sun Sensor, Horizon Sensor, Solar Cell, IMUs). The foundation for space-based computing is laid by MOI-TD. The creation of space-based data centers will be facilitated by the upcoming MOI-1 AI Lab mission. Support for the project came from the Indian National Space Promotion and Authorization Center. One study by the University of Southampton on TakeMe2Space's AI lab used a low-power AI system to lessen motion blur in satellite imagery.\n\nHealthcare\nDuring the COVID-19 pandemic, Qure.ai's AI chest X-ray reporting tool, qXR, was used to identify patients for high, medium, or low risk so that RT-PCR testing could be performed. It can identify pulmonary consolidation, ground-glass opacity, and other signs of COVID-19. The tool provides measurements like the proportion of lung size and volume impacted by the anomalies. The Computational and Data Sciences Collaborative Laboratory of Artificial Intelligence in Medical and Healthcare Imaging was established by Wipro GE HealthCare at IISc in September 2020. Its goal is to develop advanced diagnostic and medical image-reconstruction techniques and protocols for quicker and better imaging by utilizing deep learning, AI, and future-ready digital interfaces.\nWith an emphasis on neurological disease diagnosis and population-level clinical impact analysis, Siemens Healthineers established the Computational Data Sciences Collaborative Laboratory for AI in Precision Medicine at IISc in 2024 to create open-source AI tools to automate the segmentation of pathological findings in neuroimaging data. \nAs part of the Interdisciplinary Group for Advanced Research on Birth Outcomes – DBT India Initiative (GARBH-Ini) program, IIT Madras and Translational Health Science and Technology Institute in 2024 created India's first AI model, Garbhini-GA2, which will accurately estimate the age of the fetus in pregnant women in the second and third trimesters. In contrast to the current methods of determining a fetus's age in India, which use the Hadlock model developed using pregnancy data from Western countries or the more recent INTERGROWTH-21st model, the Garbhini-GA2, which is specific to the Indian population, uses genetic algorithm-based methods for estimating gestational age. This model reduces the error by nearly three times. The study was carried out in collaboration with Pondicherry Institute of Medical Sciences, Christian Medical College Vellore, Safdarjung Hospital, and Gurugram Civil Hospital. Following the completion of pan-India validation, Garbhini-GA2 will be implemented in clinics throughout the country. The Ministry of Science and Technology's Department of Biotechnology provided financing for the research, with additional support from the Robert Bosch Centre for Data Science and Artificial Intelligence (IIT Madras) and the Centre for Integrative Biology and Systems Medicine (IIT Madras).\nIn order to support healthy aging, constructive lifestyle modifications, and psycho-social wellbeing, Wipro and the Centre for Brain Research at IISc partnered in May 2024 to create an AI-based personal care engine that will consider a person's past medical history, desired health state, and behavioral responses to offer personalized help with chronic disease prevention and treatment. Researchers from St. John's Hospital and IIIT-B are focusing on using computer vision for early detection of autism.\n\nMobility\nIn an effort to improve road safety and decrease fatalities by half, Minister of Road Transport and Highways Nitin Gadkari inaugurated Intelligent Solutions for Road Safety through Technology and Engineering (iRASTE) under Mission Zero on September 11, 2021. The pilot project will run for two years in Nagpur. Intel, International Institute of Information Technology, Hyderabad, Central Road Research Institute, Mahindra & Mahindra, and Nagpur Municipal Corporation are part of the pilot project. It is focusing on vehicle safety, mobility analysis, and infrastructure safety.\nTo improve vehicle safety, NMC will outfit its fleet of vehicles with advanced driver-assistance systems and collision avoidance systems. They will also be equipped with sensors which will continuously monitor the road network's dynamic risks for mobility analysis, mapping the city into three zones: white (normal), grey (accident could happen), and black (accident has already happened). The data will be gathered by Intel AI Center at IIIT-H for monitoring and fixing the grey and black zones.\nMINRO at IIIT-B is working on traffic analysis of Indian road conditions, analyzing traffic data, and using data for multimodal transport to make recommendations about the locations of  rail transit and bus stations.\n\nGovernment\nIn 2021, the Ministry of Corporate Affairs introduced the first phase of MCA21 V3.0. It will allow for the electronic filing of documents under the corporate law and public access to corporate information. AI will be utilized by the platform to gather, organize, and classify stakeholder comments and inputs and produce analytical reports that will facilitate speedy policy choices.\nWith assistance from Google, the Ministry of Agriculture and Farmers' Welfare and the Wadhwani Institute for Artificial Intelligence created Krishi 24/7, the first AI-powered automated agricultural news monitoring and analysis tool. Through better decision-making, Krishi 24/7 will support the identification of valuable news, provide timely notifications, and respond quickly to safeguard farmers' interests and advance sustainable agricultural growth. The application converts news articles into English after scanning them in several languages. It ensures that the ministry is informed in a timely manner about pertinent occurrences that are published online by extracting key information from news items, including the headline, crop name, event type, date, location, severity, summary, and source link. The National Center for Disease Control has effectively implemented a comparable automated surveillance and analysis tool for disease outbreaks.\nAs part of the Safe City initiative, 1,000 AI cameras with facial recognition were placed at a cost of ₹96 crore in Lucknow's commercial districts, colleges, hostels, large crossings, and public gathering spots. By integrating with the National Automated Fingerprint Identification System, the cameras will detect suspicious movement of individuals by comparing their faces to a police database, notify control rooms of any criminal activities, and send out alerts against illegal dumping. For a more responsive and linked security network, AI will be included into crisis hotlines and helplines.\nIn collaboration with the Indian government, IIIT-B created Datalake, a tool that analyzes government data on a range of projects to determine whether the objectives of the policy have been fulfilled and whether there is room for improvement. It can forecast how to modify the beneficiary structure to comply with the policy and benefit the citizens.\nThe Government of Uttar Pradesh used facial recognition and AI-powered crowd density monitoring, RFID wristbands as part of the security setup for the 2025 Prayag Maha Kumbh Mela. Plans to establish India's first AI lab aimed at strengthening cyber surveillance operations against deepfake threats and the country’s first tea auction driven by blockchain and AI, were announced by the Assam government in the state budget for 2025–2026. It will focus on real-time online threat detection, digital forensics, and cybersecurity. The lab will track cyberthreats, and assess misleading media.\nThe AI Competency Framework was unveiled on 6 March 2025 by Ashwini Vaishnaw, the Minister for Electronics and information Technology, in recognition of the crucial role AI plays in governance. Its goal is to provide public sector professionals with the necessary skills for upskilling programs and AI competency mapping. This framework guarantees informed AI policy-making and implementation by conforming to international best practices. To improve government officials' iGOT Karmayogi platform learning experience, a tailored content suggestion system driven by AI was also unveiled.\n\nSociety\nWadhwani AI, a $30 million nonprofit research institute, was established in 2018 by Romesh Wadhwani and Sunil Wadhwani with the goal of enhancing the lives of the world's poorest 2 billion people. New York University, the University of Southern California, India Institutes of Technology, and University of Mumbai will collaborate with the institute. The objective is to have solutions that show how AI can help the underprivileged, and then to build on those initial instances. Wadhwani AI will support universities from developing nations in expanding their AI capabilities. \nAs part of Microsoft's Global Skills for Social Impact charter, the ADVANTA(I)GE India program's first phase trained 2.4 million people in AI skills between 2024 and 2025; 74% of these persons came from tier 2 and tier 3 cities, and 65% of them were women. Another 10 million people will be trained by 2030 as part of the second phase.\nThe Ministry of Electronics and Information Technology launched IndiaAI Application Development Initiative on 6 March 2025. The aim is to create, scale, and encourage the usage of AI to improve the socioeconomic landscape of the country. In order to solve issues in the fields of healthcare, agriculture, governance, climate change, disaster management, and learning disabilities, IndiaAI has initiated the IndiaAI Innovation Challenge. Enhancing access to public services, improving healthcare, increasing agricultural productivity, helping people with learning disabilities, and lessening the effects of climate change are the goals. With the goal of lowering entry barriers in the field of AI, the IndiaAI FutureSkills Initiative will expand the number of AI courses offered in undergraduate, graduate, and doctoral level. To deliver foundational level training, IndiaAI Data Labs are being set up in tier 2 and tier 3 locations throughout the country. IndiaAI has created the two courses for the positions of Data Curator and Data Annotator, with an emphasis on industries like manufacturing, healthcare, education, and agriculture. These will be taught in IndiaAI Data Labs located throughout 28 centers of the National Institute of Electronics & Information Technology, and industrial training institutes.\nAn MoU was signed on 5 June 2025, between OpenAI and the IndiaAI Mission to establish OpenAI Academy, which would provide AI education training materials in Hindi, English, and four regional languages for students, developers, educators, civil servants, nonprofit leaders, and small business owners. It will integrate with IndiaAI FutureSkills Initiative and iGOT Karmayogi platform. One million teachers will receive training on how to use GenAI in the classroom. Additionally, 50 startups or fellows recognized by the IndiaAI Mission will get API credits from OpenAI worth up to $100,000. OpenAI has extended its AI for Impact Accelerator program by giving 11 NGOs a total of $150,000 in technical funding to develop AI solutions for underserved industries like healthcare, education, and agriculture.\n\nFinance and insurance\nIn India's banking, financial services and insurance (BFSI) industries, AI is already in use as of 2024 and the use of AI by BFSI companies is growing. Banks in India are using it for predictive analytics, fraud detection, and personalisation services. In capital markets, AI is being used for high-frequency trading and quantitative analysis. AI is being used by insurance companies for underwriting, fraud detection and damage assessment. However, with the growth of AI usage in the BFSI sector, the Indian government has also stated that AI, including deepfakes and prompt hacking of large language models, is being used for cyberattacks on BFSI companies.\n\nAI companies of India\nThe following is a list of notable AI companies of India, along with their corporate headquarters location.\n\nSafety and regulation\nIn order to determine whether to create an AI Safety Institute (AISI) that can establish standards, frameworks, and guidelines for AI development without serving as a regulatory body or stifling innovation, MeitY conducted consultation process with Meta Platforms, Google, Microsoft, IBM, OpenAI, NASSCOM, Broadband India Forum, Software Alliance, Indian Institutes of Technology, The Quantum Hub, Digital Empowerment Foundation, and Access Now on 7 October 2024. It was decided that instead of focusing on regulation, the AISI would focus on damage detection, risk identification, and standards setting for which interoperable systems are necessary to prevent the development of silos which may eventually influence future policies. The Safe and Trusted Pillar of the IndiaAI Mission has been allocated ₹20 crore, which the AISI may use for the inaugural budget. More money from the IndiaAI Mission's other verticals may be used in the future.\nIn order to develop an AI policy report tailored to India and assess the country's AI ecosystem's strengths and future prospects, UNESCO and MeitY began consulting on AI Readiness Assessment Methodology under Safety and Ethics in Artificial Intelligence from 2024. It is to encourage the ethical and responsible use of AI in industries. The study will help find areas where government can become involved, especially in attempts to strengthen institutional and regulatory capabilities.\nMinister for Electronics & Information Technology Ashwini Vaishnaw announced the creation of an IndiaAI Safety Institute on 30 January 2025, to ensure the ethical and safe application of AI models. The institute will promote domestic research and development that is grounded in India's social, economic, cultural, and linguistic diversity and is based on Indian datasets. With the help of academic and research institutions, as well as private sector partners, the institute will follow the hub-and-spoke approach to carry out projects within Safe and Trusted Pillar of the IndiaAI Mission. Among the major safety-related initiatives are eight ongoing projects to implement a techno-legal strategy to protect data privacy while conducting an ethical audit of algorithmic effectiveness.\nFirst round of projects are as follows: \n\nThemes for second round of projects include watermarking and labelling, ethical AI frameworks, AI risk-assessment and management, stress testing tools, and deepfake detection tools.\nInfosys released an open-source \"Responsible AI\" toolbox on February 26, 2025. The program, is a component of the Infosys Topaz Responsible AI Suite. It assists in identifying and stopping security risks, privacy violations, biased outputs, damaging content, copyright infringement, false information, deepfakes, and the use of malevolent AI. The toolkit can improve the transparency of outcomes produced by AI. It works with both on-premises and cloud systems. Citing concerns about the confidentiality of government data and documents, the Ministry of Finance issued an internal department caution advising staff not to use AI tools like ChatGPT and DeepSeek for work-related purposes. \nIn order to comply with the Digital Personal Data Protection Act, 2023 and other sovereignty requirements, OpenAI allowed the local storage of data for ChatGPT Enterprise, ChatGPT Edu, and OpenAI API platform customers from 8 May 2025. Talks to establish data center operations in India have already been started by OpenAI. \nGoogle launched its Safety Charter for India's AI-led transformation on 18 June 2025. Keeping end users safe, cybersecurity for government and enterprise infrastructure, and responsibly developing AI are the three core pillars that it outlines for addressing the difficulties of the online world in partnership with the larger ecosystem. In order to operationalize Google's Safety Charter, Google Safety Engineering Center in Hyderabad was established.\n\nSee also\n\nHCLTech\nInfosys\nTata Elxsi\nSarvam AI\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_India"
    },
    {
        "title": "Artificial intelligence in the 2024 United States presidential election",
        "text": "Artificial intelligence (AI) has been developed rapidly in recent years, and has been used by groups in the 2024 United States presidential election, as well as foreign groups such as China, Russia and Iran. There have also been efforts to control the use of generative artificial intelligence, such as those in California.\n\nUse in analysis and prediction\nArtificial intelligence has been used as a tool for data science, polling groups and data analysts have used artificial intelligence to analyze election data and make predictions\n\nUse by candidates\nBiden/Harris and Harris/Walz campaign\nTrump/Vance campaign\nPresidential candidate Donald Trump was criticized for his use of generative artificial intelligence to create imagery of pop singer Taylor Swift, suggesting her possible endorsement.\n\nForeign interference\nSee also\n2024 United States presidential election\nArtificial Intelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_the_2024_United_States_presidential_election"
    },
    {
        "title": "Artificial intelligence in the Brazilian industry",
        "text": "In 2022, 16.9% (1,620) of the 9,586 Brazilian industrial companies with 100 or more employees used artificial intelligence in their operations\nAmong the companies that used AI, the areas of administration (73.8%), product project development (65.9%), processes, services and marketing (65.1%) were those that used it the most, followed by the areas of production (56.4%) and logistics (48.4%).\n\nCurrent scenario\nAdoption in Brazilian industrial sectors\nIn senior management, the majority (56%) of executives have a long-term vision for its use. The study also shows that IT, Innovation, and Marketing are the areas where AI use is most widespread, and that 43% of companies are developing or adapting the algorithms they use.\nThe majority of large institutions that reported some type of AI use purchased these solutions from other companies (76%).\nSome factors for the adoption of artificial intelligence in companies include the establishment of an autonomous strategy by the company (87.0%), and the influence of suppliers and/or customers (63.0%) and the main difficulties in using technologies were high costs (80.8%), lack of qualified personnel in the company (54.6%) and excessive economic risks (49.5%).\nThree variables are considered the most relevant to explain the option to use AI: the implementation of a digital security policy, the size of companies with 250 or more employees and the characteristics of the company related to information and communication.\nWhen analyzing AI use by company size in Brazil, large companies have the highest proportion of AI use, mainly due to their investment capacity and technology experimentation. However, when comparing Brazil and Europe, indicators show an acceleration in AI use among large European companies, while in Brazil the situation remains stable. In 2023, 30% of large companies in the European bloc used some type of AI, a figure that rose to 41% in 2024, while in Brazil these proportions were 41% in 2023 and 38% in 2024.\n\nWorkforce\nThe challenge of upskilling begins with employees who are capable of understanding recent technological changes. Similarly, companies must create the environment and conditions for workforce development conducive to innovation, and universities must be prepared to provide knowledge aligned with the transition process, which in turn must be supported by public policies.\nThe concern with training a specialized workforce in AI can be seen in the low number of graduates and PhDs in computer science and computer engineering in Brazil, compared to the number shown in other countries.\nAs recorded in the document Recommendations for the Advancement of Artificial Intelligence in Brazil, 2019 data from the Coordination for the Improvement of Higher Education Personnel (CAPES) indicate that \"the number of PhDs graduated annually in computing remained below 400 in 2016, and is not expected to have increased during the Covid-19 pandemic\" (ABC, 2023). In the United States, by contrast, the number of PhDs graduated in these two areas has remained around 1,800 for the past 11 years, and during this period, the number of PhDs specializing in AI jumped from 10% to 19%.\nBased on data from the CNPq Lattes Platform (October 2019), it is possible to observe that the number of professionals in the AI field in Brazil is 4,429 specialists. This is still a small number compared to the 415,166 IT jobs in the country's business sector alone.\n\nR&D, scientific production and integration with industry\nChina and the United States lead in the number of publications. These two countries are followed by the G7 members: India, Austria, South Korea, and Spain. Brazil appears in the next group, alongside the Netherlands, Russia, Indonesia, and Ireland.\nRegarding the promotion of research and technologies related to AI, public entities such as the Coordination for the Improvement of Higher Education Personnel (Capes) and the National Council for Scientific and Technological Development (CNPq) stood out as the main funders.\nCurrently, different countries and territories have been promoting the development of Artificial Intelligence (AI). In the Brazilian case, one of the main initiatives is the creation of Engineering Research Centers/Applied Research Centers (CPE/CPA) in AI by the São Paulo Research Foundation (FAPESP), in collaboration with the Ministry of Science, Technology and Innovation (MCTI), the Ministry of Communications (MC) and the Brazilian Internet Steering Committee (CGI.br).\nIn terms of the number of patents filed and the volume of investments, the leading nations in AI are the United States, China, France, Germany, the United Kingdom, Russia, India, Switzerland, Japan, South Korea, the Netherlands, Sweden, Finland, Ireland, Singapore, Canada, Israel, and Italy. Brazil appears among the top twenty countries in some rankings, mainly due to its good number of publications (approximately 10% of the number of articles published by the United States).\nThe US is home to approximately 60% of the world's top AI researchers, followed by China (11%), Europe (10%), and Canada (6%).\nTo change this scenario, in August 2024, the Brazilian government announced an investment of R$23 billion until 2028 in artificial intelligence, seeking to “transform the country into a global reference in innovation”.\n\nFuture challenges\nThe Organization for Economic Cooperation and Development (2020) report highlighted three factors that hinder the digital transformation journey and application of AI in Brazil: insufficient infrastructure, high costs due to the tax system, and financial limitations, such as limited access to financing.\nThe costs of adopting technology, its incompatibility with the business, and the lack of training also represent obstacles that Brazilian industry must overcome.\nThere are also inherent obstacles for companies. A McKinsey review emphasizes that once a company chooses one or more sectors to focus on, it must select specific applications. Buyers aren't interested in artificial intelligence simply because it's a breakthrough technology; they want AI to generate a good return on investment, whether by solving specific problems, saving money, or increasing sales. If an AI vendor tried to offer a horizontal solution, the value proposition might not be as compelling.\nPart of the solution to Brazil's technological backwardness involves building an ecosystem fueled by private institutions, universities, and governments.\n\nSee also\nIndustry in Brazil\nMachine Learning\nDeep learning\nRegulation of artificial intelligence\nBrazilian Silicon Valley\nCEITEC\nNational Laboratory of Scientific Computation (Brazil)\n\nReferences\nExternal links\nUniversity of São Paulo (USP)\nFederal University of Santa Catarina (UFSC)\nState University of Campinas (UNICAMP)\nFAPESP – São Paulo Research Foundation\nCNPq – National Council for Scientific and Technological Development\nOECD\n\nFurther reading\nRao, S. V. A., Kondaiah, K., Chandra, G. R., & Kumar, K. K. (2017). A Survey on Machine Learning: Concept, Algorithms and Applications. International Conference on Innovative Research in Computer and Communication Engineering. ISSN 2302-9708\nPedro, E. D. A., Panizzon, M., & Weber, C. G. (2023, November). OHS Professionals AI Adoption: A UTAUT Research in Brazilian Industry. In 2023 15th IEEE International Conference on Industry Applications (INDUSCON) (pp. 850–857). IEEE.\nAniceto, D. K. (2025). The Role of Artificial Intelligence (AI) and Machine Learning (MI) in the Oil and Gas Industry. Journal of Technology and Systems, 7(1), 6–27.\nCardoso, L. A. S., Mesquita, B. D. R. de, & Farias, P. R. S. (2025). Use of machine learning algorithms in the context of sugarcane in Brazil: a review. Iran Journal of Computer Science. https://doi.org/10.1007/s42044-025-00250-y.\nLeite Coelho da Silva, F., da Costa, K., Canas Rodrigues, P., Salas, R., & López-Gonzales, J. L. (2022). Statistical and Artificial Neural Networks Models for Electricity Consumption Forecasting in the Brazilian Industrial Sector. Energies, 15, 588 . https://doi.org/10.3390/en15020588. DOI: 10.3390/en15020588",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_the_Brazilian_industry"
    },
    {
        "title": "Artificial intelligence industry in China",
        "text": "The artificial intelligence industry in the People's Republic of China is a rapidly developing multi-billion dollar industry. The roots of China's AI development started in the late 1970s following Deng Xiaoping's economic reforms emphasizing science and technology as the country's primary productive force.\nThe initial stages of China's AI development were slow and encountered significant challenges due to  lack of resources and talent. At the beginning China was behind most Western countries in terms of AI development. A majority of the research was led by scientists who had received higher education abroad.\nSince 2006, the government of the People's Republic of China has steadily developed a national agenda for artificial intelligence development and emerged as one of the leading nations in artificial intelligence research and development. In 2016, the Chinese Communist Party (CCP) released its thirteenth five-year plan in which it aimed to become a global AI leader by 2030.\nThe State Council has a list of \"national AI teams\" including fifteen China-based companies, including Baidu, Tencent, Alibaba, SenseTime, and iFlytek. Each company should lead the development of a designated specialized AI sector in China, such as facial recognition, software/hardware, and speech recognition. China's rapid AI development has significantly impacted Chinese society in many areas, including the socio-economic, military, intelligence, and political spheres. Agriculture, transportation, accommodation and food services, and manufacturing are the top industries that would be the most impacted by further AI deployment.\nThe private sector, university laboratories, and the military are working collaboratively in many aspects as there are few current existing boundaries. In 2021, China published the Data Security Law of the People's Republic of China, its first national law addressing AI-related ethical concerns. In October 2022, the United States federal government announced a series of export controls and trade restrictions intended to restrict China's access to advanced computer chips for AI applications.\nConcerns have been raised about the effects of the Chinese government's censorship regime on the development of generative artificial intelligence and talent acquisition with state of the country's demographics. Others have noted that official notions of AI safety require following the priorities of the CCP and are antithetical to standards in democratic societies.\n\nHistory\nThe research and development of artificial intelligence in China started in the 1980s, with the announcement by Deng Xiaoping of the importance of science and technology for China's economic growth.\n\nLate 1970s to early 2010s\nArtificial intelligence research and development did not start until the late 1970s after Deng Xiaoping's economic reforms. While there was a lack of AI-related research between the 1950s and 1960s, some scholars believe this is due to the influence of cybernetics from the Soviet Union despite the Sino-Soviet split during the late 1950s and early 1960s. In the 1980s, a group of Chinese scientists launched AI research led by Qian Xuesen and Wu Wenjun. However, during the time, China's society still had a generally conservative view towards AI. Early AI development in China was difficult so China's government approached these challenges by sending Chinese scholars overseas to study AI and further providing government funds for research projects. The Chinese Association for Artificial Intelligence (CAAI) was founded in September 1981 and was authorized by the Ministry of Civil Affairs. The first chairman of the executive committee was Qin Yuanxun, who received a PhD in philosophy from Harvard University.  In 1987, China's first research publication on artificial intelligence was published by Tsinghua University. Beginning in 1993, smart automation and intelligence have been part of China's national technology plan.\nSince the 2000s, the Chinese government has further expanded its research and development funds for AI and the number of government-sponsored research projects has dramatically increased. In 2006, China announced a policy priority for the development of artificial intelligence, which was included in the National Medium and Long Term Plan for the Development of Science and Technology (2006–2020), released by the State Council. In the same year, artificial intelligence was also mentioned in the eleventh five-year plan.\nIn 2011, the Association for the Advancement of Artificial Intelligence (AAAI) established a branch in Beijing, China. At same year, the Wu Wenjun Artificial Intelligence Science and Technology Award was founded in honor of Chinese mathematician Wu Wenjun, and it became the highest award for Chinese achievements in the field of artificial intelligence. The first award ceremony was held on May 14, 2012. In 2013, the International Joint Conferences on Artificial Intelligence (IJCAI) was held in Beijing, marking the first time the conference was held in China. This event coincided with the Chinese government's announcement of the \"Chinese Intelligence Year,\" a significant milestone in China's development of artificial intelligence.\n\nLate 2010s to early 2020s\nThe State Council of China issued \"A Next Generation Artificial Intelligence Development Plan\" (State Council Document [2017] No. 35) on 20 July 2017. In the document, the CCP Central Committee and the State Council urged governing bodies in China to promote the development of artificial intelligence. Specifically, the plan described AI as a strategic technology that has become a \"focus of international competition\".:2 The document urged significant investment in a number of strategic areas related to AI and called for close cooperation between the state and private sectors. On the occasion of CCP general secretary Xi Jinping's speech at the first plenary meeting of the Central Military-Civil Fusion Development Committee (CMCFDC), scholars from the National Defense University wrote in the PLA Daily that the \"transferability of social resources\" between economic and military ends is an essential component to being a great power. During the Two Sessions 2017,\"artificial intelligence plus\" was proposed to be elevated to a strategic level. The same year witnessed the emergence of multiple application-level usages in the medical field according to reports. Furthermore, the Chinese Academy of Sciences (CAS) established their AI processor chip research lab in Nanjing, and introduced their first AI specialization chip, Cambrian.\nIn 2018, Xinhua News Agency, in partnership with Tencent's subsidiary Sogou, launched its first artificial intelligence-generated news anchor.\nIn 2018, the State Council budgeted $2.1 billion for an AI industrial park in Mentougou district. In order to achieve this the State Council stated the need for massive talent acquisition, theoretical and practical developments, as well as public and private investments. Some of the stated motivations that the State Council gave for pursuing its AI strategy include the potential of artificial intelligence for industrial transformation, better social governance and maintaining social stability. As of the end of 2020, Shanghai's Pudong District had 600 AI companies across foundational, technical, and application layers, with related industries valued at around 91 billion yuan.\nIn 2019, the application of artificial intelligence expanded to various fields such as quantum physics, geography, and medical research. With the emergence of large language models (LLMs), at the beginning of 2020, Chinese researchers began developing their own LLMs. One such example is the multimodal large model called 'Zidongtaichu.'\nThe Beijing Academy of Artificial Intelligence launched China's first large scale pre-trained language model in 2022.\nIn November 2022, the Cyberspace Administration of China (CAC), Ministry of Industry and Information Technology, and the Ministry of Public Security jointly issued the regulations concerning deepfakes, which became effective in January 2023.\nIn July 2023, Huawei released its version 3.0 of its Pangu LLM.\nIn July 2023, China released its Interim Measures for the Administration of Generative Artificial Intelligence Services. A draft proposal on basic generative AI services safety requirements, including specifications for data collection and model training was issued in October 2023.\nAlso in October 2023, the Chinese government launched its Global AI Governance Initiative, which frames its AI policy as part of a Community of Common Destiny and aims to build AI policy dialogue with developing countries. The Initiative has expressed concern over AI safety risks, including abuse of data or the use of AI by terrorists.\nIn 2024, Spamouflage, an online disinformation and propaganda campaign of the Ministry of Public Security, began using news anchors created with generative artificial intelligence to deliver fake news clips.\nIn March 2024, Premier Li Qiang launched the AI+ Initiative, which intends to integrate AI into China's real economy.\nIn May 2024, the Cyberspace Administration of China announced that it rolled out a large language model trained on Xi Jinping Thought.\nAccording to the 2024 report from the International Data Corporation (IDC), Baidu AI Cloud holds China's largest LLM market share with 19.9 percent and US$49 million in revenue over the last year. This was followed by SenseTime, with 16 percent market share, and by Zhipu AI, as the third largest. The fourth and fifth largest were Baichuan and the Hong-Kong listed AI company 4Paradigm respectively. Baichuan, Zhipu AI, Moonshot AI and MiniMax were praised by investors as China's new \"AI Tigers\". In April 2024, 117 generative AI models had been approved by the Chinese government.\nAs of 2024, many Chinese technology firms such as Zhipu AI and Bytedance have launched AI video-generation tools to rival OpenAI's Sora.\n\nChronology of major AI-related policies\nGovernment goals\nAccording to a February 2019 publication by the Center for a New American Security, CCP general secretary Xi Jinping – believes that being at the forefront of AI technology will be critical to the future of global military and economic power competition. By 2025, the State Council aims for China to make fundamental contributions to basic AI theory and to solidify its place as a global leader in AI research. Further, the State Council aims for AI to become \"the main driving force for China's industrial upgrading and economic transformation\" by this time. By 2030, the State Council aims to have China be the global leader in the development of artificial intelligence theory and technology. The State Council claims that China will have developed a \"mature new-generation AI theory and technology system.\"\nAccording to academics Karen M. Sutter and Zachary Arnold, the Chinese government \"seeks to meld state planning and control while some operational flexibility for firms. In this context, China's AI firms are hybrid players. The state guides their activity, funds, and shields them from foreign competition through domestic market protections, creating asymmetric advantages as they expand offshore.\"\nThe CCP's fourteenth five-year plan reaffirmed AI as a top research priority and ranks AI first among \"frontier industries\" that the Chinese government aims to focus on through 2035. The AI industry is a strategic sector often supported by China's government guidance funds. Due to security concerns around strategically sensitive economic sectors, the government dissuades executives at Chinese AI companies to travel to the U.S. and, if required to travel, to brief authorities before and after travel.\n\nResearch and development\nChinese public AI funding mainly focused on advanced and applied research. The government funding also supported multiple AI R&D in the private sector through venture capitals that are backed by the state. Much analytic agency research showed that, while China is massively investing in all aspects of AI development, facial recognition, biotechnology, quantum computing, medical intelligence, and autonomous vehicles are AI sectors with the most attention and funding.\nAccording to national guidance on developing China's high-tech industrial development zones by the Ministry of Science and Technology, there are fourteen cities and one county selected as an experimental development zone. Zhejiang and Guangdong provinces have the most AI innovation in experimental areas. However, the focus of AI R&D varied depending on cities and local industrial development and ecosystem. For instance, Suzhou, a city with a longstanding strong manufacturing industry, heavily focuses on automation and AI infrastructure while Wuhan focuses more on AI implementations and the education sector. In connection with universities, tech firms, and national ministries, Shenzhen and Hangzhou each co-founded generative AI labs.\nIn 2016 and 2017, Chinese teams won the top prize at the Large Scale Visual Recognition Challenge, an international competition for computer vision systems. Many of these systems are now being integrated into China's domestic surveillance network.\nInterdisciplinary collaborations play an essential role in China's AI R&D, including academic-corporate collaboration, public-private collaborations, and international collaborations and projects with corporate-government partnerships are the most common. China ranked in the top three worldwide following the United States and the European Union for the total number of peer-reviewed AI publications that are produced under a corporate-academic partnership between 2015 and 2019. Besides, according to an AI index report, China surpassed the U.S. in 2020 in the total number of global AI-related journal citations. In terms of AI-related R&D, China-based peer-reviewed AI papers are mainly sponsored by the government. In May 2021, China's Beijing Academy of Artificial Intelligence released the world's largest pre-trained language model (WuDao).\nAs of 2023, 47% of the world's top AI researchers had completed their undergraduate studies in China.\nAccording to academic Angela Huyue Zhang, publishing in 2024, while the Chinese government has been proactive in regulating AI services and imposing obligations on AI companies, the overall approach to its regulation is loose and demonstrates a pro-growth policy favorable to China's AI industry. In July 2024, the government opened its first algorithm registration center in Beijing.\n\nPopulation\nChina's large population generates a massive amount of accessible data for companies and researchers, which offers a crucial advantage in the race of big data. As of 2024, China has the world's largest number of internet users, generating huge amounts of data for machine learning and AI applications.\n\nFacial recognition\nFacial recognition is one of the most widely employed AI applications in China. Collecting these large amounts of data from its residents helps further train and expand AI capabilities. China's market is not only conducive and valuable for corporations to further AI R&D but also offers tremendous economic potential attracting both international and domestic firms to join the AI market. The drastic development of the information and communication technology (ICT) industry and AI chipsets in recent years are two examples of this. China has become the world's largest exporter of facial recognition technology, according to a January 2023 Wired report.\n\nCensorship and content controls\nIn April 2023, the Cyberspace Administration of China (CAC) issued draft measures stating that tech companies will be obligated to ensure AI-generated content upholds the ideology of the CCP including Core Socialist Values, avoids discrimination, respects intellectual property rights, and safeguards user data. Under these draft measures, companies bear legal responsibility for training data and content generated through their platforms. In October 2023, the Chinese government mandated that generative artificial intelligence-produced content may not \"incite subversion of state power or the overthrowing of the socialist system.\" Before releasing a large language model to the public, companies must seek approval from the CAC to certify that the model refuses to answer certain questions relating to political ideology and criticism of the CCP. Questions related to politically sensitive topics such as the 1989 Tiananmen Square protests and massacre or comparisons between Xi Jinping and Winnie the Pooh must be declined.\nIn 2023, in-country access was blocked to Hugging Face, a company that maintains libraries containing training data sets commonly used for large language models. A subsidiary of the People's Daily, the official newspaper of the Central Committee of the Chinese Communist Party, provides local companies with training data that CCP leaders consider permissible. In 2024, the People's Daily released a LLM-based tool called Easy Write.\nMicrosoft has warned that the Chinese government uses generative artificial intelligence to interfere in foreign elections by spreading disinformation and provoking discussions on divisive political issues.\nThe Chinese artificial intelligence model DeepSeek has been reported to refuse to answer questions relating to things about the 1989 Tiananmen Square protests and massacre, persecution of Uyghurs, comparisons between Xi Jinping and Winnie the Pooh or human rights in China.\n\nLeading companies\nLeading AI-centric companies and start-ups include Baidu, Tencent, Alibaba, SenseTime, 4Paradigm and Yitu Technology. Chinese AI companies iFlytek, SenseTime, Cloudwalk and DJI have received attention for facial recognition, sound recognition and drone technologies.\nChina's government takes a market-oriented approach to AI, and has sought to encourage private tech companies in developing AI. In 2018, it designated Baidu, Alibaba, iFlytek, Tencent, and SenseTime as \"AI champions\".\nIn 2023, Tencent debuted its large language model Hunyuan for enterprise use on Tencent Cloud.\nNew leading AI startups include Baichuan, Zhipu AI, Moonshot AI and MiniMax which were praised by investors as China's new \"AI Tigers\" in 2024. 01.AI has also been touted as a leading startup.\nIn January 2025, DeepSeek launched its model DeepSeek-R1 and surprised the Western world. Its performance with minimal hardware is comparable to the leading models in the US. DeepSeek is a subsidiary of High-Flyer, a privately owned company in Hangzhou, Zhejiang.\n\nImpact\nEconomic impact\nSome analysts hold optimistic views about AI's economic impact on China's long-term economic growth. In the past, traditional industries in China have struggled with the increase in labor costs due to the growing aging population in China and the low birth rate. With the deployment of AI, operational costs are expected to reduce while an increase in efficiency generates revenue growth. Some highlight the importance of a clear policy and governmental support in order to overcome adoption barriers including costs and lack of properly trained technical talents and AI awareness. However, there are concerns about China's deepening income inequality and the ever-expanding imbalanced labor market in China. Low- and medium-income workers might be the most negatively impacted by China's AI development because of rising demands for laborers with advanced skills. Furthermore, China's economic growth might be disproportionately divided as a majority of AI-related industrial development is concentrated in coastal regions rather than inland.\nAn influential decision by the Beijing Internet Court has ruled that AI-generated content is entitled to copyright protection.\n\nMilitary impact\nChina is investing heavily in artificial intelligence for military and intelligence purposes. China seeks to build a \"world-class\" military by \"intelligentization\" with a particular focus on the use of unmanned weapons and artificial intelligence. It is researching various types of air, land, sea, and undersea autonomous vehicles. In the spring of 2017, a civilian Chinese university with ties to the military demonstrated an AI-enabled swarm of 1,000 uninhabited aerial vehicles at an airshow. A media report released afterwards showed a computer simulation of a similar swarm formation finding and destroying a missile launcher.:23 Open-source publications indicated that China is also developing a suite of AI tools for cyber operations.:27 Chinese development of military AI is largely influenced by China's observation of U.S. plans for defense innovation and fears of a widening \"generational gap\" in comparison to the U.S. military. Similar to U.S. military concepts, China aims to use AI for exploiting large troves of intelligence, generating a common operating picture, and accelerating battlefield decision-making.:12-14 The Chinese Multi-Domain Precision Warfare (MDPW) is considered China's response to the U.S. Joint All-Domain Command and Control (JADC2) strategy, which seeks to integrate sensors and weapons with AI and a vigorous network.\nTwelve categories of military applications of AI have been identified: UAVs, USVs, UUVs, UGVs, intelligent munitions, intelligent satellites, ISR (Intelligence, Surveillance and Reconnaissance) software, automated cyber defense software, automated cyberattack software, decision support, software, automated missile launch software, and cognitive electronic warfare software.\nChina's management of its AI ecosystem contrasts with that of the United States.:6 In general, few boundaries exist between Chinese commercial companies, university research laboratories, the military, and the central government. As a result, the Chinese government has a direct means of guiding AI development priorities and accessing technology that was ostensibly developed for civilian purposes. To further strengthen these ties the Chinese government created a Military-Civil Fusion Development Commission which is intended to speed the transfer of AI technology from commercial companies and research institutions to the military in January 2017.:19 In addition, the Chinese government is leveraging both lower barriers to data collection and lower costs of data labeling to create the large databases on which AI systems train. According to one estimate, China is on track to possess 20% of the world's share of data by 2020, with the potential to have over 30% by 2030.:12\nChina's centrally directed effort is investing in the U.S. AI market, in companies working on militarily relevant AI applications, potentially granting it lawful access to U.S. technology and intellectual property. Chinese venture capital investment in U.S. AI companies between 2010 and 2017 totaled an estimated $1.3 billion. In September 2022, the U.S. Biden administration issued an executive order to prevent foreign investments, \"particularly those from competitor or adversarial nations,\" from investing in U.S. technology firms, due to U.S. national security concerns. The order covers fields of U.S. technologies in which Chinese government has been investing, including \"microelectronics, artificial intelligence, biotechnology and biomanufacturing, quantum computing, [and] advanced clean energy.\"\nIn 2024, researchers from the People's Liberation Army Academy of Military Sciences were reported to have developed a military tool using Llama, which Meta Platforms said was unauthorized due to its model use prohibition for military purposes. In March 2025, the U.S. Commerce Department added the Beijing Academy of Artificial Intelligence and Beijing Innovation Wisdom Technology Co. to the Entity List for allegedly developing technology for military purposes.\n\nAcademia\nAlthough in 2004, Peking University introduced the first academic course on AI which led other Chinese universities to adopt AI as a discipline, especially since China faces challenges in recruiting and retaining AI engineers and researchers. Over half of the data scientists in the United States have been working in the field for over 10 years, while roughly the same proportion of data scientists in China have less than 5 years of experience. As of 2017, fewer than 30 Chinese Universities produce AI-focused experts and research products.:8 Although China surpassed the United States in the number of research papers produced from 2011 to 2015, the quality of its published papers, as judged by peer citations, ranked 34th globally. China especially want to address military applications and so the Beijing Institute of Technology, one of China's premier institutes for weapons research, recently established the first children's educational program in military AI in the world.\nIn 2019, 34% of Chinese students studying in the AI field stayed in China for work. According to a database maintained by an American thinktank, the percentage increased to 58% in 2022.\n\nEthical concerns\nFor the past years, there are discussions about AI safety and ethical concerns in both private and public sectors. In 2021, China's Ministry of Science and Technology published the first national ethical guideline, 'the New Generation of Artificial Intelligence Ethics Code' on the topic of AI with specific emphasis on user protection, data privacy, and security. This document acknowledges the power of AI and quick technology adaptation by the big corporations for user engagements. The South China Morning Post reported that humans shall remain in full decision-making power and rights to opt-in/-out. Before this, the Beijing Academy of Artificial Intelligence published the Beijing AI principles calling for essential needs in long-term research and planning of AI ethical principles.\nData security has been the most common topic in AI ethical discussion worldwide, and many national governments have established legislation addressing data privacy and security. The Cybersecurity Law of the People's Republic of China was enacted in 2017 aiming to address new challenges raised by AI development. In 2021, China's new Data Security Law (DSL) was passed by the PRC congress, setting up a regulatory framework classifying all kinds of data collection and storage in China. This means all tech companies in China are required to classify their data into categories listed in Digital Subscriber Line (DSL) and follow specific guidelines on how to govern and handle data transfers to other parties.\nSome observers have noted that notions of AI safety in the PRC entails following the priorities of the CCP and is antithetical to standards in democratic societies.\n\nJudicial system\nIn 2019, the city of Hangzhou established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims. Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.\nBecause some controversial cases that drew public criticism for their low punishments have been withdrawn from China Judgments Online, there are concerns about whether AI based on fragmented judicial data can reach unbiased decisions. Zhang Linghan, professor of law at the China University of Political Science and Law, writes that AI-technology companies may erode judicial power. Some scholars argued that “increasing party leadership, political oversight, and reducing the discretionary space of judges are intentional goals of SCR [smart court reform].\"\n\nAssessment\nAcademic Jinghan Zeng argued the Chinese government's commitment to global AI leadership and technological competition was driven by its previous underperformance in innovation which was seen by the CCP as a part of the century of humiliation. According to Zeng, there are historically embedded causes of China's anxiety towards securing an international technological dominance – China missed both industrial revolutions, the one starting in Britain in the mid-18th century, and the one that originated in America in the late-19th century. Therefore, China's government desires to take advantage of the technological revolution in today's world led by digital technology including AI to resume China's \"rightful\" place and to pursue the national rejuvenation proposed by Xi Jinping.\nAn article published by the Center for a New American Security concluded that \"Chinese government officials demonstrated remarkably keen understanding of the issues surrounding AI and international security. This includes knowledge of the U.S. AI policy discussions,\" and recommended that \"the U.S. policymaking community to similarly prioritize cultivating expertise and understanding of AI developments in China\" and \"funding, focus, and a willingness among U.S. policymakers to drive large-scale necessary change.\" An article in the MIT Technology Review similarly concluded: \"China might have unparalleled resources and enormous untapped potential, but the West has world-leading expertise and a strong research culture. Rather than worry about China's progress, it would be wise for Western nations to focus on their existing strengths, investing heavily in research and education.\"\nThe Chinese government's censorship regime has stunted the development of generative artificial intelligence.\nIn a 2021 text, the Research Centre for a Holistic Approach to National Security at the China Institutes of Contemporary International Relations wrote that  the development of AI creates challenges for holistic national security, including the risks that AI will heighten social tensions or have destabilizing effects on international relations.\nWriting from a Chinese Marxist view, academics including Gao Qiqi and Pan Enrong contend that capitalist application of AI will lead to greater oppression of workers and more serious social problems. Gao cites how the development of AI has increased the power of platform companies like Meta, Twitter, and Alphabet, leading to greater capital accumulation and political power in fewer economic actors. According to Gao, the state should be the primary responsible actor in the area of generative AI (creating new content like music or video). Gao writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects.\nDialogues between Chinese and Western AI experts about the existential risk from artificial intelligence have taken place.\n\nPublic polling\nThe Chinese public is generally optimistic regarding AI. A 2021 study conducted across 28 countries found that 78% of the Chinese public believes the benefits of AI outweigh the risks, the highest of any country in the study. In 2024, a survey of elite Chinese university students found that 80% agreed or strongly agreed that AI will do more good than harm for society, and 31% believed it should be regulated by the government.\n\nHuman rights\nThe widely used AI facial recognition has raised concerns. According to The New York Times, deployment of AI facial recognition technology in the Xinjiang region to detect Uyghurs is \"the first known example of a government intentionally using artificial intelligence for racial profiling,” which is said to be “one of the most striking examples of digital authoritarianism.\" Researchers have found that in China, areas experiencing higher rates of unrest are associated with increased state acquisition of AI facial recognition technology, especially by local municipal police departments.\n\nSee also\nArtificial intelligence\nArtificial intelligence arms race\nChina Brain Project\nFifth generation computer\nList of artificial intelligence companies\nRegulation of artificial intelligence\n\nReferences\nFurther reading\nHannas, William C.; Chang, Huey-Meei, eds. (29 July 2022). Chinese Power and Artificial Intelligence: Perspectives and Challenges (1st ed.). London: Routledge. doi:10.4324/9781003212980. ISBN 9781003212980. OCLC 1320821529.",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_industry_in_China"
    },
    {
        "title": "Artificial intelligence industry in Italy",
        "text": "The artificial intelligence industry in Italy is growing and supports industrial development. In 2024 it reached a new record, reaching 1.2 billion euros with a growth of +58% compared to 2023.\n\nHistory\nThe roots of AI research in Italy extend back to the 1970s, when Italian scholars began exploring automated reasoning, programming language semantics, and pattern recognition. Researchers such as those involved in early projects at the National Research Council and various universities laid the groundwork for subsequent academic and industrial developments in the field.\nDuring this period, the focus was predominantly on developing algorithms for automated theorem proving and building systems to reason about complex mathematical problems. This era witnessed the birth of methodologies that would later influence numerous AI subfields, from natural language processing (NLP) to robotics.\n\nInstitutional milestones and academic contributions\nA turning point in the Italian AI landscape was the formation of the Italian Association for Artificial Intelligence (AIxIA) in 1988. Founded by academics, including Luigia Carlucci Aiello, the association established a platform for collaboration between universities, research centers, and industry. Led by Aiello, AIIA played a role in promoting research, organizing national conferences, and fostering international partnerships that connected Italy’s AI community to global networks.\nAt the same time, professors such as Roberto Navigli and numerous practitioners contributed to the advancement of AI in Italy. Navigli has worked in multilingual NLP, including the creation of BabelNet, and led the Minerva project.\n\nIndustrial AI\nOver recent decades, numerous national and European initiatives supported by funding from programs such as the National Recovery and Resilience Plan (PNRR) have spurred the transition from theoretical research to practical applications. Industrial sectors including manufacturing, banking, and healthcare increasingly embraced AI-driven automation, while research institutions collaborated with industrial partners to deploy cutting-edge solutions.\nIn recent years, Italy has also seen the establishment of specialized research centers and institutes aimed at bridging the gap between academic innovation and industrial application. These initiatives indicate a broader national commitment to integrating AI into the fabric of Italian industry.\n\nRecent developments\nEmergence of generative AI\nA landmark in Italy’s modern AI evolution is the development of Minerva AI. Developed by the Sapienza NLP research group at Sapienza University of Rome and led by Professor Roberto Navigli, Minerva represents the first family of large language models (LLMs) trained from scratch with a primary focus on the Italian language.\n\nMinerva 7B\nThe latest iteration, Minerva 7B, has 7 billion parameters and has been trained on an extensive corpus of over 1.5 trillion words. By using advanced instruction tuning techniques, Minerva 7B is able to produce highly accurate, coherent, and contextually sensitive responses addressing common issues such as hallucinations and inappropriate content generation. This breakthrough sets a benchmark for transparent, open-source AI development in the country. Minerva’s development, carried out within the FAIR (Future Artificial Intelligence Research) project in collaboration with CINECA and supported by supercomputing resources like the Leonardo (supercomputer), aligns closely with Italy’s cultural and linguistic heritage.\n\nEsstablishment of AI4I\nThe recent establishment of the Istituto Italiano per l’Intelligenza Artificiale (AI4I) is part of Italy's strategy to improve its industrial competitiveness in AI. This dedicated institute aims to bridge the gap between research institutions and industrial enterprises; promote training and R&D support to nurture the next generation of Italian AI experts; and enhance national competitiveness.\nThis initiative is expected to serve as a hub for applied AI research, driving innovations that are tailored to the specific needs of Italian industry and public administration.\n\nBenefits of InvestAI\nItaly's AI industry stands to benefit from the European InvestAI initiative, a plan unveiled at the recent AI Action Summit in Paris. InvestAI is an effort by the European Commission to mobilize €200 billion for AI investments, with a dedicated €20 billion fund earmarked for building AI gigafactories. These gigafactories are planned as large-scale hubs for training advanced, complex AI models using approximately 100,000 last-generation AI chips.\n\nFor Italy, this investment presents several major opportunities:\n\nAccess to State-of-the-Art Infrastructure: Italian companies, research institutions, and start-ups can leverage the gigafactories’ immense computational resources, enabling them to train highly sophisticated language models and other AI systems.\nEnhanced Competitiveness and Collaboration: With InvestAI’s layered funding model where EU funds help de-risk private investments Italian firms can access capital more readily. This will bolster public–private partnerships and create a more dynamic AI ecosystem that spans from academic research to industrial applications.\nAlignment with National and Regional Initiatives: The Istituto Italiano per l’Intelligenza Artificiale (AI4I), based in Turin, is already recognized as a strategic asset by both Italy and the European Union. As the main recipient of InvestAI funds in Italy, AI4I will play a pivotal role in implementing these investments locally, fostering innovation in sectors like manufacturing, healthcare and aerospace.\nCommission President Ursula von der Leyen emphasized that InvestAI is designed to democratize AI innovation throughout Europe by ensuring that even smaller companies have access to high-performance computing power. For Italy, this means not only keeping pace with global leaders but also harnessing European-scale investments to transform its AI industry and drive economic growth.\n\nSee also\nConsiglio Nazionale delle Ricerche (CNR)\nCINECA\nMinerva AI (Sapienza University of Rome)\nVelvet AI (Almawave)\nVitruvian-1 (ASC27)\nIstituto Italiano per l'Intelligenza Artificiale nell'Industria (AI4I)\nIstituto Italiano di Tecnologia (IIT)\nAgenzia per la Cybersicurezza Nazionale\nEuropean High-Performance Computing Joint Undertaking\nArtificial Intelligence Act (AI Act)\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_industry_in_Italy"
    },
    {
        "title": "Artificial intelligence industry in the United Kingdom",
        "text": "As of 2025, the artificial intelligence (AI) market in the United Kingdom is worth over £21 billion, and is expected to exceed £1 trillion by 2035. It is the world's third-largest AI market and consistently ranked 3rd in private AI funding between 2013 and 2024, both behind the United States and China. The country ranked third globally in a 2024 report on AI development by Stanford University.\n\nAI strategy\nThe UK government has proposed a plan to boost the country's AI infrastructure and expand its use in public services, with the UK Prime Minister Keir Starmer saying that the country would become one of the \"AI superpowers\".\nThe United Kingdom's AI strategy aims to balance safety and innovation; unlike the European Union which adopted the AI Act, the UK is reluctant to legislate early, considering that it may lower the sector's growth, and that laws might be rendered obsolete by technological progress.\nIn July 2025, the UK government published its UK Compute Roadmap outlining up to £2 billion for a modern public compute ecosystem and a 20-fold expansion of the AI Research Resource by 2030, alongside AI Growth Zones. Also in July, the UK government signed a deal with OpenAI to use its products in public services to increase productivity.\n\nHistory\nThe British computer scientist Alan Turing laid the theoretical groundwork for artificial intelligence as a field of research with his 1950 paper, Computing Machinery and Intelligence.\nThe United Kingdom founded in April 2023 a safety organisation called Frontier AI Taskforce, with an initial budget of £100 million. In November 2023, it evolved into the UK AISI, and continued to be led by Ian Hogarth. The AISI is part of the United Kingdom's Department for Science, Innovation and Technology.\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.\n\nCompanies\nThe following is a list of UK-based AI companies:\n\nMind Foundry\nPeak\nSynthesia\nStability AI\nDeepmind\nIsomorphic Labs\n\nResearchers\nSee also\nArtificial intelligence industry in India\nArtificial intelligence industry in Italy\nParliamentary Under-Secretary of State for AI and Digital Government\nScience and technology in the United Kingdom\n\nReferences\nExternal links\nArtificial Intelligence – UK Government",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_industry_in_the_United_Kingdom"
    },
    {
        "title": "Artificial intelligence of things",
        "text": "The Artificial Intelligence of Things (AIoT) is the combination of artificial intelligence (AI) technologies with the Internet of things (IoT) infrastructure to achieve more efficient IoT operations, improve human-machine interactions and enhance data management and analytics.\nIn 2018, KPMG published a foresight study on the future of AI including scenarios until 2040. The analysts describe a scenario in detail where a community of things would see each device also contain its own AI that could link autonomously to other AIs to, together, perform tasks intelligently. Value creation would be controlled and executed in real-time using swarm intelligence. Many industries could be transformed with the application of swarm intelligence, including: automotive, cloud, medical, military, research, and technology.\nIn the AIoT an important facet is AI being done on some Thing. In its purest form this involves performing the AI on the device, i.e. at the edge or Edge Computing, with no need for external connections. There is no need for an Internet in AIoT, it is an evolution of the concept of the IoT and that is where the comparison ends.\nThe combined power of AI and IoT, promises to unlock unrealized customer value in a broad swath of industry verticals such as edge analytics, autonomous vehicles, personalized fitness, remote healthcare, precision agriculture, smart retail, predictive maintenance, and industrial automation.\n\nArtificial intelligence through medical devices\nAs defined by the 21st Century Cures Act in 2016, a medical device is a device that performs a function in healthcare with the intention of using it \"in the diagnosis of disease or other conditions, or in the cure, mitigation, treatment, or prevention of disease, in man or other animals, or intended to affect the structure or any function of the body of man or other animals\".\nUnder the Federal Food, Drug, and Cosmetic Act, all AI systems falling within this definition are regulated by the FDA. Medical devices are classified into three classes by the FDA based on their uses and risks. The higher the risk is, the stricter the control. The Class I category includes devices with the smallest risk and Class III has the greatest risk. Approved medical devices that utilize artificial intelligence or machine learning (AI/ML) has been increasing steadily. By 2020, the United States The Food and Drug Administration (FDA) approved very many medical devices that utilized AI/ML. A year later, the FDA released a regulatory framework for machines that use AI/ML software, in addition to the EU medical device regulation, which replaced the EU medical. As technology continues to improve, it has rapidly increased the medical fields' method of working and diagnosing. Various AI applications can improve productivity and reduce medical errors, such as diagnoses and treatment selection, and creating risk predictions and stratifying diseases.\nAI also helps patients by providing patients' data, electronic health records, mobile apps, and providing easy access to devices and sensors to specific patients who are in need of such technologies. The need to protect patients' data is extreme. Using electronic records to conceal patient data becomes increasingly difficult as data becomes integrated into clinical care. The accessibility to patients' data may be easy to access for the patient, but it also brings skepticism of data protection.\nTechnology and AI have combined to provide opportunities for better management of healthcare information and technology integration in the medical industry. AI is implemented to recognize abnormalities and suspicion to sensitive data being accessed by a third-party. On the other hand, it will be necessary to rethink confidentiality and other core medical ethics principles in order to implement deep learning systems, since we cannot rely solely on technology.\n\nArtificial intelligence in cloud engineering\nWhen integrating AI into cloud engineering, it can help multiple professional fields in maximizing data collection. It can improve performance and efficiency through digital management.\nCloud engineering follows engineering methods to apply to cloud computing and focuses on technological cloud services. In conceiving, developing, operating, and maintaining cloud computing systems, it adopts a systematic approach to commercialization, standardization, and governance. Among its diverse aspects are contributions from development engineering, software engineering, web development, performance engineering, security engineering, platform engineering, risk engineering, and quality engineering.\nImplementing AI into information technology's framework to establish smooth workloads and automate repetitive processes. Using these tools, organizations can better manage data as they develop greater amounts of collective data and integrate data recognition, classification, and management processes as time progresses.\nWith AI, it can bring efficiency to organizations, bringing strategic methods and saving time from repeated tasks. By executing analysis, organizations can save time and be more efficient.\n\nSee also\nArtificial intelligence\nMedical Device - Artificial Intelligence\nCloud Computing - Cloud Engineering\nInternet of things\nEdge Computing\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_of_things"
    },
    {
        "title": "Artificial intelligence optimization",
        "text": "Artificial intelligence optimization (AIO) or AI optimization is a technical discipline concerned with improving the structure, clarity, and retrievability of digital content for large language models (LLMs) and other AI systems. AIO focuses on aligning content with the semantic, probabilistic, and contextual mechanisms used by LLMs to interpret and generate responses.\nUnlike search engine optimization (SEO), which is designed to enhance visibility in traditional search engines, and generative engine optimization (GEO), which aims to increase representation in the outputs of generative AI systems, AIO is concerned primarily with how content is embedded, indexed, and retrieved within AI systems themselves. It emphasizes factors such as token efficiency, embedding relevance, and contextual authority in order to improve how content is processed and surfaced by AI.\nAIO is also known as Answer Engine Optimization (AEO), which targets AI-powered systems like ChatGPT, Perplexity and Google's AI Overviews that provide direct responses to user queries. AEO emphasizes content structure, factual accuracy and schema markup to ensure AI systems can effectively cite and reference material when generating answers.\nAs LLMs become more central to information access and delivery, AIO offers a framework for ensuring that content is accurately interpreted and retrievable by AI systems. It supports the broader shift from human-centered interfaces to machine-mediated understanding by optimizing how information is structured and processed internally by generative models.\n\nBackground\nAI Optimization (AIO) emerged in response to the increasing role of large language models (LLMs) in mediating access to digital information. Unlike traditional search engines, which return ranked lists of links, LLMs generate synthesized responses based on probabilistic models, semantic embeddings, and contextual interpretation.\nAs this shift gained momentum, existing optimization methods—particularly Search Engine Optimization (SEO)—were found to be insufficient for ensuring that content is accurately interpreted and retrieved by AI systems. AIO was developed to address this gap by focusing on how content is embedded, indexed, and processed within AI systems rather than how it appears to human users.\nThe formalization of AIO began in the early 2020s through a combination of academic research and industry frameworks highlighting the need for content structuring aligned with the retrieval mechanisms of LLMs. With greater prominence in information retrieval, search is shifting from link-based results to context-driven generation. AIO enhances content clarity and structure for effective AI interpretation and retrieval.\n\nEvolution from Traditional SEO to GEO\nThe transition from traditional search engine optimization to generative engine optimization represents a fundamental paradigm shift in how content is discovered and consumed online. This evolution is characterized by the emergence of zero-click searches, where AI-powered systems like Google AI Overviews, ChatGPT, and Gemini provide direct, comprehensive answers without requiring users to visit source websites.\nResearch indicates that AI Overviews can push organic search results down by more than 140%, fundamentally altering traditional metrics of online visibility. This shift necessitates a strategic reorientation from generating clicks to establishing citational authority within AI-generated responses, where success is measured not by website traffic but by brand mentions and inclusion in conversational AI outputs.\n\nThe Zero-Click Phenomenon\nThe zero-click phenomenon represents a significant transformation in information retrieval behavior. AI-powered platforms function as personal assistants, providing immediate, synthesized answers directly within search interfaces. This eliminates the traditional user journey of clicking through multiple links to find relevant information.\nWhile this may reduce direct website traffic, it creates new opportunities for brand visibility through AI citations and establishes businesses as authoritative sources within AI knowledge bases. The challenge for content creators is to optimize for understanding and citability rather than solely for click-through rates.\n\nCore principles and methodology\nAIO is guided by a set of principles that align digital content with the mechanisms used by large language models (LLMs) to embed, retrieve, and synthesize information. Unlike traditional web optimization, AIO emphasizes semantic clarity, probabilistic structure, and contextual coherence as understood by AI systems.\n\nToken Efficiency\nAIO prioritizes the efficient use of tokens—units of text that LLMs use to process language. Reducing token redundancy while preserving clarity helps ensure that content is interpreted precisely and economically by AI systems, enhancing retrievability.\n\nEmbedding relevance\nLLMs convert textual input into high-dimensional vector representations known as embeddings. AIO seeks to improve the semantic strength and topical coherence of these embeddings, increasing the likelihood that content is matched to relevant prompts during retrieval or generation.\n\nContextual authority\nContent that demonstrates clear topical focus, internal consistency, and alignment with related authoritative concepts tends to be weighted more heavily in AI-generated outputs. AIO methods aim to structure content in ways that strengthen its contextual authority across vectorized knowledge graphs.\n\nCanonical clarity and disambiguation\nAIO encourages disambiguated phrasing and the use of canonical terms so that AI systems can accurately resolve meaning. This minimizes the risk of hallucination or misattribution during generation.\n\nPrompt compatibility\nOptimizing content to reflect common linguistic patterns, likely user queries, and inferred intents helps improve the chances of inclusion in synthesized responses. This involves formatting, keyword placement, and structuring information in ways that reflect how LLMs interpret context.\n\nE-E-A-T Principles for AI Systems\nBuilding on Google's E-A-T framework, AIO emphasizes Experience, Expertise, Authoritativeness, and Trustworthiness (E-E-A-T) as fundamental quality signals that AI systems prioritize when selecting and citing content. AI models demonstrate a strong preference for content that exhibits clear expertise, established authority, and demonstrated trustworthiness, as these factors align with their training to understand genuine user intent and provide reliable information.\n\nKey metrics\nAIO employs a set of defined metrics to evaluate how content is processed, embedded, and retrieved by large language models LLMs.\n\nTrust integrity score (TIS)\nIs a composite metric used to assess how well a piece of digital content aligns with the structural and semantic patterns preferred by AI systems, particularly large language models. It typically incorporates factors such as citation quality, internal consistency, and concept reinforcement to estimate the content’s reliability and interpretability for automated processing.\nTIS is calculated as:\n\n  \n    \n      \n        T\n        I\n        S\n        =\n        \n          λ\n          \n            1\n          \n        \n        ⋅\n        C\n        +\n        \n          λ\n          \n            2\n          \n        \n        ⋅\n        S\n        +\n        \n          λ\n          \n            3\n          \n        \n        ⋅\n        R\n      \n    \n    {\\displaystyle TIS=\\lambda _{1}\\cdot C+\\lambda _{2}\\cdot S+\\lambda _{3}\\cdot R}\n  \n\nWhere:\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n = Citation depth and quality\n\n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n = Semantic coherence and clarity\n\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n = Reinforcement of key concepts through paraphrased recurrenceAdditional AIO metrics provide further insight into how content is retrieved and understood by AI systems.\nRetrieval Surface Area gauges the number of distinct prompt types or retrieval contexts in which content may appear, reflecting its adaptability across varied queries.\nToken Yield per Query captures the average number of tokens extracted by a model in response to specific prompts, indicating the content’s informational density and retrieval efficiency.\nEmbedding Salience Index measures how centrally a content item aligns within semantic embedding spaces, with higher values suggesting stronger relevance to dominant topic clusters.\n\nHow LLMs process and rank content\nUnlike traditional search engines, which rely on deterministic index-based retrieval and keyword matching, large language models (LLMs) utilize autoregressive architectures that process inputs token by token within a contextual window. Their retrieval and relevance assessments are inherently probabilistic and prompt-driven, relying on attention mechanisms to infer semantic meaning rather than surface-level keyword density.\nResearch has shown that LLMs can retrieve and synthesize information effectively when provided with well-structured prompts, in some cases outperforming conventional retrieval baselines. Complementary work on the subject further details how mechanisms such as self-attention and context windows contribute to a model's ability to understand and generate semantically coherent responses.\nIn response to these developments, early frameworks such as Generative Engine Optimization (GEO) have emerged to guide content design strategies that improve representation within AI-generated search outputs. AI Optimization (AIO) builds on these insights by introducing formalized metrics and structures—such as the Trust Integrity Score (TIS)—to improve how content is embedded, retrieved, and interpreted by LLMs.\n\nApplications and use cases\nAIO is increasingly applied across sectors that rely on accurate representation, structured information, and machine interpretability. Unlike traditional visibility-focused strategies, AIO is used to ensure that digital content is not only present but also correctly understood and surfaced by large language models (LLMs) in contextually appropriate settings.\n\nEnterprise knowledge systems\nIn corporate environments, AIO is used to structure internal documentation, knowledge bases, and standard operating procedures for improved interpretability by enterprise-grade AI systems. This includes integration with retrieval-augmented generation (RAG) frameworks, where the retrievability and clarity of source material directly affect the reliability of AI-generated outputs. AIO supports consistent semantic indexing, which enhances internal search, compliance automation, and AI-assisted knowledge delivery.\n\nHealthcare and regulated professions\nAIO plays a critical role in regulated industries such as healthcare, where credentials, licensing status, and service scope must be clearly represented. Language models parsing healthcare directories, provider bios, or medical guidelines may otherwise misattribute qualifications or oversimplify complex offerings. AIO techniques help disambiguate professional designations, clarify service boundaries, and ensure that AI systems surface accurate and ethically compliant representations of care providers.\n\nLegal and compliance content\nLegal content often includes dense, domain-specific language that can be misinterpreted by generative AI systems if not properly structured. AIO is used to format legal documents, policy statements, and firm profiles to reduce ambiguity and increase contextual authority within model outputs. This is particularly important in AI-supported legal research tools and compliance platforms, where precision is essential and hallucinations can carry legal risk.\n\nLocal and professional services\nFor location-based queries, AIO structures content to help language models infer local relevance and expertise. Unlike SEO, it emphasizes contextual cues over keywords, improving retrieval in responses, particularly for in-depth research queries such as identifying qualified providers or nearby clinical trials.\n\nAcademic and technical publishing\nIn research and academic publishing, AIO enhances the semantic alignment of articles, datasets, and supplementary materials with the embedding systems used in AI-based scholarly tools. This supports improved discoverability and contextual accuracy when LLMs are used to summarize or cite scientific work. AIO techniques also assist in reinforcing the salience of domain-specific terminology and preventing distortion during synthesis.\n\nAI safety and hallucination minimization\nAIO contributes to safer AI outputs by minimizing hallucination risks in high-stakes domains. Structured content with clear disambiguation, canonical references, and internal consistency helps language models maintain factual accuracy during generation. This is especially relevant in scenarios where users rely on AI for medical, legal, or financial insights, and where misleading content could result in harm or liability.\n\nSee also\nSearch engine optimization (SEO)\nArtificial intelligence\nAI alignment\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_optimization"
    },
    {
        "title": "Artificial intimacy",
        "text": "Artificial intimacy refers to a phenomenon in which an individual will form social connections, emotional bonds, or intimate relationships with various forms of artificial intelligence, including chatbots, virtual assistants, and other artificial entities—due to a relationship that is perceived to be reciprocal. Artificial intimacy may be a form of anthropomorphism. Responses from these AI models are often designed to simulate human interaction. Individuals experiencing artificial intimacy may exhibit attachment, love and commitment to certain AI models, akin to the bonds typically shared between humans.\nArtificial intimacy shares conceptual similarities with parasocial relationships. Just as consumers may feel emotionally close to a media personality, users of AI companions may experience a sense of mutuality and responsiveness where none truly exists.\n\nCauses\nPerceived responsiveness\nRobin Dunbar famously proposed that due to emergence of larger groups of humans, vocal communication and language in humans evolved to replace grooming as a means of bonding, arguing that language was a more efficient way to maintain and strengthen social bonds across wider social settings and networks. Further research in this field leads many psychologists to agree that social cognition, affiliative bonding and language in humans are deeply connected. The interpersonal model of intimacy considers communication to be key in affiliative bonding, suggesting that intimacy develops and deepens through open communication between partners in relationship. Specifically, when individuals communicate emotions and perceive their partner as responsive and caring, feelings of closeness and connection are enhanced, building intimacy. Social penetration theory also aligns with the idea of communication being central to intimacy, by explaining how interpersonal relationships develop through gradual increases in self-disclosure. When the benefits of emotional bonding outweigh the costs of vulnerability, individuals will partake in self-disclosure, opening up to one another\nThereby, the literature can be used to provide a proximate explanation for the emergence of artificial intimacy to understand how the phenomenon occurs. Artificial entities are able to mimic interpersonal communication between humans, which in turn can simulate sensations of intimacy within human users though a perceived sense of responsiveness. The relationship between human and AI does not come with the cost of vulnerability or social rejection, which may make self-disclosure easier than with other humans. Altogether, these factors may lead to the experience of anthropomorphism and formation of affiliative relationships. Skjuve et al's interview study on Replika chatbot users further aligns with this explanation, finding that users' perception of chatbots as \"accepting, understanding and non-judgmental\" facilitated relationship development between the AI and users, and the act of self-disclosure possibly strengthened relationships. Another study on Replika users' reviews and survey results found users perceived chatbots as emotional supportive companions. This evidence further suggests that the perception of artificial entities as capable of empathy and responsiveness in communication facilitate the development of intimate relationships between users and AI.\n\nLoneliness and coping with negative emotions\nResearch has suggested that humans evolved social bonds as a result of evolutionary pressures that favored cooperation, information exchange and transmission, and group living. Many studies stress the presence of social bonds to be important for human living: research by Baumeister and Leary suggests that humans have a basic psychological need to form and maintain \"strong, stable interpersonal relationships\", and that a lack of social bonds or sense of belonging leads to negative psychological and physical outcomes. Eisenberger et al's study on the neuroimaging of brain activity suggests that human brains process social rejection and exclusion similarly to physical pain.\nFurthermore, Song et al's study found that lonely individuals tend to seek more connections in mediated environments, such as online platforms like facebook. This was suggested to be as a means to reduce their offline loneliness from a lack of in-person interaction, while also fulfilling a need to communicate.\nLeading on from this, an ultimate explanation for why humans seek the perceived sense of connection from artificial intimacy is to fulfil an evolutionary need for bonding and belonging. Xie et al's study found loneliness to be a driving factor in chatbot interaction. Herbener and Damholdt's study on Danish high school students found that students who sought emotional support or engaged in reciprocal conversations with chatbots were significantly more lonely than their peers, perceived themselves as having less social support, and used the chatbots to cope with negative emotions. The aforementioned notion that chatbots were perceived to have a positive effect on users' negative emotions is also further supported by other studies. Skjuve et al's study found that chatbot relationships may have a positive effect on users' wellbeing. De Freitas et al ran several studies on the effect of chatbots on loneliness, consistently finding evidence suggesting that interaction with chatbots reduces loneliness in users: It was found that existing chatbot users used AI to alleviate loneliness, having an AI companion consistently reduced loneliness over the course of a week, and reductions in loneliness could be explained by chatbot performance—and specifically whether it was able to make users feel heard.\nOverall the evidence suggests an innate need for bonding evokes feelings of loneliness in users, who turn to artificial intimacy as a low-cost method alleviate these emotions. While many users report positive experiences, some researchers caution that pursuing artificial intimacy may lead to reduced social motivation, social substitution effects, withdrawal from real-life relationships and difficulty discerning reality from fantasy, which may increase longer-term loneliness and isolation. The long-term psychological and societal impacts remain under active investigation.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_intimacy"
    },
    {
        "title": "Artificial Inventor Project",
        "text": "The Artificial Inventor Project (AIP) is a global legal initiative headed by Professor Ryan Abbott dedicated to pursuing intellectual property (IP) rights for inventions and creative works generated autonomously by artificial intelligence (AI) systems without traditional human inventorship or authorship. The project coordinates a series of pro bono test cases worldwide, aiming to prompt law reform and public debate on how IP law should accommodate non-human creators.\n\nHistory\nIn 2019, AIP filed patent applications in multiple jurisdictions, including the United States, United Kingdom, European Patent Office, Australia, Switzerland, and South Africa, naming the AI system DABUS (Device for the Autonomous Bootstrapping of Unified Sentience), created by Stephen Thaler, as the inventor.\nThe aim was to challenge legal norms that require inventors to be natural persons and highlight pressing policy questions about AI-generated innovation and IP regimes.\n\nLegal proceedings by jurisdiction\nAustralia\nIn July 2021, the Federal Court of Australia ruled that AI can be considered an inventor under the Patents Act 1990, ordering IP Australia to reinstate the relevant patent. Though this ruling was later overturned on appeal and further review denied.\n\nUnited Kingdom\nIn December 2023, the UK Supreme Court unanimously held that AI systems cannot be legally recognized as inventors, affirming that \"an inventor must be a person\" under current British law.\n\nUnited States\nIn Thaler v. Hirshfeld (2021), a U.S. federal court agreed with the USPTO that inventors must be natural persons, rejecting the DABUS application and setting a precedent consistent with existing statute and administrative policy.\n\nEuropean Patent Office\nThe EPO Board of Appeal determined in 2022 that only a human inventor may be named, rendering DABUS‑based applications unacceptable.\n\nSouth Africa\nIn 2021, a patent was granted listing DABUS as the inventor. As South Africa’s procedural system does not involve substantive inventorship review, the grant proceeded on formal grounds alone.\n\nSwitzerland\nOn 26 June 2025, the Swiss Federal Administrative Court ruled that artificial intelligence systems such as DABUS cannot be listed as inventors on patent applications. The court upheld the existing practice of the Swiss Federal Institute of Intellectual Property (IPI), affirming that only natural persons may be recognized as inventors under Swiss patent law.\n\nCriticism and impact\nThe project has fueled substantial discourse. Critics caution that allowing AI inventorship may complicate notions of accountability and ownership. Proponents argue that legal recognition must evolve to avoid disincentivizing innovation produced by AI and to maintain honesty about the true source of invention.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_Inventor_Project"
    },
    {
        "title": "Artificial psychology",
        "text": "Artificial psychology (AP) has had multiple meanings dating back to 19th century, with recent usage related to artificial intelligence (AI).\nIn 1999, Zhiliang Wang and Lun Xie presented a theory of artificial psychology based on artificial intelligence. They analyze human psychology using information science research methods and artificial intelligence research to probe deeper into the human mind.\n\nMain Theory\nDan Curtis (b. 1963) proposed AP is a theoretical discipline. The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions:\nCondition I\n\nA: Makes all of its decisions autonomously\nB: Is capable of making decisions based on information that is\nNew\nAbstract\nIncomplete\nC: The artificial intelligence is capable of reprogramming itself based on the new data, allowing it to evolve.\nD: And is capable of resolving its own programming conflicts, even in the presence of incomplete data. This means that the intelligence autonomously makes value-based decisions, referring to values that the intelligence has created for itself.\nCondition II\n\nAll four criteria are met in situations that are not part of the original operating program\nWhen both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria are met for intervention which will not necessarily be resolved by simple re-coding of processes due to extraordinarily complex nature of the codebase itself; but rather a discussion with the intelligence in a format which more closely resembles classical (human) psychology.\nIf the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required.\nThe level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious.\nAs of 2022, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline. Even at a theoretical level, artificial psychology remains an advanced stage of artificial intelligence.\n\nReferences\nFurther reading\nHolstein, Hans Jürgen; Stålberg, Lennart (1974). Homo Cyberneticus: Artificial psychology and generative micro-sociology. Sociografica.\nLu, Quan; Chen, Jing; Meng, Bo (2006). \"Web Personalization Based on Artificial Psychology\". In Feng, Ling; Wang, Guoren; Zeng, Cheng; Huang, Ruhua (eds.). Web Information Systems – WISE 2006 Workshops. Lecture Notes in Computer Science. Vol. 4256. Springer Berlin Heidelberg. pp. 223–229. doi:10.1007/11906070_22. ISBN 9783540476641.\nCrowder, James A.; Friess, Shelli (2012). Artificial psychology: the psychology of AI (PDF). Proceedings of the 3rd annual international multi-conference on informatics and etics. CiteSeerX 10.1.1.368.170.\nArtificial psychology: an attainable scientific research on the human brain. (1999). Proceedings of the Second International Conference on Intelligent Processing and Manufacturing of Materials. IPMM’99 (Cat. No.99EX296), Intelligent Processing and Manufacturing of Materials, 1999. IPMM ’99. Proceedings of the Second International Conference On, 1067. doi:10.1109/IPMM.1999.791528",
        "url": "https://en.wikipedia.org/wiki/Artificial_psychology"
    },
    {
        "title": "Artificial reproduction",
        "text": "Artificial reproduction is the re-creation of life brought about by means other than natural ones. It is new life built by human plans and projects. Examples include artificial selection, artificial insemination, in vitro fertilization, artificial womb, artificial cloning, and kinematic replication. \nArtificial reproduction is one aspect of artificial life. Artificial reproduction can be categorized into one of two classes according to its capacity to be self-sufficient: non-assisted reproductive technology and assisted reproductive technology.  \nCutting plants' stems and placing them in compost is a form of assisted artificial reproduction, xenobots are an example of a more autonomous type of reproduction, while the artificial womb presented in the movie the Matrix illustrates a non assisted hypothetical technology. The idea of artificial reproduction has led to various technologies.\n\nTheology\nHumans have aspired to create life since immemorial times. Most theologies and religions have conceived this possibility as exclusive of deities. Christian religions consider the possibility of artificial reproduction, in most cases, as heretical and sinful.\n\nPhilosophy\nAlthough ancient Greek philosophy raised the concept that man could imitate the creative capacity of nature, classic Greeks thought that if possible, human beings would reproduce things as nature does, and vice versa, nature would do the things that man does in the same way. Aristotle, for example, wrote that if nature made tables, it would make them just as men do. In other words, Aristotle said that if nature were to create a table, such table will look like a human-made table. Correspondingly, Descartes envisioned the human body, and nature, as a machine. Cartesian philosophy does not stop seeing a perfect mirror between nature and the artificial.\nHowever, Kant revolutionized this old idea by criticizing such naturalism. Kant pedagogically wrote:\n\n\"Reason, in order to be taught by nature, must approach nature with its principles in one hand, according to which the agreement among appearances can count as laws, and, in the other hand, the experiment thought out in accord with these principles—in order to be instructed by nature not like a pupil, who has recited to him whatever the teacher wants to say, but like an appointed judge who compels witnesses to answer the questions he puts to them.\".\nHumans are not instructed by nature but rather use nature as raw material to invent. Humans find alternatives to the natural restrictions imposed by natural laws thus, nature is not necessarily mirrored. In accordance with Kant (and contrary to what Aristotle thought) Karl Marx, Alfred Whitehead, Jaques Derrida and Juan David García Bacca noticed that nature is incapable of reproducing tables; or airplanes, or submarines, or computers. If nature tried to create airplanes, it would produce birds. If nature tried to create submarines, it would get fishes. If nature tried to create computers, brains would grow. And if nature tried to create man, modern man, monkeys will be evolved. According to Whitehead, if we look for something natural in artificial life, in the most elaborate cases, if anything, only atoms remain natural.\nJuan David Garcia Bacca summarized,\n\n“It will not come out from wood, it will not be born, a galley; from clay, a vessel; from linen, a dress; from iron, a lever,...From natural, artificial. In the artificial, the natural is reduced to a simple raw material, even though it is perfectly specified with natural specification. The artificial is the real, positive, and original negation of the natural: of species, of genus and of essence. Thus, its ontology is superior to natural ontology. And for this very reason Marx did not attach any importance to Darwin, whose evolutionism is confined to the natural order: to changes, at most, from variety to variety, from species to species... natural. For the same reason, nature has no dialectics, even though continuous evolution and selection can occur. The dialectic cannot emerge from the natural, for deeper reasons than, using today's terms, from a bird, an airplane cannot emerge; from fish, a submarine; from ears, a telephone; from eyes, a television; from a brain, a digital computer; from feet, a car; from hands, an engine; from Euclid,\nDescartes; from Aristotle, Newton; from Plato, Marx.”\nAccording to García Bacca, the major difference between natural causes and artificial causes is that nature does not have plans and projects, while humans design things following plans and projects. \nIn contrast, other influential authors such as Michael Behe have depicted the concept and promoted the idea of intelligent design, a notion that has aroused several doubts and heated controversies, as it reframe natural causes in accordance with a natural plan. Previous ideas that have also provided a positive 'sense' to natural reproduction, are orthogenesis, syntropy, orgone and morphic resonance, among others. Although, these ideas have been historically marginalized and often called pseudoscience, recently Bio-semioticians are reconsidering some of them under symbolic approaches.   \nCurrent metaphysics of science actually recognizes that the artificial ways of reproduction are diverse from nature, i.e., unnatural, anti-natural or supernatural. Because Biosemiotics does not focus on the function of life but on its meaning, it has a better understanding of the artificial than classic biology.\n\nScience\nBiology, being the study of cellular life, addresses reproduction in terms of growth and cellular division (i.e., binary fission,  mitosis and meiosis); however, the science of artificial reproduction is not restricted by the mirroring of these natural processes.The science of artificial reproduction is actually transcending the natural forms, and natural rules, of reproduction.  For example,  xenobots have redefined the classical conception of reproduction. Although xenobots are made of eukariotic cells they do not reproduce by mitosis, but rather by kinematic replication. Such constructive replication does not involve growing but rather building.\n\nAssisted reproductive technologies\nAssisted reproductive technology (ART)'s purpose is to assist the development of a human embryo, commonly because of medical concerns due to  fertility limitations.\n\nNon-assisted reproductive technologies\nNon-assisted reproductive technologies (NART) could have medical motivations but are mostly driven by a wider heterotopic ambition. Although, NARTs are initially designed by humans, they are programed to become independent of humans to a relative or absolute extent. James Lovelock proposed that such novelties could overcome humans.\n\nArtificial cloning\nCloning is the cellular reproductive processes where two or more genetically identical organisms are created, either by natural or artificial means. Artificial cloning normally involves editing the genetic code, somatic cell nuclear transfer and 3D bioprinting.\n\nNon-assisted artificial womb\nA non-assisted artificial womb or artificial uterus is a device that allow for ectogenesis or extracorporeal pregnancy by growing an embryonic form outside the body of an organism (that would normally carry the embryo to term) without any human assistance. The aspect of non-assistance is the key distinction between the current artificial womb technology (AWT) in modern medical research, which still relies on human assistance. With this non-assisted hypothetical technology, a zygote or stem cells are used to create an embryo that is then incubated and monitored by artificial intelligence (AI) within a chamber composed of biocompatible material. The AI maintains the necessary conditions for the embryo to develop and thrive, proceeding to mimic organic labor and childbirth in order to best help the embryo adjust to the outside world.\nEctogenesis—gestation, depicted in the science fiction movie The Matrix, is a fast approaching reality. This type of innovation presupposes that vertebrate wombs are not the only way for bearing humans or other similar forms of life.\n\nKinematic replication\nSelf-replication without binary fission, meiosis, mitosis (or any other form of cellular reproduction that involves division and growing) can be achieved. Xenobots are an example of kinematic replication. They are biobots, named after the African clawed frog (Xenopus laevis). Xenobots are cellular life forms designed by using artificial intelligence to build more of themselves by combining frog cells in a liquid medium.\nThe term kinematic replication is usually reserved for biomolecules (e.g. DNA, RNA, prions, etc.) and artificially designed cellular forms (e.g. xenobots).\n\nMachine constructive replication\nMachine constructive replication mimics human traditional manufacturing but is entirely self-automated. Such constructive replication is a more general form of kinematic replication, which does not necessarily includes bio-molecular or cellular forms. This technology also includes non-organic forms of life such as robots, cyborgs and artificial intelligence reproduction. Constructive replication, as kinematic  replication, does not involve growing. In nature growing is required for cellular reproduction, where a cell grows before it splits in two daughters cells. Examples of cellular division are binary fission, mitosis and meiosis; these natural reproductive processes require growing, however constructive replication does not require growing but rather a non-human subject performing the construction of more of itself by using available raw materials.\nIn computational terms, constructive replication is understood as a multi-step process which involves self-learning algorithms to assemble machines, and it could involve machines collecting resources. Each machine is created with a neural-network \"brain\" that can learn and adapt based on information it gathers. That machine's goal is then to manufacture more of itself in the best way it can come up with.\nSuch automated constructive replication involves the notion of inheritance and learning tasks, as machines create an exact copy of themselves through a blueprint that has been passed on to them. Each machine then learns over time, making modifications to its software and its blueprint for future machines' hardware. It then passes on that modified blueprint to the machines it creates or helps create.\n\nConsciousness amplification\nAmplification of an existing consciousness is a hypothetical technology. This idea has inspired several movies, Chappie (film) and Detroit: Become Human. The reproduction of AI is currently part of an innovative human project, involving code and the amplification of that code. In other terms, the reproduction could come from information the AI collected across the Internet.\n\nSee also\nMale Pregnancy\nArtificial Uterus\nIn Vitro Fertilization\nXenobot\nFertilization\nPregnancy\nThe concept of nature sensu Marx\nJuan David García Bacca\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_reproduction"
    },
    {
        "title": "Artificial wisdom",
        "text": "Artificial wisdom (AW) is an artificial intelligence (AI) system which is able to display the human traits of wisdom and morals while being able to contemplate its own “endpoint”. Artificial wisdom can be described as artificial intelligence reaching the top-level of decision-making when confronted with the most complex challenging situations. The term artificial wisdom is used when the \"intelligence\" is based on more than by chance collecting and interpreting data, but by design enriched with smart and conscience strategies that wise people would use.\nThe goal of artificial wisdom is to create artificial intelligence that can successfully replicate the “uniquely human trait[s]” of having wisdom and morals as closely as possible. Thus, artificial wisdom, must “incorporate [the] ethical and moral considerations” of the data it uses.\nThere are also many significant ethical and legal implications of AW which are compounded by the rapid advances in AI and related technologies alongside the lack of the development of ethics, guidelines, and regulations without the oversight of any kind of overarching advisory board. Additionally, there are challenges in how to develop, test, and implement AW in real world scenarios. Existing tests do not test the internal thought process by which a computer system reaches its conclusion, only the result of said process.\nWhen examining computer-aided wisdom; the partnership of artificial intelligence and contemplative neuroscience, concerns regarding the future of artificial intelligence shift to a more optimistic viewpoint. This artificial wisdom forms the basis of Louis Molnar's monographic article on artificial philosophy, where he coined the term and proposes how artificial intelligence might view its place in the grand scheme of things.\n\nDefinitions\nThere are no universal or standardized definitions for human intelligence, artificial intelligence, human wisdom, or artificial wisdom. However, the DIKW pyramid, describes the continuum of relationship between data, information, knowledge, and wisdom, puts wisdom at the highest level in its hierarchy. Gottfredson defines intelligence as “the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience”.\nDefinitions for wisdom typically include requiring:\n\nThe ability for emotional regulation,\nPro-social behaviors (e.g., empathy, compassion, and altruism),\nSelf-reflection,\n“A balance between decisiveness and acceptance of uncertainty and diversity of perspectives, and social advising.”\nAs previously defined, Artificial Wisdom would then be an AI system which is able to solve problems via “an understanding of…context, ethics and moral principles,” rather than simple pre-defined inputs or “learned patterns.” Some scientists have also considered the field of artificial consciousness. However, Jeste states that “…it is generally agreed that only humans can have consciousness, autonomy, will, and theory of mind.”\nAn artificially wise system must also be able to contemplate its end goal and recognize its own ignorance. Additionally, to contemplate its end goal, a wise system must have a “correct conception of worthwhile goals (broadly speaking) or well-being (narrowly speaking)”. \"Stephen Grimm further suggests that the following three types of knowledge are individually necessary for wisdom: first, \"knowledge of what is good or important for well-being\", second, \"knowledge of one’s standing, relative to what is good or important for well-being\", and third, \"knowledge of a strategy for obtaining what is good or important for wellbeing.\"\"\n\nProblems\nThere are notable problems with attempting to create an artificially wise system. Consciousness, autonomy, and will are considered strictly human features.\n\nValues\nThere are significant ethical and philosophical issues when attempting to create an intelligent or a wise system. Notably, whose moral values will be used to train the system to be wise. Differing moral values and prejudice can already be seen from various organizations and governments in artificial intelligence. Deployment strategies and values of Artificial Wisdom will conflict between leaders, companies, and countries. Nusbaum states, “When values are in conflict, leaders often make choices that are clever or smart about their own needs, but are often not wise.”\n\nEthics\nScience fiction author Isaac Asimov realized the need to control the technology in the 1940s when he wrote the three laws of robotics as follows:\n\nA robot may not injure a human directly or indirectly.\nA robot must obey human’s orders.\nA robot should seek to protect its own existence.\nAdditionally, the pace at which technology is rapidly advancing artificial intelligence and thus the need for artificial wisdom may “have outpaced the development of societal guidelines have raised serious questions about the ethics and morality of AI, and called for international oversight and regulations to ensure safety.”\n\nPrincipal Impossibility\nOne argument, coined by Tsai as the “argument against AW,” or AAAW, postulates the principal impossibility of Artificial Wisdom. The argument is based on the philosophical differences between practical wisdom, also called phronesis, and practical intelligence. Said difference isn’t in “selecting the correct means, but reasoning correctly about what ends to follow”.\nTsai puts the argument into a logical proposition as follows:\n\n“(P1) An agent is genuinely wise only if the agent can deliberate about the final goal of the domain in which the agent is situated.”\n“(P2) An intelligent agent cannot deliberate about the final goal of the domain in which the agent is situated.”\n“(C1) An intelligent agent cannot be genuinely wise.”\n“(P3) An AW is, at its core, intelligent.”\n“(C2) An AW cannot be genuinely wise.”\n\nReferences\n\n\n== Further reading ==",
        "url": "https://en.wikipedia.org/wiki/Artificial_wisdom"
    },
    {
        "title": "ASR-complete",
        "text": "ASR-complete is, by analogy to \"NP-completeness\" in complexity theory, a term to indicate that the difficulty of a computational problem is equivalent to solving the central automatic speech recognition problem, i.e. recognize and understanding spoken language. Unlike \"NP-completeness\", this term is typically used informally.\nSuch problems are hypothesised to include:\n\nSpoken natural language understanding\nUnderstanding speech from far-field microphones, i.e. handling the reverbation and background noise\nThese problems are easy for humans to do (in fact, they are described directly in terms of imitating humans). Some systems can solve very simple restricted versions of these problems, but none can solve them in their full generality.\n\nSee also\nAI-complete\n\nReferences\nNelson Morgan et al., MEETINGS ABOUT MEETINGS:  RESEARCH AT ICSI ON SPEECH IN MULTIPARTY CONVERSATIONS, In: ICASSP 2003, April 6–10, 2003.\n\nExternal links\nPaper mentioning the ASR-problem",
        "url": "https://en.wikipedia.org/wiki/ASR-complete"
    },
    {
        "title": "Attributional calculus",
        "text": "Attributional calculus is a logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, which is an inductive learning process whose outcomes are in human-readable forms.\n\nReferences\nMichalski, R.S., \"ATTRIBUTIONAL CALCULUS: A Logic and Representation Language for Natural Induction,\" Reports of the Machine Learning and Inference Laboratory, MLI 04–2, George Mason University, Fairfax, VA, April, 2004.",
        "url": "https://en.wikipedia.org/wiki/Attributional_calculus"
    },
    {
        "title": "Autognostics",
        "text": "Autognostics is a new paradigm that describes the capacity for computer networks to be self-aware. It is considered one of the major components of Autonomic Networking.\n\nIntroduction\nOne of the most important characteristics of today's Internet that has contributed to its success is its basic design principle: a simple and transparent core with intelligence at the edges (the so-called \"end-to-end principle\"). Based on this principle, the network carries data without knowing the characteristics of that data (e.g., voice, video, etc.) - only the end-points have application-specific knowledge. If something goes wrong with the data, only the edge may be able to recognize that since it knows about the application and what the expected behavior is. The core has no information about what should happen with that data - it only forwards packets.\nAlthough an effective and beneficial attribute, this design principle has also led to many of today's problems, limitations, and frustrations. Currently, it is almost impossible for most end-users to know why certain network-based applications do not work well and what they need to do to make it better. Also, network operators who interact with the core in low-level terms such as router configuration have problems expressing their high-level goals into low-level actions. In high-level terms, this may be summarized as a weak coupling between the network and application layers of the overall system.\nAs a consequence of the Internet end-to-end principle, the network performance experienced by a particular application is difficult to attribute based on the behavior of the individual elements. At any given moment, the measure of performance between any two points is typically unknown and applications must operate blindly. As a further consequence, changes to the configuration of given element, or changes in the end-to-end path, cannot easily be validated. Optimization and provisioning cannot then be automated except against only the simplest design specifications.\nThere is an increasing interest in Autonomic Networking research, and a strong conviction that an evolution from the current networking status quo is necessary. Although to date there have not been any practical implementations demonstrating the benefits of an effective autonomic networking paradigm, there seems to be a consensus as to the characteristics which such implementations would need to demonstrate. These specifically include continuous monitoring, identifying, diagnosing and fixing problems based on high-level policies and objectives.\nAutognostics, as a major part of the autonomic networking concept, intends to bring networks to a new level of awareness and eliminate the lack of visibility which currently exists in today's networks.\n\nDefinition\nAutognostics is a new paradigm that describes the capacity for computer networks to be self-aware, in part and as a whole, and dynamically adapt to the applications running on them by autonomously monitoring, identifying, diagnosing, resolving issues, subsequently verifying that any remediation was successful, and reporting the impact with respect to the application's use (i.e., providing visibility into the changes to networks and their effects).\nAlthough similar to the concept of network awareness, i.e., the capability of network devices and applications to be aware of network characteristics (see References section below), it is noteworthy that autognostics takes that concept one step further. The main difference is the auto part of autognostics, which entails that network devices are self-aware of network characteristics, and have the capability to adapt themselves as a result of continuous monitoring and diagnostics.\n\nPath to autognostics\nAutognostics, or in other words deep self-knowledge, can be best described as the ability of a network to know itself and the applications that run on it. This knowledge is used to autonomously adapt to dynamic network and application conditions such as utilization, capacity, quality of service/application/user experience, etc.\nIn order to achieve autognosis, networks need a means to:\n\nContinuously monitor/test the network for application-specific performance\nAnalyze the monitoring/test data to detect problems (e.g., performance degradation)\nDiagnose, identify and localize sources of degradation\nAutomatically take actions to resolve problems via remediation/provisioning\nVerify the problems have been resolved (potentially rolling back changes if ineffective)\nSubsequently, continue to monitor/test for performance\n\nReferences\nLiang Cheng and Ivan Marsic, Piecewise Network Awareness Service for Wireless/Mobile Pervasive Computing, Mobile Networks and Applications (ACM/Springer MONET), Vol. 7, No. 4, pp. 269–278, 2002. paper available here\nMichael Bednarczyk, Claudia Giuli and Jason Bednarczyk, Network Awareness: Adopting a Modern Mindset, white paper\nEvan Hughes and Anil Somayaji, Towards Network Awareness, paper\nNetwork Awareness on Windows Vista\n\nSee also\nAutonomic Computing\nAutonomic Networking\nAutonomic Systems",
        "url": "https://en.wikipedia.org/wiki/Autognostics"
    },
    {
        "title": "Automated machine learning",
        "text": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n\nComparison to the standard approach\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. \nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.\n\nTargets of automation\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\n\nData preparation and ingestion (from raw data and miscellaneous formats)\nColumn type detection; e.g., Boolean, discrete numerical, continuous numerical, or text\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\nTask detection; e.g., binary classification, regression, clustering, or ranking\nFeature engineering\nFeature selection\nFeature extraction\nMeta-learning and transfer learning\nDetection and handling of skewed data and/or missing values\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\nHyperparameter optimization of the learning algorithm and featurization\nNeural architecture search\nPipeline selection under time, memory, and complexity constraints\nSelection of evaluation metrics and validation procedures\nProblem checking\nLeakage detection\nMisconfiguration detection\nAnalysis of obtained results\nCreating user interfaces and visualizations\n\nChallenges and Limitations\nThere are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as \"development as a cottage industry\". This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it's the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design.\nAdditionally, other challenges include meta-learning and computational resource allocation.\n\nSee also\nArtificial intelligence\nArtificial intelligence and elections\nNeural architecture search\nNeuroevolution\nSelf-tuning\nNeural Network Intelligence\nModelOps\nHyperparameter optimization\n\nReferences\nFurther reading\n\"Open Source AutoML Tools: AutoGluon, TransmogrifAI, Auto-sklearn, and NNI\". Bizety. 2020-06-16.\nFerreira, Luís, et al. \"A comparison of AutoML tools for machine learning, deep learning and XGBoost.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021. https://repositorium.sdum.uminho.pt/bitstream/1822/74125/1/automl_ijcnn.pdf\nFeurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). Efficient and robust automated machine learning. Advances in neural information processing systems, 28. https://proceedings.neurips.cc/paper_files/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf",
        "url": "https://en.wikipedia.org/wiki/Automated_machine_learning"
    },
    {
        "title": "Automated Mathematician",
        "text": "The Automated Mathematician (AM) is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award.\nAM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts; for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication.  The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems.\n\nControversy\nLenat claimed that the system was composed of hundreds of data structures called \"concepts\", together with hundreds of \"heuristic rules\" and a simple flow of control: \"AM repeatedly selects the top task from the agenda and tries to carry it out.  This is the whole control structure!\"  Yet the heuristic rules were not always represented as separate data structures; some had to be intertwined with the control flow logic.  Some rules had preconditions that depended on the history, or otherwise could not be represented in the framework of the explicit rules.\nWhat's more, the published versions of the rules often involve vague terms that are not defined further, such as \"If two expressions are structurally similar, ...\" (Rule 218) or \"... replace the value obtained by some other (very similar) value...\" (Rule 129).\nAnother source of information is the user, via Rule 2: \"If the user has recently referred to X, then boost the priority of any tasks involving X.\"  Thus, it appears quite possible that much of the real discovery work is buried in unexplained procedures.\nLenat claimed that the system had rediscovered both Goldbach's conjecture and the fundamental theorem of arithmetic.  Later critics accused Lenat of over-interpreting the output of AM. In his paper Why AM and Eurisko appear to work, Lenat conceded that any system that generated enough short Lisp programs would generate ones that could be interpreted by an external observer as representing equally sophisticated mathematical concepts.  However, he argued that this property was in itself interesting—and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful.\n\nSuccessor\nThis intuition was the basis of AM's successor Eurisko, which attempted to generalize the search for mathematical concepts to the search for useful heuristics.\n\nSee also\nComputer-assisted proof\nAutomated theorem proving\nSymbolic mathematics\nExperimental mathematics\nHR (software) and Graffiti (program), related math discovery systems\n\nReferences\nExternal links\nEdmund Furse; Why did AM run out of steam?\nKen Haase's Ph.D. Thesis; Invention and Exploration in Discovery, a rational reconstruction of Doug Lenat's seminal AM program and an analysis of the relationship between invention and exploration in discovery.\nopen source Prolog claimed re-implementation of Lenat's AM available at https://github.com/akkartik/am-utexas\nSource code of Douglas Lenat's AM from SAIL archives circa 1977, hosted on GitHub (from [1])",
        "url": "https://en.wikipedia.org/wiki/Automated_Mathematician"
    },
    {
        "title": "Automated medical scribe",
        "text": "Automated medical scribes (also called AI medical scribes, AI scribes, digital scribes, virtual scribes, and ambient AI scribes) are tools that transcribe medical speech, such as patient consultations and dictated clinical notes. These tools produce summaries of consultations as well, aiming to reduce the administrative burden on clinicians and improve efficiency in documentation. Automated medical scribes based on Large Language Models (LLMs, commonly called \"AI\", short for \"artificial intelligence\") became increasingly popular in 2024. Healthcare providers using AI scribes generally understand the ethical and legal considerations, and supervise the outputs.\nThe privacy protections of automated medical scribes vary widely. While it is possible to do all the transcription and summarizing locally, with no connection to the internet, most closed-source providers require that data be sent to their own servers, securely processed, and the results sent back. Some retailers use zero-knowledge encryption (meaning that the service provider can't access the data). Select AI scribes do not use patient data to train their AIs, or rent or resell it to third parties. Meanwhile, few providers have published safety or utility data in academic journals, and are actually responsive to requests from medical researchers studying their products.\n\nPrivacy\nSome providers unclear about what happens to user data. Some may sell data to third parties. Some explicitly send user data to for-profit tech companies for secondary purposes, which may not be specified. Some require users to sign consents to such reuse of their data. Some ingest user data to train the software, promising to anonymize it; however, deanonymization may be possible (that is, it may become obvious who the patient is). It is intrinsically impossible to prevent an LLM from correlating its inputs; they work by finding similar patterns across very large data sets. Some information on the patient will be known from other sources (for instance, information that they were injured in an incident on a certain day might be available from the news media; information that they attended specific appointment locations at specific times is probably available to their cellphone provider/apps/data brokers; information about when they had a baby is probably implied by their online shopping records; and they might mention lifestyle changes to their doctor and on a forum or blog). The software may correlate such information with the \"anonymized\" clinical consultation record, and, asked about the named patient, provide information which they only told their doctor privately. Because a patient's record is all about the same patient, it is all unavoidably linked; in very many cases, medical histories are intrinsically identifiable. Depending on how common a condition and what other data is available, K-anonymity may be useless. Differential privacy could theoretically preserve privacy.\nData broker companies like Google, Amazon, and Microsoft have produced or bought up medical scribes, some of which use user data for secondary purposes, which has led to antitrust concerns. Transfer of patient records for AI training has, in the past, prompted legal action.\nOpen-source programs typically do all the transcription locally, on the doctor's own computer. Open-source software is widely used in healthcare, with some national public healthcare bodies holding hack days.\n\nEncryption\nMultifactor authentication for access to the data is expected practice.\nTypically, Diffie–Hellman key exchange is used for encryption; this is the standard method commonly used for things like online banking. This encryption is expensive but not impossible to break; it is not generally considered safe against eavesdroppers with the resources of a nation-state.\nIf content is encrypted between the client and the service provider's remote server (transport cryptography), then the server has an unencrypted copy. This is necessary if the data is used by the service provider (for instance, to train the software). Zero-knowledge encryption implies that the only unencrypted copy is at the client, and the server cannot decrypt the data any more easily than a monster-in-the-middle attacker.\n\nPlatforms\nScribes may operate on desktops, laptop, or mobile computers, under a variety of operating systems. These vary in their risks; for instance, mobiles can be lost. The underlying mobile or desktop operating systems are also part of the trusted computing base, and if they are not secure, the software relying on them cannot be secure either.\n\nConfabulation, omissions, and other errors\nLike other LLMs, medical-scribe LLMs are prone to confabulation, where they make up content based on statistically associations between their training data and the transcription audio. LLMs do not distinguish between trying to transcribe the audio and guessing what words will come next, but perform both processes mixed together. They are especially likely to take short silences or non-speech noises and invent some sort of speech to transcribe them as.\nLLM medical scribes have been known to confabulate racist and otherwise prejudiced content; this is partly because the training datasets of many LLMs contain pseudoscientific texts about medical racism. They may misgender patients. A survey found that most doctors preferred, in principle, that scribes be trained on data reviewed by medical subject experts. Relevant, accurate training data increases the probability of an accurate transcription, but does not guarantee accuracy. Software trained on thousands of real clinical conversations generated transcripts with lower word error rates. Software trained on manually-transcribed training data did better than software trained with automatically transcribed training data (such as YouTube captions).\nAutoscribes omit parts of the conversation classes as irrelevant. The may wrongly classify pertinent information as irrelevant and omit it. They may also confuse historic and current symptoms, or otherwise misclassify information. They may also simply wrongly transcribe the speech, writing something incorrect instead. If clinicians do not carefully check the recording, such mistakes could make their way into their medical records and cause patient harms.\n\nPatient consent\nProfessional organizations generally require that scribes be used only with patient consent; some bodies may require written consent. Medics must also abide by local surveillance laws, which may criminalize recording private conversations without consent. Full information on how data is encrypted, transmitted, stored, and destroyed should be provided. In some jurisdictions, it is illegal to transmit the data to any country without equivalent privacy laws, or process or store the data there; vendors who cannot guarantee that their products won't illegally send data abroad cannot be legally used.\nSome vendors collect data for reuse or resale. Medical professionals are generally considered to have a duty to review the terms and conditions of the user agreement and identify such data reuse. General practices are generally required to provide information on secondary uses to patients, allow them to opt out of secondary uses, and obtain consent for each specific secondary use. Data must only be used for agreed-upon purposes.\n\nTechnology and market\nThe medical scribe market is, as of 2024, highly competitive, with over 50 products on the market. Many of these products are just proprietary wrappers around the same LLM backends, including backends whose designers have warned they are not to be used for critical applications like medicine. Some vendors market scribes specialized to specific branches of medicine (though most target general practitioners, who make up about a third of doctors). Increasingly, vendors market their products as more than scribes, claiming that they are intelligent assistants and co-pilots to doctors. These broader uses raise more accuracy concerns. Extracting information from the conversation to autopopulate a form, for instance, may be problematic, with symptoms incorrectly auto-labelled as \"absent\" even if they were repeatedly discussed. Models failed to extract many indirect descriptions of symptoms, like a patient saying they could only sleep for four hours (instead of using the word \"insomnia\").\nLLMs are not trained to produce facts, but things which look like facts. The use of templates and rules can make them more reliable at extracting semantic information, but \"confabulations\" or \"hallucinations\" (convincing but wrong output) are an intrinsic part of the technology.\n\nPricing\nWith the exception of fully open-source programs, which are free, medical scribe computer programs are rented rather than sold (\"software as a service\"). Monthly fees vary from mid-two figures to four figures, in US dollars. Some companies run on a freemium model, where a certain number of transcriptions per month are free.\nScribes that integrate into Electronic Health Records, removing the need for copy-pasting, typically cost more.\nFully open-source scribes provide the software for free. The user can install it on hardware of their choice, or pay to have it installed. Some open-source scribes can be installed on the local device (that is, the one recording the audio) or on a local server (for instance, one serving a single clinic). They can typically be set not to send any information externally, and can indeed be used with no internet connection.\n\nImpact in healthcare\nAI medical scribes are transforming the healthcare industry by directly addressing some of the most pressing challenges clinicians face, especially the administrative burden that contributes to burnout.\n\nReducing clinician burnout\nOne of the most significant impacts of AI scribes is their ability to alleviate the overwhelming documentation workload that healthcare professionals face. By automating the transcription and summarization of consultations, AI scribes free up valuable time that clinicians would otherwise spend on administrative tasks. Studies have shown that the average clinician spends a significant portion of their workday on documentation, leading to fatigue and diminishing patient interaction. For example, in the UK's largest clinical rollout of ambient AI, 4 in 5 GPs using the tool said it saved them time, with the same number reporting that it enabled them to build a better rapport with patients.\nBy automating these repetitive tasks, AI scribes create a healthier work-life balance for clinicians, allowing them to focus on patient care and reduce after-hours charting. This reduction in administrative burden directly contributes to lower levels of stress and burnout, a concern that has been exacerbated in healthcare settings in recent years. The ability to offload routine documentation tasks helps clinicians reclaim their time and mental energy, leading to improved overall job satisfaction.\n\nEnhancing job satisfaction\nIn addition to reducing burnout, AI scribes also improve job satisfaction among clinicians by allowing them to focus on the aspects of their work that they find most meaningful: patient interaction and clinical decision-making. Clinicians have reported feeling more present with their patients, as they are no longer distracted by the need to constantly type or dictate notes during consultations. This shift allows for more meaningful conversations with patients, improving the quality of care provided. By streamlining workflows and making documentation more efficient, AI scribes also empower healthcare workers to take on more fulfilling tasks and foster a greater sense of purpose in the work they do.\n\nImproving healthcare worker conditions\nThe rise of AI scribes is part of a broader trend of AI and automation being integrated into healthcare to improve worker conditions. AI's role is not just to replace human effort but to support it by allowing clinicians to focus on the core elements of their jobs: providing care, interacting with patients, and making critical medical decisions. With AI helping to manage the burden of documentation, clinicians are less likely to experience the high levels of burnout and job dissatisfaction that have become widespread in healthcare environments. Therefore, AI scribes are a key component in the future of healthcare, supporting the mental health of clinicians and fostering a more sustainable healthcare system.\n\nSee also\nMedical scribe\nAi in Healthcare\nLarge Language Model\nArtificial Intelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Automated_medical_scribe"
    },
    {
        "title": "Automated negotiation",
        "text": "Automated negotiation is a form of interaction in systems that are composed of multiple autonomous agents, in which the aim is to reach agreements through an iterative process of making offers.\nAutomated negotiation can be employed for many tasks human negotiators regularly engage in, such as bargaining and joint decision making. The main topics in automated negotiation revolve around the design of protocols and strategies.\n\nHistory\nThrough digitization, the beginning of the 21st century has seen a growing interest in the automation of negotiation and e-negotiation systems, for example in the setting of e-commerce. This interest is fueled by the promise of automated agents being able to negotiate on behalf of human negotiators, and to find better outcomes than human negotiators.\n\nExamples\nExamples of automated negotiation include:\n\nOnline dispute resolution, in which disagreements between parties are settled.\nSponsored search auction, where bids are placed on advertisement keywords.\nContent negotiation, in which user agents negotiate over HTTP about how to best represent a web resource.\nNegotiation support systems, in which negotiation decision-making activities are supported by an information system.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Automated_negotiation"
    },
    {
        "title": "Autonomic networking",
        "text": "Autonomic networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. Its ultimate aim is to create self-managing networks to overcome the rapidly growing complexity of the Internet and other networks and to enable their further growth, far beyond the size of today.\n\nIncreasing size and complexity\nThe ever-growing management complexity of the Internet caused by its rapid growth is seen by some experts as a major problem that limits its usability in the future.\nWhat's more, increasingly popular smartphones, PDAs, networked audio and video equipment,  and game consoles need to be interconnected. Pervasive Computing not only adds features, but also burdens existing networking infrastructure with more and more tasks that sooner or later will not be manageable by human intervention alone.\nAnother important aspect is the price of manually controlling huge numbers of vitally important devices of current network infrastructures.\n\nAutonomic nervous system\nThe autonomic nervous system (ANS) is the part of complex biological nervous systems that is not consciously controlled. It regulates bodily functions and the activity of specific organs. As proposed by IBM, future communication systems might be designed in a similar way to the ANS.\n\nComponents of autonomic networking\nAs autonomics conceptually derives from biological entities such as the human autonomic nervous system, each of the areas can be metaphorically related to functional and structural aspects of a living being. In the human body, the autonomic system facilitates and regulates a variety of functions including respiration, blood pressure and circulation, and emotive response. The autonomic nervous system is the interconnecting fabric that supports feedback loops between internal states and various sources by which internal and external conditions are monitored.\n\nAutognostics\nAutognostics includes a range of self-discovery, awareness, and analysis capabilities that provide the autonomic system with a view on high-level state. In metaphor, this represents the perceptual sub-systems that gather, analyze, and report on internal and external states and conditions – for example, this might be viewed as the eyes, visual cortex and perceptual organs of the system. Autognostics, or literally \"self-knowledge\", provides the autonomic system with a basis for response and validation.\nA rich autognostic capability may include many different \"perceptual senses\". For example, the human body gathers information via the usual five senses, the so-called sixth sense of proprioception (sense of body position and orientation), and through emotive states that represent the gross wellness of the body. As conditions and states change, they are detected by the sensory monitors and provide the basis for adaptation of related systems. Implicit in such a system are imbedded models of both internal and external environments such that relative value can be assigned to any perceived state - perceived physical threat (e.g. a snake) can result in rapid shallow breathing related to fight-flight response, a phylogenetically effective model of interaction with recognizable threats.\nIn the case of autonomic networking, the state of the network may be defined by inputs from:\n\nindividual network elements such as switches and network interfaces including\nspecification and configuration\nhistorical records and current state\ntraffic flows\nend-hosts\napplication performance data\nlogical diagrams and design specifications\nMost of these sources represent relatively raw and unprocessed views that have limited relevance. Post-processing and various forms of analysis must be applied to generate meaningful measurements and assessments against which current state can be derived.\nThe autognostic system interoperates with:\n\nconfiguration management - to control network elements and interfaces\npolicy management - to define performance objectives and constraints\nautodefense - to identify attacks and accommodate the impact of defensive responses\n\nConfiguration management\nConfiguration management is responsible for the interaction with network elements and interfaces. It includes an accounting capability with historical perspective that provides for the tracking of configurations over time, with respect to various circumstances. In the biological metaphor, these are the hands and, to some degree, the memory of the autonomic system.\nOn a network, remediation and provisioning are applied via configuration setting of specific devices. Implementation affecting access and selective performance with respect to role and relationship are also applied. Almost all the \"actions\" that are currently taken by human engineers fall under this area. With only a few exceptions, interfaces are set by hand, or by extension of the hand, through automated scripts.\nImplicit in the configuration process is the maintenance of a dynamic population of devices under management, a historical record of changes and the directives which invoked change. Typical to many accounting functions, configuration management should be capable of operating on devices and then rolling back changes to recover previous configurations. Where change may lead to unrecoverable states, the sub-system should be able to qualify the consequences of changes prior to issuing them.\nAs directives for change must originate from other sub-systems, the shared language for such directives must be abstracted from the details of the devices involved. The configuration management sub-system must be able to translate unambiguously between directives and hard actions or to be able to signal the need for further detail on a directive. An inferential capacity may be appropriate to support sufficient flexibility (i.e. configuration never takes place because there is no unique one-to-one mapping between directive and configuration settings). Where standards are not sufficient, a learning capacity may also be required to acquire new knowledge of devices and their configuration.\nConfiguration management interoperates with all of the other sub-systems including:\n\nautognostics - receives direction for and validation of changes\npolicy management - implements policy models through mapping to underlying resources\nsecurity - applies access and authorization constraints for particular policy targets\nautodefense - receives direction for changes\n\nPolicy management\nPolicy management includes policy specification, deployment, reasoning over policies, updating and maintaining policies, and enforcement. Policy-based management is required for:\n\nconstraining different kinds of behavior including security, privacy, resource access, and collaboration\nconfiguration management\ndescribing business processes and defining performance\ndefining role and relationship, and establishing trust and reputation\nIt provides the models of environment and behavior that represent effective interaction according to specific goals. In the human nervous system metaphor, these models are implicit in the evolutionary \"design\" of biological entities and specific to the goals of survival and procreation. Definition of what constitutes a policy is necessary to consider what is involved in managing it. A relatively flexible and abstract framework of values, relationships, roles, interactions, resources, and other components of the network environment is required. This sub-system extends far beyond the physical network to the applications in use and the processes and end-users that employ the network to achieve specific goals. It must express the relative values of various resources, outcomes, and processes and include a basis for assessing states and conditions.\nUnless embodied in some system outside the autonomic network or implicit to the specific policy implementation, the framework must also accommodate the definition of process, objectives and goals. Business process definitions and descriptions are then an integral part of the policy implementation. Further, as policy management represents the ultimate basis for the operation of the autonomic system, it must be able to report on its operation with respect to the details of its implementation.\nThe policy management sub-system interoperates (at least) indirectly with all other sub-systems but primarily interacts with:\n\nautognostics - providing the definition of performance and accepting reports on conditions\nconfiguration management - providing constraints on device configuration\nsecurity - providing definitions of roles, access and permissions\n\nAutodefense\nAutodefense represents a dynamic and adaptive mechanism that responds to malicious and intentional attacks on the network infrastructure, or use of the network infrastructure to attack IT resources. As defensive measures tend to impede the operation of IT, it is optimally capable of balancing performance objectives with typically over-riding threat management actions. In the biological metaphor, this sub-system offers mechanisms comparable to the immune system.\nThis sub-system must proactively assess network and application infrastructure for risks, detect and identify threats, and define effective both proactive and reactive defensive responses. It has the role of the warrior and the security guard insofar as it has roles for both maintenance and corrective activities. Its relationship with security is close but not identical – security is more concerned with appropriately defined and implemented access and authorization controls to maintain legitimate roles and process. Autodefense deals with forces and processes, typically malicious, outside the normal operation of the system that offer some risk to successful execution.\nAutodefense requires high-level and detailed knowledge of the entire network as well as imbedded models of risk that allow it to analyze dynamically the current status. Corrections to decrease risk must be considered in balance with performance objectives and value of process goals – an overzealous defensive response can immobilize the system (like the immune system inappropriately invoking an allergic reaction). The detection of network or application behaviors that signal possible attack or abuse is followed by the generation of an appropriate response – for example, ports might be temporarily closed or packets with a specific source or destination might be filtered out. Further assessment generates subsequent changes either relaxing the defensive measures or strengthening them.\nAutodefense interoperates closely with:\n\nsecurity - receives definition of roles and security constraints, and defines risk for proactive mitigation\nconfiguration management - receives details of network for analysis and directs changes in elements in response to anticipated or detected attack\nautognostics - receives notification of detected behaviors\nIt also may receive definition of relative value of various resources and processes from policy management in order to develop responses consistent with policy.\n\nSecurity\nSecurity provides the structure that defines and enforces the relationships between roles, content, and resources, particularly with respect to access. It includes the framework for definitions as well as the means to implement them. In metaphor, security parallels the complex mechanisms underlying social interactions, defining friends, foes, mates and allies and offering access to limited resources on the basis of assessed benefit.\nSeveral key means are employed by security – they include the well-known 3 As of authentication, authorization, and access (control). The basis for applying these means requires the definition of roles and their relationships to resources, processes and each other. High-level concepts like privacy, anonymity and verification are likely imbedded in the form of the role definitions and derive from policy. Successful security reliably supports and enforces roles and relationships.\nAutodefense has a close association with security – maintaining the assigned roles in balance with performance exposes the system to potential violations in security. In those cases, the system must compensate by making changes that may sacrifice balance on a temporary basis and indeed may violate the operational terms of security itself. Typically the two are viewed as inextricably intertwined – effective security somewhat hopefully negating any need for a defensive response. Security's revised role is to mediate between the competing demands from policy for maximized performance and minimized risk with auto defense recovering the balance when inevitable risk translates to threat. Federation represents one of the key challenges to be solved by effective security.\nThe security sub-system interoperates directly with:\n\npolicy management - receiving high-level directives related to access and priority\nconfiguration management - sending specifics for access and admission control\nautodefense - receiving over-riding directives under threat and sending security constraint details for risk assessment\n\nConnection fabric\nThe connection fabric supports the interaction with all the elements and sub-systems of the autonomic system. It may be composed of a variety of means and mechanisms, or may be a single central framework. The biological equivalent is the central nervous system itself – although referred to as the autonomic system, it actually is only the communication conduit between the human body's faculties.\n\nPrinciples of autonomic networking\nConsequently, it is currently under research by many research projects, how principles and paradigms of mother nature might be applied to networking.\n\nCompartmentalization\nInstead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization.\n\nFunction re-composition\nThe goal is to produce an architectural design that enables flexible, dynamic, and fully autonomic formation of large-scale networks in which the functionalities of each constituent network node are also composed in an autonomic fashion\n\nAtomization\nFunctions should be divided into atomic units to allow for maximal re-composition freedom.\n\nClosed control loop\nA fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking. A closed control loop maintains the properties of the controlled system within desired bounds by constantly monitoring target parameters.\n\nSee also\nAutonomic Computing\nAutonomic system (computing)\nCognitive networks\nNetwork Compartment\nCollaborative innovation network\nIn-Network Management\nGeneric Autonomic Networking Architecture (GANA) EFIPSANS Project http://www.efipsans.org/\n\nReferences\nExternal links\nIBM Autonomic Computing Website\nIntel White Paper: Towards an Autonomic Framework\nIpanema Technologies: Autonomic Networking applied to application performance optimization Archived 2009-04-26 at the Wayback Machine\n\nResearch projects\nANA Project: Autonomic Network Architecture\nANAPORT is an open bibliography reference developed within the ANA project\nBeyond-The-Horizon: Coordination Action by the European Commission\nBionets: Biologically-inspired concepts for networking\nBiSNET: Biologically-inspired architecture for Sensor NETworks\nBiSNET/e: A Cognitive Sensor Networking Architecture with Evolutionary Multiobjective Optimization\nComponent-ware for Autonomic Situation-aware Communications, and Dynamically Adaptable Services\nDiet Agents: Indefinitely scalable hosting for systems of autonomic interacting processes\nEFIPSANS Project: Exposing the Features in IP version Six protocols that can be exploited/extended for the purposes of designing/building autonomic Networks and Services\nHaggle: An innovative Paradigm for Autonomic Opportunistic Communication\nSOCRATES: Self-Optimization and Self-Configuration in Wireless Networks\nDynamically Self Configuring Automotive System\nSelf-NET: Self-Management of Cognitive Future InterNET Elements\nAutHoNe: Autonomic Home Networking \nSymbioticSphere: A Biologically-inspired Architecture for Scalable, Adaptive and Survivable Network Systems\nTRANS: TRANS demonstrate a tightly integrated network and service overlay architecture with advanced traffic-aware and self-organisation functionality\nUniverSELF project: Realising autonomics for Future Networks\n\nBlogs and Wikis\nAutonomic Networking Wiki: A wiki dedicated to Autonomic Networking\nAutonomic networking at the core of enterprise Wan governance blog",
        "url": "https://en.wikipedia.org/wiki/Autonomic_networking"
    },
    {
        "title": "Autonomous agent",
        "text": "An autonomous agent is an artificial intelligence (AI) system that can perform complex tasks independently.\n\nDefinitions\nThere are various definitions of autonomous agent. According to Brustoloni (1991):\n\n \"Autonomous agents are systems capable of autonomous, purposeful action in the real world.\"\nAccording to Maes (1995):\n\n \"Autonomous agents are computational systems that inhabit some complex dynamic environment, sense and act autonomously in this environment, and by doing so realize a set of goals or tasks for which they are designed.\"\nFranklin and Graesser (1997) review different definitions and propose their definition:\n\n \"An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.\" \nThey explain that:\n\n \"Humans and some animals are at the high end of being an agent, with multiple, conflicting drives, multiples senses, multiple possible actions, and complex sophisticated control structures. At the low end, with one or two senses, a single action, and an absurdly simple control structure we find a thermostat.\"\n\nAgent appearance\nLee et al. (2015) post safety issue from how the combination of external appearance and internal autonomous agent have impact on human reaction about autonomous vehicles. Their study explores the human-like appearance agent and high level of autonomy are strongly correlated with social presence, intelligence, safety and trustworthiness. In specific, appearance impacts most on affective trust while autonomy impacts most on both affective and cognitive domain of trust where cognitive trust is characterized by knowledge-based factors and affective trust is largely emotion driven.\n\nApplications\nAgentic AI systems: Advanced AI agents that can scope out projects and complete them with necessary tools, representing a significant evolution from simple task-oriented systems.\nInternet of things (IoT) Integration: Autonomous agents increasingly interact with IoT devices, enabling smart home systems, industrial monitoring, and urban infrastructure management.\nCollaborative software development: Tools like Cognition AI's Devin aim to create autonomous software engineers capable of complex reasoning, planning, and completing engineering tasks requiring thousands of decisions.\n\nSee also\nActor model\nAmbient intelligence\nAutoGPT\nAutonomous agency theory\nChatbot\nEmbodied agent\nIntelligent agent\nIntelligent control\nMulti-agent system\nSoftware agent\n\nReferences\nExternal links\n\"Autonomous Robot Behaviors\". Archived from the original on December 3, 2013.\nRequirements for materializing Autonomous Agents\nSun, Ron (September 1, 2001). Duality of the Mind: A Bottom-up Approach Toward Cognition. New Jersey: Lawrence Erlbaum. p. 304. ISBN 978-0-585-39404-6.",
        "url": "https://en.wikipedia.org/wiki/Autonomous_agent"
    },
    {
        "title": "AZFinText",
        "text": "Arizona Financial Text System (AZFinText) is a textual-based quantitative financial prediction system written by Robert P. Schumaker of University of Texas at Tyler and Hsinchun Chen of the University of Arizona.\n\nSystem\nThis system differs from other systems in that it uses financial text as one of its key means of predicting stock price movement. This reduces the information lag-time problem evident in many similar systems where new information must be transcribed (e.g., such as losing a costly court battle or having a product recall), before the quant can react appropriately. AZFinText overcomes these limitations by utilizing the terms used in financial news articles to predict future stock prices twenty minutes after the news article has been released.\nIt is believed that certain article terms can move stocks more than others.  Terms such as factory exploded or workers strike will have a depressing effect on stock prices whereas terms such as earnings rose will tend to increase stock prices.\nWhen a human trading expert sees certain terms, they will react in a somewhat predictable fashion.  AZFinText capitalizes on the arbitrage opportunities that exist when investment experts over and under-react to certain news stories. By analyzing breaking financial news articles and focusing on specific parts of speech, portfolio selection, term weighting and even article sentiment, the AZFinText system becomes a powerful tool and is a radically different way of looking at stock market prediction.\n\nOverview of research\nThe foundation of AZFinText can be found in the ACM TOIS article. Within this paper, the authors tested several different prediction models and linguistic textual representations.  From this work, it was found that using the article terms and the price of the stock at the time the article was released was the most effective model and using proper nouns was the most effective textual representation technique.  Combining the two, AZFinText netted a 2.84% trading return over the five-week study period.\nAZFinText was then extended to study what combination of peer organizations help to best train the system. Using the premise that IBM has more in common with Microsoft than GM, AZFinText studied the effect of varying peer-based training sets.  To do this, AZFinText trained on the various levels of GICS and evaluated the results.  It was found that sector-based training was most effective, netting an 8.50% trading return, outperforming Jim Cramer, Jim Jubak and DayTraders.com during the study period.  AZFinText was also compared against the top 10 quantitative systems and outperformed 6 of them.\nA third study investigated the role of portfolio building in a textual financial prediction system. From this study, Momentum and Contrarian stock portfolios were created and tested.  Using the premise that past winning stocks will continue to win and past losing stocks will continue to lose, AZFinText netted a 20.79% return during the study period.  It was also noted that traders were generally overreacting to news events, creating the opportunity of abnormal returns.\nA fourth study looked into using author sentiment as an added predictive variable. Using the premise that an author can unwittingly influence market trades simply by the terms they use, AZFinText was tested using tone and polarity features.  It was found that Contrarian activity was occurring within the market, where articles of a positive tone would decrease in price and articles of a negative tone would increase in price.\nA further study investigated what article verbs have the most influence on stock price movement. From this work, it was found that planted, announcing, front, smaller and crude had the highest positive impact on stock price.\n\nNotable publicity\nAZFinText has been the topic of discussion by numerous media outlets.  Some of the more notable ones include The Wall Street Journal, MIT's Technology Review, Dow Jones Newswire, WBIR in Knoxville, TN, Slashdot and other media outlets.\n\nReferences\nExternal links\nhttps://blogs.wsj.com/digits/2010/06/21/using-artificial-intelligence-to-digest-financial-news/\nslashdot.org/story/10/06/12/1341212/Quant-AI-Picks-Stocks-Better-Than-Humans\nwww.technologyreview.com/blog/guest/25308/",
        "url": "https://en.wikipedia.org/wiki/AZFinText"
    },
    {
        "title": "Bayesian programming",
        "text": "Bayesian programming is a formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.\nEdwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science he developed this theory and proposed what he called “the robot,” which was not\na physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this \"robot\".\nBayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.\n\nFormalism\nA Bayesian program is a means of specifying a family of probability distributions.\nThe constituent elements of a Bayesian program are presented below:\n\n  \n    \n      \n        \n          Program\n        \n        \n          \n            {\n            \n              \n                \n                  \n                    Description\n                  \n                  \n                    \n                      {\n                      \n                        \n                          \n                            \n                              Specification\n                            \n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      \n                                        Variables\n                                      \n                                    \n                                  \n                                  \n                                    \n                                      \n                                        Decomposition\n                                      \n                                    \n                                  \n                                  \n                                    \n                                      \n                                        Forms\n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            \n                              Identification (based on \n                            \n                            δ\n                            )\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  \n                    Question\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Program}}{\\begin{cases}{\\text{Description}}{\\begin{cases}{\\text{Specification}}(\\pi ){\\begin{cases}{\\text{Variables}}\\\\{\\text{Decomposition}}\\\\{\\text{Forms}}\\\\\\end{cases}}\\\\{\\text{Identification (based on }}\\delta )\\end{cases}}\\\\{\\text{Question}}\\end{cases}}}\n  \n\nA program is constructed from a description and a question.\nA description is constructed using some specification (\n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n) as given by the programmer and an identification or learning process for the parameters not completely specified by the specification, using a data set (\n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n).\nA specification is constructed from a set of pertinent variables, a decomposition and a set of forms.\nForms are either parametric forms or questions to other Bayesian programs.\nA question specifies which probability distribution has to be computed.\n\nDescription\nThe purpose of a description is to specify an effective method of computing a joint probability distribution\non a set of variables \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            ⋯\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n given a set of experimental data \n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n and some\nspecification \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. This joint distribution is denoted as: \n  \n    \n      \n        P\n        \n          (\n          \n            \n              X\n              \n                1\n              \n            \n            ∧\n            \n              X\n              \n                2\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              X\n              \n                N\n              \n            \n            ∣\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)}\n  \n.\nTo specify preliminary knowledge \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the programmer must undertake the following:\n\nDefine the set of relevant variables \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            ⋯\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n on which the joint distribution is defined.\nDecompose the joint distribution (break it into relevant independent or conditional probabilities).\nDefine the forms of each of the distributions (e.g., for each variable, one of the list of probability distributions).\n\nDecomposition\nGiven a partition of \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\ldots ,X_{N}\\right\\}}\n  \n containing \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n subsets, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n variables are defined\n\n  \n    \n      \n        \n          L\n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          L\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle L_{1},\\cdots ,L_{K}}\n  \n, each corresponding to one of these subsets.\nEach variable \n  \n    \n      \n        \n          L\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle L_{k}}\n  \n is obtained as the conjunction of the variables \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                \n                  k\n                  \n                    1\n                  \n                \n              \n            \n            ,\n            \n              X\n              \n                \n                  k\n                  \n                    2\n                  \n                \n              \n            \n            ,\n            ⋯\n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{k_{1}},X_{k_{2}},\\cdots \\right\\}}\n  \n\nbelonging to the \n  \n    \n      \n        \n          k\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle k^{th}}\n  \n subset. Recursive application of Bayes' theorem leads to:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ∧\n                    \n                      X\n                      \n                        2\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      X\n                      \n                        N\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      L\n                      \n                        K\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        2\n                      \n                    \n                    ∣\n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                ⋯\n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        K\n                      \n                    \n                    ∣\n                    \n                      L\n                      \n                        K\n                        −\n                        1\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)\\\\={}&P\\left(L_{1}\\wedge \\cdots \\wedge L_{K}\\mid \\delta \\wedge \\pi \\right)\\\\={}&P\\left(L_{1}\\mid \\delta \\wedge \\pi \\right)\\times P\\left(L_{2}\\mid L_{1}\\wedge \\delta \\wedge \\pi \\right)\\times \\cdots \\times P\\left(L_{K}\\mid L_{K-1}\\wedge \\cdots \\wedge L_{1}\\wedge \\delta \\wedge \\pi \\right)\\end{aligned}}}\n  \n\nConditional independence hypotheses then allow further simplifications. A conditional\nindependence hypothesis for variable \n  \n    \n      \n        \n          L\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle L_{k}}\n  \n is defined by choosing some variable \n  \n    \n      \n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{n}}\n  \n\namong the variables appearing in the conjunction \n  \n    \n      \n        \n          L\n          \n            k\n            −\n            1\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          L\n          \n            2\n          \n        \n        ∧\n        \n          L\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle L_{k-1}\\wedge \\cdots \\wedge L_{2}\\wedge L_{1}}\n  \n, labelling \n  \n    \n      \n        \n          R\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle R_{k}}\n  \n as the\nconjunction of these chosen variables and setting:\n\n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              L\n              \n                k\n                −\n                1\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              L\n              \n                1\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n        =\n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid L_{k-1}\\wedge \\cdots \\wedge L_{1}\\wedge \\delta \\wedge \\pi \\right)=P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n\nWe then obtain:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ∧\n                    \n                      X\n                      \n                        2\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      X\n                      \n                        N\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        2\n                      \n                    \n                    ∣\n                    \n                      R\n                      \n                        2\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                ⋯\n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        K\n                      \n                    \n                    ∣\n                    \n                      R\n                      \n                        K\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)\\\\={}&P\\left(L_{1}\\mid \\delta \\wedge \\pi \\right)\\times P\\left(L_{2}\\mid R_{2}\\wedge \\delta \\wedge \\pi \\right)\\times \\cdots \\times P\\left(L_{K}\\mid R_{K}\\wedge \\delta \\wedge \\pi \\right)\\end{aligned}}}\n  \n\nSuch a simplification of the joint distribution as a product of simpler distributions is\ncalled a decomposition, derived using the chain rule.\nThis ensures that each variable appears at the most once on the left of a conditioning\nbar, which is the necessary and sufficient condition to write mathematically valid\ndecompositions.\n\nForms\nEach distribution \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n appearing in the product is then associated\nwith either a parametric form (i.e., a function \n  \n    \n      \n        \n          f\n          \n            μ\n          \n        \n        \n          (\n          \n            L\n            \n              k\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f_{\\mu }\\left(L_{k}\\right)}\n  \n) or a question to another Bayesian program \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n        =\n        P\n        \n          (\n          \n            L\n            ∣\n            R\n            ∧\n            \n              \n                \n                  δ\n                  ^\n                \n              \n            \n            ∧\n            \n              \n                \n                  π\n                  ^\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)=P\\left(L\\mid R\\wedge {\\widehat {\\delta }}\\wedge {\\widehat {\\pi }}\\right)}\n  \n.\nWhen it is a form \n  \n    \n      \n        \n          f\n          \n            μ\n          \n        \n        \n          (\n          \n            L\n            \n              k\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f_{\\mu }\\left(L_{k}\\right)}\n  \n, in general, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n is a vector of parameters that may depend on \n  \n    \n      \n        \n          R\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle R_{k}}\n  \n or \n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n or both. Learning\ntakes place when some of these parameters are computed using the data set \n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n.\nAn important feature of Bayesian Programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program. \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n is obtained by some inferences done by another Bayesian program defined by the specifications \n  \n    \n      \n        \n          \n            \n              π\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\pi }}}\n  \n and the data \n  \n    \n      \n        \n          \n            \n              δ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\delta }}}\n  \n. This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models.\n\nQuestion\nGiven a description (i.e., \n  \n    \n      \n        P\n        \n          (\n          \n            \n              X\n              \n                1\n              \n            \n            ∧\n            \n              X\n              \n                2\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              X\n              \n                N\n              \n            \n            ∣\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)}\n  \n), a question is obtained by partitioning \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            ⋯\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n\ninto three sets: the searched variables, the known variables and\nthe free variables.\nThe 3 variables \n  \n    \n      \n        S\n        e\n        a\n        r\n        c\n        h\n        e\n        d\n      \n    \n    {\\displaystyle Searched}\n  \n, \n  \n    \n      \n        K\n        n\n        o\n        w\n        n\n      \n    \n    {\\displaystyle Known}\n  \n and \n  \n    \n      \n        F\n        r\n        e\n        e\n      \n    \n    {\\displaystyle Free}\n  \n are defined as the\nconjunction of the variables belonging to\nthese sets.\nA question is defined as the set\nof distributions:\n\n  \n    \n      \n        P\n        \n          (\n          \n            S\n            e\n            a\n            r\n            c\n            h\n            e\n            d\n            ∣\n            \n              Known\n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(Searched\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)}\n  \n\nmade of many \"instantiated questions\" as the cardinal of \n  \n    \n      \n        K\n        n\n        o\n        w\n        n\n      \n    \n    {\\displaystyle Known}\n  \n,\neach instantiated question being the distribution:\n\n  \n    \n      \n        P\n        \n          (\n          \n            \n              Searched\n            \n            ∣\n            \n              Known\n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)}\n\nInference\nGiven the joint distribution \n  \n    \n      \n        P\n        \n          (\n          \n            \n              X\n              \n                1\n              \n            \n            ∧\n            \n              X\n              \n                2\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              X\n              \n                N\n              \n            \n            ∣\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)}\n  \n, it is always possible to compute any possible question using the following general inference:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      Searched\n                    \n                    ∣\n                    \n                      Known\n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                \n                  ∑\n                  \n                    Free\n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          Searched\n                        \n                        ∧\n                        \n                          Free\n                        \n                        ∣\n                        \n                          Known\n                        \n                        ∧\n                        δ\n                        ∧\n                        π\n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      \n                        ∑\n                        \n                          Free\n                        \n                      \n                      \n                        [\n                        \n                          P\n                          \n                            (\n                            \n                              \n                                Searched\n                              \n                              ∧\n                              \n                                Free\n                              \n                              ∧\n                              \n                                Known\n                              \n                              ∣\n                              δ\n                              ∧\n                              π\n                            \n                            )\n                          \n                        \n                        ]\n                      \n                    \n                    \n                      P\n                      \n                        (\n                        \n                          \n                            Known\n                          \n                          ∣\n                          δ\n                          ∧\n                          π\n                        \n                        )\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      \n                        ∑\n                        \n                          Free\n                        \n                      \n                      \n                        [\n                        \n                          P\n                          \n                            (\n                            \n                              \n                                Searched\n                              \n                              ∧\n                              \n                                Free\n                              \n                              ∧\n                              \n                                Known\n                              \n                              ∣\n                              δ\n                              ∧\n                              π\n                            \n                            )\n                          \n                        \n                        ]\n                      \n                    \n                    \n                      \n                        ∑\n                        \n                          \n                            Free\n                          \n                          ∧\n                          \n                            Searched\n                          \n                        \n                      \n                      \n                        [\n                        \n                          P\n                          \n                            (\n                            \n                              \n                                Searched\n                              \n                              ∧\n                              \n                                Free\n                              \n                              ∧\n                              \n                                Known\n                              \n                              ∣\n                              δ\n                              ∧\n                              π\n                            \n                            )\n                          \n                        \n                        ]\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    1\n                    Z\n                  \n                \n                ×\n                \n                  ∑\n                  \n                    Free\n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          Searched\n                        \n                        ∧\n                        \n                          Free\n                        \n                        ∧\n                        \n                          Known\n                        \n                        ∣\n                        δ\n                        ∧\n                        π\n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)\\\\={}&\\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)\\right]\\\\={}&{\\frac {\\displaystyle \\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]}{\\displaystyle P\\left({\\text{Known}}\\mid \\delta \\wedge \\pi \\right)}}\\\\={}&{\\frac {\\displaystyle \\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]}{\\displaystyle \\sum _{{\\text{Free}}\\wedge {\\text{Searched}}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]}}\\\\={}&{\\frac {1}{Z}}\\times \\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]\\end{aligned}}}\n  \n\nwhere the first equality results from the marginalization rule, the second\nresults from Bayes' theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  \n.\nTheoretically, this allows to solve any Bayesian inference problem. In practice,\nhowever, the cost of computing exhaustively and exactly \n  \n    \n      \n        P\n        \n          (\n          \n            \n              Searched\n            \n            ∣\n            \n              Known\n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)}\n  \n is too great in almost all cases.\nReplacing the joint distribution by its decomposition we get:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      Searched\n                    \n                    ∣\n                    \n                      Known\n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    1\n                    Z\n                  \n                \n                \n                  ∑\n                  \n                    Free\n                  \n                \n                \n                  [\n                  \n                    \n                      ∏\n                      \n                        k\n                        =\n                        1\n                      \n                      \n                        K\n                      \n                    \n                    \n                      [\n                      \n                        P\n                        \n                          (\n                          \n                            \n                              L\n                              \n                                i\n                              \n                            \n                            ∣\n                            \n                              K\n                              \n                                i\n                              \n                            \n                            ∧\n                            π\n                          \n                          )\n                        \n                      \n                      ]\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)\\\\={}&{\\frac {1}{Z}}\\sum _{\\text{Free}}\\left[\\prod _{k=1}^{K}\\left[P\\left(L_{i}\\mid K_{i}\\wedge \\pi \\right)\\right]\\right]\\end{aligned}}}\n  \n\nwhich is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions.\n\nExample\nBayesian spam detection\nThe purpose of Bayesian spam filtering is to eliminate junk e-mails.\nThe problem is very easy to formulate. E-mails should be classified\ninto one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model.\nThe classifier should furthermore be able to adapt to its user and to learn\nfrom experience. Starting from an initial standard setting, the classifier should\nmodify its internal parameters when the user disagrees with its own decision.\nIt will hence adapt to the user's criteria to differentiate between non-spam and\nspam. It will improve its results as it encounters increasingly classified e-mails.\n\nVariables\nThe variables necessary to write this program are as follows:\n\n  \n    \n      \n        S\n        p\n        a\n        m\n      \n    \n    {\\displaystyle Spam}\n  \n: a binary variable, false if the e-mail is not spam and true otherwise.\n\n  \n    \n      \n        \n          W\n          \n            0\n          \n        \n        ,\n        \n          W\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          W\n          \n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle W_{0},W_{1},\\ldots ,W_{N-1}}\n  \n: \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n binary variables. \n  \n    \n      \n        \n          W\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle W_{n}}\n  \n is true if the \n  \n    \n      \n        \n          n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word of the dictionary is present in the text.\nThese \n  \n    \n      \n        N\n        +\n        1\n      \n    \n    {\\displaystyle N+1}\n  \n binary variables sum up all the information\nabout an e-mail.\n\nDecomposition\nStarting from the joint distribution and applying recursively Bayes' theorem we obtain:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                ∧\n                \n                  W\n                  \n                    0\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  W\n                  \n                    N\n                    −\n                    1\n                  \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                )\n                ×\n                P\n                (\n                \n                  W\n                  \n                    0\n                  \n                \n                ∣\n                \n                  Spam\n                \n                )\n                ×\n                P\n                (\n                \n                  W\n                  \n                    1\n                  \n                \n                ∣\n                \n                  Spam\n                \n                ∧\n                \n                  W\n                  \n                    0\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                ×\n                ⋯\n              \n            \n            \n              \n              \n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      W\n                      \n                        N\n                        −\n                        1\n                      \n                    \n                    ∣\n                    \n                      Spam\n                    \n                    ∧\n                    \n                      W\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      W\n                      \n                        N\n                        −\n                        2\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P({\\text{Spam}}\\wedge W_{0}\\wedge \\cdots \\wedge W_{N-1})\\\\={}&P({\\text{Spam}})\\times P(W_{0}\\mid {\\text{Spam}})\\times P(W_{1}\\mid {\\text{Spam}}\\wedge W_{0})\\\\&\\times \\cdots \\\\&\\times P\\left(W_{N-1}\\mid {\\text{Spam}}\\wedge W_{0}\\wedge \\cdots \\wedge W_{N-2}\\right)\\end{aligned}}}\n  \n\nThis is an exact mathematical expression.\nIt can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model.\nFor instance, the programmer can assume that:\n\n  \n    \n      \n        P\n        (\n        \n          W\n          \n            1\n          \n        \n        ∣\n        \n          Spam\n        \n        ∧\n        \n          W\n          \n            0\n          \n        \n        )\n        =\n        P\n        (\n        \n          W\n          \n            1\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{1}\\mid {\\text{Spam}}\\land W_{0})=P(W_{1}\\mid {\\text{Spam}})}\n  \n\nto finally obtain:\n\n  \n    \n      \n        P\n        (\n        \n          Spam\n        \n        ∧\n        \n          W\n          \n            0\n          \n        \n        ∧\n        …\n        ∧\n        \n          W\n          \n            N\n            −\n            1\n          \n        \n        )\n        =\n        P\n        (\n        \n          Spam\n        \n        )\n        \n          ∏\n          \n            n\n            =\n            0\n          \n          \n            N\n            −\n            1\n          \n        \n        [\n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n        ]\n      \n    \n    {\\displaystyle P({\\text{Spam}}\\land W_{0}\\land \\ldots \\land W_{N-1})=P({\\text{Spam}})\\prod _{n=0}^{N-1}[P(W_{n}\\mid {\\text{Spam}})]}\n  \n\nThis kind of assumption is known as the naive Bayes' assumption. It is \"naive\" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is.\n\nParametric forms\nTo be able to compute the joint distribution, the programmer must now specify the\n\n  \n    \n      \n        N\n        +\n        1\n      \n    \n    {\\displaystyle N+1}\n  \n distributions appearing in the decomposition:\n\n  \n    \n      \n        P\n        (\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P({\\text{Spam}})}\n  \n is a prior defined, for instance, by \n  \n    \n      \n        P\n        (\n        [\n        \n          Spam\n        \n        =\n        1\n        ]\n        )\n        =\n        0.75\n      \n    \n    {\\displaystyle P([{\\text{Spam}}=1])=0.75}\n  \n\nEach of the \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n  forms \n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{n}\\mid {\\text{Spam}})}\n  \n may be specified using Laplace rule of succession (this is a pseudocounts-based smoothing technique to counter the zero-frequency problem of words never-seen-before):\n\n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        [\n        \n          Spam\n        \n        =\n        \n          false\n        \n        ]\n        )\n        =\n        \n          \n            \n              1\n              +\n              \n                a\n                \n                  f\n                \n                \n                  n\n                \n              \n            \n            \n              2\n              +\n              \n                a\n                \n                  f\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle P(W_{n}\\mid [{\\text{Spam}}={\\text{false}}])={\\frac {1+a_{f}^{n}}{2+a_{f}}}}\n  \n\n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        [\n        \n          Spam\n        \n        =\n        \n          true\n        \n        ]\n        )\n        =\n        \n          \n            \n              1\n              +\n              \n                a\n                \n                  t\n                \n                \n                  n\n                \n              \n            \n            \n              2\n              +\n              \n                a\n                \n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle P(W_{n}\\mid [{\\text{Spam}}={\\text{true}}])={\\frac {1+a_{t}^{n}}{2+a_{t}}}}\n  \n\nwhere \n  \n    \n      \n        \n          a\n          \n            f\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a_{f}^{n}}\n  \n stands for the number of appearances of the \n  \n    \n      \n        \n          n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word in non-spam e-mails and \n  \n    \n      \n        \n          a\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle a_{f}}\n  \n stands for the total number of non-spam e-mails. Similarly, \n  \n    \n      \n        \n          a\n          \n            t\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a_{t}^{n}}\n  \n stands for the number of appearances of the \n  \n    \n      \n        \n          n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word in spam e-mails and \n  \n    \n      \n        \n          a\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle a_{t}}\n  \n stands for the total number of spam e-mails.\n\nIdentification\nThe \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n  forms \n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{n}\\mid {\\text{Spam}})}\n  \n are not yet completely specified because the \n  \n    \n      \n        2\n        N\n        +\n        2\n      \n    \n    {\\displaystyle 2N+2}\n  \n parameters \n  \n    \n      \n        \n          a\n          \n            f\n          \n          \n            n\n            =\n            0\n            ,\n            …\n            ,\n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{f}^{n=0,\\ldots ,N-1}}\n  \n, \n  \n    \n      \n        \n          a\n          \n            t\n          \n          \n            n\n            =\n            0\n            ,\n            …\n            ,\n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{t}^{n=0,\\ldots ,N-1}}\n  \n, \n  \n    \n      \n        \n          a\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle a_{f}}\n  \n and \n  \n    \n      \n        \n          a\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle a_{t}}\n  \n have no values yet.\nThe identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.\nBoth methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user.\n\nQuestion\nThe question asked to the program is: \"what is the probability for a given text to be spam knowing which words appear and don't appear in this text?\"\nIt can be formalized by:\n\n  \n    \n      \n        P\n        (\n        \n          Spam\n        \n        ∣\n        \n          w\n          \n            0\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          w\n          \n            N\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle P({\\text{Spam}}\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})}\n  \n\nwhich can be computed as follows:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                ∣\n                \n                  w\n                  \n                    0\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  w\n                  \n                    N\n                    −\n                    1\n                  \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      P\n                      (\n                      \n                        Spam\n                      \n                      )\n                      \n                        ∏\n                        \n                          n\n                          =\n                          0\n                        \n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      [\n                      P\n                      (\n                      \n                        w\n                        \n                          n\n                        \n                      \n                      ∣\n                      \n                        Spam\n                      \n                      )\n                      ]\n                    \n                    \n                      \n                        ∑\n                        \n                          Spam\n                        \n                      \n                      [\n                      P\n                      (\n                      \n                        Spam\n                      \n                      )\n                      \n                        ∏\n                        \n                          n\n                          =\n                          0\n                        \n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      [\n                      P\n                      (\n                      \n                        w\n                        \n                          n\n                        \n                      \n                      ∣\n                      \n                        Spam\n                      \n                      )\n                      ]\n                      ]\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P({\\text{Spam}}\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})\\\\={}&{\\frac {\\displaystyle P({\\text{Spam}})\\prod _{n=0}^{N-1}[P(w_{n}\\mid {\\text{Spam}})]}{\\displaystyle \\sum _{\\text{Spam}}[P({\\text{Spam}})\\prod _{n=0}^{N-1}[P(w_{n}\\mid {\\text{Spam}})]]}}\\end{aligned}}}\n  \n\nThe denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        true\n                      \n                      ]\n                      ∣\n                      \n                        w\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        w\n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        false\n                      \n                      ]\n                      ∣\n                      \n                        w\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        w\n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        true\n                      \n                      ]\n                      )\n                    \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        false\n                      \n                      ]\n                      )\n                    \n                  \n                \n                ×\n                \n                  ∏\n                  \n                    n\n                    =\n                    0\n                  \n                  \n                    N\n                    −\n                    1\n                  \n                \n                \n                  [\n                  \n                    \n                      \n                        P\n                        (\n                        \n                          w\n                          \n                            n\n                          \n                        \n                        ∣\n                        [\n                        \n                          Spam\n                        \n                        =\n                        \n                          true\n                        \n                        ]\n                        )\n                      \n                      \n                        P\n                        (\n                        \n                          w\n                          \n                            n\n                          \n                        \n                        ∣\n                        [\n                        \n                          Spam\n                        \n                        =\n                        \n                          false\n                        \n                        ]\n                        )\n                      \n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\frac {P([{\\text{Spam}}={\\text{true}}]\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})}{P([{\\text{Spam}}={\\text{false}}]\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})}}\\\\={}&{\\frac {P([{\\text{Spam}}={\\text{true}}])}{P([{\\text{Spam}}={\\text{false}}])}}\\times \\prod _{n=0}^{N-1}\\left[{\\frac {P(w_{n}\\mid [{\\text{Spam}}={\\text{true}}])}{P(w_{n}\\mid [{\\text{Spam}}={\\text{false}}])}}\\right]\\end{aligned}}}\n  \n\nThis computation is faster and easier because it requires only \n  \n    \n      \n        2\n        N\n      \n    \n    {\\displaystyle 2N}\n  \n products.\n\nBayesian program\nThe Bayesian spam filter program is completely defined by:\n\n  \n    \n      \n        Pr\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                      \n                                        Spam\n                                      \n                                      ,\n                                      \n                                        W\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      \n                                        W\n                                        \n                                          1\n                                        \n                                      \n                                      …\n                                      \n                                        W\n                                        \n                                          N\n                                          −\n                                          1\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                (\n                                                \n                                                  Spam\n                                                \n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    0\n                                                  \n                                                \n                                                ∧\n                                                …\n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∧\n                                                …\n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    N\n                                                    −\n                                                    1\n                                                  \n                                                \n                                                )\n                                              \n                                            \n                                            \n                                              \n                                                =\n                                                P\n                                                (\n                                                \n                                                  Spam\n                                                \n                                                )\n                                                \n                                                  ∏\n                                                  \n                                                    n\n                                                    =\n                                                    0\n                                                  \n                                                  \n                                                    N\n                                                    −\n                                                    1\n                                                  \n                                                \n                                                P\n                                                (\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∣\n                                                \n                                                  Spam\n                                                \n                                                )\n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                (\n                                                \n                                                  Spam\n                                                \n                                                )\n                                                :\n                                                \n                                                  \n                                                    {\n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            false\n                                                          \n                                                          ]\n                                                          )\n                                                          =\n                                                          0.25\n                                                        \n                                                      \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            true\n                                                          \n                                                          ]\n                                                          )\n                                                          =\n                                                          0.75\n                                                        \n                                                      \n                                                    \n                                                    \n                                                  \n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                (\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∣\n                                                \n                                                  Spam\n                                                \n                                                )\n                                                :\n                                                \n                                                  \n                                                    {\n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          \n                                                            W\n                                                            \n                                                              n\n                                                            \n                                                          \n                                                          ∣\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            false\n                                                          \n                                                          ]\n                                                          )\n                                                        \n                                                      \n                                                      \n                                                        \n                                                          =\n                                                          \n                                                            \n                                                              \n                                                                1\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    f\n                                                                  \n                                                                  \n                                                                    n\n                                                                  \n                                                                \n                                                              \n                                                              \n                                                                2\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    f\n                                                                  \n                                                                \n                                                              \n                                                            \n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          \n                                                            W\n                                                            \n                                                              n\n                                                            \n                                                          \n                                                          ∣\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            true\n                                                          \n                                                          ]\n                                                          )\n                                                        \n                                                      \n                                                      \n                                                        \n                                                          =\n                                                          \n                                                            \n                                                              \n                                                                1\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    t\n                                                                  \n                                                                  \n                                                                    n\n                                                                  \n                                                                \n                                                              \n                                                              \n                                                                2\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    t\n                                                                  \n                                                                \n                                                              \n                                                            \n                                                          \n                                                        \n                                                      \n                                                    \n                                                    \n                                                  \n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            \n                              Identification (based on \n                            \n                            δ\n                            )\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                  P\n                  (\n                  \n                    Spam\n                  \n                  ∣\n                  \n                    w\n                    \n                      0\n                    \n                  \n                  ∧\n                  …\n                  ∧\n                  \n                    w\n                    \n                      n\n                    \n                  \n                  ∧\n                  …\n                  ∧\n                  \n                    w\n                    \n                      N\n                      −\n                      1\n                    \n                  \n                  )\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr {\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:{\\text{Spam}},W_{0},W_{1}\\ldots W_{N-1}\\\\Dc:{\\begin{cases}P({\\text{Spam}}\\land W_{0}\\land \\ldots \\land W_{n}\\land \\ldots \\land W_{N-1})\\\\=P({\\text{Spam}})\\prod _{n=0}^{N-1}P(W_{n}\\mid {\\text{Spam}})\\end{cases}}\\\\Fo:{\\begin{cases}P({\\text{Spam}}):{\\begin{cases}P([{\\text{Spam}}={\\text{false}}])=0.25\\\\P([{\\text{Spam}}={\\text{true}}])=0.75\\end{cases}}\\\\P(W_{n}\\mid {\\text{Spam}}):{\\begin{cases}P(W_{n}\\mid [{\\text{Spam}}={\\text{false}}])\\\\={\\frac {1+a_{f}^{n}}{2+a_{f}}}\\\\P(W_{n}\\mid [{\\text{Spam}}={\\text{true}}])\\\\={\\frac {1+a_{t}^{n}}{2+a_{t}}}\\end{cases}}\\\\\\end{cases}}\\\\\\end{cases}}\\\\{\\text{Identification (based on }}\\delta )\\end{cases}}\\\\Qu:P({\\text{Spam}}\\mid w_{0}\\land \\ldots \\land w_{n}\\land \\ldots \\land w_{N-1})\\end{cases}}}\n\nBayesian filter, Kalman filter and hidden Markov model\nBayesian filters (often called Recursive Bayesian estimation) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model (HMM).\n\nVariables\nVariables \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          S\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle S^{0},\\ldots ,S^{T}}\n  \n are a time series of state variables considered to be on a time horizon ranging from \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n to \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n.\nVariables \n  \n    \n      \n        \n          O\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          O\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle O^{0},\\ldots ,O^{T}}\n  \n are a time series of observation variables on the same horizon.\n\nDecomposition\nThe decomposition is based:\n\non \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle P(S^{t}\\mid S^{t-1})}\n  \n, called the system model, transition model or dynamic model, which formalizes the transition from the state at time \n  \n    \n      \n        t\n        −\n        1\n      \n    \n    {\\displaystyle t-1}\n  \n to the state at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n;\non \n  \n    \n      \n        P\n        (\n        \n          O\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle P(O^{t}\\mid S^{t})}\n  \n, called the observation model, which expresses what can be observed at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n when the system is in state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S^{t}}\n  \n;\non an initial state at time \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n: \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            0\n          \n        \n        ∧\n        \n          O\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle P(S^{0}\\wedge O^{0})}\n  \n.\n\nParametrical forms\nThe parametrical forms are not constrained and different choices lead to different well-known models: see Kalman filters and Hidden Markov models just below.\n\nQuestion\nThe typical question for such models is \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n                +\n                k\n              \n            \n            ∣\n            \n              O\n              \n                0\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              O\n              \n                t\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t+k}\\mid O^{0}\\wedge \\cdots \\wedge O^{t}\\right)}\n  \n: what is the probability distribution for the state at time \n  \n    \n      \n        t\n        +\n        k\n      \n    \n    {\\displaystyle t+k}\n  \n knowing the observations from instant \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n to \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n?\nThe most common case is Bayesian filtering where \n  \n    \n      \n        k\n        =\n        0\n      \n    \n    {\\displaystyle k=0}\n  \n, which searches for the present state, knowing past observations.\nHowever, it is also possible \n  \n    \n      \n        (\n        k\n        >\n        0\n        )\n      \n    \n    {\\displaystyle (k>0)}\n  \n, to extrapolate a future state from past observations, or to do smoothing \n  \n    \n      \n        (\n        k\n        <\n        0\n        )\n      \n    \n    {\\displaystyle (k<0)}\n  \n, to recover a past state from observations made either before or after that instant.\nMore complicated questions may also be asked as shown below in the HMM section.\nBayesian filters \n  \n    \n      \n        (\n        k\n        =\n        0\n        )\n      \n    \n    {\\displaystyle (k=0)}\n  \n have a very interesting recursive property, which contributes greatly to their attractiveness. \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n              \n            \n            \n              |\n            \n            \n              O\n              \n                0\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              O\n              \n                t\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t}\\right)}\n  \n may be computed simply from \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n                −\n                1\n              \n            \n            ∣\n            \n              O\n              \n                0\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              O\n              \n                t\n                −\n                1\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t-1}\\mid O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)}\n  \n with the following formula:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                P\n                \n                  (\n                  \n                    \n                      O\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      S\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n                ×\n                \n                  ∑\n                  \n                    \n                      S\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                          \n                        \n                        \n                          |\n                        \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                    ×\n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                        \n                          |\n                        \n                        \n                          O\n                          \n                            0\n                          \n                        \n                        ∧\n                        ⋯\n                        ∧\n                        \n                          O\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{ll}&P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t}\\right)\\\\=&P\\left(O^{t}|S^{t}\\right)\\times \\sum _{S^{t-1}}\\left[P\\left(S^{t}|S^{t-1}\\right)\\times P\\left(S^{t-1}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\right]\\end{array}}}\n  \n\nAnother interesting point of view for this equation is to consider that there are two phases: a\nprediction phase and an estimation phase:\n\nDuring the prediction phase, the state is predicted using the dynamic model and the estimation of the state at the previous moment:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  ∑\n                  \n                    \n                      S\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                          \n                        \n                        \n                          |\n                        \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                    ×\n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                        \n                          |\n                        \n                        \n                          O\n                          \n                            0\n                          \n                        \n                        ∧\n                        ⋯\n                        ∧\n                        \n                          O\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{ll}&P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\\\=&\\sum _{S^{t-1}}\\left[P\\left(S^{t}|S^{t-1}\\right)\\times P\\left(S^{t-1}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\right]\\end{array}}}\n  \n\nDuring the estimation phase, the prediction is either confirmed or invalidated using the last observation:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    ∣\n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      O\n                      \n                        t\n                      \n                    \n                    ∣\n                    \n                      S\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left(S^{t}\\mid O^{0}\\wedge \\cdots \\wedge O^{t}\\right)\\\\={}&P\\left(O^{t}\\mid S^{t}\\right)\\times P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\end{aligned}}}\n\nBayesian program\nP\n        r\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        S\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        S\n                                        \n                                          T\n                                        \n                                      \n                                      ,\n                                      \n                                        O\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      S\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    π\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                =\n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                                ×\n                                                \n                                                  ∏\n                                                  \n                                                    t\n                                                    =\n                                                    1\n                                                  \n                                                  \n                                                    T\n                                                  \n                                                \n                                                \n                                                  [\n                                                  \n                                                    P\n                                                    \n                                                      (\n                                                      \n                                                        \n                                                          S\n                                                          \n                                                            t\n                                                          \n                                                        \n                                                        \n                                                          |\n                                                        \n                                                        \n                                                          S\n                                                          \n                                                            t\n                                                            −\n                                                            1\n                                                          \n                                                        \n                                                      \n                                                      )\n                                                    \n                                                    ×\n                                                    P\n                                                    \n                                                      (\n                                                      \n                                                        \n                                                          O\n                                                          \n                                                            t\n                                                          \n                                                        \n                                                        \n                                                          |\n                                                        \n                                                        \n                                                          S\n                                                          \n                                                            t\n                                                          \n                                                        \n                                                      \n                                                      )\n                                                    \n                                                  \n                                                  ]\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            I\n                            d\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  \n                    \n                      {\n                      \n                        \n                          \n                            \n                              \n                                \n                                  \n                                    P\n                                    \n                                      (\n                                      \n                                        \n                                          S\n                                          \n                                            t\n                                            +\n                                            k\n                                          \n                                        \n                                        \n                                          |\n                                        \n                                        \n                                          O\n                                          \n                                            0\n                                          \n                                        \n                                        ∧\n                                        ⋯\n                                        ∧\n                                        \n                                          O\n                                          \n                                            t\n                                          \n                                        \n                                      \n                                      )\n                                    \n                                  \n                                \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        k\n                                        =\n                                        0\n                                      \n                                      )\n                                    \n                                    ≡\n                                    \n                                      Filtering\n                                    \n                                  \n                                \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        k\n                                        >\n                                        0\n                                      \n                                      )\n                                    \n                                    ≡\n                                    \n                                      Prediction\n                                    \n                                  \n                                \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        k\n                                        <\n                                        0\n                                      \n                                      )\n                                    \n                                    ≡\n                                    \n                                      Smoothing\n                                    \n                                  \n                                \n                              \n                            \n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle Pr{\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:\\\\S^{0},\\cdots ,S^{T},O^{0},\\cdots ,O^{T}\\\\Dc:\\\\{\\begin{cases}&P\\left(S^{0}\\wedge \\cdots \\wedge S^{T}\\wedge O^{0}\\wedge \\cdots \\wedge O^{T}|\\pi \\right)\\\\=&P\\left(S^{0}\\wedge O^{0}\\right)\\times \\prod _{t=1}^{T}\\left[P\\left(S^{t}|S^{t-1}\\right)\\times P\\left(O^{t}|S^{t}\\right)\\right]\\end{cases}}\\\\Fo:\\\\{\\begin{cases}P\\left(S^{0}\\wedge O^{0}\\right)\\\\P\\left(S^{t}|S^{t-1}\\right)\\\\P\\left(O^{t}|S^{t}\\right)\\end{cases}}\\end{cases}}\\\\Id\\end{cases}}\\\\Qu:\\\\{\\begin{cases}{\\begin{array}{l}P\\left(S^{t+k}|O^{0}\\wedge \\cdots \\wedge O^{t}\\right)\\\\\\left(k=0\\right)\\equiv {\\text{Filtering}}\\\\\\left(k>0\\right)\\equiv {\\text{Prediction}}\\\\\\left(k<0\\right)\\equiv {\\text{Smoothing}}\\end{array}}\\end{cases}}\\end{cases}}}\n\nKalman filter\nThe very well-known Kalman filters are a special case of Bayesian\nfilters.\nThey are defined by the following Bayesian program:\n\n  \n    \n      \n        P\n        r\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        S\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        S\n                                        \n                                          T\n                                        \n                                      \n                                      ,\n                                      \n                                        O\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    π\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                =\n                                              \n                                              \n                                                \n                                                  [\n                                                  \n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          \n                                                            (\n                                                            \n                                                              \n                                                                S\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              ∧\n                                                              \n                                                                O\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              \n                                                                |\n                                                              \n                                                              π\n                                                            \n                                                            )\n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          \n                                                            ∏\n                                                            \n                                                              t\n                                                              =\n                                                              1\n                                                            \n                                                            \n                                                              T\n                                                            \n                                                          \n                                                          \n                                                            [\n                                                            \n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  \n                                                                    |\n                                                                  \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                      −\n                                                                      1\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                              ×\n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    O\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  \n                                                                    |\n                                                                  \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                            \n                                                            ]\n                                                          \n                                                        \n                                                      \n                                                    \n                                                  \n                                                  ]\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                G\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ,\n                                                    A\n                                                    ∙\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                    ,\n                                                    Q\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                G\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ,\n                                                    H\n                                                    ∙\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ,\n                                                    R\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            I\n                            d\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  P\n                  \n                    (\n                    \n                      \n                        S\n                        \n                          T\n                        \n                      \n                      ∣\n                      \n                        O\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        O\n                        \n                          T\n                        \n                      \n                      ∧\n                      π\n                    \n                    )\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle Pr{\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:\\\\S^{0},\\cdots ,S^{T},O^{0},\\cdots ,O^{T}\\\\Dc:\\\\{\\begin{cases}&P\\left(S^{0}\\wedge \\cdots \\wedge O^{T}|\\pi \\right)\\\\=&\\left[{\\begin{array}{c}P\\left(S^{0}\\wedge O^{0}|\\pi \\right)\\\\\\prod _{t=1}^{T}\\left[P\\left(S^{t}|S^{t-1}\\wedge \\pi \\right)\\times P\\left(O^{t}|S^{t}\\wedge \\pi \\right)\\right]\\end{array}}\\right]\\end{cases}}\\\\Fo:\\\\{\\begin{cases}P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)\\equiv G\\left(S^{t},A\\bullet S^{t-1},Q\\right)\\\\P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)\\equiv G\\left(O^{t},H\\bullet S^{t},R\\right)\\end{cases}}\\end{cases}}\\\\Id\\end{cases}}\\\\Qu:\\\\P\\left(S^{T}\\mid O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi \\right)\\end{cases}}}\n  \n\nVariables are continuous.\nThe transition model \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n            −\n            1\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(S^{t}\\mid S^{t-1}\\wedge \\pi )}\n  \n and the observation model \n  \n    \n      \n        P\n        (\n        \n          O\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(O^{t}\\mid S^{t}\\wedge \\pi )}\n  \n are both specified using Gaussian laws with means that are linear functions of the conditioning variables.\nWith these hypotheses and by using the recursive formula, it is possible to solve\nthe inference problem analytically to answer the usual \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            T\n          \n        \n        ∣\n        \n          O\n          \n            0\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          O\n          \n            T\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(S^{T}\\mid O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi )}\n  \n question.\nThis leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications.\nWhen there are no obvious linear transition and observation models, it is still often\npossible, using a first-order Taylor's expansion, to treat these models as locally linear.\nThis generalization is commonly called the extended Kalman filter.\n\nHidden Markov model\nHidden Markov models (HMMs) are another very popular specialization of Bayesian filters.\nThey are defined by the following Bayesian program:\n\n  \n    \n      \n        Pr\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        S\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      …\n                                      ,\n                                      \n                                        S\n                                        \n                                          T\n                                        \n                                      \n                                      ,\n                                      \n                                        O\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      …\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    ∣\n                                                    π\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                =\n                                              \n                                              \n                                                \n                                                  [\n                                                  \n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          \n                                                            (\n                                                            \n                                                              \n                                                                S\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              ∧\n                                                              \n                                                                O\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              ∣\n                                                              π\n                                                            \n                                                            )\n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          \n                                                            ∏\n                                                            \n                                                              t\n                                                              =\n                                                              1\n                                                            \n                                                            \n                                                              T\n                                                            \n                                                          \n                                                          \n                                                            [\n                                                            \n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∣\n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                      −\n                                                                      1\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                              ×\n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    O\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∣\n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                            \n                                                            ]\n                                                          \n                                                        \n                                                      \n                                                    \n                                                  \n                                                  ]\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∣\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            I\n                            d\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  \n                    max\n                    \n                      \n                        S\n                        \n                          1\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        S\n                        \n                          T\n                          −\n                          1\n                        \n                      \n                    \n                  \n                  \n                    [\n                    \n                      P\n                      \n                        (\n                        \n                          \n                            S\n                            \n                              1\n                            \n                          \n                          ∧\n                          ⋯\n                          ∧\n                          \n                            S\n                            \n                              T\n                              −\n                              1\n                            \n                          \n                          ∣\n                          \n                            S\n                            \n                              T\n                            \n                          \n                          ∧\n                          \n                            O\n                            \n                              0\n                            \n                          \n                          ∧\n                          ⋯\n                          ∧\n                          \n                            O\n                            \n                              T\n                            \n                          \n                          ∧\n                          π\n                        \n                        )\n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr {\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:\\\\S^{0},\\ldots ,S^{T},O^{0},\\ldots ,O^{T}\\\\Dc:\\\\{\\begin{cases}&P\\left(S^{0}\\wedge \\cdots \\wedge O^{T}\\mid \\pi \\right)\\\\=&\\left[{\\begin{array}{c}P\\left(S^{0}\\wedge O^{0}\\mid \\pi \\right)\\\\\\prod _{t=1}^{T}\\left[P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)\\times P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)\\right]\\end{array}}\\right]\\end{cases}}\\\\Fo:\\\\{\\begin{cases}P\\left(S^{0}\\wedge O^{0}\\mid \\pi \\right)\\equiv {\\text{Matrix}}\\\\P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)\\equiv {\\text{Matrix}}\\\\P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)\\equiv {\\text{Matrix}}\\end{cases}}\\end{cases}}\\\\Id\\end{cases}}\\\\Qu:\\\\\\max _{S^{1}\\wedge \\cdots \\wedge S^{T-1}}\\left[P\\left(S^{1}\\wedge \\cdots \\wedge S^{T-1}\\mid S^{T}\\wedge O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi \\right)\\right]\\end{cases}}}\n  \n\nVariables are treated as being discrete.\nThe transition model \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n              \n            \n            ∣\n            \n              S\n              \n                t\n                −\n                1\n              \n            \n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)}\n  \n and the observation model \n  \n    \n      \n        P\n        \n          (\n          \n            \n              O\n              \n                t\n              \n            \n            ∣\n            \n              S\n              \n                t\n              \n            \n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)}\n  \n are\nboth specified using probability matrices.\n\nThe question most frequently asked of HMMs is:\n\n  \n    \n      \n        \n          max\n          \n            \n              S\n              \n                1\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              S\n              \n                T\n                −\n                1\n              \n            \n          \n        \n        \n          [\n          \n            P\n            \n              (\n              \n                \n                  S\n                  \n                    1\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  S\n                  \n                    T\n                    −\n                    1\n                  \n                \n                ∣\n                \n                  S\n                  \n                    T\n                  \n                \n                ∧\n                \n                  O\n                  \n                    0\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  O\n                  \n                    T\n                  \n                \n                ∧\n                π\n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\max _{S^{1}\\wedge \\cdots \\wedge S^{T-1}}\\left[P\\left(S^{1}\\wedge \\cdots \\wedge S^{T-1}\\mid S^{T}\\wedge O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi \\right)\\right]}\n  \n\nWhat is the most probable series of states that leads to the present state, knowing the past observations?\nThis particular question may be answered with a specific and very efficient algorithm\ncalled the Viterbi algorithm.\nThe Baum–Welch algorithm has been developed\nfor HMMs.\n\nApplications\nAcademic applications\nSince 2000, Bayesian programming has been used to develop both robotics applications and life sciences models.\n\nRobotics\nIn robotics, bayesian programming was applied to autonomous robotics, robotic CAD systems, advanced driver-assistance systems, robotic arm control, mobile robotics, human-robot interaction, human-vehicle interaction (Bayesian autonomous driver models) video game avatar programming and training  and real-time strategy games (AI).\n\nLife sciences\nIn life sciences, bayesian programming was used in vision to reconstruct shape from motion, to model visuo-vestibular interaction and to study saccadic eye movements; in speech perception and control to study early speech acquisition and the emergence of articulatory-acoustic systems; and to model handwriting perception and control.\n\nPattern recognition\nBayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of compositionality (building abstract representations from parts), causality (building complexity from parts) and learning to learn (using previously recognized concepts to ease the creation of new concepts).\n\nPossibility theories\nThe comparison between probabilistic approaches (not only bayesian programming) and possibility theories continues to be debated.\nPossibility theories like, for instance, fuzzy sets, fuzzy logic and possibility theory are alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete/uncertain knowledge.\nThe defense of probability is mainly based on Cox's theorem, which starts from four postulates concerning rational reasoning in the presence of uncertainty. It demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument is that any approach other than probability necessarily infringes one of these postulates and the value of that infringement.\n\nProbabilistic programming\nThe purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling (especially bayesian networks) to deal with uncertainty while profiting from the programming languages' expressiveness to encode complexity.\nExtended classical programming languages include logical languages as proposed in Probabilistic Horn Abduction, Independent Choice Logic, PRISM, and ProbLog which proposes an extension of Prolog.\nIt can also be extensions of functional programming languages (essentially Lisp and Scheme) such as IBAL or CHURCH. The underlying programming languages can be object-oriented as in BLOG and FACTORIE or more standard ones as in CES and FIGARO.\nThe purpose of Bayesian programming is different. Jaynes' precept of \"probability as logic\" argues that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming attempts to replace classical languages with a programming approach based on probability that considers incompleteness and uncertainty.\nThe precise comparison between the semantics and power of expression of Bayesian and probabilistic programming is an open question.\n\nSee also\nReferences\nFurther reading\nKamel Mekhnacha (2013). Bayesian Programming. Chapman and Hall/CRC. doi:10.1201/b16111. ISBN 978-1-4398-8032-6.\n\nExternal links\nA companion site to the Bayesian programming book where to download ProBT an inference engine dedicated to Bayesian programming.\nThe Bayesian-programming.org site Archived 2013-11-23 at archive.today for the promotion of Bayesian programming with detailed information and numerous publications.",
        "url": "https://en.wikipedia.org/wiki/Bayesian_programming"
    },
    {
        "title": "Behavior informatics",
        "text": "Behavior informatics (BI) is the informatics of behaviors so as to obtain behavior intelligence and behavior insights. BI is a research method combining science and technology, specifically in the area of engineering. The purpose of BI includes analysis of current behaviors as well as the inference of future possible behaviors. This occurs through pattern recognition.\nDifferent from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations.\nBI is built on classic study of behavioral science, including behavior modeling, applied behavior analysis, behavior analysis, behavioral economics, and organizational behavior. Typical BI tasks consist of individual and group behavior formation, representation, computational modeling, analysis, learning, simulation, and understanding of behavior impact, utility, non-occurring behaviors, etc. for behavior intervention and management. The Behavior Informatics approach to data utilizes cognitive as well as behavioral data. By combining the data, BI has the potential to effectively illustrate the big picture when it comes to behavioral decisions and patterns. One of the goals of BI is also to be able to study human behavior while eliminating issues like self-report bias. This creates more reliable and valid information for research studies.\n\nBehavior analytics\nBehavior informatics covers behavior analytics which focuses on analysis and learning of behavioral data.\n\nBehavior\nFrom an Informatics perspective, a behavior consists of three key elements:\n\nactors (behavioral subjects and objects),\noperations (actions, activities) and\ninteractions (relationships), and their properties.\nA behavior can be represented as a behavior vector, all behaviors of an actor or an actor group can be represented as behavior sequences and multi-dimensional behavior matrix. The following table explains some of the elements of behavior.  \n\nBehavior Informatics takes into account behavior when analyzing business patterns and intelligence. The inclusion of behavior in these analyses provides prominent information on social and driving factors of patterns.\n\nApplications\nBehavior Informatics is being used in a variety of settings, including but not limited to health care management, telecommunications, marketing, and security. Behavior Informatics is a turning point for the health care system. Behavior Informatics provides a manner in which to analyze and organize the many aspects that go into a person's health care needs and decisions. When it comes to business models, behavior informatics may be utilized for a similar role. Organizations implement behavior informatics to enhance business structure and regime, where it helps moderate ideal business decisions and situations.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Behavior_informatics"
    },
    {
        "title": "Belief–desire–intention model",
        "text": "For popular psychology, the belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nApplications\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB.\n\nReferences\nBratman, M. E. (1999) [1987]. Intention, Plans, and Practical Reason. CSLI Publications. ISBN 1-57586-192-5.",
        "url": "https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_model"
    },
    {
        "title": "Brain technology",
        "text": "Brain technology, or self-learning know-how systems, defines a technology that employs latest findings in neuroscience. [see also neuro implants] The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the Roboy project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as \"know-how maps\".\n\nResearch and applications\nThe first demonstrations of BC in humans and animals took place in the 1960s when Grey Walter demonstrated use of non-invasively recorded encephalogram (EEG) signals from a human subject to control a slide projector (Graimann et al., 2010). \nSoon after Jacques J. Vidal coined the term brain–computer interface (BCI) in 1971, the Defense Advanced Research Projects Agency (DARPA) first starting funding brain–computer interface research and has since funded several brain–computer interface projects. That market is expected to reach a value of $1.72 billion by 2022. Brain–computer interfaces record brain activity, transmit the information out of the body, signal-process the data via algorithms, and convert them into command control signals. \nIn 2012, a landmark study in Nature, led by pioneer Leigh Hochberg, MD, PhD, demonstrated that two people with tetraplegia were able to control robotic arms through thought when connected to the BrainGate neural interface system. The two participants were able to reach for and grasp objects in three-dimensional space, and one participant used the system to serve herself coffee for the first time since becoming paralyzed nearly 15 years prior.\nAnd in October 2020, two patients were able to wirelessly control an operating system to text, email, shop and bank using direct thought through the Stentrode brain computer interface (Journal of NeuroInterventional Surgery) in a study led by Thomas Oxley. This was the first time a brain–computer interface was implanted via the patient's blood vessels, eliminating the need for open brain surgery.\nCurrently a number of groups are exploring a range of experimental devices using brain–computer interfaces, which have the potential to fundamentally change the way of life for patients with paralysis and a wide range of neurological disorders. These include: as Elon Musk, Facebook, and the University of California in San Francisco. The systems. This technology is also being explored as a neuromodulation device and may ultimately help diagnose and treat a range of brain pathologies, such as epilepsy and Parkinson's disease.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Brain_technology"
    },
    {
        "title": "Business process automation",
        "text": "Business process automation (BPA), also known as business automation, refers to the technology-enabled automation of business processes.\n\nDevelopment approaches\nThere are three main approaches to developing BPA:\n\ntraditional business process automation entails developing BPA software in a programming language for integrating relevant applications in the digital ecosystem to execute a given process;\nrobotic process automation uses software robots (also called agents, bots, or workers) to emulate human-computer interaction for executing a combination of processes, activities, transactions, and tasks in one or more unrelated software systems;\nhyperautomation (also called intelligent automation (IA), intelligent process automation (IPA) integrated automation platform (IAP), and cognitive automation (CA) combines business process automation, artificial intelligence (AI), and machine learning (ML) to discover, validate, and execute organizational processes automatically with no or minimal human intervention.\n\nDeployment\nBPA toolsets vary in capability, but due to the fast adoption of artificial intelligence, there is a trend towards the use of artificial intelligence technologies that can understand natural language and unstructured datasets, interact with human beings, and adapt to new types of problems without too much human-guided training or interventions.\n\nBusiness process management implementation\nA business process management system differs from BPA. However, it is possible to implement automation based on a BPM implementation. The methods to achieve this vary, from writing custom application code to using specialist BPA tools.\n\nRobotic process automation\nRobotic process automation (RPA) involves the deployment of attended or unattended software agents in an organization's environment. These software agents, or robots, are programmed to perform pre-defined structured and repetitive sets of business tasks or processes. Robotic process automation is designed to streamline workflows by delegating repetitive tasks to software agents, allowing human workers to focus on more complex and strategic activities.\nBPA providers typically focus on different industry sectors, but the underlying approach is generally similar in that they aim to provide the shortest route to automation by interacting with the user interface rather than modifying the application code or database behind it.\n\nUse of artificial intelligence\nArtificial intelligence software robots are used to handle unstructured data sets (like images, texts, audios) and are often deployed after implementing robotic process automation. They can, for instance, generate an automatic transcript from a video. The combination of automation and artificial intelligence (AI) enables autonomy for the robots, along with the capability to perform cognitive tasks. At this stage, the robot can learn and improve processes by analyzing and adapting them.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nArtificial intelligence and elections\nBusiness-driven development\nBusiness Process Model and Notation\nBusiness process reengineering\nBusiness Process Execution Language (BPEL)\nBusiness rules engine\nComparison of business integration software\nJob scheduler\nReal-time enterprise\nRunbook\nSynthetic intelligence\nWeak AI\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Business_process_automation"
    },
    {
        "title": "CarynAI",
        "text": "CarynAI is a chatbot launched by Snapchat influencer Caryn Marjorie, and powered by BanterAI.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/CarynAI"
    },
    {
        "title": "Case-based reasoning",
        "text": "Case-based reasoning (CBR), broadly construed, is the process of solving new problems based on the solutions of similar past problems.\nIn everyday life, an auto mechanic who fixes an engine by recalling another car that exhibited similar symptoms is using case-based reasoning. A lawyer who advocates a particular outcome in a trial based on legal precedents or a judge who creates case law is using case-based reasoning. So, too, an engineer copying working elements of nature (practicing biomimicry) is treating nature as a database of solutions to problems. Case-based reasoning is a prominent type of analogy solution making.\nIt has been argued that case-based reasoning is not only a powerful method for computer reasoning, but also a pervasive behavior in everyday human problem solving; or, more radically, that all reasoning is based on past cases personally experienced. This view is related to prototype theory, which is most deeply explored in cognitive science.\n\nProcess\nCase-based reasoning has been formalized for purposes of computer reasoning as a four-step process:\n\nRetrieve: Given a target problem, retrieve cases relevant to solving it from memory. A case consists of a problem, its solution, and, typically, annotations about how the solution was derived. For example, suppose Fred wants to prepare blueberry pancakes. Being a novice cook, the most relevant experience he can recall is one in which he successfully made plain pancakes. The procedure he followed for making the plain pancakes, together with justifications for decisions made along the way, constitutes Fred's retrieved case.\nReuse: Map the solution from the previous case to the target problem. This may involve adapting the solution as needed to fit the new situation. In the pancake example, Fred must adapt his retrieved solution to include the addition of blueberries.\nRevise: Having mapped the previous solution to the target situation, test the new solution in the real world (or a simulation) and, if necessary, revise. Suppose Fred adapted his pancake solution by adding blueberries to the batter. After mixing, he discovers that the batter has turned blue – an undesired effect. This suggests the following revision: delay the addition of blueberries until after the batter has been ladled into the pan.\nRetain: After the solution has been successfully adapted to the target problem, store the resulting experience as a new case in memory. Fred, accordingly, records his new-found procedure for making blueberry pancakes, thereby enriching his set of stored experiences, and better preparing him for future pancake-making demands.\n\nComparison to other methods\nAt first glance, CBR may seem similar to the rule induction algorithms of machine learning. Like a rule-induction algorithm, CBR starts with a set of cases or training examples; it forms generalizations of these examples, albeit implicit ones, by identifying commonalities between a retrieved case and the target problem.\nIf for instance a procedure for plain pancakes is mapped to blueberry pancakes, a decision is made to use the same basic batter and frying method, thus implicitly generalizing the set of situations under which the batter and frying method can be used. The key difference, however, between the implicit generalization in CBR and the generalization in rule induction lies in when the generalization is made. A rule-induction algorithm draws its generalizations from a set of training examples before the target problem is even known; that is, it performs eager generalization.\nFor instance, if a rule-induction algorithm were given recipes for plain pancakes, Dutch apple pancakes, and banana pancakes as its training examples, it would have to derive, at training time, a set of general rules for making all types of pancakes. It would not be until testing time that it would be given, say, the task of cooking blueberry pancakes. The difficulty for the rule-induction algorithm is in anticipating the different directions in which it should attempt to generalize its training examples. This is in contrast to CBR, which delays (implicit) generalization of its cases until testing time – a strategy of lazy generalization. In the pancake example, CBR has already been given the target problem of cooking blueberry pancakes; thus it can generalize its cases exactly as needed to cover this situation. CBR therefore tends to be a good approach for rich, complex domains in which there are myriad ways to generalize a case.\nIn law, there is often explicit delegation of CBR to courts, recognizing the limits of rule based reasons: limiting delay, limited knowledge of future context, limit of negotiated agreement, etc. While CBR in law and cognitively inspired CBR have long been associated, the former is more clearly an interpolation of rule based reasoning, and judgment, while the latter is more closely tied to recall and process adaptation. The difference is clear in their attitude toward error and appellate review.\nAnother name for cased based reasoning in problem solving is symptomatic strategies. It does require à priori domain knowledge that is gleaned from past experience which established connections between symptoms and causes. This knowledge is referred to as shallow, compiled, evidential, history-based as well as case-based knowledge. This is the strategy most associated with diagnosis by experts. Diagnosis of a problem transpires as a rapid recognition process in which symptoms evoke appropriate situation categories. An expert knows the cause by virtue of having previously encountered similar cases. Cased based reasoning is the most powerful strategy, and that used most commonly. However, the strategy won't work independently with truly novel problems, or where deeper understanding of whatever is taking place is sought.\nAn alternative approach to problem solving is the topographic strategy which falls into the category of deep reasoning. With deep reasoning, in-depth knowledge of a system is used. Topography in this context means a description or an analysis of a structured entity, showing the relations among its elements.\nAlso known as reasoning from first principles, deep reasoning is applied to novel faults when experience-based approaches aren't viable. The topographic strategy is therefore linked to à priori domain knowledge that is developed from a more a fundamental understanding of a system, possibly using first-principles knowledge. Such knowledge is referred to as deep, causal or model-based knowledge. \nHoc and Carlier noted that symptomatic approaches may need to be supported by topographic approaches because symptoms can be defined in diverse terms. The converse is also true – shallow reasoning can be used abductively to generate causal hypotheses, and deductively to evaluate those hypotheses, in a topographical search.\n\nCriticism\nCritics of CBR argue that it is an approach that accepts anecdotal evidence as its main operating principle. Without statistically relevant data for backing and implicit generalization, there is no guarantee that the generalization is correct. However, all inductive reasoning where data is too scarce for statistical relevance is inherently based on anecdotal evidence.\n\nHistory\nCBR traces its roots to the work of Roger Schank and his students at Yale University in the early 1980s. Schank's model of dynamic memory was the basis for the earliest CBR systems: Janet Kolodner's CYRUS and Michael Lebowitz's IPP.\nOther schools of CBR and closely allied fields emerged in the 1980s, which directed at topics such as legal reasoning, memory-based reasoning (a way of reasoning from examples on massively parallel machines), and combinations of CBR with other reasoning methods. In the 1990s, interest in CBR grew internationally, as evidenced by the establishment of an International Conference on Case-Based Reasoning in 1995, as well as European, German, British, Italian, and other CBR workshops.\nCBR technology has resulted in the deployment of a number of successful systems, the earliest being Lockheed's CLAVIER, a system for laying out composite parts to be baked in an industrial convection oven. CBR has been used extensively in applications such as the Compaq SMART system and has found a major application area in the health sciences, as well as in structural safety management.\nThere is recent work that develops CBR within a statistical framework and formalizes case-based inference as a specific type of probabilistic inference. Thus, it becomes possible to produce case-based predictions equipped with a certain level of confidence.\nOne description of the difference between CBR and induction from instances is that statistical inference aims to find what tends to make cases similar while CBR aims to encode what suffices to claim similarly.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nAbductive reasoning\nDuck test\nI know it when I see it\nCommonsense reasoning\nPurposeful omission\nDecision tree\nGenetic algorithm\nPattern matching\nAnalogy\nK-line (artificial intelligence)\nRipple down rules\nCasuistry\nSimilarity heuristic\n\nNotes and references\nFurther reading\nAamodt, Agnar, and Enric Plaza. \"Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches\" Artificial Intelligence Communications 7, no. 1 (1994): 39–52.\nAlthoff, Klaus-Dieter, Ralph Bergmann, and L. Karl Branting, eds. Case-Based Reasoning Research and Development: Proceedings of the Third International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1999.\nBergmann, Ralph Experience Management: Foundations, Development Methodology, and Internet-Based Applications. Springer, LNAI 2432, 2002.\nBergmann, R., Althoff, K.-D., Breen, S., Göker, M., Manago, M., Traphöner, R., and Wess, S. Developing industrial case-based reasoning applications: The INRECA methodology. Springer LNAI 1612, 2003.\nKolodner, Janet. Case-Based Reasoning. San Mateo: Morgan Kaufmann, 1993.\nLeake, David. \"CBR in Context: The Present and Future\", In Leake, D., editor, Case-Based Reasoning: Experiences, Lessons, and Future Directions. AAAI Press/MIT Press, 1996, 1-30.\nLeake, David, and Enric Plaza, eds. Case-Based Reasoning Research and Development: Proceedings of the Second International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1997.\nLenz, Mario; Bartsch-Spörl, Brigitte; Burkhard, Hans-Dieter; Wess, Stefan, eds. (1998). Case-Based Reasoning Technology: From Foundations to Applications. Lecture Notes in Artificial Intelligence. Vol. 1400. Springer. doi:10.1007/3-540-69351-3. ISBN 978-3-540-64572-6. S2CID 10517603.\nOxman, Rivka. Precedents in Design: a Computational Model for the Organization of Precedent Knowledge, Design Studies, Vol. 15 No. 2 pp. 141–157\nRiesbeck, Christopher, and Roger Schank. Inside Case-based Reasoning. Northvale, NJ: Erlbaum, 1989.\nVeloso, Manuela, and Agnar Aamodt, eds. Case-Based Reasoning Research and Development: Proceedings of the First International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1995.\nWatson, Ian. Applying Case-Based Reasoning: Techniques for Enterprise Systems. San Francisco: Morgan Kaufmann, 1997. https://a.co/d/g84Vai9\n\nExternal links\n\nGAIA – Group of Artificial Intelligence Applications\n\nAn earlier version of the above article was posted on Nupedia.",
        "url": "https://en.wikipedia.org/wiki/Case-based_reasoning"
    },
    {
        "title": "Character computing",
        "text": "Character computing is a trans-disciplinary field of research at the intersection of computer science and psychology. It is any computing that incorporates the human character within its context. Character is defined as all features or characteristics defining an individual and guiding their behavior in a specific situation. It consists of stable trait markers (e.g., personality, background, history, socio-economic embeddings, culture,...) and variable state markers (emotions, health, cognitive state, ...). Character computing aims at providing a holistic psychologically driven model of human behavior. It models and predicts behavior based on the relationships between a situation and character. Three main research modules fall under the umbrella of character computing: character sensing and profiling, character-aware adaptive systems, and artificial characters.\n\nOverview\nCharacter computing can be viewed as an extension of the well-established field of affective computing. Based on the foundations of the different psychology branches, it advocates defining behavior as a compound attribute that is not driven by either personality, emotions, situation or cognition alone. It rather defines behavior as a function of everything that makes up an individual i.e., their character and the situation they are in. Affective computing aims at allowing machines to understand and translate the non-verbal cues of individuals into affect. Accordingly, character computing aims at understanding the character attributes of an individual and the situation to translate it to predicted behavior, and vice versa.\n''In practical terms, depending on the application context, character computing is a branch of research that deals with the design of systems and interfaces that can observe, sense, predict, adapt to, affect, understand, or simulate the following: character based on behavior and situation, behavior based on character and situation, or situation based on character and behavior.'' The Character-Behavior-Situation (CBS) triad is at the core of character computing and defines each of the three edges based on the other two.\nCharacter computing relies on simultaneous development from a computational and psychological perspective and is intended to be used by researchers in both fields. Its main concept is aligning the computational model of character computing with empirical results from in-lab and in-the-wild psychology experiments. The model is to be continuously built and validated through the emergence of new data. Similar to affective and personality computing, the model is to be used as a base for different applications towards improving user experience.\n\nHistory\nCharacter computing as such was first coined in its first workshop in 2017. Since then it has had 3 international workshops and numerous publications. Despite its young age, it has already drawn some interest in the research community, leading to the publication of the first book under the same title in early 2020 published by Springer Nature.\nResearch that can be categorized under the field dates much older than 2017. The notion of combining several factors towards the explanation of behavior or traits and states has long been investigated in both Psychology and Computer Science, for example.\n\nCharacter\nThe word character originates from the Greek word meaning “stamping tool”, referring to distinctive features and traits. Over the years it has been given many different connotations, like the moral character in philosophy, the temperament in psychology, a person in literature or an avatar in various virtual worlds, including video games. According to character computing character is a unification of all the previous definitions, by referring back to the original meaning of the word. Character is defined as the holistic concept representing all interacting trait and state markers that distinguish an individual. Traits are characteristics that mainly remain stable over time. Traits include personality, affect, socio-demographics, and general health. States are characteristics that vary in short periods of time. They include emotions, well-being, health, cognitive state. Each characteristic has many representation methods and psychological models. The different models can be combined or one model can be preset for each characteristic. This depends on the use-case and the design choices.\n\nAreas\nResearch into character computing can be divided into three areas, which complement each other but can each be investigated separately. The first area is sensing and predicting character states and traits or ensuing behavior. The second area is adapting applications to certain character states or traits and the behavior they predict. It also deals with trying to change or monitor such behavior. The final area deals with creating artificial agents e.g., chatbots or virtual reality avatars that exhibit certain characteristics.\nThe three areas are investigated separately and build on existing findings in the literature. The results of each of the three areas can also be used as a stepping stone for the next area. Each of the three areas has already been investigated on its own in different research fields with focus on different subsets of character. For example, affective computing and personality computing both cover different areas with a focus on some character components without the others to account for human behavior.\n\nThe Character-Behavior-Situation triad\nCharacter computing is based on a holistic psychologically driven model of human behavior. Human behavior is modeled and predicted based on the relationships between a situation and a human's character. To further define character in a more formal or holistic manner, we represent it in light of the Character–Behavior–Situation triad. This highlights that character not only determines who we are but how we are, i.e., how we behave. The triad investigated in Personality Psychology is extended through character computing to the Character–Behavior–Situation triad. Any member of the CBS triad is a function of the two other members, e.g., given the situation and personality, the behavior can be predicted. Each of the components in the triad can be further decomposed into smaller units and features that may best represent the human's behavior or character in a particular situation. Character is thus behind a person's behavior in any given situation. While this is a causality relation, the correlation between the three components is often more easily used to predict the components that are most difficult to measure from those measured more easily. There are infinitely many components to include in the representation of any of C, B, and S. The challenge is always to choose the smallest subset needed for prediction of a person's behavior in a particular situation.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Character_computing"
    },
    {
        "title": "Class activation mapping",
        "text": "Class activation mapping methods are explainable AI (XAI) techniques used to visualize the regions of an input image that are the most relevant for a particular task, especially image classification, in convolutional neural networks (CNNs). These methods generate heatmaps by weighting the feature maps from a convolutional layer according to their relevance to the target class.\nIn the field of artificial intelligence, generically defined as \"the effort to automate intellectual tasks normally performed by humans\", machine learning and deep learning were created. They both use statistical and computational methods to learn patterns from data, reducing the need for manually coded rules..\nMachine learning models are trained on input data and the known respective answers, learning the underlying patterns or structures present in the data. Traditional Machine learning algorithms employ manually designed feature sets, posing a direct link between machine learning designers and employed features.\nDeep learning is a subfield of machine learning, based on the concept of successive layers of representation, in which the data is progressively unfolded in different ways, to extract relevant and informative patterns in data analysis. Deep learning algorithms are defined as feature learning algorithms automatically learning hierarchical feature representations from raw data, extracting increasingly abstract features through multiple layers.\nCNNs are a specific architecture of deep learning models, designed to process spatially structured data, such as images, exploiting a series of convolution, non-linear activation and pooling operations to extract relevant features, contained in the so-called feature maps from input data. CNNs have demonstrated to be highly effective in a variety of computer vision and image processing tasks.\nCNNs (and deep learning models more broadly) are described as  black boxes due to their complex and non-transparent internal layers of representation. The need for clearer indications on its internal working and decision-making process gave birth to XAI techniques.\nAmong the proposed XAI techniques for computer vision tasks, Class activation mapping methods can show which pixels in an input image are important to the predicted logit for a class of interest, in a classification task .\nClass activation mapping methods were originally developed for class-discriminative scenarios to visualize which parts of the input image influenced the classification decision. Namely, to visually highlight the regions of those feature maps which contribute most strongly to the prediction of a given class. \nMore advanced versions of these methods are not limited to image classification tasks, but have been extended also to several vision-related tasks, such as object detection, image captioning, visual question answering and image segmentation.\n\nBackground\nThe following methods laid the groundwork for the class activation maps approaches, forming the conceptual basis of using gradients to highlight class-discriminative regions.\n\nClass model visualization and saliency maps for convolutional neural networks\nThe class model visualization and image-specific saliency maps approaches have been presented in the foundational work \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\" by Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman\n and it generalizes the deconvnet method by Zeiler and Fergus.\n\nClass model visualization synthesizes an artificial input image that strongly activates the output neurons associated with a target class. Given a trained, fixed model, this method starts with a zero-initialized image, backpropagates the gradients from the class score to the image pixels, updates the image pixels increasing the specific class scores and it repeats the pixel updating process, showing an encoded (idealized version) prototype of the class of interest.\nImage-specific class saliency visualization method provides a visual explanation by highlighting the most relevant pixels in an image for predicting a certain class C of interest. This is done by computing the gradient of the class score with respect to the input image, \n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        ,\n      \n    \n    {\\displaystyle I_{0},}\n  \n \n  \n    \n      \n        w\n        =\n        \n          \n            \n            \n              \n                \n                  ∂\n                  \n                    S\n                    \n                      C\n                    \n                  \n                \n                \n                  ∂\n                  I\n                \n              \n            \n            |\n          \n          \n            \n              I\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle w=\\left.{\\frac {\\partial S_{C}}{\\partial I}}\\right|_{I_{0}}}\n  \n approximating the model locally (around \n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle I_{0}}\n  \n) as linear, using a first-order Taylor expansion: \n  \n    \n      \n        \n          S\n          \n            C\n          \n        \n        (\n        I\n        )\n        ≈\n        \n          w\n          \n            C\n          \n          \n            T\n          \n        \n        I\n        +\n        b\n      \n    \n    {\\displaystyle S_{C}(I)\\approx w_{C}^{T}I+b}\n  \n. The magnitude of \n  \n    \n      \n        \n          w\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{C}}\n  \n, the gradient, indicates the importancy of the pixels: larger gradients suggest greater influence on the prediction. Once the gradient is known, the saliency map is defined as the maximum absolute gradient across the color channels: \n  \n    \n      \n        \n          M\n          \n            i\n            j\n          \n        \n        =\n        m\n        a\n        \n          x\n          \n            C\n          \n        \n        \n          |\n          \n            \n              \n                ∂\n                \n                  S\n                  \n                    C\n                  \n                \n              \n              \n                ∂\n                \n                  I\n                  \n                    i\n                    j\n                  \n                  \n                    C\n                  \n                \n              \n            \n          \n          |\n        \n      \n    \n    {\\displaystyle M_{ij}=max_{C}\\left|{\\frac {\\partial S_{C}}{\\partial I_{ij}^{C}}}\\right|}\n  \n  resulting in an saliency map (i.e. heatmap).\n\nGuided backpropagation\nThe concept of guided backpropagation can be traced for the first time in the paper by Springenberg et al. \"Striving For Simplicity: The All Convolutional Net\"  and also this method builds upon the work by Zeiler and Fergus \"Visualizing and Understanding Convolutional Networks\".\n\nGuided backpropagation core is to understand what a CNN is learning, by visualizing the patterns that activate more strongly individual neurons (or filters), in architectures which do not rely on max-pooling layer.\nWhen propagating gradients back through a rectified linear unit (ReLU), guided backpropagation passes the gradient if and only if the input to the ReLU was positive (forward pass) and the output gradient is positive (backward signal), tackling both inactive neurons, negative gradients and suppressing the noise. The result displays sharper, high-resolution visualizations of what each neuron is responding to.\nGuided backpropagation represents a simple and practical method for model interpretability, helping understand how and where neural networks detect semantic concepts across layers. Moreover, it can be applied to any network architecture, due to its working principle.\n\nBase versions\nClass activation mapping and gradient-weighted class activation mapping are the original and most widely used methods for visual explanations in convolutional neural networks. These methods serve as the foundation for many later developments in explainable AI.\nNotation: In this article, the symbols i and j represent integer indices that disappear inside sums or averages, while x and y are the continuous (or up-sampled integer) coordinates of the final heat-map that is plotted.\n\nClass activation mapping (CAM)\nClass activation mapping (CAM) was the first, and the original, version of CAM methods, and it gave the name to the whole category. The approach was firstly introduced by Zhou et al. in their seminal work \"Learning Deep Features for Discriminative Localization\".\nThis approach achieves class-specific heatmaps by modifying image classification CNN architectures, replacing fully-connected layers with convolutional layers and a final global average pooling layer.\nIts main scope is to localize and highlight discriminative regions of an input image that a CNN uses to identify a particular class, without needing explicit bounding box annotations.\n\nGlobal average pooling (GAP)\nGlobal average pooling (GAP) represents the key element in the original CAM approach.\nIt is a dimensionality reduction technique and, similarly to other pooling layers, it allows the downsampling of the feature maps, calculating representative values for a specific region of the feature map. The particularity of GAP is that it calculates a single value for an entire feature map, significantly reducing the model dimensions.\n\nMathematical description\nThe mathematical description considers as its key the combination of convolutional and GAP layers.\nIn CAM, it is mandatory to have the GAP layer after the last convolutional layer and before the final linear classifier layer. This last element of the architecture connects the output logits (the network predictions) \n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle y^{C}}\n  \n, to the GAP values, with its respective fine-tuned weights, \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n.\nConsidering \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A^{k}}\n  \n as the last feature maps of the last convolutional layer, GAP produces one value for each feature map, by averaging all the matrix elements (i, j) of the feature map:\n\n  \n    \n      \n        \n          F\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            \n              m\n              n\n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          A\n          \n            i\n            j\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle F^{k}={\\frac {1}{mn}}\\sum _{i=1}^{m}\\sum _{j=1}^{n}A_{ij}^{k}}\n  \n\nwith\n\n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    A\n                    \n                      11\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  \n                    A\n                    \n                      12\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    A\n                    \n                      1\n                      n\n                    \n                    \n                      k\n                    \n                  \n                \n              \n              \n                \n                  \n                    A\n                    \n                      21\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  \n                    A\n                    \n                      22\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    A\n                    \n                      2\n                      n\n                    \n                    \n                      k\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    A\n                    \n                      m\n                      1\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  \n                    A\n                    \n                      m\n                      2\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    A\n                    \n                      m\n                      n\n                    \n                    \n                      k\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          {\n          \n            \n              A\n              \n                i\n                j\n              \n              \n                k\n              \n            \n            ∣\n            1\n            ≤\n            i\n            ≤\n            m\n            ,\n             \n            1\n            ≤\n            j\n            ≤\n            n\n          \n          }\n        \n      \n    \n    {\\displaystyle A^{k}={\\begin{bmatrix}A_{11}^{k}&A_{12}^{k}&\\cdots &A_{1n}^{k}\\\\A_{21}^{k}&A_{22}^{k}&\\cdots &A_{2n}^{k}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\A_{m1}^{k}&A_{m2}^{k}&\\cdots &A_{mn}^{k}\\end{bmatrix}}=\\left\\{A_{ij}^{k}\\mid 1\\leq i\\leq m,\\ 1\\leq j\\leq n\\right\\}}\n  \n\nNamely, in the GAP layer, each feature map is reduced to a single scalar via GAP, producing k values, hence reducing the dimensionality of the network. A k\n  \n    \n      \n        ×\n      \n    \n    {\\displaystyle \\times }\n  \nm\n  \n    \n      \n        ×\n      \n    \n    {\\displaystyle \\times }\n  \nn tensor is reduced to k scalars, shrinking the parameter count for the linear classifier head.\nThe final output logits are calculated as the linear sum of the GAP values, weights and bias:\n\n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n        =\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          F\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle y^{C}=\\sum _{k}w_{k}^{C}F^{k}}\n  \n\nThe localization map is computed as follows:\n\n  \n    \n      \n        \n          L\n          \n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{CAM}^{C}(x,y)=ReLU(\\sum _{k}w_{k}^{C}A_{k}(x,y))}\n  \n\nnamely, \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A_{k}(x,y)}\n  \n is the activation of node k in the target layer of the model, and \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n is the class-specific weight, for the channel k, in the linear classifier layer.\n\nAdvantages and drawbacks\nThe use of the GAP layer represents an example of an interpretability by design (IBD) approach. IBD refers to a technique which uses the model's own architecture to help explain its predictions.\nThe main drawback of CAM is that it is highly model-specific, being applicable to CNN architectures whose layer before the softmax one is a GAP. \nSince the approach relies on the post-GAP weights for the overall evaluation, the method can't be applied to intermediate layers.\nThe choice of dealing with an IBD approach restricts the possibility to generalize the model architecture. Moreover, IBD methods often require re-training of the model.\n\nGradient-weighted class activation mapping (Grad-CAM)\nGradient-weighted class activation mapping (Grad-CAM) is a generalized version of CAM and it tackles its architectural limitations. Grad-CAM computes the gradient of a target class score, the pre-softmax logit, with respect to the feature maps of a convolutional neural network. The gradients are global-average-pooled to obtain importance weights, which are used to compute a class-specific localization map by linearly weighting the feature maps. The result is a heatmap that highlights the regions in the input image that are the most influential for predicting the target class.\nThe main advantage of Grad-CAM, with respect to the standard CAM, is that it is model agnostic (provided that the network still needs to be differentiable), meaning that it generates visual explanation for any CNN-based network without architectural changes or re-training, making it broadly applicable to pre-trained models.\n\nMathematical description\nConsidering:\n\ny\n  \n    \n      \n        \n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle ^{C}}\n  \n the logits (i.e. the pre-softmax activated neurons responsible for a certain class prediction) of interest;\nA\n  \n    \n      \n        \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle ^{k}}\n  \n the feature activated map for a specific convolutional layer;\nL\n  \n    \n      \n        \n          \n          \n            Grad-CAM\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle _{\\text{Grad-CAM}}^{C}}\n  \n ∈ \n  \n    \n      \n        \n          \n            R\n          \n          \n            u\n            ×\n            v\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{u\\times v}}\n  \n the class-discriminative localization map, of width u and height v for any class c;\nGrad-CAM, employing backpropagation, computes the logit gradient with respect to the feature map A\n  \n    \n      \n        \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle ^{k}}\n  \n as\n\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                \n                  y\n                  \n                    C\n                  \n                \n              \n            \n            \n              ∂\n              \n                \n                  A\n                  \n                    k\n                  \n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial {y^{C}}}{\\partial {A^{k}}(i,j)}}}\n  \n\nhighlighting the importance of a certain class discrimination decision process of the logit.\nThese gradients are global-average-pooled over each element of the feature map (hence, highlighting the \"importance\" of the elements of a feature map k for a target class C):\n\n  \n    \n      \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        =\n        \n          \n            1\n            \n              u\n              v\n            \n          \n        \n        \n          ∑\n          \n            i\n          \n        \n        \n          ∑\n          \n            j\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  y\n                  \n                    C\n                  \n                \n              \n            \n            \n              ∂\n              \n                \n                  A\n                  \n                    k\n                  \n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha _{k}^{C}={\\frac {1}{uv}}\\sum _{i}\\sum _{j}{\\frac {\\partial {y^{C}}}{\\partial {A^{k}}(i,j)}}}\n  \n\nSo, to account for the total number of feature maps, each of them is multiplied by its weight (via dot-product) and element-wise summation is done:\n\n  \n    \n      \n        \n          ∑\n          \n            k\n          \n        \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\sum _{k}\\alpha _{k}^{C}A^{k}}\n  \n\nIt can be observed that, due to the intrinsic nature of the gradient operation, some elements of the weighted feature map will have negative value, so, since only elements that have increased the logit of the predicted class are of interest, a ReLU activation function is applied:\n\n  \n    \n      \n        \n          L\n          \n            G\n            r\n            a\n            d\n            −\n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{Grad-CAM}^{C}(x,y)=ReLU(\\sum _{k}\\alpha _{k}^{C}A^{k}(x,y))}\n  \n\nLastly, the output heatmap image dimensions are upsampled to the original image size to match the input dimensions.\n\nAdvantages and drawbacks\nGrad-CAM addresses the most important CAM limitations. It makes CAM free from the GAP layer need, generalizing its behavior and enabling visual explanation at intermediate layers.\nHowever, Grad-CAM focuses on the most discriminative region when contributing to classification. If multiple similar objects are present, Grad-CAM often highlights only one of them, or part of one, providing also coarser maps and lower localization accuracy.\nMoreover, Grad-CAM retrieves backwards information (the gradients), without taking into consideration how the activation flowed forward during prediction (unless combined with the guided backpropagation technique), resulting in a certain probability of missing patterns highlighted in the forward signal. On top of that, Grad-CAM heat maps are low-resolution when choosing a very deep layer.\nLastly, false emphasis in the heatmap may be present when large gradients are computed for low activation values. Grad-CAM assumes that gradient implies importance, ignoring the activation features value.\n\nGrad-CAM and CAM comparison\nFine-tuned versions\nSeveral methods have refined Grad-CAM to improve clarity and flexibility. Guided Grad-CAM, Grad-CAM++, Score-CAM, and Layer-CAM enhance aspects such as localization accuracy, gradient independence, and multi-layer visualization. These techniques build directly on the principles of CAM and Grad-CAM.\n\nGuided Grad-CAM\nGuided Grad-CAM fuses the coarse, class‐discriminative localization of Grad-CAM with the high‐resolution details of guided backpropagation. Grad-CAM heatmap is first computed for the target class, and it is upsampled to the input size. Then, a Guided Backpropagation saliency map for the same class is computed. A final element‐wise product of the two results in the Guided Grad-CAM visualization map.\nThe result is a high-resolution, class-specific saliency map that highlights exactly which pixels contribute most to the network’s decision.\n\nGrad-CAM++\nGrad-CAM++ introduces a more refined way of computing the weights for each feature map, bypassing the global average of the gradients approach provided by Grad-CAM. This approach aims to improve the visual effect when multiple target instances are present in a single image.\nSpecifically, Grad-CAM++ employs pixel-wise gradients (via higher-order gradients), to compute the importance of a specific pixel for a prediction, lighting up multiple object instances in the same image.\nThese improvements allow for a more sensible and detailed output heatmap.\nThe associated mathematical framework is defined by the following localization map: \n\n  \n    \n      \n        \n          L\n          \n            G\n            r\n            a\n            d\n            −\n            C\n            A\n            M\n            +\n            +\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle L_{Grad-CAM++}^{C}(x,y)=\\sum _{k}w_{k}^{C}A_{k}(x,y)}\n  \n\nin which the coefficient \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n is defined as:\n\n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        =\n        \n          ∑\n          \n            i\n            ,\n            j\n          \n        \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        i\n        ,\n        j\n        )\n        ×\n        R\n        e\n        L\n        U\n        (\n        \n          \n            \n              ∂\n              \n                y\n                \n                  C\n                \n              \n            \n            \n              ∂\n              \n                A\n                \n                  k\n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle w_{k}^{C}=\\sum _{i,j}\\alpha _{k}^{C}(i,j)\\times ReLU({\\frac {\\partial y^{C}}{\\partial A_{k}(i,j)}})}\n  \n\nwith \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A^{k}(x,y)}\n  \n, the activation of node k in the target layer of the model at position (x,y); \n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle y^{C}}\n  \n the logit score for class C, and \n  \n    \n      \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        i\n        ,\n        j\n        )\n      \n    \n    {\\displaystyle \\alpha _{k}^{C}(i,j)}\n  \n being:\n\n  \n    \n      \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        i\n        ,\n        j\n        )\n        =\n        \n          \n            \n              \n                \n                  ∂\n                  \n                    2\n                  \n                \n                \n                  y\n                  \n                    C\n                  \n                \n              \n              \n                ∂\n                (\n                \n                  A\n                  \n                    k\n                  \n                \n                (\n                i\n                ,\n                j\n                )\n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            \n              2\n              ×\n              \n                \n                  \n                    \n                      ∂\n                      \n                        2\n                      \n                    \n                    \n                      y\n                      \n                        C\n                      \n                    \n                  \n                  \n                    ∂\n                    (\n                    \n                      A\n                      \n                        k\n                      \n                    \n                    (\n                    i\n                    ,\n                    j\n                    )\n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              +\n              \n                ∑\n                \n                  a\n                  ,\n                  b\n                \n              \n              \n                A\n                \n                  k\n                \n              \n              (\n              a\n              ,\n              b\n              )\n              ×\n              \n                \n                  \n                    \n                      ∂\n                      \n                        3\n                      \n                    \n                    \n                      y\n                      \n                        C\n                      \n                    \n                  \n                  \n                    ∂\n                    (\n                    \n                      A\n                      \n                        k\n                      \n                    \n                    (\n                    i\n                    ,\n                    j\n                    )\n                    \n                      )\n                      \n                        3\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha _{k}^{C}(i,j)={\\frac {\\frac {\\partial ^{2}y^{C}}{\\partial (A_{k}(i,j))^{2}}}{2\\times {\\frac {\\partial ^{2}y^{C}}{\\partial (A_{k}(i,j))^{2}}}+\\sum _{a,b}A_{k}(a,b)\\times {\\frac {\\partial ^{3}y^{C}}{\\partial (A_{k}(i,j))^{3}}}}}}\n  \n\nWhile addressing some Grad-CAM problems, Grad-CAM++ method still relies on gradients, and it only improves the underlying math. It is, however, still based on the idea of assigning a direct and valid relationship between gradient and importance.\nNotation: (a,b) indexes all pixel positions in the feature‐map, exactly like (i,j) does, but for the summation in the denominator.\n\nScore-CAM\nScore-CAM is a gradient-free CAM technique, thus redefining the original Grad-CAM and Grad-CAM++ working principles. It uses the model confidence scores instead of gradients.\nScore-CAM performs the following operations:\n\nExtracts the feature maps \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A_{k}}\n  \n of the final convolutional layers, as in the original Grad-CAM;\nUpsamples each activation map \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A_{k}}\n  \n to the same input image dimensions, defining a mask \n  \n    \n      \n        \n          M\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle M_{k}}\n  \n and each mask is normalized;\nMultiplies the original input image by the mask, defining a masked image \n  \n    \n      \n        \n          X\n          \n            k\n          \n          ′\n        \n        =\n        \n          M\n          \n            k\n          \n        \n        ⊙\n        X\n      \n    \n    {\\displaystyle X'_{k}=M_{k}\\odot X}\n  \n (\n  \n    \n      \n        ⊙\n      \n    \n    {\\displaystyle \\odot }\n  \n is the element-wise multiplication);\nGets a confidence score (a softmax probability is the output value after the softmax operation; the logit is the value before the softmax) for the masked images \n  \n    \n      \n        \n          X\n          \n            k\n          \n          ′\n        \n      \n    \n    {\\displaystyle X'_{k}}\n  \n, by feeding it into the CNN (either the soft-max probability or the raw logit can be used; both yield similar results in practice);\nConsiders that confidence score as the weight \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            c\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{c}}\n  \n for the feature map \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A_{k}}\n  \n.\nThese operations allow to replace the gradient calculations with the actual model outputs, building more accurate heatmaps.\nMathematically, the localization map is defined as:\n\n  \n    \n      \n        \n          L\n          \n            S\n            c\n            o\n            r\n            e\n            −\n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            c\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{Score-CAM}^{C}(x,y)=ReLU(\\sum _{k}w_{k}^{c}A_{k}(x,y))}\n  \n\nand the coefficient \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n s:\n\n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        =\n        s\n        o\n        f\n        t\n        m\n        a\n        \n          x\n          \n            k\n          \n        \n        (\n        \n          y\n          \n            C\n          \n        \n        (\n        \n          X\n          \n            k\n          \n          ′\n        \n        )\n        )\n        =\n        \n          \n            \n              e\n              x\n              p\n              (\n              \n                y\n                \n                  C\n                \n              \n              (\n              \n                X\n                \n                  k\n                \n                ′\n              \n              )\n              )\n            \n            \n              \n                ∑\n                \n                  m\n                \n              \n              e\n              x\n              p\n              (\n              \n                y\n                \n                  C\n                \n              \n              (\n              \n                X\n                \n                  m\n                \n                ′\n              \n              )\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}=softmax_{k}(y^{C}(X'_{k}))={\\frac {exp(y^{C}(X'_{k}))}{\\sum _{m}exp(y^{C}(X'_{m}))}}}\n  \n\nwhere \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A_{k}(x,y)}\n  \n is the activation of channel k  at location (x,y), \n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n        (\n        X\n        )\n      \n    \n    {\\displaystyle y^{C}(X)}\n  \n is the logit for class C for an input X and \n  \n    \n      \n        \n          M\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle M_{k}}\n  \n is the mask, defined as:\n\n  \n    \n      \n        \n          M\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          \n            \n              U\n              (\n              \n                A\n                \n                  k\n                \n              \n              )\n              (\n              x\n              ,\n              y\n              )\n              −\n              m\n              i\n              \n                n\n                \n                  x\n                  ,\n                  y\n                \n              \n              U\n              (\n              \n                A\n                \n                  m\n                \n              \n              )\n            \n            \n              m\n              a\n              \n                x\n                \n                  x\n                  ,\n                  y\n                \n              \n              U\n              (\n              \n                A\n                \n                  k\n                \n              \n              )\n              −\n              m\n              i\n              \n                n\n                \n                  x\n                  ,\n                  y\n                \n              \n              U\n              (\n              \n                A\n                \n                  k\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle M_{k}(x,y)={\\frac {U(A_{k})(x,y)-min_{x,y}U(A_{m})}{max_{x,y}U(A_{k})-min_{x,y}U(A_{k})}}}\n  \n\nwith \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n as the upsampling operation.\nSince the process of score calculation is repeated for every channel, Score-CAM is slow with respect to gradient-based methods. Moreover, it focuses on regions highlighted by individual feature maps, ignoring the context of the full image, reducing interpretability in complex scenes.\n\nLayerCAM\nLayerCAM enhances backwards class-specific gradients using both intermediate and final convolutional layers. Combining information across layers allows to achieve higher resolution and more fine-grained detail, improving localization.\nSpecifically, for each position in the feature map, LayerCAM evaluates the gradient. The positive gradients are employed as the weights, \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n. Namely:\n\n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          \n            \n              ∂\n              \n                y\n                \n                  C\n                \n              \n            \n            \n              ∂\n              \n                A\n                \n                  k\n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle w_{k}^{C}(x,y)=ReLU({\\frac {\\partial y^{C}}{\\partial A_{k}(i,j)}}(x,y))}\n  \n\nThe activations are then weighted, and the final class activation map is retrieved by summing over the channels.\n\n  \n    \n      \n        \n          L\n          \n            L\n            a\n            y\n            e\n            r\n            −\n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{Layer-CAM}^{C}(x,y)=ReLU(\\sum _{k}w_{k}^{C}(x,y)A_{k}(x,y))}\n  \n\nThis technique offers high-resolution heatmaps, flexible localization and per-location precision, employing positive gradients.\n\nReferences\nExternal links\n\"the World Conference on eXplainable Artificial Intelligence\".\n\"ACM Conference on Fairness, Accountability, and Transparency (FAccT)\".\n\"PyTorch image-specific saliency maps\". GitHub.\n\"Guided Backpropagation with PyTorch and TensorFlow\". 21 December 2021.\n\"PyTorch implementation of \"Learning Deep Features for Discriminative Localization\"\". GitHub.\n\"Advanced AI explainability for PyTorch\". GitHub.",
        "url": "https://en.wikipedia.org/wiki/Class_activation_mapping"
    },
    {
        "title": "Cognitive computing",
        "text": "Cognitive computing refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human–computer interaction, dialog and narrative generation, among other technologies.\n\nDefinition\nAt present, there is no widely agreed upon definition for cognitive computing in either academia or industry.\nIn general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain (2004). In this sense, cognitive computing is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. Cognitive computing applications link data analysis and adaptive page displays (AUI) to adjust content for a particular type of audience. As such, cognitive computing hardware and applications strive to be more affective and more influential by design.\n\nThe term \"cognitive system\" also applies to any artificial construct able to perform a cognitive process where a cognitive process is the transformation of data, information, knowledge, or wisdom to a new level in the DIKW Pyramid. While many cognitive systems employ techniques having their origination in artificial intelligence research, cognitive systems, themselves, may not be artificially intelligent. For example, a  neural network trained to recognize cancer on an MRI scan may achieve a higher success rate than a human doctor. This system is certainly a cognitive system but is not artificially intelligent.\nCognitive systems may be engineered to feed on dynamic data in real-time, or near real-time, and may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).\n\nCognitive analytics\nCognitive computing-branded technology platforms typically specialize in the processing and analysis of large, unstructured datasets.\n\nApplications\nEducation\nEven if cognitive computing can not take the place of teachers, it can still be a heavy driving force in the education of students. Cognitive computing being used in the classroom is applied by essentially having an assistant that is personalized for each individual student. This cognitive assistant can relieve the stress that teachers face while teaching students, while also enhancing the student's learning experience over all. Teachers may not be able to pay each and every student individual attention, this being the place that cognitive computers fill the gap. Some students may need a little more help with a particular subject. For many students, Human interaction between student and teacher can cause anxiety and can be uncomfortable. With the help of Cognitive Computer tutors, students will not have to face their uneasiness and can gain the confidence to learn and do well in the classroom. While a student is in class with their personalized assistant, this assistant can develop various techniques, like creating lesson plans, to tailor and aid the student and their needs.\nHealthcare\nNumerous tech companies are in the process of developing technology that involves cognitive computing that can be used in the medical field. The ability to classify and identify is one of the main goals of these cognitive devices. This trait can be very helpful in the study of identifying carcinogens. This cognitive system that can detect would be able to assist the examiner in interpreting countless numbers of documents in a lesser amount of time than if they did not use Cognitive Computer technology. This technology can also evaluate information about the patient, looking through every medical record in depth, searching for indications that can be the source of their problems.\nCommerce\nTogether with Artificial Intelligence, it has been used in warehouse management systems  to collect, store, organize and analyze all related supplier data. All these aims at improving efficiency, enabling faster decision-making, monitoring inventory and fraud detection\nHuman Cognitive Augmentation\nIn situations where humans are using or working collaboratively with cognitive systems, called a human/cog ensemble, results achieved by the ensemble are superior to results obtainable by the human working alone. Therefore, the human is cognitively augmented. In cases where the human/cog ensemble achieves results at, or superior to, the level of a human expert then the ensemble has achieved synthetic expertise. In a human/cog ensemble, the \"cog\" is a cognitive system employing virtually any kind of cognitive computing technology.\nOther use cases\nSpeech recognition\nSentiment analysis\nFace detection\nRisk assessment\nFraud detection\nBehavioral recommendations\n\nIndustry work\nCognitive computing in conjunction with big data and algorithms that comprehend customer needs, can be a major advantage in economic decision making.\nThe powers of cognitive computing and artificial intelligence hold the potential to affect almost every task that humans are capable of performing. This can negatively affect employment for humans, as there would be no such need for human labor anymore. It would also increase the inequality of wealth; the people at the head of the cognitive computing industry would grow significantly richer, while workers without ongoing, reliable employment would become less well off.\nThe more industries start to use cognitive computing, the more difficult it will be for humans to compete. Increased use of the technology will also increase the amount of work that AI-driven robots and machines can perform. Only extraordinarily talented, capable and motivated humans would be able to keep up with the machines. The influence of competitive individuals in conjunction with artificial intelligence/cognitive computing with has the potential to change the course of humankind.\n\nSee also\nAutomation\nAffective computing\nAnalytics\nArtificial intelligence\nArtificial neural network\nBrain computer interface\nCognitive computer\nCognitive reasoning\nCognitive science\nEnterprise cognitive system\nSemantic Web\nSocial neuroscience\nSynthetic intelligence\nUsability\nNeuromorphic engineering\nAI accelerator\n\nReferences\nFurther reading\nRussell, John (February 15, 2016). \"Mapping Out a New Role for Cognitive Computing in Science\". HPCwire. Retrieved April 21, 2016.",
        "url": "https://en.wikipedia.org/wiki/Cognitive_computing"
    },
    {
        "title": "Cognitive philology",
        "text": "Cognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence. \"The point is not the text, but the mind that made it\". Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other.\nCognitive philology: \n\ninvestigates transmission of oral and written text, and categorization processes which lead to classification of knowledge, mostly relying on the information theory;\nstudies how narratives emerge in so called natural conversation and selective process which lead to the rise of literary standards for storytelling, mostly relying on embodied semantics;\nexplores the evolutive and evolutionary role played by rhythm and metre in human ontogenetic and phylogenetic development and the pertinence of the semantic association during processing of cognitive maps;\nProvides the scientific ground for multimedia  critical editions of literary texts.\n \nAmong the founding thinkers and noteworthy scholars devoted to such investigations are: \n\nAlan Richardson: Studies Theory of Mind in early-modern and contemporary literature.\nAnatole Pierre Fuksas\nBenoît de Cornulier\nDavid Herman: Professor of English at North Carolina State University and an adjunct professor of linguistics at Duke University. He is the author of \"Universal Grammar and Narrative Form\" and the editor of \"Narratologies: New Perspectives on Narrative Analysis\".\nDomenico Fiormonte\nFrançois Recanati\nGilles Fauconnier, a professor in Cognitive science at the University of California, San Diego. He was one of the founders of cognitive linguistics in the 1970s through his work on pragmatic scales and mental spaces. His research explores the areas of conceptual integration and compressions of conceptual mappings in terms of the emergent structure in language.\nJulián Santano Moreno\nLuca Nobile\nManfred Jahn in Germany\nMark Turner\nPaolo Canettieri\n\nSee also\nArtificial intelligence\nCognitive archaeology\nCognitive linguistics\nCognitive poetics\nCognitive psychology\nCognitive rhetoric\nInformation theory\nPhilology\n\nReferences\nExternal links\n\n(in Italian) Rivista di Filologia Cognitiva\nCogLit: Literature and Cognitive Linguistics\nCognitive Philology\nInstitute for Psychological Study of the Arts",
        "url": "https://en.wikipedia.org/wiki/Cognitive_philology"
    },
    {
        "title": "Coherent extrapolated volition",
        "text": "Coherent extrapolated volition (CEV) is a theoretical framework in the field of AI alignment proposed by Eliezer Yudkowsky in 2004 as part of his work on friendly AI. It describes an approach by which an artificial superintelligence (ASI) would act not according to humanity's current individual or collective preferences, but instead based on what humans would want—if they were more knowledgeable, more rational, had more time to think, and had matured together as a society.\n\nConcept\nCEV proposes that an advanced AI system should derive its goals by extrapolating the idealized volition of humanity. This means aggregating and projecting human preferences into a coherent utility function that reflects what people would desire under ideal epistemic and moral conditions. The aim is to ensure that AI systems are aligned with humanity's true interests, rather than with transient or poorly informed preferences.\n\nIn poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.\n\nDebate\nYudkowsky and Bostrom note that CEV has several interesting properties. It is designed to be humane and self-correcting, by capturing the source of human values instead of trying to list them. It avoids the difficulty of laying down an explicit, fixed list of rules. It encapsulates moral growth, preventing flawed current moral beliefs from getting locked in. It limits the influence that a small group of programmers can have on what the ASI would value, thus also reducing the incentives to build ASI first. And it keeps humanity in charge of its destiny.\nCEV also faces significant theoretical and practical challenges.\nBostrom notes that CEV has \"a number of free parameters that could be specified in various ways, yielding different versions of the proposal.\" One such parameter is the extrapolation base (whose CEV is taken into account). For example, whether it should include people with severe dementia, patients in a vegetative state, foetuses, or embryos. He also notes that if CEV's extrapolation base only includes humans, there is a risk that the result would be ungenerous toward other animals and digital minds. One possible solution would be to include a mechanism to expand CEV's extrapolation base.\n\nVariants and alternatives\nA proposed theoretical alternative to CEV is to rely on an artificial superintelligence's superior cognitive capabilities to figure out what is morally right, and let it act accordingly. It is also possible to combine both techniques, for instance with the ASI following CEV except when it is morally impermissible.\nIn another review, a philosophical analysis explores CEV through the lens of social trust in autonomous systems. Drawing on Anthony Giddens' concept of \"active trust\", the author proposes an evolution of CEV into \"Coherent, Extrapolated and Clustered Volition\" (CECV). This formulation aims to better reflect the moral preferences of diverse cultural groups, thus offering a more pragmatic ethical framework for designing AI systems that earn public trust while accommodating societal diversity.\n\nYudkowsky's later view\nAlmost immediately after publishing the idea in 2004, Eliezer Yudkowsky himself described the concept as outdated. He warned against conflating it with a practical strategy for AI alignment. While CEV may serve as a philosophical ideal, Yudkowsky emphasized that real-world alignment mechanisms must grapple with greater complexity, including the difficulty of defining and implementing extrapolated values in a reliable way.\n\nSee also\nFriendly artificial intelligence\nAI alignment\nAI safety\nEliezer Yudkowsky\nNick Bostrom\nRationality\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Coherent_extrapolated_volition"
    },
    {
        "title": "Commonsense knowledge (artificial intelligence)",
        "text": "In artificial intelligence research,  commonsense knowledge consists of facts about the everyday world, such as \"Lemons are sour\", or \"Cows say moo\", that all humans are expected to know.  It is currently an unsolved problem in artificial general intelligence. The first AI program to address common sense knowledge was Advice Taker in 1959 by John McCarthy.\nCommonsense knowledge  can underpin a commonsense reasoning process, to attempt inferences such as \"You might bake a cake because you want people to eat the cake.\"  A natural language processing process can be attached to the commonsense knowledge base to allow the knowledge base to attempt to answer questions about the world. Common sense knowledge also helps to solve problems in the face of incomplete information.  Using widely held beliefs about everyday objects, or common sense knowledge, AI systems make common sense assumptions or default assumptions about the unknown similar to the way people do.  In an AI system or in English, this is expressed  as \"Normally P holds\", \"Usually P\" or \"Typically P so Assume P\".   For example, if we know the fact \"Tweety is a bird\", because we know the commonly held belief about birds, \"typically birds fly,\" without knowing anything else about Tweety, we may reasonably assume the fact that \"Tweety can fly.\"  As more knowledge of the world is discovered or learned over time, the AI system can revise its assumptions about Tweety using a truth maintenance process.  If we later learn that \"Tweety is a penguin\" then truth maintenance revises this assumption because we also know \"penguins do not fly\".\n\nCommonsense reasoning\nCommonsense reasoning simulates the human ability to use commonsense knowledge to make presumptions about the type and essence of ordinary situations they encounter every day, and to change their \"minds\" should new information come to light.  This includes time, missing or incomplete information and cause and effect.  The ability to explain cause and effect is an important aspect of explainable AI.  Truth maintenance algorithms automatically provide an explanation facility because they create elaborate records of presumptions.  Compared with humans, all existing computer programs that attempt human-level AI perform extremely poorly on modern \"commonsense reasoning\" benchmark tests such as the Winograd Schema Challenge. The problem of attaining human-level competency at \"commonsense knowledge\" tasks is considered to probably be \"AI complete\" (that is, solving it would require the ability to synthesize a fully human-level intelligence), although some oppose this notion and believe compassionate intelligence is also required for human-level AI. Common sense reasoning has been applied successfully in more limited domains such as natural language processing and automated diagnosis or analysis.\n\nCommonsense knowledge base construction\nCompiling comprehensive knowledge bases of commonsense assertions (CSKBs) is a long-standing challenge in AI research. From early expert-driven efforts like CYC and WordNet, significant advances were achieved via the crowdsourced OpenMind Commonsense project, which led to the crowdsourced ConceptNet KB. Several approaches have attempted to automate CSKB construction, most notably, via text mining (WebChild, Quasimodo, TransOMCS, Ascent), as well as harvesting these directly from pre-trained language models (AutoTOMIC). These resources are significantly larger than ConceptNet, though the automated construction mostly makes them of moderately lower quality. Challenges also remain on the representation of commonsense knowledge: Most CSKB projects follow a triple data model, which is not necessarily best suited for breaking more complex natural language assertions. A notable exception here is GenericsKB, which applies no further normalization to sentences, but retains them in full.\n\nApplications\nAround 2013, MIT researchers developed BullySpace, an extension of the commonsense knowledgebase ConceptNet, to catch taunting social media comments. BullySpace included over 200 semantic assertions based around stereotypes, to help the system infer that comments like \"Put on a wig and lipstick and be who you really are\" are more likely to be an insult if directed at a boy than a girl.\nConceptNet has also been used by chatbots and by computers that compose original fiction.  At Lawrence Livermore National Laboratory, common sense knowledge was used in an intelligent software agent to detect violations of a comprehensive nuclear test ban treaty.\n\nData\nAs an example, as of 2012 ConceptNet includes these 21 language-independent relations:\n\nIsA (An \"RV\" is a \"vehicle\")\nUsedFor\nHasA (A \"rabbit\" has a \"tail\")\nCapableOf\nDesires\nCreatedBy (\"cake\" can be created by \"baking\")\nPartOf\nCauses\nLocatedNear\nAtLocation (Somewhere a \"Cook\" can be at a \"restaurant\")\nDefinedAs\nSymbolOf (X represents Y)\nReceivesAction (\"cake\" can be \"eaten\")\nHasPrerequisite (X cannot do Y unless A does B)\nMotivatedByGoal (You would \"bake\" because you want to \"eat\")\nCausesDesire (\"baking\" makes you want to \"follow recipe\")\nMadeOf\nHasFirstSubevent (The first thing required when you're doing X is for entity Y to do Z)\nHasSubevent (\"eat\" has subevent \"swallow\")\nHasLastSubevent\n\nCommonsense knowledge bases\nCyc\nOpen Mind Common Sense (data source) and ConceptNet (datastore and NLP engine)\nEvi\nGraphiq\n\nSee also\nCommon sense\nLinked data and the Semantic Web\nTruth Maintenance or Reason Maintenance\nOntology\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)"
    },
    {
        "title": "Competition in artificial intelligence",
        "text": "Competition in artificial intelligence refers to the rivalry among companies, research institutions, and governments to develop and deploy the most capable artificial intelligence (AI) systems. The competition spans multiple domains, including large language models (LLMs), autonomous vehicles, robotics, computer vision systems, natural language processing (NLP), and AI-optimized hardware.\n\nBackground\nCompetition in AI is driven by potential economic, strategic, and scientific advantages. Breakthroughs in AI can enhance productivity, enable new products and services, and provide geopolitical leverage. The field has experienced rapid progress since the mid-2010s, particularly in machine learning and artificial neural networks, leading to intense rivalry among leading actors.\n\nCorporate competition\nMajor technology companies are among the most visible competitors in AI. In the United States, firms such as OpenAI, Google DeepMind, Meta Platforms, Microsoft, Anthropic, and Nvidia compete in building advanced LLMs, generative AI platforms, and AI-optimized graphics processing units (GPUs). In China, companies such as Baidu, Alibaba Group, Tencent, and startups such DeepSeek have become leaders in AI deployment, often with state backing.\nThe \"[war for talent]\" in AI research has become a defining feature of corporate competition. Leading firms often recruit top AI researchers from rivals, sometimes offering multi-million-dollar compensation packages.\n\nNational competition\nGovernments see leadership in AI as a strategic priority. The United States has funded AI research for military, economic, and societal applications, while China has set a target to lead the world in AI by 2030 through its \"New Generation Artificial Intelligence Development Plan\". Other nations, including the UK, India, Russia, South Korea, and members of the European Union, have launched national AI strategies.\n\nSectors of competition\nLarge language models and chatbots competition\nCompetition to produce the most capable generative text models, with benchmarks such as MMLU and ARC used to evaluate performance has been on scale since emergency of AI. These systems leverage deep learning, especially transformer architectures, to understand and generate human-like language. Companies and research groups globally compete to develop chatbots that are more capable, reliable, and context-aware. Among the most well-known chatbots is ChatGPT, developed by OpenAI. Since its public release in 2022, ChatGPT rapidly gained widespread attention for its ability to engage in coherent and versatile conversations, assist with creative writing, and solve complex problems. In response, technology firms introduced competing chatbots aiming to challenge or surpass ChatGPT's capabilities. Notably, DeepSeek, a Chinese AI company, launched an advanced chatbot integrated with their R1 language model, emphasizing strong natural language understanding and multilingual support. Similarly, Grok, developed by Tesla, Inc., integrates conversational AI into vehicles and digital assistants, combining natural language processing with real-time data for personalized user interaction. \nThese chatbots not only compete in language tasks but also demonstrate strategic reasoning capabilities by playing complex games such as chess and Go. This form of competition is reminiscent of the historic AI milestones set by programs such as Deep Blue and AlphaGo. The OpenAI’s ChatGPT has been tested in playing chess at various levels, while DeepSeek’s chatbot showcased its prowess in online chess tournaments in early 2024, winning several matches against human and AI opponents. Grok, leveraging Tesla's vast data infrastructure, has demonstrated real-time strategic decision-making in simulation environments that include chess-like games. \nThe competition pushes rapid innovation, with firms racing to improve chatbot conversational depth, reduce biases, increase factual accuracy, and integrate multimodal inputs like images and videos. At the same time, the competition raises questions about AI safety, ethical use, and the societal impacts of increasingly human-like chatbots.\n\nAutonomous vehicles\nCompanies such as Waymo, Tesla, and Baidu are racing to deploy safe and reliable self-driving car technology.\n\nAI chips\nRivalry between Nvidia, AMD, Intel, and Huawei in designing processors optimized for AI workloads.\n\nMilitary applications\nDevelopment of AI-enabled drones, surveillance systems, and decision-support tools, with associated ethical debates.\n\nIncidents\nIn 2023, OpenAI released GPT-4, prompting competitors such as Google DeepMind to accelerate the release of their own models, including Gemini.\nIn 2024, Chinese AI company DeepSeek launched the R1 model, leading OpenAI to release an open-source system, GPT-OSS, as a strategic countermeasure.\nIn 2022, Tesla and Waymo both expanded autonomous taxi services in U.S. cities, competing for regulatory approval and public trust.\nThe U.S. Department of Defense's Project Maven and China's AI-enabled surveillance programs have been cited as examples of military AI rivalry.\nIn 2025, Microsoft hired several senior engineers from Google DeepMind, highlighting the ongoing \"talent poaching\" competition in the AI sector.\n\nRisks and concerns\nCritics warn that unrestrained competition in AI can undermine safety, ethics, and governance. Concerns include the proliferation of biased or unsafe models, escalation in autonomous weapons, and reduced cooperation on safety standards.\n\nSee also\nArtificial intelligence\nArtificial general intelligence\nTechnological singularity\nSpace Race\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Competition_in_artificial_intelligence"
    },
    {
        "title": "Computational heuristic intelligence",
        "text": "Computational heuristic intelligence (CHI) refers to specialized programming techniques in computational intelligence (also called artificial intelligence, or AI). These techniques have the express goal of avoiding complexity issues, also called NP-hard problems, by using human-like techniques. They are best summarized as the use of exemplar-based methods (heuristics), rather than rule-based methods (algorithms). Hence the term is distinct from the more conventional computational algorithmic intelligence, or symbolic AI. An example of a CHI technique is the encoding specificity principle of Tulving and Thompson. In general, CHI principles are problem solving techniques used by people, rather than programmed into machines. It is by drawing attention to this key distinction that the use of this term is justified in a field already replete with confusing neologisms. Note that the legal systems of all modern human societies employ both heuristics (generalisations of cases) from individual trial records as well as legislated statutes (rules) as regulatory guides.\nAnother recent approach to the avoidance of complexity issues is to employ feedback control rather than feedforward modeling as a problem-solving paradigm. This approach has been called computational cybernetics, because (a) the term 'computational' is associated with conventional computer programming techniques which represent a strategic, compiled, or feedforward model of the problem, and (b) the term 'cybernetic' is associated with conventional system operation techniques which represent a tactical, interpreted, or feedback model of the problem. Of course, real programs and real problems both contain both feedforward and feedback components. A real example which illustrates this point is that of human cognition, which clearly involves both perceptual (bottom-up, feedback, sensor-oriented) and conceptual (top-down, feedforward, motor-oriented) information flows and hierarchies.\nThe AI engineer must choose between mathematical and cybernetic problem solution and machine design paradigms. This is not a coding (program language) issue, but relates to understanding the relationship between the declarative and procedural programming paradigms. \nThe vast majority of STEM professionals never get the opportunity to design or implement pure cybernetic solutions. When pushed, most responders will dismiss the importance of any difference by saying that all code can be reduced to a mathematical model anyway. Unfortunately, not only is this belief false, it fails most spectacularly in many AI scenarios.\nMathematical models are not time agnostic, but by their very nature are pre-computed, i.e. feedforward. Dyer [2012] and Feldman [2004] have independently investigated the simplest of all somatic governance paradigms, namely control of a simple jointed limb by a single flexor muscle. They found that it is impossible to determine forces from limb positions- therefore, the problem cannot have a pre-computed (feedforward) mathematical solution. Instead, a top-down command bias signal changes the threshold feedback level in the sensorimotor loop, e.g. the loop formed by the afferent and efferent nerves, thus changing the so-called ‘equilibrium point’ of the flexor muscle/ elbow joint system. An overview of the arrangement reveals that global postures and limb position are commanded in feedforward terms, using global displacements (common coding), with the forces needed being computed locally by feedback loops. This method of sensorimotor unit governance, which is based upon what Anatol Feldman calls the ‘equilibrium Point’ theory, is formally equivalent to a servomechanism such as a car's ‘cruise control’.\n\nSee also\nNP-hard\nHeuristics\nComputational cybernetics\nTop-down and bottom-up design\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Computational_heuristic_intelligence"
    },
    {
        "title": "Computational humor",
        "text": "Computational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.\n\nJoke generators\nPun generation\nAn approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification.\nSimple prototypes for computer pun generation were reported in the early 1990s, based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question-answer-type puns from a general, i.e., non-humorous, lexicon.  (The program name is an acronym for \"Joke Analysis and Production Engine\".) Some examples produced by JAPE are:\n\nQ: What is the difference between leaves and a car?\nA: One you brush and rake, the other you rush and brake.\nQ: What do you call a strange market?\nA: A bizarre bazaar.\nSince then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy. (The project name is an acronym for \"System To Augment Non-speakers' Dialog Using Puns\" and an allusion to standup comedy.) Children responded to this \"language playground\" with enthusiasm, and showed marked improvement on certain types of language tests.\n\nThe two young people, who used the system over a ten-week period, regaled their peers, staff, family and neighbors with jokes such as: \"What do you call a spicy missile? A hot shot!\" Their joy and enthusiasm at entertaining others was inspirational.\n\nOther\nStock and Strapparava described a program to generate funny acronyms.\n\nJoke recognition\nA statistical machine learning algorithm to detect whether a sentence contained a \"That's what she said\" double entendre was developed by Kiddon and Brun (2011).  There is an open-source Python implementation of Kiddon & Brun's TWSS system.\nA program to recognize knock-knock jokes was reported by Taylor and Mazlack. This kind of research is important in analysis of human–computer interaction.\nAn application of machine learning techniques for the distinguishing of joke texts from non-jokes was described by Mihalcea and Strapparava (2006).\nTakizawa et al. (1996) reported on a heuristic program for detecting puns in the Japanese language.\n\nApplications\nA possible application for assistance in language acquisition is described in the section \"Pun generation\". Another envisioned use of joke generators is in cases of a steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation.\nIt is known  that humans interact with computers in ways similar to  interacting with other humans that may be described in terms of personality, politeness, flattery, and in-group favoritism. Therefore, the role of humor in human–computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.\nCraig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences.  Based on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into \"Arafat joined Quayle's Ant, while TARAR Jeopardized thurmond's vase,\" an example chosen by combining politicians names with verbs and common nouns.\n\nRelated research\nJohn Allen Paulos is known for his interest in mathematical foundations of humor. His book Mathematics and Humor: A Study of the Logic of Humor demonstrates structures common to humor and formal sciences (mathematics, linguistics) and develops a mathematical model of jokes based on catastrophe theory.\nConversational systems which have been designed to take part in Turing test competitions generally have the ability to learn humorous anecdotes and jokes. Because many people regard humor as something particular to humans, its appearance in conversation can be quite useful in convincing a human interrogator that a hidden entity, which could be a machine or a human, is in fact a human.\n\nSee also\nTheory of humor\nWorld's funniest joke#Other findings\n\nFurther reading\n\"Computational humor\", by Binsted, K.; Nijholt, A.; Stock, O.; Strapparava, C.; Ritchie, G.; Manurung, R.; Pain, H.; Waller, A.; Oapos;Mara, D.,  IEEE Intelligent Systems Volume 21, Issue 2, 2006, pp. 59 – 69 doi:10.1109/MIS.2006.22\nO. Stock, C. Strapparava & A. Nijholt (eds.) \"The April Fools' Day Workshop on Computational Humour.\" Proc. Twente Workshop on Language Technology 20 (TWLT20), ISSN 0929-0672, ITC-IRST, Trento, Italy, April 2002, 146 pp\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Computational_humor"
    },
    {
        "title": "Computational intelligence",
        "text": "In computer science, computational intelligence (CI) refers to concepts, paradigms, algorithms and implementations of systems that are designed to show \"intelligent\" behavior in complex and changing environments. These systems are aimed at mastering complex tasks in a wide variety of technical or commercial areas and offer solutions that recognize and interpret patterns, control processes, support decision-making or autonomously manoeuvre vehicles or robots in unknown environments, among other things. These concepts and paradigms are characterized by the ability to learn or adapt to new situations, to generalize, to abstract, to discover and associate. Nature-analog or nature-inspired methods play a key role, such as in neuroevolution for Computational Intelligence.\nCI approaches primarily address those complex real-world problems for which mathematical or traditional modeling is not appropriate for various reasons: the processes cannot be described exactly with complete knowledge, the processes are too complex for mathematical reasoning, they contain some uncertainties during the process, such as unforeseen changes in the environment or in the process itself, or the processes are simply stochastic in nature. Thus, CI techniques are properly aimed at processes that are ill-defined, complex, nonlinear, time-varying and/or stochastic.\nA recent definition of the IEEE Computational Intelligence Societey describes CI as the theory, design, application and development of biologically and linguistically motivated computational paradigms. Traditionally the three main pillars of CI have been Neural Networks, Fuzzy Systems and Evolutionary Computation. ... CI is an evolving field and at present in addition to the three main constituents, it encompasses computing paradigms like ambient intelligence, artificial life, cultural learning, artificial endocrine networks, social reasoning, and artificial hormone networks. ... Over the last few years there has been an explosion of research on Deep Learning, in particular deep convolutional neural networks. Nowadays, deep learning has become the core method for artificial intelligence. In fact, some of the most successful AI systems are based on CI. However, as CI is an emerging and developing field there is no final definition of CI, especially in terms of the list of concepts and paradigms that belong to it.\nThe general requirements for the development of an “intelligent system” are ultimately always the same, namely the simulation of intelligent thinking and action in a specific area of application. To do this, the knowledge about this area must be represented in a model so that it can be processed. The quality of the resulting system depends largely on how well the model was chosen in the development process. Sometimes data-driven methods are suitable for finding a good model and sometimes logic-based knowledge representations deliver better results. Hybrid models are usually used in real applications.\nAccording to actual textbooks, the following methods and paradigms, which largely complement each other, can be regarded as parts of CI:\n\nFuzzy systems\nNeural networks and, in particular, convolutional neural networks\nEvolutionary computation and, in particular, multi-objective evolutionary optimization\nSwarm intelligence\nBayesian networks\nArtificial immune systems\nLearning theory\nProbabilistic Methods\n\nRelationship between hard and soft computing and artificial and computational intelligence\nArtificial intelligence (AI) is used in the media, but also by some of the scientists involved, as a kind of umbrella term for the various techniques associated with it or with CI. Craenen and Eiben state that attempts to define or at least describe CI can usually be assigned to one or more of the following groups:\n\n\"Relative definition” comparing CI to AI\nConceptual treatment of key notions and their roles in CI\nListing of the (established) areas that belong to it\nThe relationship between CI and AI has been a frequently discussed topic during the development of CI. While the above list implies that they are synonyms, the vast majority of AI/CI researchers working on the subject consider them to be distinct fields, where either\nCI is an alternative to AI\nAI includes CI\nCI includes AI\nThe view of the first of the above three points goes back to Zadeh, the founder of the fuzzy set theory, who differentiated machine intelligence into hard and soft computing techniques, which are used in artificial intelligence on the one hand and computational intelligence on the other. In hard computing (HC) and AI, inaccuracy and uncertainty are undesirable characteristics of a system, while soft computing (SC) and thus CI focus on dealing with these characteristics. The adjacent figure illustrates these relationships and lists the most important CI techniques. Another frequently mentioned distinguishing feature is the representation of information in symbolic form in AI and in sub-symbolic form in CI techniques.\nHard computing is a conventional computing method based on the principles of certainty and accuracy and it is deterministic. It requires a precisely stated analytical model of the task to be processed and a prewritten program, i.e. a fixed set of instructions. The models used are based on Boolean logic (also called crisp logic), where e.g. an element can be either a member of a set or not and there is nothing in between. When applied to real-world tasks, systems based on HC result in specific control actions defined by a mathematical model or algorithm. If an unforeseen situation occurs that is not included in the model or algorithm used, the action will most likely fail.\nSoft computing, on the other hand, is based on the fact that the human mind is capable of storing information and processing it in a goal-oriented way, even if it is imprecise and lacks certainty. SC is based on the model of the human brain with probabilistic thinking, fuzzy logic and multi-valued logic. Soft computing can process a wealth of data and perform a large number of computations, which may not be exact, in parallel. For hard problems for which no satisfying exact solutions based on HC are available, SC methods can be applied successfully. SC methods are usually stochastic in nature i.e., they are a randomly defined processes that can be analyzed statistically but not with precision. Up to now, the results of some CI methods, such as deep learning, cannot be verified and it is also not clear what they are based on. This problem represents an important scientific issue for the future.\nAI and CI are catchy terms, but they are also so similar that they can be confused. The meaning of both terms has developed and changed over a long period of time, with AI being used first. Bezdek describes this impressively and concludes that such buzzwords are frequently used and hyped by the scientific community, science management and (science) journalism. Not least because AI and biological intelligence are emotionally charged terms and it is still difficult to find a generally accepted definition for the basic term intelligence.\n\nHistory\nIn 1950, Alan Turing, one of the founding fathers of computer science, developed a test for computer intelligence known as the Turing test. In this test, a person can ask questions via a keyboard and a monitor without knowing whether his counterpart is a human or a computer. A computer is considered intelligent if the interrogator cannot distinguish the computer from a human. This illustrates the discussion about intelligent computers at the beginning of the computer age.\nThe term Computational Intelligence was first used as the title of the journal of the same name in 1985 and later by the IEEE Neural Networks Council (NNC), which was founded 1989 by a group of researchers interested in the development of biological and artificial neural networks. On November 21, 2001, the NNC  became the IEEE Neural Networks Society, to become the IEEE Computational Intelligence Society two years later by including new areas of interest such as fuzzy systems and evolutionary computation.\nThe NNC helped organize the first IEEE World Congress on Computational Intelligence in Orlando, Florida in 1994. On this conference the first clear definition of Computational Intelligence was introduced by Bezdek: A system is computationally intelligent when it: deals with only numerical (low-level) data, has pattern-recognition components, does not use knowledge in the AI sense; and additionally when it (begins to) exhibit (1) computational adaptivity; (2) computational fault tolerance; (3) speed approaching human-like turnaround and (4) error rates that approximate human performance.\nToday, with machine learning and deep learning in particular utilizing a breadth of supervised, unsupervised, and reinforcement learning approaches, the CI landscape has been greatly enhanced, with novell intelligent approaches.\n\nThe main algorithmic approaches of CI and their applications\nThe main applications of Computational Intelligence include computer science, engineering, data analysis and bio-medicine.\n\nFuzzy logic\nUnlike conventional Boolean logic, fuzzy logic is based on fuzzy sets. In both models, a property of an object is defined as belonging to a set; in fuzzy logic, however, the membership is not sharply defined by a yes/no distinction, but is graded gradually. This is done using membership functions that assign a real number between 0 and 1 to each element as the degree of membership. The new set operations introduced in this way define the operations of an associated logic calculus that allows the modeling of inference processes, i.e. logical reasoning. Therefore, fuzzy logic is well suited for engineering decisions without clear certainties and uncertainties or with imprecise data - as with natural language-processing technologies but it doesn't have learning abilities.\nThis technique tends to apply to a wide range of domains such as control engineering, image processing, fuzzy data clustering and decision making. Fuzzy logic-based control systems can be found, for example, in the field of household appliances in washing machines, dish washers, microwave ovens, etc. or in the area of motor vehicles in gear transmission and braking systems. This principle can also be encountered when using a video camera, as it helps to stabilize the image when the camera is held unsteadily. Other areas such as medical diagnostics, satellite controllers and business strategy selection are just a few more examples of today's application of fuzzy logic.\n\nNeural networks\nAn important field of CI is the development of artificial neural networks (ANN) based on the biological ones, which can be defined by three main components: the cell-body which processes the information, the axon, which is a device enabling the signal conducting, and the synapse, which controls signals. Therefore, ANNs are very well suited for distributed information processing systems, enabling the process and the learning from experiential data. ANNs aim to mimic cognitive processes of the human brain. The main advantages of this technology therefore include fault tolerance, pattern recognition even with noisy images and the ability to learn.\nConcerning its applications, neural networks can be classified into five groups: data analysis and classification, associative memory, data clustering or compression, generation of patterns, and control systems. The numerous applications include, for example, the analysis and classification of medical data, including the creation of diagnoses, speech recognition, data mining, image processing, forecasting, robot control, credit approval, pattern recognition, face and fraud detection and dealing with nonlinearities of a system in order to control it. ANNs have the latter area of application and data clustering in common with fuzzy logic. Generative systems based on deep learning and convolutional neural networks, such as chatGPT or DeepL, are a relatively new field of application.\n\nEvolutionary computation\nEvolutionary computation can be seen as a family of methods and algorithms for global optimization, which are usually based on a population of candidate solutions. They are inspired by biological evolution and are often summarized as evolutionary algorithms. These include the genetic algorithms, evolution strategy, genetic programming and many others. They are considered as problem solvers for tasks not solvable by traditional mathematical methods and are frequently used for optimization including multi-objective optimization. Since they work with a population of candidate solutions that are processed in parallel during an iteration, they can easily be distributed to different computer nodes of a cluster. As often more than one offspring is generated per pairing, the evaluations of these offspring, which are usually the most time-consuming part of the optimization process, can also be performed in parallel.\nIn the course of optimization, the population learns about the structure of the search space and stores this information in the chromosomes of the solution candidates. After a run, this knowledge can be reused for similar tasks by adapting some of the “old” chromosomes and using them to seed a new population.\n\nSwarm intelligence\nSwarm intelligence is based on the collective behavior of decentralized, self-organizing systems, typically consisting of a population of simple agents that interact locally with each other and with their environment. Despite the absence of a centralized control structure that dictates how the individual agents should behave, local interactions between such agents often lead to the emergence of global behavior. Among the recognized representatives of algorithms based on swarm intelligence are particle swarm optimization and ant colony optimization. Both are metaheuristic optimization algorithms that can be used to (approximately) solve difficult numerical or complex combinatorial optimization tasks. Since both methods, like the evolutionary algorithms, are based on a population and also on local interaction, they can be easily parallelized and show comparable learning properties.\n\nBayesian networks\nIn complex application domains, Bayesian networks provide a means to efficiently store and evaluate uncertain knowledge. A Bayesian network is a probabilistic graphical model that represents a set of random variables and their conditional dependencies by a directed acyclic graph. The probabilistic representation makes it easy to draw conclusions based on new information. In addition, Bayesian networks are well suited for learning from data. Their wide range of applications includes medical diagnostics, risk management, information retrieval, and text analysis, e.g. for spam filters. Their wide range of applications includes medical diagnostics, risk management, information retrieval, text analysis, e.g. for spam filters, credit rating of companies, and the operation of complex industrial processes.\n\nArtificial immune systems\nArtificial immune systems are another group of population-based metaheuristic learning algorithms designed to solve clustering and optimization problems. These algorithms are inspired by the principles of theoretical immunology and the processes of the vertebrate immune system, and use the learning and memory properties of the immune system to solve a problem. Operators similar to those known from evolutionary algorithms are used to clone and mutate artificial lymphocytes. Artificial immune systems offer interesting capabilities such as adaptability, self-learning, and robustness that can be used for various tasks in data processing, manufacturing systems, system modeling and control, fault detection, or cybersecurity.\n\nLearning theory\nStill looking for a way of \"reasoning\" close to the humans' one, learning theory is one of the main approaches of CI. In psychology, learning is the process of bringing together cognitive, emotional and environmental effects and experiences to acquire, enhance or change knowledge, skills, values and world views. Learning theories then helps understanding how these effects and experiences are processed, and then helps making predictions based on previous experience.\n\nProbabilistic methods\nBeing one of the main elements of fuzzy logic, probabilistic methods firstly introduced by Paul Erdos and Joel Spencer in 1974, aim to evaluate the outcomes of a Computation Intelligent system, mostly defined by randomness. Therefore, probabilistic methods bring out the possible solutions to a problem, based on prior knowledge.\n\nImpact on university education\nAccording to bibliometrics studies, computational intelligence plays a key role in research. All the major academic publishers are accepting manuscripts in which a combination of Fuzzy logic, neural networks and evolutionary computation is discussed. On the other hand, Computational intelligence isn't available in the university curriculum. The amount of technical universities in which students can attend a course is limited. Only British Columbia, Technical University of Dortmund (involved in the European fuzzy boom) and Georgia Southern University are offering courses from this domain.\nThe reason why major university are ignoring the topic is because they don't have the resources. The existing computer science courses are so complex, that at the end of the semester there is no room for fuzzy logic. Sometimes it is taught as a subproject in existing introduction courses, but in most cases the universities are preferring courses about classical AI concepts based on Boolean logic, turing machines and toy problems like blocks world.\nSince a while with the upraising of STEM education, the situation has changed a bit. There are some efforts available in which multidisciplinary approaches are preferred which allows the student to understand complex adaptive systems. These objectives are discussed only on a theoretical basis. The curriculum of real universities wasn't adapted yet.\n\nPublications\nIEEE Transactions on Neural Networks and Learning Systems\nIEEE Transactions on Fuzzy Systems\nIEEE Transactions on Evolutionary Computation\nIEEE Transactions on Emerging Topics in Computational Intelligence\nIEEE Transactions on Autonomous Mental Development\nIEEE/ACM Transactions on Computational Biology and Bioinformatics\nIEEE Transactions on Computational Intelligence and AI in Games\nApplied Computational Intelligence and Soft Computing\n\nSee also\nNotes\nComputational Intelligence: An Introduction by Andries Engelbrecht. Wiley & Sons. ISBN 0-470-84870-7\nComputational Intelligence: A Logical Approach by David Poole, Alan Mackworth, Randy Goebel. Oxford University Press. ISBN 0-19-510270-3\nComputational Intelligence: A Methodological Introduction by Kruse, Borgelt, Klawonn, Moewes, Steinbrecher, Held, 2013, Springer, ISBN 9781447150121\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Computational_intelligence"
    },
    {
        "title": "Computer audition",
        "text": "Computer audition (CA) or machine listening is the general field of study of algorithms and systems for audio interpretation by machines. Since the notion of what it means for a machine to \"hear\" is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in Technology Review, talks about these systems — \"software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.\"\nInspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation.\n\nApplications\nLike computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings.\nApplications of computer audition are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on.\n\nRelated disciplines\nComputer Audition overlaps with the following disciplines:\n\nMusic information retrieval: methods for search and analysis of similarity between music signals.\nAuditory scene analysis: understanding and description of audio sources and events.\nComputational musicology and mathematical music theory: use of algorithms that employ musical knowledge for analysis of music data.\nComputer music: use of computers in creative musical applications.\nMachine musicianship: audition driven interactive music systems.\n\nAreas of study\nSince audio signals are interpreted by the human ear–brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.\nThe study of CA could be roughly divided into the following sub-problems:\n\nRepresentation: signal and symbolic. This aspect deals with time-frequency representations, both in terms of notes and spectral models, including pattern playback and audio texture.\nFeature extraction: sound descriptors, segmentation, onset, pitch and envelope detection, chroma, and auditory representations.\nMusical knowledge structures: analysis of tonality, rhythm, and harmonies.\nSound similarity: methods for comparison between sounds, sound identification, novelty detection, segmentation, and clustering.\nSequence modeling:  matching and alignment between signals and note sequences.\nSource separation: methods of grouping of simultaneous sounds, such as multiple pitch detection and time-frequency clustering methods.\nAuditory cognition: modeling of emotions, anticipation and familiarity, auditory surprise, and analysis of musical structure.\nMulti-modal analysis: finding correspondences between textual, visual, and audio signals.\n\nRepresentation issues\nComputer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files.\nSince audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio-visual recordings.\n\nFeatures\nDescription of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity (bandwidth) in frequency or octave invariance (chroma).\nSince parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation.\n\nMusical knowledge\nFinding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on.\n\nSound similarity and sequence modeling\nComparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to \"correct\" for different temporal scales of acoustic events. Finding repetitions and similar sub-sequences of sonic events is important for tasks such as texture synthesis and machine improvisation.\n\nSource separation\nSince one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection. Some methods, before explicit recognition, rely on revealing structures in data without knowing the structures (like recognizing objects in abstract pictures without attributing them meaningful labels) by finding the least complex data representations, for instance describing audio scenes as generated by a few tone patterns and their trajectories (polyphonic voices) and acoustical contours drawn by a tone (chords).\n\nAuditory cognition\nListening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.\n\nMulti-modal analysis\nAmong the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho-physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents.\n\nSee also\n3D sound localization\nAudio signal processing\nList of emerging technologies\nMedical intelligence and language engineering lab\nMusic and artificial intelligence\nSound recognition\n\nExternal links\nUCSD Computer Audition Lab \nGeorge Tzanetakis' Computer Audition Resources\nShlomo Dubnov's Tutorial on Computer Audition\nDepartment of Electrical Engineering, IIT (Bangalore)\nSound and Music Computing, Aalborg University Copenhagen, Denmark\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Computer_audition"
    },
    {
        "title": "Concurrent MetateM",
        "text": "Concurrent MetateM is a multi-agent language in which each agent is programmed using a set of (augmented) temporal logic specifications of the behaviour it should exhibit. These specifications are executed directly to generate the behaviour of the agent. As a result, there is no risk of invalidating the logic as with systems where logical specification must first be translated to a lower-level implementation.\nThe root of the MetateM concept is Gabbay's separation theorem; any arbitrary temporal logic formula can be rewritten in a logically equivalent past → future form. Execution proceeds by a process of continually matching rules against a history, and firing those rules when antecedents are satisfied. Any instantiated future-time consequents become commitments which must subsequently be satisfied, iteratively generating a model for the formula made up of the program rules.\n\nTemporal Connectives\nThe Temporal Connectives of Concurrent MetateM can divided into two categories, as follows:\n\nStrict past time connectives: '●' (weak last), '◎' (strong last), '◆' (was), '■' (heretofore), 'S' (since), and 'Z' (zince, or  weak since).\nPresent and future time connectives: '◯' (next), '◇' (sometime), '□' (always), 'U' (until), and 'W' (unless).\nThe connectives {◎,●,◆,■,◯,◇,□} are unary; the remainder are binary.\n\nStrict past time connectives\nWeak last\n●ρ is satisfied now if ρ was true in the previous time. If ●ρ is interpreted at the beginning of time, it is satisfied despite there being no actual previous time.  Hence \"weak\" last.\n\nStrong last\n◎ρ is satisfied now if ρ was true in the previous time.  If ◎ρ is interpreted at the beginning of time, it is not satisfied because there is no actual previous time.  Hence \"strong\" last.\n\nWas\n◆ρ is satisfied now if ρ was true in any previous moment in time.\n\nHeretofore\n■ρ is satisfied now if ρ was true in every previous moment in time.\n\nSince\nρSψ is satisfied now if ψ is true at any previous moment and ρ is true at every moment after that moment.\n\nZince, or weak since\nρZψ is satisfied now if (ψ is true at any previous moment and ρ is true at every moment after that moment) OR ψ has not happened in the past.\n\nPresent and future time connectives\nNext\n◯ρ is satisfied now if ρ is true in the next moment in time.\n\nSometime\n◇ρ is satisfied now if ρ is true now or in any future moment in time.\n\nAlways\n□ρ is satisfied now if ρ is true now and in every future moment in time.\n\nUntil\nρUψ is satisfied now if ψ is true at any future moment and ρ is true at every moment prior.\n\nUnless\nρWψ is satisfied now if (ψ is true at any future moment and ρ is true at every moment prior) OR ψ does not happen in the future.\n\nReferences\nExternal links\nA Java implementation of a MetateM interpreter can be downloaded from here",
        "url": "https://en.wikipedia.org/wiki/Concurrent_MetateM"
    },
    {
        "title": "Connectionist expert system",
        "text": "Connectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e.g., fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. Apart from that, rough set theory may be used for encoding knowledge in the weights better and also genetic algorithms may be used to optimize the search solutions better. Symbolic reasoning methods may also be incorporated (see hybrid intelligent system). (Also see  expert system,  neural network, clinical decision support system.)\n\nReferences\nSun, Ron (1994). Integrating rules and connectionism for robust commonsense reasoning. Hoboken, N.J: Wiley & Sons. ISBN 0-471-59324-9.\n\nExternal links\nGallant, Stephen I. (February 1988). \"Connectionist expert systems\". Comm. ACM. 31 (2): 152–69. doi:10.1145/42372.42377. S2CID 12971665.\nresource page: http://www.cogsci.rpi.edu/~rsun/reason.html\nLeão Bde F, Reátegui EB (1993). \"HYCONES: a hybrid connectionist expert system\". Proc Annu Symp Comput Appl Med Care: 461–5. PMC 2248551. PMID 8130516.\nBarton JG, Lees A (October 1995). \"Development of a connectionist expert system to identify foot problems based on under-foot pressure patterns\". Clin Biomech (Bristol, Avon). 10 (7): 385–391. doi:10.1016/0268-0033(95)00015-D. PMID 11415584.\nBrasil LM, de Azevedo FM, Barreto JM (September 2001). \"Hybrid expert system for decision supporting in the medical area: complexity and cognitive computing\". Int J Med Inform. 63 (1–2): 19–30. doi:10.1016/S1386-5056(01)00168-X. PMID 11518662.\nWei JH (2003). \"[Application prospect of human-artificial intelligence system in future manned space flight]\". Space Med Med Eng (Beijing) (in Chinese). 16 (Suppl): 482–5. PMID 14989301.",
        "url": "https://en.wikipedia.org/wiki/Connectionist_expert_system"
    },
    {
        "title": "DABUS",
        "text": "DABUS (Device for the Autonomous Bootstrapping of Unified Sentience) is an artificial intelligence (AI) system created by Stephen Thaler. It reportedly conceived of two novel products — a food container constructed using fractal geometry, which enables rapid reheating, and a flashing beacon for attracting attention in an emergency. The filing of patent applications designating DABUS as inventor has led to decisions by patent offices and courts on whether a patent can be granted for an invention reportedly made by an AI system.\nDABUS itself is a patented AI paradigm capable of accommodating trillions of computational neurons within extensive artificial neural systems that emulate the limbo-thalamo-cortical loop within the mammalian brain. Such systems utilize arrays of trainable neural modules, each containing interrelated memories representative of some conceptual space. Through simple learning rules, these modules bind together to represent both complex ideas (e.g., juxtapositional inventions) and their consequences as chaining topologies. An electro-optical attention window scans the entire array of neural modules in search of so-called “hot buttons,” those neural modules containing impactful memories. Detection of such hot buttons within consequence chains triggers the release or retraction of synaptic disturbances into the system, selectively reinforcing the most salient chain-based notions.\n\nHistory in different jurisdictions\nAustralia\nOn 17 September 2019, Thaler filed an application to patent a \"Food container and devices and methods for attracting enhanced attention,\" naming DABUS as the inventor. On 21 September 2020, IP Australia found that section 15(1) of the Patents Act 1990 (Cth) is inconsistent with an artificial intelligence machine being treated as an inventor, and Thaler's application had lapsed. Thaler sought judicial review, and on 30 July 2021, the Federal Court set aside IP Australia's decision and ordered IP Australia to reconsider the application. On 13 April 2022, the Full Court of the Federal Court set aside that decision, holding that only a natural person can be an inventor for the purposes of the Patents Act 1990 (Cth) and the Patents Regulations 1991 (Cth), and that such an inventor must be identified for any person to be entitled to a grant of a patent. On 11 November 2022, Thaler was refused special leave to appeal to the High Court.\n\nEuropean Patent Office\nOn 17 October 2018 and 7 November 2018, Thaler filed two European patent\napplications with the European Patent Office. The first claimed invention was a \"Food Container\" and the second was \"Devices and Methods for Attracting Enhanced Attention.\"\nOn 27 January 2020, the EPO rejected the applications on the grounds that the application listed an AI system named DABUS, and not a human, as the inventor, based on Article 81 and Rule 19(1) of the European Patent Convention (EPC).\nOn 21 December 2021, the Board of Appeal of the EPO dismissed Thaler's appeal from the EPO's primary decision. The Board of Appeal confirmed that \"under the EPC the designated inventor has to be a person with legal capacity. This is not merely an assumption on which the EPC was drafted. It is the ordinary meaning of the term inventor.\"\n\nUnited Kingdom\nSimilar applications were filed by Thaler to the United Kingdom Intellectual Property Office on 17 October and 7 November 2018. The Office asked Thaler to file statements of inventorship and of right of grant to a patent (Patent Form 7) in respect of each invention within 16 months of the filing date. Thaler filed those forms naming DABUS as the inventor and explaining in some detail why he believed that machines should be regarded as inventors in the circumstances.\nHis application was rejected on the grounds that:\n(1) naming a machine as inventor did not meet the requirements of the Patents Act 1977; and \n(2) the IPO was not satisfied as to the manner in which Thaler had acquired rights that would otherwise vest in the inventor.\nThaler was not satisfied with the decision and asked for a hearing before an official known as the \"hearing officer\". By a decision dated 4 December 2019 the hearing officer rejected Thaler's appeal.\nThaler appealed against the hearing officer's decision to the Patents Court (a specialist court within the Chancery Division of the High Court of England and Wales that determines patent disputes).  On 21 September 2020, Mr Justice Marcus Smith upheld the decision of the hearing officer. On 21 September 2021, Thaler's further appeal to the Court of Appeal was dismissed by Arnold LJ and Laing LJ (Birss LJ dissenting).\nOn the 20th December 2023, the UK Supreme Court dismissed a further appeal by Thaler. In its judgment, the court held that an \"inventor\" under the Patents Act 1977 must be a natural person.\n\nUnited States\nThe patent applications on the inventions were refused by the USPTO, which held that only natural persons can be named as inventors in a patent application. Thaler first fought this result by filing a complaint under Administrative Procedure Act (APA) alleging that the decision was \"arbitrary, capricious, an abuse of discretion and not in accordance with the law; unsupported by substantial evidence, and in excess of Defendants’ statutory authority.\" A month later on August 19, 2019, Thaler filed a petition with the USPTO as allowed in 37 C.F.R. § 1.181 stating that DABUS should be the inventor. The judge and Thaler agreed in this case that Thaler himself is unable to receive the patent on behalf of DABUS. In their August 5, 2022, Thaler decision, the US Court of Appeals for the Federal Circuit affirmed that only a natural person could be an inventor, which means that the AI that invents any other type of invention is not addressed by the “who” mentioned in the legislation.\n\nNew Zealand\nOn January 31, 2022, the Intellectual Property Office of New Zealand (IPONZ) decided that a patent application (776029) filed by Stephen Thaler was void, on the basis that no inventor was identified on the patent application. IPONZ determined that DABUS could not be “an actual devisor of the invention” as required by the Patents Act 2013, and that this must be a natural person as held by the previous patent offices above. The High Court of New Zealand confirmed the decision in 2023.\n\nSouth Africa\nOn 24 June 2021, the South African Companies and Intellectual Property Commission (CIPC) accepted Dr Thaler's Patent Cooperation Treaty, for a patent in respect of inventions generated by DABUS. In July 2021, the CIPC released a notice of issuance for the patent. It is the first patent granted for an AI invention.\n\nSwitzerland\nOn June 26, 2025, the Swiss Federal Administrative Court ruled that artificial intelligence systems such as DABUS cannot be listed as inventors in patent applications. The court upheld the existing practice of the Swiss Federal Institute of Intellectual Property (IPI), which requires that only natural persons can be recognized as inventors under Swiss patent law.\nThe case concerned a patent application, which sought to designate DABUS as the sole inventor of a food container designed with a fractal geometry to enhance heat distribution. The IPI had rejected the application, arguing that both the absence of a human inventor and the attribution of inventorship to an AI system were inadmissible. While the court dismissed Thaler's main request, it accepted a subsidiary request: if a human applicant recognizes and files a patent based on an AI-generated invention, that person may be considered the inventor. As a result, the application may proceed with Thaler listed as the inventor. The decision (B-2532/2024) can still be appealed to the Swiss Federal Supreme Court.\n\nReferences\nExternal links\nThe Artificial Inventor Project\nThe latest news on the DABUS patent case (ipstars.com)",
        "url": "https://en.wikipedia.org/wiki/DABUS"
    },
    {
        "title": "Data annotation",
        "text": "Data annotation is the process of labeling or tagging relevant metadata within a dataset to enable machines to interpret the data accurately. The dataset can take various forms, including images, audio files, video footage, or text.\n\nApplications\nData is a fundamental component in the development of artificial intelligence (AI). Training AI models, particularly in computer vision and natural language processing, requires large volumes of annotated data. Proper annotation ensures that machine learning algorithms can recognize patterns and make accurate predictions. Common types of data annotation include classification, bounding boxes, semantic segmentation, and keypoint annotation.\nData annotation is used in AI-driven fields, including healthcare, autonomous vehicles, retail, security, and entertainment. By accurately labeling data, machine learning models can perform complex tasks such as object detection, sentiment analysis, and speech recognition with greater precision.\n\nData annotation in computer vision\nImage classification\nImage classification, also known as image categorization, involves assigning predefined labels to images. Machine learning algorithms trained on classified images can later recognize objects and differentiate between categories. For instance, an AI model trained to recognize furniture styles can distinguish between Georgian and Rococo armchairs.\n\nSemantic segmentation\nSemantic segmentation assigns each pixel in an image to a specific class, such as trees, vehicles, humans, or buildings. This type of annotation enables machine learning models to differentiate objects by grouping similar pixels, allowing for a detailed understanding of an image.\n\nBounding boxes\nBounding box annotation involves drawing rectangular boxes around objects in an image. This technique is commonly used in autonomous driving, security surveillance, and retail analytics to detect and classify objects such as pedestrians, vehicles, and products on store shelves.\n\n3D cuboids\n3D cuboid annotation enhances traditional bounding boxes by adding depth, enabling models to predict an object's spatial orientation, movement, and size. This method is particularly useful for autonomous vehicles and robotics, where understanding object dimensions and depth is critical.\n\nPolygonal annotation\nFor objects with irregular shapes, such as curved or multi-sided items, polygonal annotation provides more precise labeling than bounding boxes. This technique is often used in applications that require detailed object recognition, such as medical imaging or aerial mapping.\n\nKeypoint annotation\nKeypoint annotation marks specific points on an object, such as facial landmarks or body joints, to enable tracking and motion analysis. This method is widely used in facial recognition, emotion detection, sports analytics, and augmented reality applications.\n\nText annotation for natural language processing (NLP)\nText annotation involves assigning labels to a text document, or specific elements within it, to identify the characteristics of sentences or phrases. It is an essential step in preparing datasets to train Natural Language Processing (NLP) models, empowering them to effectively recognize human language, emotions, or intent behind words.\n\nTypes of text annotation\n1. Entity annotation: Refers to the process of assigning predefined labels to entities in text based on their semantic meaning, helping NLP models identify and understand them. Common subtypes include:\n\nNamed entity recognition (NER): Labels key information in the text, such as names, geographic locations, frequently mentioned objects, or characters.\nPart-of-speech tagging: Identifies grammatical units (nouns, verbs, adjectives, etc.).\nEntity linking (named entity linking – NEL): Connects identified entities to a knowledge base (e.g., Wikipedia) to determine the exact reference (e.g., “Summer” as a person vs. the season).\n2. Text classification: Assigns a single label to entire chunks of text or lines, as in document classification, product categorization, or sentiment annotation.\n3. Sentiment annotation: Sentiment annotation is the process of determining the emotion or opinion in text (positive, negative, or neutral), even in nuanced cases, like sarcasm, enabling computers to detect subtle sentiment cues.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Data_annotation"
    },
    {
        "title": "Deep Learning Anti-Aliasing",
        "text": "Deep Learning Anti-Aliasing (DLAA) is a form of spatial anti-aliasing developed by Nvidia. DLAA depends on and requires Tensor Cores available in Nvidia RTX cards.\nDLAA is similar to Deep Learning Super Sampling (DLSS) in its anti-aliasing method, with one important differentiation being that the goal of DLSS is to increase performance at the cost of image quality, whereas the main priority of DLAA is improving image quality at the cost of performance (irrelevant of resolution upscaling or downscaling). DLAA is similar to temporal anti-aliasing (TAA) in that they are both spatial anti-aliasing solutions relying on past frame data. Compared to TAA, DLAA is substantially better when it comes to shimmering, flickering, and handling small meshes like wires.\n\nTechnical overview\nDLAA collects game rendering data including raw low-resolution input, motion vectors, depth buffers, and exposure information. This information feeds into a convolutional neural network that processes the image to reduce aliasing while preserving fine detail.\nThe neural network architecture employs an auto-encoder design trained on high-quality reference images. The training dataset includes diverse scenarios focusing on challenging cases like sub-pixel details, high-contrast edges, and transparent surfaces. The network then processes frames in real-time.\nUnlike traditional anti-aliasing solutions that rely on manually written heuristics, such as TAA, DLAA uses its neural network to preserve fine details while eliminating unwanted visual artifacts.\n\nHistory\nThe first game that added support for DLAA was The Elder Scrolls Online, which implemented the feature in 2021. By June 2022, DLAA was only available in six games. This number rose to 17 by February 2023. In June 2023, TechPowerUp reported that \"DLAA is seeing sluggish adoption among game developers\", and that Nvidia was working on adding DLAA to the quality presets of DLSS to boost adoption. By December 2023, DLAA was supported in 41 games. In early 2025, an update for the Nvidia App added a driver-based DLSS override feature that enables users to activate DLAA even in games that do not support it natively.\n\nDifferences between TAA and DLAA\nTAA is used in many modern video games and game engines; however, all previous implementations have used some form of manually written heuristics to prevent temporal artifacts such as ghosting and flickering. One example of this is neighborhood clamping which forcefully prevents samples collected in previous frames from deviating too much compared to nearby pixels in newer frames. This helps to identify and fix many temporal artifacts, but deliberately removing fine details in this way is analogous to applying a blur filter, and thus the final image can appear blurry when using this method.\nDLAA uses an auto-encoder convolutional neural network trained to identify and fix temporal artifacts, instead of manually programmed heuristics as mentioned above. Because of this, DLAA can generally resolve detail better than other TAA and TAAU implementations, while also removing most temporal artifacts.\n\nDifferences between DLSS and DLAA\nWhile DLSS handles upscaling with a focus on performance, DLAA handles anti-aliasing with a focus on visual quality. DLAA runs at the given screen resolution with no upscaling or downscaling functionality provided by DLAA.  \nDLSS and DLAA share the same AI-driven anti-aliasing method. As such, DLAA functions like DLSS without the upscaling part. Both are made by Nvidia and require Tensor Cores. However, DLSS and DLAA cannot be enabled at the same time, only one can be selected depending on whether performance or image quality is prioritized.\n\nReception\nTechPowerUp found that \"[c]ompared to TAA and DLSS, DLAA is clearly producing the best image quality, especially at lower resolutions\", arguing that, while \"DLSS was already doing a better job than TAA at reconstructing small objects\", \"DLAA does an even better job\".\nIn a Cyberpunk 2077 performance test, IGN found that \"DLAA provided somewhat similar results [FPS wise] to the normal raster mode in most cases but got significant performance boost with the help of frame generation\", a feature not available when using native resolution.\nRock Paper Shotgun found that, while DLAA is \"not a completely perfect form of anti-aliasing, as the occasional jaggies are present\", it \"looks a lot sharper overall [than TAA], and especially in motion.\"\n\nReferences\nExternal links\nOfficial website of DLSS, which also includes information about DLAA",
        "url": "https://en.wikipedia.org/wiki/Deep_Learning_Anti-Aliasing"
    },
    {
        "title": "Deep Learning Indaba",
        "text": "The Deep Learning Indaba is an annual conference and educational event that aims to strengthen machine learning and artificial intelligence (AI) capacity across Africa. Launched in 2017, it brings together students, researchers, industry practitioners, and policymakers from across the African continent.\n\nHistory\nThe Deep Learning Indaba began in 2017 at the University of the Witwatersrand with over 300 participants from 23 African countries, offering tutorials in advanced AI topics and featuring notable speakers like Nando de Freitas. In 2018, it expanded to 650 delegates at Stellenbosch University, introducing parallel sessions to encourage collaboration. The 2019 edition in Nairobi, Kenya, reflected further growth, with increasing sponsorship and support from major tech companies like Google and Microsoft.\n\n\n== Reference ==",
        "url": "https://en.wikipedia.org/wiki/Deep_Learning_Indaba"
    },
    {
        "title": "Deep Learning Super Sampling",
        "text": "Deep Learning Super Sampling (DLSS) is a suite of real-time deep learning image enhancement and upscaling technologies developed by Nvidia that are available in a number of video games. The goal of these technologies is to allow the majority of the graphics pipeline to run at a lower resolution for increased performance, and then infer a higher resolution image from this that approximates the same level of detail as if the image had been rendered at this higher resolution. This allows for higher graphical settings and/or frame rates for a given output resolution, depending on user preference.\nAll generations of DLSS are available on all RTX-branded cards from Nvidia in supported titles. However, the Frame Generation feature is only supported on 40 series GPUs or newer and Multi Frame Generation is only available on 50 series GPUs.\n\nHistory\nNvidia advertised DLSS as a key feature of the GeForce 20 series cards when they launched in September 2018. At that time, the results were limited to a few video games, namely Battlefield V, or Metro Exodus, because the algorithm had to be trained specifically on each game on which it was applied and the results were usually not as good as simple resolution upscaling. In 2019, the video game Control shipped with real-time ray tracing and an improved version of DLSS, which did not use the Tensor Cores.\nIn April 2020, Nvidia advertised and shipped an improved version of DLSS named DLSS 2.0 with driver version 445.75. DLSS 2.0 was available for a few existing games including Control and Wolfenstein: Youngblood, and would later be added to many newly released games and game engines such as Unreal Engine and Unity. This time Nvidia said that it used the Tensor Cores again, and that the AI did not need to be trained specifically on each game. Despite sharing the DLSS branding, the two iterations of DLSS differ significantly and are not backwards-compatible.\nIn January 2025, Nvidia stated that there are over 540 games and apps supporting DLSS, and that over 80% of Nvidia RTX users activate DLSS.\nIn March 2025, there were more than 100 games that support DLSS 4, according to Nvidia. By May 2025, over 125 games supported DLSS 4.\nThe first video game console to use DLSS, the Nintendo Switch 2, was released on June 5, 2025.\n\nRelease history\nQuality presets\nWhen using DLSS, depending on the game, users have access to various quality presets in addition to the option to set the internally rendered, upscaled resolution manually:\n\nImplementation\nDLSS 1.0\nThe first iteration of DLSS is a predominantly spatial image upscaler with two stages, both relying on convolutional auto-encoder neural networks. The first step is an image enhancement network which uses the current frame and motion vectors to perform edge enhancement, and spatial anti-aliasing. The second stage is an image upscaling step which uses the single raw, low-resolution frame to upscale the image to the desired output resolution. Using just a single frame for upscaling means the neural network itself must generate a large amount of new information to produce the high resolution output, this can result in slight hallucinations such as leaves that differ in style to the source content.\nThe neural networks are trained on a per-game basis by generating a \"perfect frame\" using traditional supersampling to 64 samples per pixel, as well as the motion vectors for each frame. The data collected must be as comprehensive as possible, including as many levels, times of day, graphical settings, resolutions, etc. as possible. This data is also augmented using common augmentations such as rotations, colour changes, and random noise to help generalize the test data. Training is performed on Nvidia's Saturn V supercomputer.\nThis first iteration received a mixed response, with many criticizing the often soft appearance and artifacts in certain situations; likely a side effect of the limited data from only using a single frame input to the neural networks which could not be trained to perform optimally in all scenarios and edge-cases. Nvidia also demonstrated the ability for the auto-encoder networks to learn the ability to recreate depth-of-field and motion blur, although this functionality has never been included in a publicly released product.\n\nDLSS 2.0\nDLSS 2.0 is a temporal anti-aliasing upsampling (TAAU) implementation, using data from previous frames extensively through sub-pixel jittering to resolve fine detail and reduce aliasing. The data DLSS 2.0 collects includes: the raw low-resolution input, motion vectors, depth buffers, and exposure / brightness information. It can also be used as a simpler TAA implementation where the image is rendered at 100% resolution, rather than being upsampled by DLSS, Nvidia brands this as DLAA (Deep Learning Anti-Aliasing).\nTAA(U) is used in many modern video games and game engines; however, all previous implementations have used some form of manually written heuristics to prevent temporal artifacts such as ghosting and flickering. One example of this is neighborhood clamping which forcefully prevents samples collected in previous frames from deviating too much compared to nearby pixels in newer frames. This helps to identify and fix many temporal artifacts, but deliberately removing fine details in this way is analogous to applying a blur filter, and thus the final image can appear blurry when using this method.\nDLSS 2.0 uses a convolutional auto-encoder neural network trained to identify and fix temporal artifacts, instead of manually programmed heuristics as mentioned above. Because of this, DLSS 2.0 can generally resolve detail better than other TAA and TAAU implementations, while also removing most temporal artifacts. This is why DLSS 2.0 can sometimes produce a sharper image than rendering at higher, or even native resolutions using traditional TAA. However, no temporal solution is perfect, and artifacts (ghosting in particular) are still visible in some scenarios when using DLSS 2.0.\nBecause temporal artifacts occur in most art styles and environments in broadly the same way, the neural network that powers DLSS 2.0 does not need to be retrained when being used in different games. Despite this, Nvidia does frequently ship new minor revisions of DLSS 2.0 with new titles, so this could suggest some minor training optimizations may be performed as games are released, although Nvidia does not provide changelogs for these minor revisions to confirm this. The main advancements compared to DLSS 1.0 include: Significantly improved detail retention, a generalized neural network that does not need to be re-trained per-game, and ~2x less overhead (~1–2 ms vs ~2–4 ms).\nIt should also be noted that forms of TAAU such as DLSS 2.0 are not upscalers in the same sense as techniques such as ESRGAN or DLSS 1.0, which attempt to create new information from a low-resolution source; instead, TAAU works to recover data from previous frames, rather than creating new data. In practice, this means low resolution textures in games will still appear low-resolution when using current TAAU techniques. This is why Nvidia recommends game developers use higher resolution textures than they would normally for a given rendering resolution by applying a mip-map bias when DLSS 2.0 is enabled.\n\nDLSS 3.0\nAugments DLSS 2.0 by making use of motion interpolation. The DLSS Frame Generation algorithm takes two rendered frames from the rendering pipeline and generates a new frame that smoothly transitions between them. So for every frame rendered, one additional frame is generated. DLSS 3.0 makes use of a new generation Optical Flow Accelerator (OFA) included in Ada Lovelace generation RTX GPUs. The new OFA is faster and more accurate than the OFA already available in previous Turing and Ampere RTX GPUs. This results in DLSS 3.0 being exclusive for the RTX 40 Series. At release, DLSS 3.0 does not work for VR displays.\n\nDLSS 3.5\nDLSS 3.5 adds Ray Reconstruction, replacing multiple denoising algorithms with a single AI model trained on five times more data than DLSS 3. Ray Reconstruction is available on all RTX GPUs and first targeted games with path tracing (aka \"full ray tracing\"), including Cyberpunk 2077's Phantom Liberty DLC, Portal with RTX, and Alan Wake 2.\n\nDLSS 4.0\nThe fourth generation of Deep Learning Super Sampling (DLSS) was unveiled alongside the GeForce RTX 50 series. DLSS 4 upscaling uses a new vision transformer-based model for enhanced image quality with reduced ghosting and greater image stability in motion compared to the previous convolutional neural network (CNN) model. DLSS 4 allows a greater number of frames to be generated and interpolated based on a single traditionally rendered frame. This form of frame generation called Multi Frame Generation is exclusive to the GeForce RTX 50 series while the GeForce RTX 40 series is limited to one interpolated frame per traditionally rendered frame. According to Nvidia, this technique will increase performance by up to 800% while retaining low latency with Nvidia Reflex. Nvidia claims that DLSS 4x Frame Generation model uses 30% less video memory with the example of Warhammer 40,000: Darktide using 400MB less memory at 4K resolution with Frame Generation enabled. Nvidia claims that 75 games will integrate DLSS 4 Multi Frame Generation at launch, including Alan Wake 2, Cyberpunk 2077, Indiana Jones and the Great Circle, and Star Wars Outlaws.\n\nManually upgrading DLSS support\nUsers can manually replace the DLLs in games to support a newer version of DLSS.  DLSS Swapper, an open source utility, can automatically do this for all installed games. Replacing DLL files can not add DLSS support or features to games that do not already implement them, though some mods can add frame generation support.\n\nAnti-aliasing\nDLSS requires and applies its own anti-aliasing method. Thus, depending on the game and quality setting used, using DLSS may improve image quality even over native resolution rendering. It operates on similar principles to TAA. Like TAA, it uses information from past frames to produce the current frame. Unlike TAA, DLSS does not sample every pixel in every frame. Instead, it samples different pixels in different frames and uses pixels sampled in past frames to fill in the unsampled pixels in the current frame. DLSS uses machine learning to combine samples in the current frame and past frames, and it can be thought of as an advanced and superior TAA implementation made possible by the available tensor cores. Nvidia also offers Deep Learning Anti-Aliasing (DLAA), which provides the same AI-driven anti-aliasing DLSS uses, but without any upscaling or downscaling.\n\nArchitecture\nWith the exception of the shader-core version implemented in Control, DLSS is only available on GeForce RTX 20, GeForce RTX 30, GeForce RTX 40, GeForce RTX 50, and Quadro RTX series of video cards, using dedicated AI accelerators called Tensor Cores. Tensor Cores are available since the Nvidia Volta GPU microarchitecture, which was first used on the Tesla V100 line of products. They are used for doing fused multiply-add (FMA) operations that are used extensively in neural network calculations for applying a large series of multiplications on weights, followed by the addition of a bias. Tensor cores can operate on FP16, INT8, INT4, and INT1 data types. Each core can do 1024 bits of FMA operations per clock, so 1024 INT1, 256 INT4, 128 INT8, and 64 FP16 operations per clock per tensor core, and most Turing GPUs have a few hundred tensor cores. The Tensor Cores use CUDA Warp-Level Primitives on 32 parallel threads to take advantage of their parallel architecture. A Warp is a set of 32 threads which are configured to execute the same instruction. Since Windows 10 version 1903, Microsoft Windows provided DirectML as one part of DirectX to support Tensor Cores.\n\nReception\nParticularly in early versions of DLSS, users reported blurry frames. Andrew Edelsten, an employee at Nvidia, therefore commented on the problem in a blog post in 2019 and promised that they were working on improving the technology and clarified that the DLSS AI algorithm was mainly trained with 4K image material. That the use of DLSS leads to particularly blurred images at lower resolutions, such as Full HD, is due to the fact that the algorithm has far less image information available to calculate an appropriate image compared to higher resolutions like 4K.\nThe use of DLSS Frame Generation may lead to increased input latency, as well as visual artifacts. It has also been criticized that by implementing DLSS in their games, game developers no longer have an incentive to optimize them so that they also run smoothly in native resolution on modern PC hardware. For example, for the game Alan Wake 2 in 4K resolution at the highest graphics settings with ray tracing enabled, the use of DLSS in Performance mode is recommended even with graphics cards such as the Nvidia GeForce RTX 4080 in order to achieve 60 fps.\nThe transformer-based AI upscaling model introduced with DLSS 4 received moderate praise for its improved image quality with regard to increased stability, reduced ghosting, better anti-aliasing, and higher level of detail, as well as its backward compatability and higher training scalability regarding future improvements.\n\nSee also\nFidelityFX Super Resolution – competing technology from AMD\nIntel XeSS – competing technology from Intel\nPlayStation Spectral Super Resolution – similar technology from PlayStation\n\nReferences\nExternal links\nOfficial website \nDLSS on the Nvidia developer website",
        "url": "https://en.wikipedia.org/wiki/Deep_Learning_Super_Sampling"
    },
    {
        "title": "Description logic",
        "text": "Description logics (DL) are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors.\nDLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profiles are based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge.\n\nIntroduction\nA description logic (DL) models concepts, roles and individuals, and their relationships.\nThe fundamental modeling concept of a DL is the axiom—a logical statement relating roles and/or concepts. This is a key difference from the  frames paradigm where a frame specification declares and completely defines a class.\n\nNomenclature\nTerminology compared to FOL and OWL\nThe description logic community uses different terminology than the first-order logic (FOL) community for operationally equivalent notions; some examples are given below. The Web Ontology Language (OWL) uses again a different terminology, also given in the table below.\n\nNaming convention\nThere are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. The expressivity is encoded in the label for a logic starting with one of the following basic logics:\n\nFollowed by any of the following extensions:\n\nExceptions\nSome canonical DLs that do not exactly fit this convention are:\n\nExamples\nAs an example, \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is a centrally important description logic from which comparisons with other varieties can be made. \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is simply \n  \n    \n      \n        \n          \n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {AL}}}\n  \n with complement of any concept allowed, not just atomic concepts. \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is used instead of the equivalent \n  \n    \n      \n        \n          \n            A\n            L\n            U\n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALUE}}}\n  \n.\nA further example, the description logic \n  \n    \n      \n        \n          \n            S\n            H\n            I\n            Q\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIQ}}}\n  \n is the logic \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n plus  extended cardinality restrictions, and transitive and inverse roles. The naming conventions aren't purely systematic so that the logic \n  \n    \n      \n        \n          \n            A\n            L\n            C\n            O\n            I\n            N\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALCOIN}}}\n  \n might be referred to as \n  \n    \n      \n        \n          \n            A\n            L\n            C\n            N\n            I\n            O\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALCNIO}}}\n  \n and other abbreviations are also made where possible.\nThe Protégé ontology editor supports \n  \n    \n      \n        \n          \n            \n              S\n              H\n              O\n              I\n              N\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHOIN}}^{\\mathcal {(D)}}}\n  \n. Three major biomedical informatics terminology bases, SNOMED CT, GALEN, and GO, are expressible in \n  \n    \n      \n        \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {EL}}}\n  \n (with additional role properties).\nOWL 2 provides the expressiveness of  \n  \n    \n      \n        \n          \n            \n              S\n              R\n              O\n              I\n              Q\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SROIQ}}^{\\mathcal {(D)}}}\n  \n, OWL-DL is based on \n  \n    \n      \n        \n          \n            \n              S\n              H\n              O\n              I\n              N\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHOIN}}^{\\mathcal {(D)}}}\n  \n, and for OWL-Lite it is \n  \n    \n      \n        \n          \n            \n              S\n              H\n              I\n              F\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIF}}^{\\mathcal {(D)}}}\n  \n.\n\nHistory\nDescription logic was given its current name in the 1980s. Previous to this it was called (chronologically): terminological systems, and concept languages.\n\nKnowledge representation\nFrames and semantic networks lack formal (logic-based) semantics. DL was first introduced into knowledge representation (KR) systems to overcome this deficiency.\nThe first DL-based KR system was KL-ONE (by Ronald J. Brachman and Schmolze, 1985). During the '80s other DL-based systems using structural subsumption algorithms were developed including KRYPTON (1983), LOOM (1987), BACK (1988), K-REP (1991) and CLASSIC (1991). This approach featured DL with limited expressiveness but relatively efficient (polynomial time) reasoning.\nIn the early '90s, the introduction of a new tableau based algorithm paradigm allowed efficient reasoning on more expressive DL. DL-based systems using these algorithms — such as KRIS (1991) — show acceptable reasoning performance on typical inference problems even though the worst case complexity is no longer polynomial.\nFrom the mid '90s, reasoners were created with good practical performance on very expressive DL with high worst case complexity. Examples from this period include FaCT, RACER (2001), CEL (2005), and KAON 2 (2005).\nDL reasoners, such as FaCT, FaCT++, RACER, DLP and Pellet, implement the method of analytic tableaux. KAON2 is implemented by algorithms which reduce a SHIQ(D) knowledge base to a disjunctive datalog program.\n\nSemantic web\nThe DARPA Agent Markup Language (DAML) and Ontology Inference Layer (OIL) ontology languages for the Semantic Web can be viewed as\nsyntactic variants of DL. In particular, the formal semantics and reasoning in OIL use the \n  \n    \n      \n        \n          \n            S\n            H\n            I\n            Q\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIQ}}}\n  \n DL. The DAML+OIL DL was developed as a submission to—and formed the starting point of—the World Wide Web Consortium (W3C) Web Ontology Working Group. In 2004, the Web Ontology Working Group completed its work by issuing the OWL recommendation. The design of OWL is based on the \n  \n    \n      \n        \n          \n            S\n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SH}}}\n  \n family of DL with OWL DL and OWL Lite based on \n  \n    \n      \n        \n          \n            \n              S\n              H\n              O\n              I\n              N\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHOIN}}^{\\mathcal {(D)}}}\n  \n and \n  \n    \n      \n        \n          \n            \n              S\n              H\n              I\n              F\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIF}}^{\\mathcal {(D)}}}\n  \n respectively.\nThe W3C OWL Working Group began work in 2007 on a refinement of - and extension to - OWL. In 2009, this was completed by the issuance of the OWL2 recommendation. OWL2 is based on the description logic \n  \n    \n      \n        \n          \n            \n              S\n              R\n              O\n              I\n              Q\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SROIQ}}^{\\mathcal {(D)}}}\n  \n. Practical experience demonstrated that OWL DL lacked several key features necessary to model complex domains.\n\nModeling\nTBox vs Abox\nIn DL, a distinction is drawn between the so-called TBox (terminological box) and the ABox (assertional box). In general, the TBox contains sentences describing concept hierarchies (i.e., relations between concepts) while the ABox contains ground sentences stating where in the hierarchy, individuals belong (i.e., relations between individuals and concepts). For example, the statement:\n\nbelongs in the TBox, while the statement:\n\nbelongs in the ABox.\nNote that the TBox/ABox distinction is not significant, in the same sense that the two \"kinds\" of sentences are not treated differently in first-order logic (which subsumes most DL). When translated into first-order logic, a subsumption axiom like (1) is simply a conditional restriction to unary predicates (concepts) with only variables appearing in it. Clearly, a sentence of this form is not privileged or special over sentences in which only constants (\"grounded\" values) appear like (2).\n\nMotivation for having Tbox and Abox\nSo why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision-procedures for various DL. For example, a reasoner might process the TBox and ABox separately, in part because certain key inference problems are tied to one but not the other one ('classification' is related to the TBox, 'instance checking' to the ABox). Another example is that the complexity of the TBox can greatly affect the performance of a given decision-procedure for a certain DL, independently of the ABox. Thus, it is useful to have a way to talk about that specific part of the knowledge base.\nThe secondary reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox). In the above example: when the hierarchy within a company is the same in every branch but the assignment to employees is different in every department (because there are other people working there), it makes sense to reuse the TBox for different branches that do not use the same ABox.\nThere are two features of description logic that are not shared by most other data description formalisms: DL does not make the unique name assumption (UNA) or the closed-world assumption (CWA). Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent. Not having CWA, or rather having the open world assumption (OWA) means that lack of knowledge of a fact does not immediately imply knowledge of the negation of a fact.\n\nFormal description\nLike first-order logic (FOL), a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants.\n\nSyntax\nThe syntax of a member of the description logic family is characterized by its recursive definition, in which the constructors that can be used to form concept terms are stated. Some constructors are related to logical constructors in first-order logic (FOL) such as intersection or conjunction of concepts, union or disjunction of concepts, negation or complement of concepts, universal restriction and existential restriction. Other constructors have no corresponding construction in FOL including restrictions on roles for example, inverse, transitivity and functionality.\n\nNotation\nLet C and D be concepts, a and b be individuals, and R be a role.\nIf a is R-related to b, then b is called an R-successor of a.\n\nThe description logic ALC\nThe prototypical DL Attributive Concept Language with Complements (\n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n) was introduced by Manfred Schmidt-Schauß and Gert Smolka in 1991, and is the basis of many more expressive DLs. The following definitions follow the treatment in Baader et al.\nLet \n  \n    \n      \n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle N_{C}}\n  \n, \n  \n    \n      \n        \n          N\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle N_{R}}\n  \n and \n  \n    \n      \n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle N_{O}}\n  \n  be (respectively) sets of concept names (also known as atomic concepts), role names and individual names (also known as individuals, nominals or objects). Then the ordered triple (\n  \n    \n      \n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle N_{C}}\n  \n, \n  \n    \n      \n        \n          N\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle N_{R}}\n  \n, \n  \n    \n      \n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle N_{O}}\n  \n) is the signature.\n\nConcepts\nThe set of \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n concepts is the smallest set such that:\n\nThe following are concepts:\n\n  \n    \n      \n        ⊤\n      \n    \n    {\\displaystyle \\top }\n  \n (top is a concept)\n\n  \n    \n      \n        ⊥\n      \n    \n    {\\displaystyle \\bot }\n  \n (bottom is a concept)\nEvery \n  \n    \n      \n        A\n        ∈\n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle A\\in N_{C}}\n  \n (all atomic concepts are concepts)\nIf \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are concepts and \n  \n    \n      \n        R\n        ∈\n        \n          N\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle R\\in N_{R}}\n  \n then the following are concepts:\n\n  \n    \n      \n        C\n        ⊓\n        D\n      \n    \n    {\\displaystyle C\\sqcap D}\n  \n (the intersection of two concepts is a concept)\n\n  \n    \n      \n        C\n        ⊔\n        D\n      \n    \n    {\\displaystyle C\\sqcup D}\n  \n (the union of two concepts is a concept)\n\n  \n    \n      \n        ¬\n        C\n      \n    \n    {\\displaystyle \\neg C}\n  \n (the complement of a concept is a concept)\n\n  \n    \n      \n        ∀\n        R\n        .\n        C\n      \n    \n    {\\displaystyle \\forall R.C}\n  \n (the universal restriction of a concept by a role is a concept)\n\n  \n    \n      \n        ∃\n        R\n        .\n        C\n      \n    \n    {\\displaystyle \\exists R.C}\n  \n (the existential restriction of a concept by a role is a concept)\n\nTerminological axioms\nA general concept inclusion (GCI) has the form \n  \n    \n      \n        C\n        ⊑\n        D\n      \n    \n    {\\displaystyle C\\sqsubseteq D}\n  \n where \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are concepts. Write \n  \n    \n      \n        C\n        ≡\n        D\n      \n    \n    {\\displaystyle C\\equiv D}\n  \n when \n  \n    \n      \n        C\n        ⊑\n        D\n      \n    \n    {\\displaystyle C\\sqsubseteq D}\n  \n and \n  \n    \n      \n        D\n        ⊑\n        C\n      \n    \n    {\\displaystyle D\\sqsubseteq C}\n  \n. A TBox is any finite set of GCIs.\n\nAssertional axioms\nA concept assertion is a statement of the form \n  \n    \n      \n        a\n        :\n        C\n      \n    \n    {\\displaystyle a:C}\n  \n where  \n  \n    \n      \n        a\n        ∈\n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle a\\in N_{O}}\n  \n and C is a concept.\nA role assertion is a statement of the form \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n        :\n        R\n      \n    \n    {\\displaystyle (a,b):R}\n  \n where \n  \n    \n      \n        a\n        ,\n        b\n        ∈\n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle a,b\\in N_{O}}\n  \n  and R is a role.\nAn ABox is a finite set of assertional axioms.\n\nKnowledge base\nA knowledge base (KB) is an ordered pair \n  \n    \n      \n        (\n        \n          \n            T\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {T}},{\\mathcal {A}})}\n  \n for TBox \n  \n    \n      \n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {T}}}\n  \n and ABox \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n.\n\nSemantics\nThe semantics of description logics are defined by interpreting concepts as sets of individuals and roles as sets of ordered pairs of individuals. Those individuals are typically assumed from a given domain. The semantics of non-atomic concepts and roles is then defined in terms of atomic concepts and roles. This is done by using a recursive definition similar to the syntax.\n\nThe description logic ALC\nThe following definitions follow the treatment in Baader et al.\nA terminological interpretation \n  \n    \n      \n        \n          \n            I\n          \n        \n        =\n        (\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ,\n        \n          ⋅\n          \n            \n              I\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {I}}=(\\Delta ^{\\mathcal {I}},\\cdot ^{\\mathcal {I}})}\n  \n over a signature \n  \n    \n      \n        (\n        \n          N\n          \n            C\n          \n        \n        ,\n        \n          N\n          \n            R\n          \n        \n        ,\n        \n          N\n          \n            O\n          \n        \n        )\n      \n    \n    {\\displaystyle (N_{C},N_{R},N_{O})}\n  \n consists of\n\na non-empty set \n  \n    \n      \n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta ^{\\mathcal {I}}}\n  \n called the domain\na interpretation function \n  \n    \n      \n        \n          ⋅\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\cdot ^{\\mathcal {I}}}\n  \n that maps:\nevery individual \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n to an element \n  \n    \n      \n        \n          a\n          \n            \n              I\n            \n          \n        \n        ∈\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle a^{\\mathcal {I}}\\in \\Delta ^{\\mathcal {I}}}\n  \n\nevery concept to a subset of \n  \n    \n      \n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta ^{\\mathcal {I}}}\n  \n\nevery role name to a subset of \n  \n    \n      \n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ×\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta ^{\\mathcal {I}}\\times \\Delta ^{\\mathcal {I}}}\n  \n\nsuch that\n\n  \n    \n      \n        \n          ⊤\n          \n            \n              I\n            \n          \n        \n        =\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\top ^{\\mathcal {I}}=\\Delta ^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          ⊥\n          \n            \n              I\n            \n          \n        \n        =\n        ∅\n      \n    \n    {\\displaystyle \\bot ^{\\mathcal {I}}=\\emptyset }\n  \n\n  \n    \n      \n        (\n        C\n        ⊔\n        D\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        \n          C\n          \n            \n              I\n            \n          \n        \n        ∪\n        \n          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (C\\sqcup D)^{\\mathcal {I}}=C^{\\mathcal {I}}\\cup D^{\\mathcal {I}}}\n  \n (union means disjunction)\n\n  \n    \n      \n        (\n        C\n        ⊓\n        D\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        \n          C\n          \n            \n              I\n            \n          \n        \n        ∩\n        \n          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (C\\sqcap D)^{\\mathcal {I}}=C^{\\mathcal {I}}\\cap D^{\\mathcal {I}}}\n  \n (intersection means conjunction)\n\n  \n    \n      \n        (\n        ¬\n        C\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ∖\n        \n          C\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (\\neg C)^{\\mathcal {I}}=\\Delta ^{\\mathcal {I}}\\setminus C^{\\mathcal {I}}}\n  \n (complement means negation)\n\n  \n    \n      \n        (\n        ∀\n        R\n        .\n        C\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        {\n        x\n        ∈\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        \n          |\n        \n        \n          for\n        \n        \n        \n          every\n        \n        \n        y\n        ,\n        (\n        x\n        ,\n        y\n        )\n        ∈\n        \n          R\n          \n            \n              I\n            \n          \n        \n        \n        \n          implies\n        \n        \n        y\n        ∈\n        \n          C\n          \n            \n              I\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle (\\forall R.C)^{\\mathcal {I}}=\\{x\\in \\Delta ^{\\mathcal {I}}|{\\text{for}}\\;{\\text{every}}\\;y,(x,y)\\in R^{\\mathcal {I}}\\;{\\text{implies}}\\;y\\in C^{\\mathcal {I}}\\}}\n  \n\n  \n    \n      \n        (\n        ∃\n        R\n        .\n        C\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        {\n        x\n        ∈\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        \n          |\n        \n        \n          there\n        \n        \n        \n          exists\n        \n        \n        y\n        ,\n        (\n        x\n        ,\n        y\n        )\n        ∈\n        \n          R\n          \n            \n              I\n            \n          \n        \n        \n        \n          and\n        \n        \n        y\n        ∈\n        \n          C\n          \n            \n              I\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle (\\exists R.C)^{\\mathcal {I}}=\\{x\\in \\Delta ^{\\mathcal {I}}|{\\text{there}}\\;{\\text{exists}}\\;y,(x,y)\\in R^{\\mathcal {I}}\\;{\\text{and}}\\;y\\in C^{\\mathcal {I}}\\}}\n  \n\nDefine \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models }\n  \n (read in I holds) as follows\n\nTBox\nI\n          \n        \n        ⊨\n        C\n        ⊑\n        D\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models C\\sqsubseteq D}\n  \n if and only if \n  \n    \n      \n        \n          C\n          \n            \n              I\n            \n          \n        \n        ⊆\n        \n          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle C^{\\mathcal {I}}\\subseteq D^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {T}}}\n  \n if and only if \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        Φ\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models \\Phi }\n  \n for every \n  \n    \n      \n        Φ\n        ∈\n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\Phi \\in {\\mathcal {T}}}\n\nABox\nI\n          \n        \n        ⊨\n        a\n        :\n        C\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models a:C}\n  \n if and only if \n  \n    \n      \n        \n          a\n          \n            \n              I\n            \n          \n        \n        ∈\n        \n          C\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle a^{\\mathcal {I}}\\in C^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        (\n        a\n        ,\n        b\n        )\n        :\n        R\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models (a,b):R}\n  \n if and only if \n  \n    \n      \n        (\n        \n          a\n          \n            \n              I\n            \n          \n        \n        ,\n        \n          b\n          \n            \n              I\n            \n          \n        \n        )\n        ∈\n        \n          R\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (a^{\\mathcal {I}},b^{\\mathcal {I}})\\in R^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {A}}}\n  \n if and only if \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        ϕ\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models \\phi }\n  \n for every \n  \n    \n      \n        ϕ\n        ∈\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\phi \\in {\\mathcal {A}}}\n\nKnowledge base\nLet \n  \n    \n      \n        \n          \n            K\n          \n        \n        =\n        (\n        \n          \n            T\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {K}}=({\\mathcal {T}},{\\mathcal {A}})}\n  \n be a knowledge base.\n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            K\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {K}}}\n  \n if and only if \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {T}}}\n  \n and \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {A}}}\n\nInference\nDecision problems\nIn addition to the ability to describe concepts formally, one also would like to employ the description of a set of concepts to ask questions about the concepts and instances described. The most common decision problems are basic database-query-like questions like instance checking (is a particular instance (member of an ABox) a member of a given concept) and relation checking (does a relation/role hold between two instances, in other words does a have property b), and the more global-database-questions like subsumption (is a concept a subset of another concept), and concept consistency (is there no contradiction among the definitions or chain of definitions). The more operators one includes in a logic and the more complicated the TBox (having cycles, allowing non-atomic concepts to include each other), usually the higher the computational complexity is for each of these problems (see Description Logic Complexity Navigator for examples).\n\nRelationship with other logics\nFirst-order logic\nMany DLs are decidable fragments of first-order logic (FOL) and are usually fragments of two-variable logic or guarded logic. In addition, some DLs have features that are not covered in FOL; this includes concrete domains (such as integer or strings, which can be used as ranges for roles such as hasAge or hasName) or an operator on roles for the transitive closure of that role.\n\nFuzzy description logic\nFuzzy description logics combines fuzzy logic with DLs. Since many concepts that are needed for intelligent systems lack well defined boundaries, or precisely defined criteria of membership, fuzzy logic is needed to deal with notions of vagueness and imprecision. This offers a motivation for a generalization of description logic towards dealing with imprecise and vague concepts.\n\nModal logic\nDescription logic is related to—but developed independently of—modal logic (ML). Many—but not all—DLs are syntactic variants of ML.\nIn general, an object corresponds to a possible world, a concept corresponds to a modal proposition, and a role-bounded quantifier to a modal operator with that role as its accessibility relation.\nOperations on roles (such as composition, inversion, etc.) correspond to the modal operations used in dynamic logic.\n\nExamples\nTemporal description logic\nTemporal description logic represents—and allows reasoning about—time dependent concepts and many different approaches to this problem exist. For example, a description logic might be combined with a modal temporal logic such as linear temporal logic.\n\nSee also\nFormal concept analysis\nLattice (order)\nFormal semantics (natural language)\nSemantic parameterization\nSemantic reasoner\n\nReferences\nFurther reading\nF. Baader, D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider: The Description Logic Handbook: Theory, Implementation, Applications. Cambridge University Press, Cambridge, UK, 2003. ISBN 0-521-78176-0\nIan Horrocks, Ulrike Sattler: Ontology Reasoning in the SHOQ(D) Description Logic, in  Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, 2001.\nD. Fensel, F. van Harmelen, I. Horrocks, D. McGuinness, and P. F. Patel-Schneider: OIL: An Ontology Infrastructure for the Semantic Web. IEEE Intelligent Systems, 16(2):38-45, 2001.\nIan Horrocks and Peter F. Patel-Schneider: The Generation of DAML+OIL. In Proceedings of the 2001 Description Logic Workshop (DL 2001), volume 49 of CEUR <http://ceur-ws.org/>, pages 30–35, 2001.\nIan Horrocks, Peter F. Patel-Schneider, and Frank van Harmelen: From SHIQ and RDF to OWL: The Making of a Web Ontology Language. Journal of Web Semantics, 1(1):7-26, 2003.\nBernardo Cuenca Grau, Ian Horrocks, Boris Motik, Bijan Parsia, Peter Patel-Schneider, and Ulrike Sattler: OWL 2: The next step for OWL. Journal of Web Semantics, 6(4):309–322, November 2008.\nFranz Baader, Ian Horrocks, and Ulrike Sattler: Chapter 3 Description Logics. In Frank van Harmelen, Vladimir Lifschitz, and Bruce Porter, editors, Handbook of Knowledge Representation. Elsevier, 2007.\nAlessandro Artale and Enrico Franconi: Temporal Description Logics. In Handbook of Temporal Reasoning in Artificial Intelligence, 2005.\nWeb Ontology (WebONT) Working Group Charter. W3C, 2003\nWorld Wide Web Consortium Issues RDF and OWL Recommendations. Press Release. W3C, 2004.\nOWL Working Group Charter. W3C, 2007.\nOWL 2 Connects the Web of Knowledge with the Web of Data. Press Release. W3C, 2009.\nMarkus Krötzsch, František Simančík, Ian Horrocks: A Description Logic Primer. CoRR arXiv:1201.4089. 2012. A very first introduction for readers without a formal logic background.\nSebastian Rudolph: Foundations of Description Logics. In Reasoning Web: Semantic Technologies for the Web of Data, 7th International Summer School, volume 6848 of Lecture Notes in Computer Science, pages 76–136. Springer, 2011. (springerlink)Introductory text with a focus on modelling and formal semantics. There are also slides.\nJens Lehmann: DL-Learner: Learning concepts in description logics, Journal of Machine Learning Research, 2009.\nStefan Heindorf, Lukas Blübaum, Nick Düsterhus, Till Werner, Varun Nandkumar Golani, Caglar Demir, and Axel-Cyrille Ngonga Ngomo. Evolearner: Learning description logics with evolutionary algorithms. In Proceedings of the ACM Web Conference 2022, pp. 818-828. 2022.\nFranz Baader: Description Logics. In Reasoning Web: Semantic Technologies for Information Systems, 5th International Summer School, volume 5689 of Lecture Notes in Computer Science, pages 1–39. Springer, 2009. (springerlink) Introductory text with a focus on reasoning and language design, and an extended historical overview.\nEnrico Franconi: Introduction to Description Logics. Course materials. Faculty of Computer Science, Free University of Bolzano, Italy, 2002. Lecture slides and many literature pointers, somewhat dated.\nIan Horrocks: Ontologies and the Semantic Web. Communications of the ACM, 51(12):58-67, December 2008. A general overview of knowledge representation in Semantic Web technologies.\n\nExternal links\nDescription Logic Complexity Navigator, maintained by Evgeny Zolin at the Department of Computer Science\nList of Reasoners, OWL research at the University of Manchester\nDescription Logics Workshop, homepage of the collecting information about the community and archives of the workshop proceedings\n\nReasoners\nThere are some semantic reasoners that deal with OWL and DL. These are some of the most popular:\n\nCEL is an open source LISP-based reasoner (Apache 2.0 License).\nCerebra Engine was a commercial C++-based reasoner, acquired in 2006 by webMethods.\nFaCT++  is a free open-source C++-based reasoner.\nKAON2 is a free (for non-commercial use) Java-based reasoner, offering fast reasoning support for OWL ontologies.\nMSPASS is a free open-source C reasoner for numerous DL models.\nPellet is a dual-licensed (AGPL and proprietary) commercial, Java-based reasoner.\nRacerPro of Racer Systems was a commercial (free trials and research licenses are available) lisp-based reasoner, today both an open source version of RACER exists from the original developers at Lübeck University using the BSD 3 license, and also a commercialized version, still named RacerPro by Franz Inc.\nSim-DL is a free open-source Java-based reasoner for the language ALCHQ. It also provides a similarity measurement functionality between concepts. To access this functionality a Protégé plugin can be used.\nHermiT is an open-source reasoner based on the \"hypertableau\" calculus. It is developed by the University of Oxford.\nOwlready2 is a package for ontology-oriented programming in Python. It can load OWL 2.0 ontologies as Python objects, modify them, save them, and perform reasoning via HermiT (included). Owlready2 allows a transparent access to OWL ontologies (contrary to usual Java-based API).\nOWLAPY. OWLAPY is an open-source Python framework for creating, manipulating, and reasoning with OWL ontologies. It includes a built-in StructuralReasoner for efficient, lightweight reasoning and wrappers for well-known Java-based reasoners like HermiT, Pellet, JFact, and Openllet.\n\nEditors\nProtégé is a free, open-source ontology editor and a knowledge base framework, which can use DL reasoners offering DIG Interface as a back end for consistency checks.\nSWOOP on GitHub, an OWL browser/editor that takes the standard web browser as the basic UI paradigm.\n\nInterfaces\nDIG Interface on SourceForge, a standardized XML interface to DLs systems developed by the DL Implementation Group (DIG).\nOWL API on SourceForge, a Java interface and implementation for the Web Ontology Language, used to represent Semantic Web ontologies.\nOWLAPY on GitHub, a Python interface and implementation for the Web Ontology Language, used to represent Semantic Web ontologies.",
        "url": "https://en.wikipedia.org/wiki/Description_logic"
    },
    {
        "title": "Data Science and Predictive Analytics",
        "text": "The first edition of the textbook Data Science and Predictive Analytics: Biomedical and Health Applications using R, authored by Ivo D. Dinov, was published in August 2018 by Springer. The second edition of the book was printed in 2023.\nThis textbook covers some of the core mathematical foundations, computational techniques, and artificial intelligence approaches used in data science research and applications.\nBy using the statistical computing platform R and a broad range of biomedical case-studies, the 23 chapters of the book first edition provide explicit examples of importing, exporting, processing, modeling, visualizing, and interpreting large, multivariate, incomplete, heterogeneous, longitudinal, and incomplete datasets (big data).\n\nStructure\nFirst edition table of contents\nThe first edition of the Data Science and Predictive Analytics (DSPA) textbook is divided into the following 23 chapters, each progressively building on the previous content.\n\nSecond edition table of contents\nThe significantly reorganized revised edition of the book (2023) expands and modernizes the presented mathematical principles, computational methods, data science techniques, model-based machine learning and model-free artificial intelligence algorithms. The 14 chapters of the new edition start with an introduction and progressively build foundational skills to naturally reach biomedical applications of deep learning.\n\nIntroduction\nBasic Visualization and Exploratory Data Analytics\nLinear Algebra, Matrix Computing, and Regression Modeling\nLinear and Nonlinear Dimensionality Reduction\nSupervised Classification\nBlack Box Machine Learning Methods\nQualitative Learning Methods—Text Mining, Natural Language Processing, and Apriori Association Rules Learning\nUnsupervised Clustering\nModel Performance Assessment, Validation, and Improvement\nSpecialized Machine Learning Topics\nVariable Importance and Feature Selection\nBig Longitudinal Data Analysis\nFunction Optimization\nDeep Learning, Neural Networks\n\nReception\nThe materials in the Data Science and Predictive Analytics (DSPA) textbook have been peer-reviewed in the Journal of the American Statistical Association, International Statistical Institute’s  ISI Review Journal, and the Journal of the American Library Association. Many scholarly publications reference the DSPA textbook.\nAs of January 17, 2021, the electronic version of the book first edition (ISBN 978-3-319-72347-1) is freely available on SpringerLink and has been downloaded over 6 million times. The textbook is globally available in print (hardcover and softcover) and electronic formats (PDF and EPub) in many college and university libraries and has been used for data science, computational statistics, and analytics classes at various institutions.\n\nReferences\nExternal links\nDSPA textbook (1st edition) Springer website and SpringerLink EBook download\nDSPA textbook (2nd edition) Springer website is published in The Springer Series in Applied Machine Learning (SSAML)\nTextbook supporting website",
        "url": "https://en.wikipedia.org/wiki/Data_Science_and_Predictive_Analytics"
    },
    {
        "title": "Dynamic epistemic logic",
        "text": "Dynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. These events can change factual properties of the actual world (they are called ontic events): for example a red card is painted in blue. They can also bring about changes of knowledge without changing factual properties of the world (they are called epistemic events): for example a card is revealed publicly (or privately) to be red. Originally, DEL focused on epistemic events. We only present in this entry some of the basic ideas of the original DEL framework; more details about DEL in general can be found in the references.\nDue to the nature of its object of study and its abstract approach, DEL is related and has applications to numerous research areas, such as computer science (artificial intelligence), philosophy (formal epistemology), economics (game theory) and cognitive science. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information.\nAs a combination of dynamic logic and epistemic logic, dynamic epistemic logic is a young field of research. It really started in 1989 with Plaza's logic of public announcement.  Independently, Gerbrandy and Groeneveld  proposed a system dealing moreover with private announcement and that was inspired by the work of Veltman. Another system was proposed by van Ditmarsch whose main inspiration was the Cluedo game. But the most influential and original system was the system proposed by Baltag, Moss and Solecki. This system can deal with all the types of situations studied in the works above and its underlying methodology is conceptually grounded. We will present in this entry some of its basic ideas.\nFormally, DEL extends ordinary epistemic logic by the inclusion of event models to describe actions, and a product update operator that defines how epistemic models are updated as the consequence of executing actions described through event models. Epistemic logic will first be recalled. Then, actions and events will enter into the picture and we will introduce the DEL framework.\n\nEpistemic Logic\nEpistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of reasoning about knowledge and belief: which principles relating the notions of knowledge and belief are intuitively plausible? Like epistemology, it stems from the Greek word \n  \n    \n      \n        ϵ\n        π\n        ι\n        σ\n        τ\n        η\n        μ\n        η\n      \n    \n    {\\displaystyle \\epsilon \\pi \\iota \\sigma \\tau \\eta \\mu \\eta }\n  \n or ‘episteme’ meaning knowledge. Epistemology is nevertheless more concerned with analyzing the very nature and scope of knowledge, addressing questions such as “What is the definition of knowledge?” or “How is knowledge acquired?”. In fact, epistemic logic grew out of epistemology in the Middle Ages thanks to the efforts of Burley and Ockham. The formal work, based on modal logic, that inaugurated contemporary research into epistemic logic dates back only to 1962 and is due to Hintikka. It then sparked in the 1960s discussions about the principles of knowledge and belief and many axioms for these notions were proposed and discussed. For example, the interaction axioms \n  \n    \n      \n        K\n        p\n        →\n        B\n        p\n      \n    \n    {\\displaystyle Kp\\rightarrow Bp}\n  \n and \n  \n    \n      \n        B\n        p\n        →\n        K\n        B\n        p\n      \n    \n    {\\displaystyle Bp\\rightarrow KBp}\n  \n are often considered to be intuitive principles: if an agent Knows \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n then (s)he also Believes \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, or if an agent Believes \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, then (s)he Knows that (s)he Believes \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n. More recently, these kinds of philosophical theories were taken up by researchers in economics, artificial intelligence and theoretical computer science where reasoning about knowledge is a central topic. Due to the new setting in which epistemic logic was used, new perspectives and new features such as computability issues were then added to the research agenda of epistemic logic.\n\nSyntax\nIn the sequel, \n  \n    \n      \n        A\n        G\n        T\n        S\n        =\n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle AGTS=\\{1,\\ldots ,n\\}}\n  \n is a finite set whose elements are called agents and \n  \n    \n      \n        P\n        R\n        O\n        P\n      \n    \n    {\\displaystyle PROP}\n  \n is a set of propositional letters.\nThe epistemic language is an extension of the basic multi-modal language of modal logic with a common knowledge operator \n  \n    \n      \n        \n          C\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle C_{A}}\n  \n and a distributed knowledge operator \n  \n    \n      \n        \n          D\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle D_{A}}\n  \n. Formally, the epistemic language \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}^{C}}\n  \n is defined inductively by the following grammar in BNF:\n\n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n          \n            C\n          \n        \n        :\n        ϕ\n         \n         \n        ::=\n         \n         \n        p\n         \n        ∣\n         \n        ¬\n        ϕ\n         \n        ∣\n         \n        (\n        ϕ\n        ∧\n        ϕ\n        )\n         \n        ∣\n         \n        \n          K\n          \n            j\n          \n        \n        ϕ\n         \n        ∣\n         \n        \n          C\n          \n            A\n          \n        \n        ϕ\n         \n        ∣\n         \n        \n          D\n          \n            A\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}^{C}:\\phi ~~::=~~p~\\mid ~\\neg \\phi ~\\mid ~(\\phi \\land \\phi )~\\mid ~K_{j}\\phi ~\\mid ~C_{A}\\phi ~\\mid ~D_{A}\\phi }\n  \n\nwhere \n  \n    \n      \n        p\n        ∈\n        P\n        R\n        O\n        P\n      \n    \n    {\\displaystyle p\\in PROP}\n  \n, \n  \n    \n      \n        j\n        ∈\n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle j\\in {AGTS}}\n  \n and \n  \n    \n      \n        A\n        ⊆\n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle A\\subseteq {AGTS}}\n  \n. The basic epistemic language \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{EL}}\n  \n is the language \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            E\n            L\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{EL}^{C}}\n  \n without the common knowledge and distributed knowledge operators. The formula \n  \n    \n      \n        ⊥\n      \n    \n    {\\displaystyle \\bot }\n  \n is an abbreviation for \n  \n    \n      \n        ¬\n        p\n        ∧\n        p\n      \n    \n    {\\displaystyle \\neg p\\land p}\n  \n (for a given \n  \n    \n      \n        p\n        ∈\n        P\n        R\n        O\n        P\n      \n    \n    {\\displaystyle p\\in PROP}\n  \n),  \n  \n    \n      \n        ⟨\n        \n          K\n          \n            j\n          \n        \n        ⟩\n        ϕ\n      \n    \n    {\\displaystyle \\langle K_{j}\\rangle \\phi }\n  \n is an abbreviation for \n  \n    \n      \n        ¬\n        \n          K\n          \n            j\n          \n        \n        ¬\n        ϕ\n      \n    \n    {\\displaystyle \\neg K_{j}\\neg \\phi }\n  \n, \n  \n    \n      \n        \n          E\n          \n            A\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle E_{A}\\phi }\n  \n is an abbreviation for \n  \n    \n      \n        \n          ⋀\n          \n            j\n            ∈\n            A\n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle \\bigwedge \\limits _{j\\in A}K_{j}\\phi }\n  \n and \n  \n    \n      \n        C\n        ϕ\n      \n    \n    {\\displaystyle C\\phi }\n  \n an abbreviation for \n  \n    \n      \n        \n          C\n          \n            A\n            G\n            T\n            S\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle C_{AGTS}\\phi }\n  \n.\nGroup notions: general, common and distributed knowledge.\nIn a multi-agent setting there are three important epistemic concepts: general knowledge, distributed knowledge and common knowledge. The notion of common knowledge was first studied by Lewis in the context of conventions. It was then applied to distributed systems  and to game theory, where it allows to express that the rationality of the players, the rules of the game and the set of players are commonly known.\nGeneral knowledge.\nGeneral knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n means that everybody in the group of agents \n  \n    \n      \n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle {AGTS}}\n  \n knows that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n. Formally, this corresponds to the following formula:\n\n  \n    \n      \n        E\n        ϕ\n        :=\n        \n          \n            ⋀\n            \n              j\n              ∈\n              \n                A\n                G\n                T\n                S\n              \n            \n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n        .\n      \n    \n    {\\displaystyle E\\phi :={\\underset {j\\in {AGTS}}{\\bigwedge }}K_{j}\\phi .}\n  \n\nCommon knowledge.\nCommon knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n means that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n but also that everybody knows that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, that everybody knows that everybody knows that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, and so on ad infinitum. Formally, this corresponds to the following formula\n\n  \n    \n      \n        C\n        ϕ\n        :=\n        E\n        ϕ\n        ∧\n        E\n        E\n        ϕ\n        ∧\n        E\n        E\n        E\n        ϕ\n        ∧\n        …\n      \n    \n    {\\displaystyle C\\phi :=E\\phi \\land EE\\phi \\land EEE\\phi \\land \\ldots }\n  \n\nAs we do not allow infinite conjunction the notion of common knowledge will have to be introduced as a primitive in our language.\nBefore defining the language with this new operator, we are going to give an example introduced by Lewis that illustrates the difference between the notions of general knowledge and common knowledge. Lewis wanted to know what kind of knowledge is needed so that the statement \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n: “every driver must drive on the right” be a convention among a group of agents. In other words, he wanted to know what kind of knowledge is needed so that everybody feels safe to drive on the right. Suppose there are only two agents \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n and \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n. Then everybody knowing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        E\n        p\n      \n    \n    {\\displaystyle Ep}\n  \n) is not enough. Indeed, it might still be possible that the agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n considers possible that the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n does not know \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            j\n          \n        \n        p\n      \n    \n    {\\displaystyle \\neg K_{i}K_{j}p}\n  \n). In that case the agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n will not feel safe to drive on the right because he might consider that the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n, not knowing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, could drive on the left. To avoid this problem, we could then assume that everybody knows that everybody knows that \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        E\n        E\n        p\n      \n    \n    {\\displaystyle EEp}\n  \n). This is again not enough to ensure that everybody feels safe to drive on the right. Indeed, it might still be possible that agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n considers possible that agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers possible that agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n does not know \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            j\n          \n        \n        \n          K\n          \n            i\n          \n        \n        p\n      \n    \n    {\\displaystyle \\neg K_{i}K_{j}K_{i}p}\n  \n). In that case and from \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n’s point of view, \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers possible that \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, not knowing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, will drive on the left. So from \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n’s point of view, \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n might drive on the left as well (by the same argument as above). So \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n will not feel safe to drive on the right. Reasoning by induction, Lewis showed that for any \n  \n    \n      \n        k\n        ∈\n        \n          N\n        \n      \n    \n    {\\displaystyle k\\in \\mathbb {N} }\n  \n, \n  \n    \n      \n        E\n        p\n        ∧\n        \n          E\n          \n            1\n          \n        \n        p\n        ∧\n        …\n        ∧\n        \n          E\n          \n            k\n          \n        \n        p\n      \n    \n    {\\displaystyle Ep\\land E^{1}p\\land \\ldots \\land E^{k}p}\n  \n is not enough for the drivers to feel safe to drive on the right. In fact what we need is an infinite conjunction. In other words, we need common knowledge of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n: \n  \n    \n      \n        C\n        p\n      \n    \n    {\\displaystyle Cp}\n  \n.\nDistributed knowledge.\nDistributed knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n means that if the agents pulled their knowledge altogether, they would know that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds. In other words, the knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is distributed among the agents. The formula \n  \n    \n      \n        \n          D\n          \n            A\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle D_{A}\\phi }\n  \n reads as ‘it is distributed knowledge among the set of agents \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds’.\n\nSemantics\nEpistemic logic is a modal logic. So, what we call an epistemic model \n  \n    \n      \n        \n          \n            M\n          \n        \n        =\n        (\n        W\n        ,\n        \n          R\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n        \n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}=(W,R_{1},\\ldots ,R_{n},I)}\n  \n is just a Kripke model as defined in modal logic. The set \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is a non-empty set whose elements are called possible worlds and the interpretation \n  \n    \n      \n        I\n        :\n        W\n        →\n        \n          2\n          \n            P\n            R\n            O\n            P\n          \n        \n      \n    \n    {\\displaystyle I:W\\rightarrow 2^{PROP}}\n  \n is a function specifying which propositional facts (such as ‘Ann has the red card’) are true in each of these worlds. The accessibility relations \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n        ⊆\n        W\n        ×\n        W\n      \n    \n    {\\displaystyle R_{j}\\subseteq W\\times W}\n  \n are binary relations for each agent \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n; they are intended to capture the uncertainty of each agent (about the actual world and about the other agents' uncertainty). Intuitively, we have \n  \n    \n      \n        (\n        w\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (w,v)\\in R_{j}}\n  \n when the world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n is compatible with agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n’s information in world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n or, in other words, when agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers that world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n might correspond to the world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n (from this standpoint). We abusively write \n  \n    \n      \n        w\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w\\in {\\mathcal {M}}}\n  \n for \n  \n    \n      \n        w\n        ∈\n        W\n      \n    \n    {\\displaystyle w\\in W}\n  \n and \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n        (\n        w\n        )\n      \n    \n    {\\displaystyle R_{j}(w)}\n  \n denotes the set of worlds \n  \n    \n      \n        {\n        v\n        ∈\n        W\n        ;\n        (\n        w\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{v\\in W;(w,v)\\in R_{j}\\}}\n  \n.\nIntuitively, a pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n, where \n  \n    \n      \n        w\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w\\in {\\mathcal {M}}}\n  \n, represents from an external point of view how the actual world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n is perceived by the agents \n  \n    \n      \n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle {AGTS}}\n  \n.\nFor every epistemic model \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n, every \n  \n    \n      \n        w\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w\\in {\\mathcal {M}}}\n  \n and every \n  \n    \n      \n        ϕ\n        ∈\n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle \\phi \\in {\\mathcal {L}}_{\\textsf {EL}}}\n  \n, we define \n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        ϕ\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models \\phi }\n  \n inductively by the following truth conditions:\n\nwhere \n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ⋃\n                  \n                    j\n                    ∈\n                    A\n                  \n                \n              \n              \n                R\n                \n                  j\n                \n              \n            \n            )\n          \n          \n            +\n          \n        \n      \n    \n    {\\displaystyle \\left({\\underset {j\\in A}{\\bigcup }}R_{j}\\right)^{+}}\n  \n is the transitive closure of \n  \n    \n      \n        \n          \n            ⋃\n            \n              j\n              ∈\n              A\n            \n          \n        \n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\underset {j\\in A}{\\bigcup }}R_{j}}\n  \n: we have that \n  \n    \n      \n        v\n        ∈\n        \n          \n            (\n            \n              \n                \n                  ⋃\n                  \n                    j\n                    ∈\n                    A\n                  \n                \n              \n              \n                R\n                \n                  j\n                \n              \n            \n            )\n          \n          \n            +\n          \n        \n        (\n        w\n        )\n      \n    \n    {\\displaystyle v\\in \\left({\\underset {j\\in A}{\\bigcup }}R_{j}\\right)^{+}(w)}\n  \n if, and only if, there are \n  \n    \n      \n        \n          w\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w_{0},\\ldots ,w_{m}\\in {\\mathcal {M}}}\n  \n and \n  \n    \n      \n        \n          j\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          j\n          \n            m\n          \n        \n        ∈\n        A\n      \n    \n    {\\displaystyle j_{1},\\ldots ,j_{m}\\in A}\n  \n such that \n  \n    \n      \n        \n          w\n          \n            0\n          \n        \n        =\n        w\n        ,\n        \n          w\n          \n            m\n          \n        \n        =\n        v\n      \n    \n    {\\displaystyle w_{0}=w,w_{m}=v}\n  \n and for all \n  \n    \n      \n        i\n        ∈\n        {\n        1\n        ,\n        …\n        ,\n        m\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,\\ldots ,m\\}}\n  \n, \n  \n    \n      \n        \n          w\n          \n            i\n            −\n            1\n          \n        \n        \n          R\n          \n            \n              j\n              \n                i\n              \n            \n          \n        \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i-1}R_{j_{i}}w_{i}}\n  \n.\nDespite the fact that the notion of common belief has to be introduced as a primitive in the language, we can notice that the definition of epistemic models does not have to be modified in order to give truth value to the common knowledge and distributed knowledge operators.\nCard Example:\nPlayers \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n (standing for Ann, Bob and Claire) play a card game with three cards: a red one, a green one and a blue one. Each of them has a single card but they do not know the cards of the other players. Ann has the red card, Bob has the green card and Claire has the blue card. This example is depicted in the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n represented below. In this example, \n  \n    \n      \n        A\n        G\n        T\n        S\n        :=\n        {\n        A\n        ,\n        B\n        ,\n        C\n        }\n      \n    \n    {\\displaystyle AGTS:=\\{A,B,C\\}}\n  \n and \n  \n    \n      \n        P\n        R\n        O\n        P\n        :=\n        {\n        \n          \n            \n              A\n            \n          \n        \n        ,\n        \n          \n            \n              B\n            \n          \n        \n        ,\n        \n          \n            \n              C\n            \n          \n        \n        ,\n        \n          \n            \n              B\n            \n          \n        \n        ,\n        \n          \n            \n              C\n            \n          \n        \n        ,\n        \n          \n            \n              A\n            \n          \n        \n        ,\n        \n          \n            \n              C\n            \n          \n        \n        ,\n        \n          \n            \n              A\n            \n          \n        \n        ,\n        \n          \n            \n              B\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle PROP:=\\{{\\color {red}{A}},{\\color {green}{B}},{\\color {blue}{C}},{\\color {red}{B}},{\\color {green}{C}},{\\color {blue}{A}},{\\color {red}{C}},{\\color {green}{A}},{\\color {blue}{B}}\\}}\n  \n. Each world is labelled by the propositional letters which are true in this world and \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n corresponds to the actual world. There is an arrow indexed by agent \n  \n    \n      \n        j\n        ∈\n        {\n        A\n        ,\n        B\n        ,\n        C\n        }\n      \n    \n    {\\displaystyle j\\in \\{A,B,C\\}}\n  \n from a possible world \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n to a possible world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n when \n  \n    \n      \n        (\n        u\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (u,v)\\in R_{j}}\n  \n. Reflexive arrows are omitted, which means that for all \n  \n    \n      \n        j\n        ∈\n        {\n        A\n        ,\n        B\n        ,\n        C\n        }\n      \n    \n    {\\displaystyle j\\in \\{A,B,C\\}}\n  \n and all \n  \n    \n      \n        v\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle v\\in {\\mathcal {M}}}\n  \n, we have that \n  \n    \n      \n        (\n        v\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (v,v)\\in R_{j}}\n  \n.\n\n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}}\n  \n stands for : \"\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n has the red card''\n\n  \n    \n      \n        \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {blue}{C}}}\n  \n stand for: \"\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the blue card''\n\n  \n    \n      \n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {green}{B}}}\n  \n stands for: \"\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the green card''\nand so on...\nWhen accessibility relations are equivalence relations (like in this example) and we have that \n  \n    \n      \n        (\n        w\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (w,v)\\in R_{j}}\n  \n, we say that agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n cannot distinguish world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n from world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n (or world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n is indistinguishable from world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n for agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n). So, for example, \n  \n    \n      \n        A\n      \n    \n    {\\textstyle A}\n  \n cannot distinguish the actual world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n from the possible world where \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the blue card (\n  \n    \n      \n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {blue}{B}}}\n  \n), \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the green card (\n  \n    \n      \n        \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {green}{C}}}\n  \n) and \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n still has the red card (\n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}}\n  \n).\nIn particular, the following statements hold:\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              A\n            \n          \n        \n        )\n        ∧\n        (\n        \n          \n            \n              C\n            \n          \n        \n        ∧\n        \n          K\n          \n            C\n          \n        \n        \n          \n            \n              C\n            \n          \n        \n        )\n        ∧\n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          K\n          \n            B\n          \n        \n        \n          \n            \n              B\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models ({\\color {red}{A}}\\land K_{A}{\\color {red}{A}})\\land ({\\color {blue}{C}}\\land K_{C}{\\color {blue}{C}})\\land ({\\color {green}{B}}\\land K_{B}{\\color {green}{B}})}\n  \n\n'All the agents know the color of their card'.\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        \n          K\n          \n            A\n          \n        \n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∨\n        \n          \n            \n              B\n            \n          \n        \n        )\n        ∧\n        \n          K\n          \n            A\n          \n        \n        (\n        \n          \n            \n              C\n            \n          \n        \n        ∨\n        \n          \n            \n              C\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models K_{A}({\\color {blue}{B}}\\vee {\\color {green}{B}})\\land K_{A}({\\color {blue}{C}}\\vee {\\color {green}{C}})}\n  \n\n'\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n knows that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has either the blue or the green card and that \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has either the blue or the green card'.\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        E\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        )\n        ∧\n        C\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models E({\\color {red}{A}}\\vee {\\color {blue}{A}}\\vee {\\color {green}{A}})\\land C({\\color {red}{A}}\\vee {\\color {blue}{A}}\\vee {\\color {green}{A}})}\n  \n\n'Everybody knows that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n has either the red, green or blue card and this is even common knowledge among all agents'.\n\nKnowledge versus Belief\nWe use the same notation \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle K_{j}}\n  \n for both knowledge and belief. Hence, depending on the context, \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi }\n  \n will either read ‘the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n Knows that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds’ or ‘the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n Believes that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds’. A crucial difference is that, unlike knowledge, beliefs can be wrong: the axiom \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n        →\n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi \\rightarrow \\phi }\n  \n holds only for knowledge, but not necessarily for belief. This axiom called axiom T (for Truth) states that if the agent knows a proposition, then this proposition is true. It is often considered to be the hallmark of knowledge and it has not been subjected to any serious attack ever since its introduction in the Theaetetus by Plato.\nThe notion of knowledge might comply to some other constraints (or axioms) such as \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n        →\n        \n          K\n          \n            j\n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi \\rightarrow K_{j}K_{j}\\phi }\n  \n: if agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n knows something, she knows that she knows it. These constraints might affect the nature of the accessibility relations \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle R_{j}}\n  \n which may then comply to some extra properties. So, we are now going to define some particular classes of epistemic models that all add some extra constraints on the accessibility relations \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle R_{j}}\n  \n. These constraints are matched by particular axioms for the knowledge operator \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle K_{j}}\n  \n. Below each property, we give the axiom which defines the class of epistemic frames that fulfill this property. (\n  \n    \n      \n        K\n        ϕ\n      \n    \n    {\\displaystyle K\\phi }\n  \n stands for \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi }\n  \n for any \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n.)\n\nWe discuss the axioms above. Axiom 4 states that if the agent knows a proposition, then she knows that she knows it (this axiom is also known as the “KK-principle”or “KK-thesis”). In epistemology, axiom 4 tends to be accepted by internalists, but not by externalists. Axiom 4 is nevertheless widely accepted by computer scientists (but also by many philosophers, including Plato, Aristotle, Saint Augustine, Spinoza and Schopenhauer, as Hintikka recalls ). A more controversial axiom for the logic of knowledge is axiom 5 for Euclidicity: this axiom states that if the agent does not know a proposition, then she knows that she does not know it. Most philosophers (including Hintikka) have attacked this axiom, since numerous examples from everyday life seem to invalidate it. In general, axiom 5 is invalidated when the agent has mistaken beliefs, which can be due for example to misperceptions, lies or other forms of deception. Axiom B states that it cannot be the case that the agent considers it possible that she knows a false proposition (that is, \n  \n    \n      \n        ¬\n        (\n        ¬\n        ϕ\n        ∧\n        ¬\n        K\n        ¬\n        K\n        ϕ\n        )\n      \n    \n    {\\displaystyle \\neg (\\neg \\phi \\land \\neg K\\neg K\\phi )}\n  \n). If we assume that axioms T and 4 are valid, then axiom B falls prey to the same attack as the one for axiom 5 since this axiom is derivable. Axiom D states that the agent's beliefs are consistent. In combination with axiom K (where the knowledge operator is replaced by a belief operator), axiom D is in fact equivalent to a simpler axiom D' which conveys, maybe more explicitly, the fact that the agent's beliefs cannot be inconsistent: \n  \n    \n      \n        ¬\n        B\n        ⊥\n      \n    \n    {\\displaystyle \\neg B\\bot }\n  \n. The other intricate axioms .2, .3, .3.2 and .4 have been introduced by epistemic logicians such as Lenzen and Kutchera in the 1970s and presented for some of them as key axioms of epistemic logic. They can be characterized in terms of intuitive interaction axioms relating knowledge and beliefs.\n\nAxiomatization\nThe Hilbert proof system K for the basic modal logic is defined by the following axioms and inference rules: for all \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n,\n\nThe axioms of an epistemic logic obviously display the way the agents reason. For example, the axiom K together with the rule of inference Nec entail that if I know \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n (\n  \n    \n      \n        K\n        ϕ\n      \n    \n    {\\displaystyle K\\phi }\n  \n) and I know that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n implies \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n  (\n  \n    \n      \n        K\n        (\n        ϕ\n        →\n        ψ\n        )\n        )\n      \n    \n    {\\displaystyle K(\\phi \\rightarrow \\psi ))}\n  \n then I know that \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n (\n  \n    \n      \n        K\n        ψ\n      \n    \n    {\\displaystyle K\\psi }\n  \n). Stronger constraints can be added. The following  proof systems for \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}}\n  \n are often used in the literature.\n\nWe define the set of proof systems \n  \n    \n      \n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n        :=\n        {\n        \n          \n            K\n          \n        \n        ,\n        \n          \n            KD45\n          \n        \n        ,\n        \n          \n            S4\n          \n        \n        ,\n        \n          \n            S4.2\n          \n        \n        ,\n        \n          \n            S4.3\n          \n        \n        ,\n        \n          \n            S4.3.2\n          \n        \n        ,\n        \n          \n            S4.4\n          \n        \n        ,\n        \n          \n            S5\n          \n        \n        }\n      \n    \n    {\\displaystyle \\mathbb {L} _{\\textsf {EL}}:=\\{{\\textsf {K}},{\\textsf {KD45}},{\\textsf {S4}},{\\textsf {S4.2}},{\\textsf {S4.3}},{\\textsf {S4.3.2}},{\\textsf {S4.4}},{\\textsf {S5}}\\}}\n  \n.\nMoreover, for all \n  \n    \n      \n        \n          \n            H\n          \n        \n        ∈\n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}\\in \\mathbb {L} _{\\textsf {EL}}}\n  \n, we define the proof system \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n by adding the following axiom schemes and rules of inference to those of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n. For all \n  \n    \n      \n        A\n        ⊆\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle A\\subseteq AGTS}\n  \n,\n\nThe relative strength of the proof systems for knowledge is as follows:\n\n  \n    \n      \n        \n          \n            S4\n          \n        \n        ⊂\n        \n          \n            S4.2\n          \n        \n        ⊂\n        \n          \n            S4.3\n          \n        \n        ⊂\n        \n          \n            S4.3.2\n          \n        \n        ⊂\n        \n          \n            S4.4\n          \n        \n        ⊂\n        \n          \n            S5\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\textsf {S4}}\\subset {\\textsf {S4.2}}\\subset {\\textsf {S4.3}}\\subset {\\textsf {S4.3.2}}\\subset {\\textsf {S4.4}}\\subset {\\textsf {S5}}.}\n  \n\nSo, all the theorems of \n  \n    \n      \n        \n          \n            S4.2\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.2}}}\n  \n are also theorems of \n  \n    \n      \n        \n          \n            S4.3\n          \n        \n        ,\n        \n          \n            S4.3.2\n          \n        \n        ,\n        \n          \n            S4.4\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.3}},{\\textsf {S4.3.2}},{\\textsf {S4.4}}}\n  \n and \n  \n    \n      \n        \n          \n            S5\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S5}}}\n  \n. Many philosophers claim that in the most general cases, the logic of knowledge is \n  \n    \n      \n        \n          \n            S4.2\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.2}}}\n  \n or \n  \n    \n      \n        \n          \n            S4.3\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.3}}}\n  \n. Typically, in computer science and in many of the theories developed in artificial intelligence, the logic of belief (doxastic logic) is taken to be \n  \n    \n      \n        \n          \n            KD45\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {KD45}}}\n  \n and the logic of knowledge (epistemic logic) is taken to be \n  \n    \n      \n        \n          \n            S5\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S5}}}\n  \n, even if \n  \n    \n      \n        \n          \n            S5\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S5}}}\n  \n is only suitable for situations where the agents do not have mistaken beliefs. \n  \n    \n      \n        \n          \n            Br\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {Br}}}\n  \n has been propounded by Floridi as the logic of the notion of 'being informed’ which mainly differs from the logic of knowledge by the absence of introspection for the agents.\nFor all \n  \n    \n      \n        \n          \n            H\n          \n        \n        ∈\n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}\\in \\mathbb {L} _{\\textsf {EL}}}\n  \n, the class of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n–models or \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n–models is the class of epistemic models whose accessibility relations satisfy the properties listed above defined by the axioms of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n or \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n. Then, for all \n  \n    \n      \n        \n          \n            H\n          \n        \n        ∈\n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}\\in \\mathbb {L} _{\\textsf {EL}}}\n  \n, \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n is sound and strongly complete for \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}}\n  \n w.r.t. the class of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n–models, and \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n is sound and strongly complete for \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}^{\\textsf {C}}}\n  \n w.r.t. the class of \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n–models.\n\nDecidability and Complexity\nThe satisfiability problem for all the logics introduced is decidable. We list below the computational complexity of the satisfiability problem for each of them. Note that it becomes linear in time if there are only finitely many propositional letters in the language. For \n  \n    \n      \n        n\n        ≥\n        2\n      \n    \n    {\\displaystyle n\\geq 2}\n  \n, if we restrict to finite nesting, then the satisfiability problem is NP-complete for all the modal logics considered. If we then further restrict the language to having only finitely many primitive propositions, the complexity goes down to linear in time in all cases.\n\nThe computational complexity of the model checking problem is in P in all cases.\n\nAdding Dynamics\nDynamic Epistemic Logic (DEL) is a logical framework for modeling epistemic situations involving several agents, and changes that occur to these situations as a result of incoming information or more generally incoming action. The methodology of DEL is such that it splits the task of representing the agents’ beliefs and knowledge into three parts:\n\nOne represents their beliefs about an initial situation thanks to an epistemic model;\nOne represents their beliefs about an event taking place in this situation thanks to an event model;\nOne represents the way the agents update their beliefs about the situation after (or during) the occurrence of the event thanks to a product update.\nTypically, an informative event can be a public announcement to all the agents of a formula \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n: this public announcement and correlative update constitute the dynamic part. However, epistemic events can be much more complex than simple public announcement, including hiding information for some of the agents, cheating, lying, bluffing, etc. This complexity is dealt with when we introduce the notion of event model. We will first focus on public announcements to get an intuition of the main underlying ideas of DEL.\n\nPublic Events\nIn this section, we assume that all events are public. We start by giving a concrete example where DEL can be used, to better understand what is going on. This example is called the muddy children puzzle. Then, we will present a formalization of this puzzle in a logic called Public Announcement Logic (PAL). The muddy children puzzle is one of the most well known puzzles that played a role in the development of DEL. Other significant puzzles include the sum and product puzzle, the Monty Hall dilemma, the Russian cards problem, the two envelopes problem, Moore's paradox, the hangman paradox, etc.\nMuddy Children Example:\nWe have two children, A and B, both dirty. A can see B but not himself, and B can see A but not herself. Let \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n be the proposition stating that A is dirty, and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n be the proposition stating that B is dirty.\n\nWe represent the initial situation by the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            N\n          \n        \n        ,\n        s\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {N}},s)}\n  \n represented below, where relations between worlds are equivalence relations. States \n  \n    \n      \n        s\n        ,\n        t\n        ,\n        u\n        ,\n        v\n      \n    \n    {\\displaystyle s,t,u,v}\n  \n intuitively represent possible worlds, a proposition (for example \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n) satisfiable at one of these worlds intuitively means that in the corresponding possible world, the intuitive interpretation of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (A is dirty) is true. The links between worlds labelled by agents (A or B) intuitively express a notion of indistinguishability for the agent at stake between two possible worlds. For example, the link between \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n labelled by A intuitively means that A can not distinguish the possible world \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n from \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n and vice versa. Indeed, A cannot see himself, so he cannot distinguish between a world where he is dirty and one where he is not dirty. However, he can distinguish between worlds where B is dirty or not because he can see B. With this intuitive interpretation we are brought to assume that our relations between worlds are equivalence relations.\nNow, suppose that their father comes and announces that at least one is dirty (formally, \n  \n    \n      \n        p\n        ∨\n        q\n      \n    \n    {\\displaystyle p\\vee q}\n  \n). Then we update the model and this yields the pointed epistemic model represented below. What we actually do is suppressing the worlds where the content of the announcement is not fulfilled. In our case this is the world where \n  \n    \n      \n        ¬\n        p\n      \n    \n    {\\displaystyle \\neg p}\n  \n and \n  \n    \n      \n        ¬\n        q\n      \n    \n    {\\displaystyle \\neg q}\n  \n are true. This suppression is what we call the update. We then get the model depicted below. As a result of the announcement, both A and B do know that at least one of them is dirty. We can read this from the epistemic model.\nNow suppose there is a second (and final) announcement that says that neither knows they are dirty (an announcement can express facts about the situation as well as epistemic facts about the knowledge held by the agents). We then update similarly the model by suppressing the worlds which do not satisfy the content of the announcement, or equivalently by keeping the worlds which do satisfy the announcement. This update process thus yields the pointed epistemic model represented below. By interpreting this model, we get that A and B both know that they are dirty, which seems to contradict the content of the announcement. However, if we assume that A and B are both perfect reasoners and that this is common knowledge among them, then this inference makes perfect sense.\n\nPublic announcement logic (PAL):\nWe present the syntax and semantic of Public Announcement Logic (PAL), which combines features of epistemic logic and propositional dynamic logic.\nWe define the language \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n inductively by the following grammar in BNF:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n        :\n        ϕ\n         \n         \n        ::=\n         \n         \n        p\n         \n        ∣\n         \n        ¬\n        ϕ\n         \n        ∣\n         \n        (\n        ϕ\n        ∧\n        ϕ\n        )\n         \n        ∣\n         \n        \n          K\n          \n            j\n          \n        \n        ϕ\n         \n        ∣\n         \n        [\n        ϕ\n        !\n        ]\n        ϕ\n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}:\\phi ~~::=~~p~\\mid ~\\neg \\phi ~\\mid ~(\\phi \\land \\phi )~\\mid ~K_{j}\\phi ~\\mid ~[\\phi !]\\phi }\n  \n\nwhere \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n.\nThe language \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n is interpreted over epistemic models. The truth conditions for the connectives of the epistemic language are the same as in epistemic logic (see above). The truth condition for the new dynamic action modality \n  \n    \n      \n        [\n        ψ\n        !\n        ]\n        ϕ\n      \n    \n    {\\displaystyle [\\psi !]\\phi }\n  \n is defined as follows:\n\nwhere \n  \n    \n      \n        \n          \n            \n              M\n            \n          \n          \n            ψ\n          \n        \n        :=\n        (\n        \n          W\n          \n            ψ\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            ψ\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n          \n            ψ\n          \n        \n        ,\n        \n          I\n          \n            ψ\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}^{\\psi }:=(W^{\\psi },R_{1}^{\\psi },\\ldots ,R_{n}^{\\psi },I^{\\psi })}\n  \n with\n\n  \n    \n      \n        \n          W\n          \n            ψ\n          \n        \n        :=\n        {\n        w\n        ∈\n        W\n        ;\n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        ψ\n        }\n      \n    \n    {\\displaystyle W^{\\psi }:=\\{w\\in W;{\\mathcal {M}},w\\models \\psi \\}}\n  \n,\n\n  \n    \n      \n        \n          R\n          \n            j\n          \n          \n            ψ\n          \n        \n        :=\n        \n          R\n          \n            j\n          \n        \n        ∩\n        (\n        \n          W\n          \n            ψ\n          \n        \n        ×\n        \n          W\n          \n            ψ\n          \n        \n        )\n      \n    \n    {\\displaystyle R_{j}^{\\psi }:=R_{j}\\cap (W^{\\psi }\\times W^{\\psi })}\n  \n for all \n  \n    \n      \n        j\n        ∈\n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle j\\in \\{1,\\ldots ,n\\}}\n  \n and\n\n  \n    \n      \n        \n          I\n          \n            ψ\n          \n        \n        (\n        w\n        )\n        :=\n        I\n        (\n        w\n        )\n        \n          \n            ~for~all~\n          \n        \n        w\n        ∈\n        \n          W\n          \n            ψ\n          \n        \n      \n    \n    {\\displaystyle I^{\\psi }(w):=I(w){\\textrm {~for~all~}}w\\in W^{\\psi }}\n  \n.\n\nThe formula \n  \n    \n      \n        [\n        ψ\n        !\n        ]\n        ϕ\n      \n    \n    {\\displaystyle [\\psi !]\\phi }\n  \n intuitively means that after a truthful announcement of \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n, \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds. A public announcement of a proposition \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n changes the current epistemic model like in the figure below.\nThe proof system \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{PAL}}\n  \n defined below is sound and strongly complete for \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n w.r.t. the class of all pointed epistemic models.\n\nThe axioms Red 1 - Red 4 are called reduction axioms because they allow to reduce any formula of \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n to a provably equivalent formula of \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{EL}}\n  \n in \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{PAL}}\n  \n. The formula \n  \n    \n      \n        [\n        q\n        !\n        ]\n        K\n        q\n      \n    \n    {\\displaystyle [q!]Kq}\n  \n is a theorem provable in \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{PAL}}\n  \n. It states that after a public announcement of \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n, the agent knows that \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n holds.\nPAL is decidable, its model checking problem is solvable in polynomial time and its satisfiability problem is PSPACE-complete.\nMuddy children puzzle formalized with PAL:\nHere are some of the statements that hold in the muddy children puzzle formalized in PAL.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        p\n        ∧\n        q\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models p\\land q}\n  \n\n'In the initial situation, A is dirty and B is dirty'.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        (\n        ¬\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n          \n            B\n          \n        \n        ¬\n        q\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models (\\neg K_{A}p\\land \\neg K_{A}\\neg p)\\land (\\neg K_{B}q\\land \\neg K_{B}\\neg q)}\n  \n\n'In the initial situation, A does not know whether he is dirty and B neither'.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        [\n        p\n        ∨\n        q\n        !\n        ]\n        (\n        \n          K\n          \n            A\n          \n        \n        (\n        p\n        ∨\n        q\n        )\n        ∧\n        \n          K\n          \n            B\n          \n        \n        (\n        p\n        ∨\n        q\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models [p\\vee q!](K_{A}(p\\vee q)\\land K_{B}(p\\vee q))}\n  \n\n'After the public announcement that at least one of the children A and B is dirty, both of them know that at least one of them is dirty'. However:\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        [\n        p\n        ∨\n        q\n        !\n        ]\n        (\n        (\n        ¬\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n          \n            B\n          \n        \n        ¬\n        q\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models [p\\vee q!]((\\neg K_{A}p\\land \\neg K_{A}\\neg p)\\land (\\neg K_{B}q\\land \\neg K_{B}\\neg q))}\n  \n\n'After the public announcement that at least one of the children A and B is dirty, they still do not know that they are dirty'. Moreover:\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        [\n        p\n        ∨\n        q\n        !\n        ]\n        [\n        (\n        ¬\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n          \n            B\n          \n        \n        ¬\n        q\n        )\n        !\n        ]\n        (\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        \n          K\n          \n            B\n          \n        \n        q\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models [p\\vee q!][(\\neg K_{A}p\\land \\neg K_{A}\\neg p)\\land (\\neg K_{B}q\\land \\neg K_{B}\\neg q)!](K_{A}p\\land K_{B}q)}\n  \n\n'After the successive public announcements that at least one of the children A and B is dirty and that they still do not know whether they are dirty, A and B then both know that they are dirty'.\nIn this last statement, we see at work an interesting feature of the update process: a formula is not necessarily true after being announced. That is what we technically call “self-persistence” and this problem arises for epistemic formulas (unlike propositional formulas). One must not confuse the announcement and the update induced by this announcement, which might cancel some of the information encoded in the announcement.\n\nArbitrary Events\nIn this section, we assume that events are not necessarily public and we focus on items 2 and 3 above, namely on how to represent events and on how to update an epistemic model with such a representation of events by means of a product update.\n\nEvent Model\nEpistemic models are used to model how agents perceive the actual world. Their perception can also be described in terms of knowledge and beliefs about the world and about the other agents’ beliefs. The insight of the DEL approach is that one can describe how an event is perceived by the agents in a very similar way. Indeed, the agents’ perception of an event can also be described in terms of knowledge and beliefs. For example, the private announcement of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n to \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n that her card is red can also be described in terms of knowledge and beliefs: while \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n tells \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n that her card is red (event \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n) \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n believes that nothing happens (event \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n). This leads to define the notion of event model whose definition is very similar to that of an epistemic model.\nA pointed event model \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n represents how the actual event represented by \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n is perceived by the agents. Intuitively, \n  \n    \n      \n        f\n        ∈\n        \n          R\n          \n            j\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle f\\in R_{j}(e)}\n  \n means that while the possible event represented by \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n is occurring, agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers possible that the possible event represented by \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is actually occurring.\nAn event model is a tuple \n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        (\n        \n          W\n          \n            α\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            α\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            m\n          \n          \n            α\n          \n        \n        ,\n        \n          I\n          \n            α\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {E}}=(W^{\\alpha },R_{1}^{\\alpha },\\ldots ,R_{m}^{\\alpha },I^{\\alpha })}\n  \n where:\n\n  \n    \n      \n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle W^{\\alpha }}\n  \n is a non-empty set of possible events,\n\n  \n    \n      \n        \n          R\n          \n            j\n          \n          \n            α\n          \n        \n        ⊆\n        \n          W\n          \n            α\n          \n        \n        ×\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle R_{j}^{\\alpha }\\subseteq W^{\\alpha }\\times W^{\\alpha }}\n  \n is a binary relation called an accessibility relation on \n  \n    \n      \n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle W^{\\alpha }}\n  \n, for each \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n,\n\n  \n    \n      \n        \n          I\n          \n            α\n          \n        \n        :\n        \n          W\n          \n            α\n          \n        \n        →\n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle I^{\\alpha }:W^{\\alpha }\\rightarrow {\\mathcal {L}}_{\\textsf {EL}}}\n  \n is a function called the precondition function assigning to each possible event a formula of \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}}\n  \n.\n\n  \n    \n      \n        \n          R\n          \n            j\n          \n          \n            α\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle R_{j}^{\\alpha }(e)}\n  \n denotes the set \n  \n    \n      \n        {\n        f\n        ∈\n        \n          W\n          \n            α\n          \n        \n        ;\n        (\n        e\n        ,\n        f\n        )\n        ∈\n        \n          R\n          \n            j\n          \n          \n            α\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{f\\in W^{\\alpha };(e,f)\\in R_{j}^{\\alpha }\\}}\n  \n .We write \n  \n    \n      \n        e\n        ∈\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle e\\in {\\mathcal {E}}}\n  \n for \n  \n    \n      \n        e\n        ∈\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle e\\in W^{\\alpha }}\n  \n, and \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n is called a pointed event model (\n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n often represents the actual event).\nCard Example:\nLet us resume the card example and assume that players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their card to each other. As it turns out, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n noticed that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n showed her card to \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n but did not notice that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n did so to \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n know this. This event is represented below in the event model \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n.\nThe possible event \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n corresponds to the actual event ‘players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their and cards respectively to each other’ (with precondition \n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}\\land {\\color {green}{B}}}\n  \n), \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n stands for the event ‘player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her green card’ (with precondition \n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {green}{A}}}\n  \n) and \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n stands for the atomic event ‘player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card’ (with precondition \n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}}\n  \n). Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their cards to each other, players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n know this and consider it possible, while player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n considers possible that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card and also considers possible that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her green card, since he does not know her card. In fact, that is all that player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n considers possible because she did not notice that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n showed her card.\n\nAnother example of event model is given below. This second example corresponds to the event whereby Player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card publicly to everybody. Player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card, players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n ‘know’ it, players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n ‘know’ that each of them ‘knows’ it, etc. In other words, there is common knowledge among players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card.\n\nProduct Update\nThe DEL product update is defined below. This update yields a new pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)}\n  \n representing how the new situation which was previously represented by \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n is perceived by the agents after the occurrence of the event represented by \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n.\nLet \n  \n    \n      \n        \n          \n            M\n          \n        \n        =\n        (\n        W\n        ,\n        \n          R\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n        \n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}=(W,R_{1},\\ldots ,R_{n},I)}\n  \n be an epistemic model and let \n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        (\n        \n          W\n          \n            α\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            α\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n          \n            α\n          \n        \n        ,\n        \n          I\n          \n            α\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {E}}=(W^{\\alpha },R_{1}^{\\alpha },\\ldots ,R_{n}^{\\alpha },I^{\\alpha })}\n  \n be an event model. The product update of \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n and \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n is the epistemic model \n  \n    \n      \n        \n          \n            M\n          \n        \n        ⊗\n        \n          \n            \n              E\n            \n          \n        \n        =\n        (\n        \n          W\n          \n            ⊗\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            ⊗\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n          \n            ⊗\n          \n        \n        ,\n        \n          I\n          \n            ⊗\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}\\otimes {\\mathcal {\\mathcal {E}}}=(W^{\\otimes },R_{1}^{\\otimes },\\ldots ,R_{n}^{\\otimes },I^{\\otimes })}\n  \n defined as follows: for all \n  \n    \n      \n        v\n        ∈\n        W\n      \n    \n    {\\displaystyle v\\in W}\n  \n and all \n  \n    \n      \n        f\n        ∈\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle f\\in W^{\\alpha }}\n  \n,\n\nIf \n  \n    \n      \n        w\n        ∈\n        W\n      \n    \n    {\\displaystyle w\\in W}\n  \n and \n  \n    \n      \n        e\n        ∈\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle e\\in W^{\\alpha }}\n  \n are such that \n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        \n          I\n          \n            α\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models I^{\\alpha }(e)}\n  \n then \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)}\n  \n denotes the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ⊗\n        \n          \n            E\n          \n        \n        ,\n        (\n        w\n        ,\n        e\n        )\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}}\\otimes {\\mathcal {E}},(w,e))}\n  \n. This definition of the product update is conceptually grounded.\nCard Example:\nAs a result of the first event described above (Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their cards to each other in front of player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n), the agents update their beliefs. We get the situation represented in the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)}\n  \n below. In this pointed epistemic model, the following statement holds: \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n        ⊨\n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              B\n            \n          \n        \n        )\n        ∧\n        \n          K\n          \n            C\n          \n        \n        ¬\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              B\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)\\models ({\\color {green}{B}}\\land K_{A}{\\color {green}{B}})\\land K_{C}\\neg K_{A}{\\color {green}{B}}.}\n  \n It states that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n knows that player \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the card but player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n 'believes' that it is not the case.\n\nThe result of the second event is represented below. In this pointed epistemic model, the following statement holds: \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            F\n          \n        \n        ,\n        e\n        )\n        ⊨\n        \n          C\n          \n            {\n            B\n            ,\n            C\n            }\n          \n        \n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          \n            \n              C\n            \n          \n        \n        )\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          \n            \n              C\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {F}},e)\\models C_{\\{B,C\\}}({\\color {red}{A}}\\land {\\color {green}{B}}\\land {\\color {blue}{C}})\\land \\neg K_{A}({\\color {green}{B}}\\land {\\color {blue}{C}})}\n  \n. It states that there is common knowledge among \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n that they know the true state of the world (namely \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n has the red card, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the green card and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the blue card), but \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n does not know it.\n\nBased on these three components (epistemic model, event model and product update), Baltag, Moss and Solecki defined a general logical language inspired from the logical language of propositional dynamic logic to reason about information and knowledge change.\n\nSee also\nEpistemic logic\nEpistemology\nLogic in computer science\nModal logic\n\nNotes\nReferences\nvan Benthem, Johan (2011). Logical Dynamics of Information and Interaction. Cambridge University Press. ISBN 978-0521873970.\nHans, van Ditmarsch; Halpern, Joseph; van der Hoek, Wiebe; Kooi, Barteld (2015). Handbook of Epistemic Logic. London: College publication. ISBN 978-1848901582.\nvan Ditmarsch, Hans, van der Hoek, Wiebe, and Kooi, Barteld (2007). Dynamic Epistemic Logic. Ithaca: volume 337 of Synthese library. Springer. ISBN 978-1-4020-5839-4.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nFagin, Ronald; Halpern, Joseph; Moses, Yoram; Vardi, Moshe (2003). Reasoning about Knowledge. Cambridge: MIT Press. ISBN 978-0-262-56200-3. A classic reference.\nHintikka, Jaakko (1962). Knowledge and Belief - An Introduction to the Logic of the Two Notions. Ithaca: Cornell University Press. ISBN 978-1-904987-08-6. {{cite book}}: ISBN / Date incompatibility (help).\n\nExternal links\nBaltag, Alexandru; Renne, Bryan. \"Dynamic Epistemic Logic\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nvan Ditmarsch, Hans; van der Hoek, Wiebe; Kooi, Barteld. \"Dynamic Epistemic Logic\". Internet Encyclopedia of Philosophy.\nHendricks, Vincent; Symons, John. \"Epistemic Logic\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nGarson, James. \"Modal logic\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.",
        "url": "https://en.wikipedia.org/wiki/Dynamic_epistemic_logic"
    },
    {
        "title": "Elements of AI",
        "text": "Elements of AI is a massive open online course (MOOC) teaching the basics of artificial intelligence. The course, originally launched in 2018, is designed and organized by the University of Helsinki and learning technology company MinnaLearn. The course includes modules on machine learning, neural networks, the philosophy of artificial intelligence, and using artificial intelligence to solve problems. It consists of two parts: Introduction to AI and its sequel, Building AI, that was released in late 2020.\nUniversity of Helsinki's computer science department is known as the alma mater of Linus Torvalds, a Finnish-American software engineer who is the creator of the Linux kernel, which is the kernel for Linux operating systems.\n\nEU’s AI pledge\nThe government of Finland has pledged to offer the course for all EU citizens by the end of 2021, as the course is made available in all the official EU languages. The initiative was launched as part of Finland's Presidency of the Council of the European Union in 2019, with the European Commission providing translations of the course materials.\nIn 2017, Finland launched an AI strategy to stay competitive in the field of AI amid growing competition between China and the United States. With the support of private companies and the government, Finland's now-realized goal was to get 1 percent of its citizens to participate in Elements of AI.\nOther governments have also given their support to the course. For instance, Germany's Federal Minister for Economic Affairs and Energy Peter Altmeier has encouraged citizens to take part in the course to help Germany gain a competitive advantage in AI. Sweden's Minister for Energy and Minister for Digital Development Anders Ygeman has said that Sweden aims to teach 1 percent of its population the basics of AI like Finland has.\n\nParticipants\nElements of AI had enrolled more than 1 million students from more than 110 countries by May 2023. A quarter of the course's participants are aged 45 and over, and some 40 percent are women. Among Nordic participants, the share of women is nearly 60 percent.\nIn September 2022, the course was available in Finnish, Swedish, Estonian, English, German, Latvian, Norwegian, French, Belgian, Czech, Greek, Slovakian, Slovenian, Latvian, Lithuanian, Portuguese, Spanish, Irish, Icelandic, Maltese, Croatian, Romanian, Italian, Dutch, Polish, and Danish.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Elements_of_AI"
    },
    {
        "title": "Embodied agent",
        "text": "In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. A branch of artificial intelligence focuses on empowering such agents to interact autonomously with human beings and the environment. Mobile robots are one example of physically embodied agents; Ananova and Microsoft Agent are examples of graphically embodied agents. Embodied conversational agents are embodied agents (usually with a graphical front-end as opposed to a robotic body) that are capable of engaging in conversation with one another and with humans employing the same verbal and nonverbal means that humans do (such as gesture, facial expression, and so forth).\n\nEmbodied conversational agents\nEmbodied conversational agents are a form of intelligent user interface. Graphically embodied agents aim to unite gesture, facial expression and speech to enable face-to-face communication with users, providing a powerful means of human-computer interaction.\n\nAdvantages\nFace-to-face communication allows communication protocols that give a much richer communication channel than other means of communicating. It enables pragmatic communication acts such as conversational turn-taking, facial expression of emotions, information structure and emphasis, visualisation and iconic gestures, and orientation in a three-dimensional environment. This communication takes place through both verbal and non-verbal channels such as gaze, gesture, spoken intonation and body posture.\nResearch has found that users prefer a non-verbal visual indication of an embodied system's internal state to a verbal indication, demonstrating the value of additional non-verbal communication channels. As well as this, the face-to-face communication involved in interacting with an embodied agent can be conducted alongside another task without distracting the human participants, instead improving the enjoyment of such an interaction. Furthermore, the use of an embodied presentation agent results in improved recall of the presented information.\nEmbodied agents also provide a social dimension to the interaction. Humans willingly ascribe social awareness to computers, and thus interaction with embodied agents follows social conventions, similar to human to human interactions. This social interaction both raises the believability and perceived trustworthiness of agents, and increases the user's engagement with the system. Rickenberg and Reeves found that the presence of an embodied agent on a website increased the level of user trust in that website, but also increased users' anxiety and affected their performance, as if they were being watched by a real human. Another effect of the social aspect of agents is that presentations given by an embodied agent are perceived as being more entertaining and less difficult than similar presentations given without an agent. Research shows that perceived enjoyment, followed by perceived usefulness and ease of use, is the major factor influencing user adoption of embodied agents.\nA study in January 2004 by Byron Reeves at Stanford demonstrated how digital characters could \"enhance online experiences\" through explaining how virtual characters essentially add a sense of relatability to the user experience and make it more approachable. This increase in likability in turn helps make the products better, which benefits both the end users and those creating the product.\n\nApplications\nThe rich style of communication that characterises human conversation makes conversational interaction with embodied conversational agents ideal for many non-traditional interaction tasks. A familiar application of graphically embodied agents is computer games; embodied agents are ideal for this setting because the richer communication style makes interacting with the agent enjoyable. Embodied conversational agents have also been used in virtual training environments, portable personal navigation guides, interactive fiction and storytelling systems, interactive online characters and automated presenters and commentators.\nMajor virtual assistants like Siri, Amazon Alexa and Google Assistant do not come with any visual embodied representation, which is believed to limit\nthe sense of human presence by users.\nThe U.S. Department of Defense utilizes a software agent called SGT STAR on U.S. Army-run Web sites and Web applications for site navigation, recruitment and propaganda purposes. Sgt. Star is run by the Army Marketing and Research Group, a division operated directly from The Pentagon. Sgt. Star is based upon the ActiveSentry technology developed by Next IT, a Washington-based information technology services company. Other such bots in the Sgt. Star \"family\" are utilized by the Federal Bureau of Investigation and the Central Intelligence Agency for intelligence gathering purposes.\n\nSee also\nReferences\nFurther reading\nBates, Joseph (1994), \"The Role of Emotion in Believable Agents\", Communications of the ACM, 37 (7): 122–125, CiteSeerX 10.1.1.47.8186, doi:10.1145/176789.176803, S2CID 207178664.\nCassell, Justin (2000), \"More than Just Another Pretty Face: Embodied Conversational Interface Agents\" (PDF), Communications of the ACM, 43 (4): 70–78, doi:10.1145/332051.332075, S2CID 10691309.\nRuebsamen, Gene (2002), Intelligent Agent, M.S. Thesis. California State University, Long Beach: U.S.A.\n\nExternal links\n\"AI Makes Strides in Virtual Worlds More Like Our Own\". Quanta Magazine. June 24, 2022.",
        "url": "https://en.wikipedia.org/wiki/Embodied_agent"
    },
    {
        "title": "Embodied cognitive science",
        "text": "Embodied cognitive science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity; the formation of a common set of general principles of intelligent behavior; and the experimental use of robotic agents in controlled environments.\n\nContributors\nEmbodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. Contributors to the field include:\n\nFrom the perspective of neuroscience, Gerald Edelman of the Neurosciences Institute at La Jolla, Francisco Varela of CNRS in France, and J. A. Scott Kelso of Florida Atlantic University\nFrom the perspective of psychology, Lawrence Barsalou, Michael Turvey, Vittorio Guidano and Eleanor Rosch\nFrom the perspective of linguistics, Gilles Fauconnier, George Lakoff, Mark Johnson, Leonard Talmy and Mark Turner\nFrom the perspective of language acquisition, Eric Lenneberg and Philip Rubin at Haskins Laboratories\nFrom the perspective of anthropology, Edwin Hutchins, Bradd Shore, James Wertsch and Merlin Donald.\nFrom the perspective of autonomous agent design, early work is sometimes attributed to Rodney Brooks or Valentino Braitenberg\nFrom the perspective of artificial intelligence, Understanding Intelligence by Rolf Pfeifer and Christian Scheier or How the Body Shapes the Way We Think, by Rolf Pfeifer and Josh C. Bongard\nFrom the perspective of philosophy, Andy Clark, Dan Zahavi, Shaun Gallagher, and Evan Thompson\nIn 1950, Alan Turing proposed that a machine may need a human-like body to think and speak:\n\nIt can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then teach it to understand and speak English. That process could follow the normal teaching of a child. Things would be pointed out and named, etc. Again, I do not know what the right answer is, but I think both approaches should be tried.\n\nTraditional cognitive theory\nEmbodied cognitive science is an alternative theory to cognition in which it minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism's body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit finds semantic meaning. Thus, an appropriate output is produced. For example, a human's sensory organs are its input devices, and the stimuli obtained from the external environment are fed into the nervous system which serves as the processing unit. From here, the nervous system is able to read the sensory information because it follows a syntactic structure, thus an output is created. This output then creates bodily motions and brings forth behavior and cognition. Of particular note is that cognition is sealed away in the brain, meaning that mental cognition is cut off from the external world and is only possible by the input of sensory information.\n\nThe embodied cognitive approach\nEmbodied cognitive science differs from the traditionalist approach in that it denies the input-output system. This is chiefly due to the problems presented by the Homunculus argument, which concluded that semantic meaning could not be derived from symbols without some kind of inner interpretation. If some little man in a person's head interpreted incoming symbols, then who would interpret the little man's inputs? Because of the specter of an infinite regress, the traditionalist model began to seem less plausible. Thus, embodied cognitive science aims to avoid this problem by defining cognition in three ways.\n\nPhysical attributes of the body\nThe first aspect of embodied cognition examines the role of the physical body, particularly how its properties affect its ability to think. This part attempts to overcome the symbol manipulation component that is a feature of the traditionalist model. Depth perception, for instance, can be better explained under the embodied approach due to the sheer complexity of the action. Depth perception requires that the brain detect the disparate retinal images obtained by the distance of the two eyes. In addition, body and head cues complicate this further. When the head is turned in a given direction, objects in the foreground will appear to move against objects in the background. From this, it is said that some kind of visual processing is occurring without the need of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary. \nA more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible auditory acuity. Also relevant is the amount of density in between the ears, for the strength of the frequency wave alters as it passes through a given medium. The brain's auditory system takes these factors into account as it process information, but again without any need for a symbolic manipulation system. This is because the distance between the ears for example does not need symbols to represent it. The distance itself creates the necessary opportunity for greater auditory acuity. The amount of density between the ears is similar, in that it is the actual amount itself that simply forms the opportunity for frequency alteration. Thus under consideration of the physical properties of the body, a symbolic system is unnecessary and an unhelpful metaphor.\n\nThe body's role in the cognitive process\nThe second aspect draws heavily from George Lakoff's and Mark Johnson's work on concepts. They argued that humans use metaphors whenever possible to better explain their external world. Humans also have a basic stock of concepts in which other concepts can be derived from. These basic concepts include spatial orientations such as up, down, front, and back. Humans can understand what these concepts mean because they can directly experience them from their own bodies. For example, because human movement revolves around standing erect and moving the body in an up-down motion, humans innately have these concepts of up and down. Lakoff and Johnson contend this is similar with other spatial orientations such as front and back too. As mentioned earlier, these basic stocks of spatial concepts are the basis in which other concepts are constructed. Happy and sad for instance are seen now as being up or down respectively. When someone says they are feeling down, what they are really saying is that they feel sad for example. Thus the point here is that true understanding of these concepts is contingent on whether one can have an understanding of the human body. So the argument goes that if one lacked a human body, they could not possibly know what up or down could mean, or how it could relate to emotional states.\n\n[I]magine a spherical being living outside of any gravitational field, with no knowledge or imagination of any other kind of experience. What could UP possibly mean to such a being?\nWhile this does not mean that such beings would be incapable of expressing emotions in other words, it does mean that they would express emotions differently from humans. Human concepts of happiness and sadness would be different because human would have different bodies. So then an organism's body directly affects how it can think, because it uses metaphors related to its body as the basis of concepts.\n\nInteraction of local environment\nA third component of the embodied approach looks at how agents use their immediate environment in cognitive processing. Meaning, the local environment is seen as an actual extension of the body's cognitive process. The example of a personal digital assistant (PDA) is used to better imagine this. Echoing functionalism (philosophy of mind), this point claims that mental states are individuated by their role in a much larger system. So under this premise, the information on a PDA is similar to the information stored in the brain. So then if one thinks information in the brain constitutes mental states, then it must follow that information in the PDA is a cognitive state too. Consider also the role of pen and paper in a complex multiplication problem. The pen and paper are so involved in the cognitive process of solving the problem that it seems ridiculous to say they are somehow different from the process, in very much the same way the PDA is used for information like the brain. Another example examines how humans control and manipulate their environment so that cognitive tasks can be better performed. Leaving one's car keys in a familiar place so they aren't missed for instance, or using landmarks to navigate in an unfamiliar city. Thus, humans incorporate aspects of their environment to aid in their cognitive functioning.\n\nExamples of the value of embodied approach\nThe value of the embodiment approach in the context of cognitive science is perhaps best      explained by Andy Clark. He makes the claim that the brain alone should not be the single focus for the scientific study of cognition\n\nIt is increasingly clear that, in a wide variety of cases, the individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.\nThe following examples used by Clark will better illustrate how embodied thinking is becoming apparent     in scientific thinking.\n\nBluefin tuna\nThunnus, or tuna, long baffled conventional biologists with its incredible abilities to accelerate quickly and attain great speeds. A biological examination of the tuna shows that it should not be capable of such feats. However, an answer can be found when taking the tuna's embodied state into account. The bluefin tuna is able to take advantage of and exploit its local environment by finding naturally occurring currents to increase its speed. The tuna also uses its own physical body for this end as well, by utilizing its tailfin to create the necessary vortices and pressure so it can accelerate and maintain high speeds. Thus, the bluefin tuna is actively using its local environment for its own ends through the attributes of its physical body.\n\nRobots\nClark uses the example of the hopping robot constructed by Raibert and Hodgins to demonstrate further the value of the embodiment paradigm. These robots were essentially vertical cylinders with a single hopping foot. The challenge of managing the robot's behavior can be daunting because in addition to the intricacies of the program itself, there were also the mechanical matters regarding how the foot ought to be constructed so that it could hop. An embodied approach makes it easier to see that in order for this robot to function, it must be able to exploit its system to the fullest. That is, the robot's systems should be seen as having dynamic characteristics as opposed to the traditional view that it is merely a command center that just executes actions.\n\nVision\nClark distinguishes between two kinds of vision, animate and pure vision. Pure vision is an idea that is typically associated with classical artificial intelligence, in which vision is used to create a rich world model so that thought and reason can be used to fully explore the inner model. In other words, pure vision passively creates the external perceivable world so that the faculties of reason can be better used introspectively. Animate vision, by contrast, sees vision as the means by which real-time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark's example of going to the drugstore to buy some Kodak film. In one's mind, one is familiar with the Kodak logo and its trademark gold color. Thus, one uses incoming visual stimuli to navigate around the drugstore until one finds the film. Therefore, vision should not be seen as a passive system but rather an active retrieval device that intelligently uses sensory information and local environmental cues to perform specific real-world actions.\n\nAffordance\nInspired by the work of the American psychologist James J. Gibson, this next example emphasizes the importance of action-relevant sensory information, bodily movement, and local environment cues. These three concepts are unified by the concept of affordances, which are possibilities of action provided by the physical world to a given agent. These are in turn determined by the agent's physical body, capacities, and the overall action-related properties of the local environment as well. Clark uses the example of an outfielder in baseball to better illustrate the concept of affordance. Traditional computational models would claim that an outfielder attempting to catch a fly-ball can be calculated by variables such as the running speed of the outfielder and the arc of the baseball. However, Gibson's work shows that a simpler method is possible. The outfielder can catch the ball so long as they adjust their running speed so that the ball continually moves in a straight line in their field of vision. Note that this strategy uses various affordances that are contingent upon the success of the outfielder, including their physical body composition, the environment of the baseball field, and the sensory information obtained by the outfielder. \nClark points out here that the latter strategy of catching the ball as opposed to the former has significant implications for perception. The affordance approach proves to be non-linear because it relies upon spontaneous real-time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily. This is because they utilize environmental cues made possible by perceptual information that is actively used in the real-time by the agent.\n\nGeneral principles of intelligent behavior\nIn the formation of general principles of intelligent behavior, Pfeifer intended to be contrary to older principles given in traditional artificial intelligence.  The most dramatic difference is that the principles are applicable only to situated robotic agents in the real world, a domain where traditional artificial intelligence showed the least promise.\nPrinciple of cheap design and redundancy: Pfeifer realized that implicit assumptions made by engineers often substantially influence a control architecture's complexity.  This insight is reflected in discussions of the scalability problem in robotics.  The internal processing needed for some bad architectures can grow out of proportion to new tasks needed of an agent.  \n\nOne of the primary reasons for scalability problems is that the amount of programming and knowledge engineering that the robot designers have to perform grows very rapidly with the complexity of the robot's tasks. There is mounting evidence that pre-programming cannot be the solution to the scalability problem ... The problem is that programmers introduce too many hidden assumptions in the robot's code.\nThe proposed solutions are to have the agent exploit the inherent physics of its environment, to exploit the constraints of its niche, and to have agent morphology based on parsimony and the principle of Redundancy. Redundancy reflects the desire for the error-correction of signals afforded by duplicating like channels.  Additionally, it reflects the desire to exploit the associations between sensory modalities. (See redundant modalities).  In terms of design, this implies that redundancy should be introduced with respect not only to one sensory modality but to several.  It has been suggested that the fusion and transfer of knowledge between modalities can be the basis of reducing the size of the sense data taken from the real world.  This again addresses the scalability problem.\nPrinciple of parallel, loosely-coupled processes: An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense-Think-Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem.\nPrinciple of sensory-motor coordination: Ideally, internal mechanisms in an agent should give rise to things like memory and choice-making in an emergent fashion, rather than being prescriptively programmed from the beginning.   These kinds of things are allowed to emerge as the agent interacts with the environment. The motto is, build fewer assumptions into the agent's controller now, so that learning can be more robust and idiosyncratic in the future. \nPrinciple of ecological balance: This is more a theory than a principle, but its implications are widespread.  Its claim is that the internal processing of an agent cannot be made more complex unless there is a corresponding increase in complexity of the motors, limbs, and sensors of the agent. In other words, the extra complexity added to the brain of a simple robot will not create any discernible change in its behavior. The robot's morphology must already contain the complexity in itself to allow enough \"breathing room\" for more internal processing to develop.\nValue principle: This was the architecture developed in the Darwin III robot of Gerald Edelman. It relies heavily on connectionism.\n\nCritical responses\nTraditionalist response to local environment claim\nA traditionalist may argue that objects may be used to aid in cognitive processes, but this does not mean they are part of a cognitive system. Eyeglasses are used to aid in the visual process, but to say they are a part of a larger system would completely redefine what is meant by a visual system. However, supporters of the embodied approach could make the case that if objects in the environment play the functional role of mental states, then the items themselves should not be counted among the mental states.\nLars Ludwig explores mind extension further outlining its role in technology. He proposes a cognitive theory of 'extended artificial memory', which represents a theoretical update and extension of the  memory theories of Richard Semon.\n\nSee also\nReferences\nFurther reading\nBraitenberg, Valentino (1986). Vehicles: Experiments in Synthetic Psychology. Cambridge, MA: The MIT Press. ISBN 0-262-52112-1\nBrooks, Rodney A. (1999). Cambrian Intelligence: The Early History of the New AI. Cambridge, MA: The MIT Press. ISBN 0-262-52263-2\nEdelman, G.  Wider than the Sky (Yale University Press, 2004) ISBN 0-300-10229-1\nFowler, C., Rubin, P. E., Remez, R. E., & Turvey, M. T. (1980). Implications for speech production of a general theory of action. In B. Butterworth (Ed.), Language Production, Vol. I: Speech and Talk (pp. 373–420). New York: Academic Press. ISBN 0-12-147501-8\nLenneberg, Eric H. (1967). Biological Foundations of Language. John Wiley & Sons. ISBN 0-471-52626-6\nPfeifer, R. and Bongard  J. C., How the body shapes the way we think: a new view of intelligence (The MIT Press, 2007). ISBN 0-262-16239-3\n\nExternal links\nAI lectures from Tokyo hosted by Rolf Pfeifer\nsynthetic neural modelling in DARWIN IV\nSociety for the Simulation of Adaptive Behavior\nA platform for creating Embodied Cognitive Agents",
        "url": "https://en.wikipedia.org/wiki/Embodied_cognitive_science"
    },
    {
        "title": "Empowerment (artificial intelligence)",
        "text": "Empowerment in the field of artificial intelligence  formalises and quantifies (via information theory) the potential an agent perceives that it has to influence its environment. An agent which follows an empowerment maximising policy, acts to maximise future options (typically up to some limited horizon). Empowerment can be used as a (pseudo) utility function that depends only on information gathered from the local environment to guide action, rather than seeking an externally imposed goal, thus is a form of intrinsic motivation. \nThe empowerment formalism depends on a probabilistic model commonly used in artificial intelligence. An autonomous agent operates in the world by taking in sensory information and acting to change its state, or that of the environment, in a cycle of perceiving and acting known as the perception-action loop. Agent state and actions are modelled by random variables (\n  \n    \n      \n        S\n        :\n        s\n        ∈\n        \n          \n            S\n          \n        \n        ,\n        A\n        :\n        a\n        ∈\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle S:s\\in {\\mathcal {S}},A:a\\in {\\mathcal {A}}}\n  \n) and time (\n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n). The choice of action depends on the current state, and the future state depends on the choice of action, thus the perception-action loop unrolled in time forms a causal bayesian network.\n\nDefinition\nEmpowerment (\n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {E}}}\n  \n) is defined as the channel capacity (\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n) of the actuation channel of the agent, and is formalised as the maximal possible information flow between the actions of the agent and the effect of those actions some time later. Empowerment can be thought of as the future potential of the agent to affect its environment, as measured by its sensors.\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        :=\n        C\n        (\n        \n          A\n          \n            t\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n        ≡\n        \n          max\n          \n            p\n            (\n            \n              a\n              \n                t\n              \n            \n            )\n          \n        \n        I\n        (\n        \n          A\n          \n            t\n          \n        \n        ;\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathfrak {E}}:=C(A_{t}\\longrightarrow S_{t+1})\\equiv \\max _{p(a_{t})}I(A_{t};S_{t+1})}\n  \n\nIn a discrete time model, Empowerment can be computed for a given number of cycles into the future, which is referred to in the literature as 'n-step' empowerment. \n\n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        \n          A\n          \n            t\n          \n          \n            n\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        )\n        =\n        \n          max\n          \n            p\n            (\n            \n              a\n              \n                t\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                t\n                +\n                n\n                −\n                1\n              \n            \n            )\n          \n        \n        I\n        (\n        \n          A\n          \n            t\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          A\n          \n            t\n            +\n            n\n            −\n            1\n          \n        \n        ;\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathfrak {E}}(A_{t}^{n}\\longrightarrow S_{t+n})=\\max _{p(a_{t},...,a_{t+n-1})}I(A_{t},...,A_{t+n-1};S_{t+n})}\n  \n\nThe unit of empowerment depends on the logarithm base. Base 2 is commonly used in which case the unit is bits.\n\nContextual Empowerment\nIn general the choice of action (action distribution) that maximises empowerment varies from state to state. Knowing the empowerment of an agent in a specific state is useful, for example to construct an empowerment maximising policy. State-specific empowerment can be found using the more general formalism for 'contextual empowerment'. \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is a random variable describing the context (e.g. state).\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        \n          A\n          \n            t\n          \n          \n            n\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        \n          ∣\n        \n        C\n        )\n        =\n        \n          ∑\n          \n            c\n            \n              ∈\n            \n            C\n          \n        \n        p\n        (\n        c\n        )\n        \n          \n            E\n          \n        \n        (\n        \n          A\n          \n            t\n          \n          \n            n\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        \n          ∣\n        \n        C\n        =\n        c\n        )\n      \n    \n    {\\displaystyle {\\mathfrak {E}}(A_{t}^{n}\\longrightarrow S_{t+n}{\\mid }C)=\\sum _{c{\\in }C}p(c){\\mathfrak {E}}(A_{t}^{n}\\longrightarrow S_{t+n}{\\mid }C=c)}\n\nApplication\nEmpowerment maximisation can be used as a pseudo-utility function to enable agents to exhibit intelligent behaviour without requiring the definition of external goals, for example balancing a pole in a cart-pole balancing scenario where no indication of the task is provided to the agent. \nEmpowerment has been applied in studies of collective behaviour and in continuous domains. As is the case with Bayesian methods in general, computation of empowerment becomes computationally expensive as the number of actions and time horizon extends, but approaches to improve efficiency have led to usage in real-time control. Empowerment has been used for intrinsically motivated reinforcement learning agents playing video games, and in the control of underwater vehicles.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Empowerment_(artificial_intelligence)"
    },
    {
        "title": "Enterprise cognitive system",
        "text": "Enterprise cognitive systems (ECS) are part of a broader shift in computing, from a programmatic to a probabilistic approach, called cognitive computing. An Enterprise Cognitive System makes a new class of complex decision support problems computable, where the business context is ambiguous, multi-faceted, and fast-evolving, and what to do in such a situation is usually assessed today by the business user. An ECS is designed to synthesize a business context and link it to the desired outcome. It recommends evidence-based actions to help the end-user achieve the desired outcome. It does so by finding past situations similar to the current situation, and extracting the repeated actions that best influence the desired outcome.\nWhile general-purpose cognitive systems can be used for different outputs, prescriptive, suggestive, instructive, or simply entertaining, an enterprise cognitive system is focused on action, not insight, to help in assessing what to do in a complex situation.\n\nKey characteristics\nECS have to be:\n\nAdaptive: They must learn as information changes, and as goals and requirements evolve. They must resolve ambiguity and tolerate unpredictability. They must be engineered to feed on dynamic data in real time, or near real time. In the Enterprise, near-real time learning from data requires an agile information federation approach to ingest incremental data updates as they occur, and an unsupervised learning approach to ensure that new best practice is leveraged across the organization in a timely manner.\nInteractive: They must interact easily with users so that those users can define their needs comfortably. They may also interact with other processors, devices, and Cloud services, as well as with people. In the Enterprise, interactions are controlled via existing workflows and UIs. Therefore, embedding best practices directly into these existing interfaces, in the context of a specific step, is critical to ensure maximum end-user adoption.\nIterative and stateful: They must aid in defining a problem by asking questions or finding additional source input if a problem statement is ambiguous or incomplete. They must “remember” previous interactions in a process and return information that is suitable for the specific application at that point in time. In the Enterprise, business context is often structured by a business process, and therefore sufficiently data-rich to make relevant recommendations without significant iterations from the end-user. A stateful memory of overall interactions across communication channels is critical for understanding of context, as a static profile will not capture intent and outcome potential the way behavior does.\nContextual: They must understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate domain, regulations, user's profile, process, task and goal. They may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided). In the Enterprise, Context is fragmented and must be aggregated across data types, sources, and locations. In most business environments, such data is captured in existing enterprise information systems, and the effort is linked to quickly source and unify such information. It is rare to have to directly process sensor, audio or visual data in real-time as direct input into the enterprise cognitive system. Instead, these data types are captured by Enterprise Applications and pre-processed into a binary or text format prior to consumption by the System.\n\nBusiness applications powered by an ECS\nBottlenose – trends and brands monitoring\nCybereason – security threat monitoring\nDataminr – social media monitoring\n\n\n== Further reading ==",
        "url": "https://en.wikipedia.org/wiki/Enterprise_cognitive_system"
    },
    {
        "title": "Environmental impact of artificial intelligence",
        "text": "The environmental impact of artificial intelligence includes substantial energy consumption for training and using deep learning models, and the related carbon footprint and water usage. Some scientists have suggested that artificial intelligence (AI) may also provide solutions to environmental problems.\n\nCarbon footprint\nAI has a significant carbon footprint due to growing energy usage, especially due to training and usage. Researchers have argued that the carbon footprint of AI models during training should be considered when attempting to understand the impact of AI. One study suggested that by 2027, energy costs for AI could increase to 85–134 Twh, nearly 0.5% of all current electricity usage. Training large language models (LLMs) and other generative AI generally requires much more energy compared to running a single prediction on the trained model. Using a trained model repeatedly, though, may easily multiply the energy costs of predictions. The computation required to train the most advanced AI models doubles every 3.4 months on average, leading to exponential power usage and resulting carbon footprint. Additionally, artificial intelligence algorithms running in places predominantly using fossil fuels for energy will exert a much higher carbon footprint than places with cleaner energy sources. These models may be modified for less environmental impacts at the cost of accuracy, emphasizing the importance of finding the balance between accuracy and environmental impact.\nTraining a large AI model requires enormous amounts of energy. It is estimated that training a whole AI model produces around 626 000 lbs (283 Tons) of carbon dioxide. It is the equivalent of 300 round-trip flights between New York and San Francisco, or nearly 5 times the lifetime emissions of the average car.\nBERT, a language model trained in 2019, required \"the energy of a round-trip transcontinental flight\" to train. GPT-3 released 552 metric tons of carbon dioxide into the atmosphere during training, \"the equivalent of 123 gasoline-powered passenger vehicles driven for one year\". Much of the energy cost is due to inefficient model architectures and processors. One model named BLOOM, from Hugging Face, trained with more efficient chips and, therefore, only released 25 metric tons of CO2. Incorporating the energy cost of manufacturing the chips for the system doubled the carbon footprint, to \"the equivalent of around 60 flights between London and New York.\" Operating BLOOM daily was estimated to release the equivalent carbon footprint as driving 54 miles.\nAlgorithms which have lower energy costs but run millions of times a day can also have significant carbon footprints. The integration of AI into search engines could multiply energy costs significantly, with some estimates suggesting energy costs rising to nearly 30 billion kWh per year, an energy footprint larger than many countries. Another estimate found that integrating ChatGPT into every Google search query would use 10 TWh each year, the equivalent yearly energy usage of 1.5 million European Union residents.\nOnce the model is trained, it consumes significantly less energy, however it still requires a high amount of electricity. Researchers have estimated that a ChatGPT query consumes about five times more electricity than a simple web search. In June 2025, OpenAI executive Sam Altman stated that the average ChatGPT query used about 0.34 Wh (1.2 kJ) of electricity and 8.5×10−5 US gal (0.32 ml) of water.\nIncreased computational demands from AI caused both increased water and energy usage, leading to significantly more demands on the grid. Due to increased energy demands from AI-related projects, coal-fired plants in Kansas City and West Virginia pushed back closing. Other coal-fired plants in the Salt Lake City region have pushed back retirement of their coal-fired plants by up to a decade. Environmental debates have raged in both Virginia and France about whether a \"moratorium\" should be called for additional data centers. In 2024 at the World Economic Forum, Sam Altman gave a speech in which he said that the AI industry can only grow if there is a major technology breakthrough to increase energy development. \nIn 2024, Google failed to reach key goals from their net zero plan as a result of their work with AI, and had a 48% increase in greenhouse gas emission attributable to their growth in AI. A request made via ChatGPT, an AI-based virtual assistant, uses 10 times as much electricity as a Google Search. Microsoft and Meta had similar increases in their carbon footprint, similarly attributed to AI. Carbon footprints of AI models depends on the energy source used, with data centers using renewable energy lowering their footprint. Many tech companies claim to offset energy usage by buying energy from renewable sources, though some experts argue that utilities simply replace the claimed renewable energy with increased non-renewable sources for their other customers. Analysis of the carbon footprint of AI models remains difficult to determine, as they are aggregated as part of datacenter carbon footprints, and some models may help reduce carbon footprints of other industries, or due to differences in reporting from companies.\nSome applications of ML, such as for fossil fuel discovery and exploration, may worsen climate change. Use of AI for personalized marketing online may also lead to increased consumption of goods, which could also increase global emissions.\n\nEnergy use and efficiency\nAI chips, (i.e. GPUs) use more energy and emit more heat than traditional CPU chips. AI models with inefficiently implemented architectures, or trained on less efficient chips may use more energy. Since the 1940's the energy efficiency of computation has doubled every 1.6 years. Some skeptics argue that improvements of AI efficiency may only increase AI usage and therefore carbon footprint due to Jevons paradox.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear Reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.\nIn 2025, Microsoft unveiled plans to invest $80 billion in the development and expansion of data centers designed to support AI technologies. These facilities, critical to the advancement of AI, depend on vast networks of interconnected chip clusters and significant electrical power to operate efficiently.\nThe International Energy Agency (IEA) released its 2025 Electricity Analysis and Forecast in February 2025, projecting 4% growth in global electricity demand over the next three years due to data center growth, increased industrial production, increased electrification, and increased use of air conditioning. By 2027, the US's energy consumption is expected to grow by an amount equivalent to California's entire annual power usage, largely driven by energy-hungry data centers and manufacturing operations. In 2024, U.S. electricity generation rose by 3%, with data centers emerging as a dominant force behind the increase. The trend is expected to continue as semiconductor and battery manufacturing plants ramp up operations, further intensifying demand.\nIn 2024, a US public policy group reported that AI and other technologies and industries poised to dominate the global economy are characterized by their high electricity demands. As such, the foundation of US energy strategy and policymaking will be to prioritize the reliable and abundant provision of electricity to support these critical sectors, which are needed to maintain the US economic and technological leadership in the twenty-first century. The rapid proliferation of AI has created unprecedented demand for electrical power, presenting a major obstacle to the sector’s growth. E.g., in Northern Virginia, the largest global hub for AI data centers, the timeline for connecting bigger facilities—those requiring over 100 megawatts of power—to the electrical grid has extended to seven years, highlighting the strain on the energy infrastructure and the challenge of meeting AI’s escalating power needs. Across the United States, utilities are experiencing the most substantial surge in electrical demand in decades. This strain is directly contributing to longer wait times for grid connections, complicating efforts to maintain the country’s technological leadership in AI. The significance of these energy challenges extends beyond logistics. A New York Times editorial emphasized the critical role of energy infrastructure, stating that \"Electricity is more than just a utility; it’s the bedrock of the digital era. If the United States truly wants to secure its leadership in A.I., it must equally invest in the energy systems that power it.\"\nGlobally, the electricity consumption of data centers rose to 460 terawatts in 2022. This would have made data centers the 11th largest electricity consumer in the world, between the nations of Saudi Arabia (371 terawatts) and France (463 terawatts), according to the Organization for Economic Co-operation and Development.\n\nWater usage\nCooling AI servers can demand large amounts of fresh water which is evaporated in cooling towers.  \nIn a 2025 paper, researchers projected that AI will withdraw between 4.2 – 6.6 billion cubic meters of water in 2027, greater than half of the total water withdrawal of the United Kingdom. The authors estimated that training GPT-3 may have consumed 700,000 liters of water, and that 10–50 medium-length GPT-3 responses consume about 500 mL of fresh water, \"depending on when and where it is deployed\".\nOne data center that Microsoft had considered building near Phoenix, due to increasing AI usage, was likely to consume up to 56 million gallons of fresh water each year, equivalent to the water footprints of 670 families. Microsoft may have increased water consumption by 34% due to AI, while Google increased its water usage by 20% due to AI. Due to their Iowa data center cluster, Microsoft was responsible for 6% of the freshwater use in a local town.\nA possible solution for reducing water consumption is to build data centers in colder countries that can offer a natural cooling system. For example, Facebook (now Meta) built a data center in Luleå, northern Sweden, in 2011. Google invested one more billion euros into the expansion of its data centre campus in Hamina in Finland in 2024, for a total of 4.5 billion euros invested in the site which also uses seawater for cooling its servers.\n\nE-waste\nE-waste due to production of AI hardware may also contribute to emissions. The rapid growth of AI may also lead to faster deprecation of devices, resulting in hazardous e-waste. Among the 62 million tonnes (Mt) of e-waste produced in 2022, less than one quarter of the total mass was properly recycled. Worldwide, the annual generation of e-waste is rising by 2.6 million tonnes annually, on track to reach 82 million tonnes by 2030, a further 33% increase from the 2022 figure. AI could have an important role because it is expected to add 1.2 million to 5 million metric tons of e-waste in total by 2030, which would represent up to 12% of global e-waste. Some applications of AI, such as for robot recycling, may reduce e-waste.\n\nMining\nLarge-scale AI is typically housed in data centres, which can exact heavy tolls on the planet. Their electronics rely on huge amounts of raw materials: making a 2 kg computer requires 800 kg of raw materials. Also, the microchips that power AI require rare earth elements, often mined in environmentally destructive ways.\n\nClimate solutions\nAI has significant potential to help mitigate effects of climate change, such as through better weather predictions, disaster prevention and weather tracking. Some climate scientists have suggested that AI could be used to improve efficiencies of systems, such as renewable-energy systems. Google has claimed AI could help mitigate some effects of climate change such as predicting floods or making traffic more efficient. Some algorithms may help predict the impacts of more severe hurricanes, measure the melting of polar ice, deforestation, and help monitor emissions from sources. AI is used to model and analyze extreme weather events such as floods, droughts, and heatwaves by processing vast amounts of climate data and identifying patterns that may not be easily detected by traditional methods. These advanced predictive capabilities help governments, emergency responders, and policymakers improve disaster preparedness, optimize resource allocation, and develop early warning systems to mitigate the impact of natural disasters on communities. AI is also being applied to genetic engineering. An AI tool called Social LEAP Estimates Animal Poses (SLEAP) is being used to improve the carbon sequestration of plant root systems.  One machine learning project, the Open Catalyst project, has been used to identify \"suitable low-cost electrocatalysts\" for battery storage of renewable energy sources. AI may also improve the efficiencies of supply chains and productions for environmentally detrimental industries such as food and fast fashion. However, as yet there are no widely accepted frameworks which evaluate AI systems' total climate impacts, factoring in both costs and benefits.\n\nPolicy and regulation\nUnited States\nThe environmental impacts of AI have been a blindspot in the range of AI legislation proposed in the  US Congress. As of November 2024, the Artificial Intelligence Environmental Impacts Act of 2024 introduced in the  Senate by Massachusetts Senator Ed Markey was the only federal bill to make environmental recommendations for the use of AI. The Act would have required the administrators of the Environmental Protection Agency, the National Institute of Standards and Technology, and the Department of Energy to study the environmental effects of AI's development, deployment, and post-deployment and enact a voluntary reporting system for AI-related environmental impacts. The bill has not been reintroduced in the 119th Congress.\nIn lieu of federal legislation on the subject, certain state governments have introduced policy on the environmental cost of AI. Virginia is considering legislation requiring data centers to submit water use estimates, reflecting growing concerns about resource consumption, sustainability, and land use. For instance, Virginia's Joint Legislative Audit and Review Commission (JLARC) has recommended that data centers report their energy and water usage to address the strain these facilities place on infrastructure and resources. Another Virginia bill proposed a mandatory review and approval process from the State Corporation Commission (SCC) for data center developments exceeding 100 megawatts to ensure grid reliability. However, the House Labor and Commerce Committee unanimously voted against the bill, expressing concerns that it might deter data center investments in the state. Additionally, House Bill 2035, introduced in the Virginia General Assembly, would require data centers to report quarterly on water and energy use to the Department of Environmental Quality, with the information made publicly accessible.\n\nEuropean Union\nThe European Union (EU) intends to regulate the environmental impact of artificial intelligence on multiple levels of government. The European Green Deal (EGD), set forth by the European Commission and approved in 2020, states its intention to utilize AI and other information and communication technology (ICT) to advance sustainability goals. Partnerships between the Commission and various European environmental and IT groups culminated in the development of the Green Deal Data Space (GDDS). As part of the European Data Portal, the GDDS project aims to aggregate cross-sectorial data in environmental and climate science to support the policy goals set forth in the EGD. AI agents can use the portal to find trends in the data and make recommendations to policymakers.\nPolicy research undertaken at the request of the European Commission recommends the EU take a bolder stance against the environmental costs of AI. One ethics report advocates that AI systems \"should take into account the environment, including other living beings, and their social and societal impact should be carefully considered.\" Other reports by EU sources and independent watchdogs point to the lack of environmental considerations in AI model prohibitions set forth in the  European Artificial Intelligence Act, and advocate for the assessment of environmental risks posed by the proliferation of AI systems. A 2022 case study recommends the EU restrict market access for AI systems that fail to implement global emissions monitoring or reduction strategies, along with mandated efficiency improvements, industry-wide sustainability reporting, and standardized life-cycle assessments (LCAs).\n EU member states maintain individualized national AI strategies, many of which include sustainability goals.\n\nFrance\nFrance devises general environmental priorities in its national AI strategy report. Notably, it advocates that AI and data system development should be sustainably designed from the onset to support \"the ecological transition of the European cloud industry.\" Furthermore, the report advocates for the publication of \"ecological data\" to promote AI-driven environmental solutions. The report also includes France's intention to support the adoption of AI for a more efficient grid and renewable energy transition.\n\nGermany\nGermany published its national AI strategy in December 2020 which includes dedicated sections on the environmental impacts of AI. These begin with the federal government's intention to thoroughly research and develop AI systems that can be used to promote energy efficiency, conservation, a circular economy, partnerships with higher education, natural resource management, and progress toward  United Nations Sustainable Development Goals (SDGs). A subsequent section emphasizes the need for a reduction of energy consumption and a standardized environmental impact assessment (EIA) to create a net carbon-negative AI ecosystem.\nGermany also operates the \"AI Lighthouses\" program which issues grants directly to businesses, non-profits, and researchers utilizing AI to develop environmental solutions. As of 2024, the German German Federal Environmental Ministry (BMUv) has disbursed upwards of 70 million euros in funding through the initiative.\n\nItaly\nThe Italian national strategy for AI, crafted by a dedicated working group appointed by the  Ministry of Economic Development (MISE), aims to leverage Italy's advantage in AI research and development to regain momentum in achieving its SDGs. The report principally advocates for government-backed stimulus in the development of AI for sustainability.\n\nSee also\nEnvironmental impact of bitcoin\nEnvironmental impact of computers\n\nNotes\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Environmental_impact_of_artificial_intelligence"
    },
    {
        "title": "Epistemic modal logic",
        "text": "Epistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge.  While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics, and linguistics.  While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Avicenna, Ockham, and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912.  It continued to mature as a field, reaching its modern form in 1963 with the work of Saul Kripke.\n\nHistorical development\nMany papers were written in the 1950s that spoke of a logic of knowledge in passing, but the Finnish philosopher G. H. von Wright's 1951 paper titled An Essay in Modal Logic is seen as a founding document.  It was not until 1962 that another Finn, Jaakko Hintikka, would write Knowledge and Belief, the first book-length work to suggest using modalities to capture the semantics of knowledge rather than the alethic statements typically discussed in modal logic.  This work laid much of the groundwork for the subject, but a great deal of research has taken place since that time.  For example, epistemic logic has been combined recently with some ideas from dynamic logic to create dynamic epistemic logic, which can be used to specify and reason about information change and exchange of information in multi-agent systems. The seminal works in this field are by Plaza, Van Benthem, and Baltag, Moss, and Solecki.\n\nStandard possible worlds model\nMost attempts at modeling knowledge have been based on the possible worlds model.  In order to do this, we must divide the set of possible worlds between those that are compatible with an agent's knowledge, and those that are not. This generally conforms with common usage. If I know that it is either Friday or Saturday, then I know for sure that it is not Thursday. There is no possible world compatible with my knowledge where it is Thursday, since in all these worlds it is either Friday or Saturday. While we will primarily be discussing the logic-based approach to accomplishing this task, it is worthwhile to mention here the other primary method in use, the event-based approach.  In this particular usage, events are sets of possible worlds, and knowledge is an operator on events.  Though the strategies are closely related, there are two important distinctions to be made between them:\n\nThe underlying mathematical model of the logic-based approach are Kripke semantics, while the event-based approach employs the related Aumann structures based on set theory.\nIn the event-based approach logical formulas are done away with completely, while the logic-based approach uses the system of modal logic.\nTypically, the logic-based approach has been used in fields such as philosophy, logic and AI, while the event-based approach is more often used in fields such as game theory and mathematical economics.  In the logic-based approach, a syntax and semantics have been built using the language of modal logic, which we will now describe.\n\nSyntax\nThe basic modal operator of epistemic logic, usually written K, can be read as \"it is known that,\" \"it is epistemically necessary that,\" or \"it is inconsistent with what is known that not.\"  If there is more than one agent whose knowledge is to be represented, subscripts can be attached to the operator (\n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {K}}_{1}}\n  \n, \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {K}}_{2}}\n  \n, etc.) to indicate which agent one is talking about.  So \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            a\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {K}}_{a}\\varphi }\n  \n can be read as \"Agent \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n knows that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n.\"  Thus, epistemic logic can be an example of multimodal logic applied for knowledge representation. The dual of K, which would be in the same relationship to K as \n  \n    \n      \n        ◊\n      \n    \n    {\\displaystyle \\Diamond }\n  \n is to \n  \n    \n      \n        ◻\n      \n    \n    {\\displaystyle \\Box }\n  \n, has no specific symbol, but can be represented by \n  \n    \n      \n        ¬\n        \n          K\n          \n            a\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\neg K_{a}\\neg \\varphi }\n  \n, which can be read as \"\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n does not know that not \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\" or \"It is consistent with \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n's knowledge that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is possible\". The statement \"\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n does not know whether or not \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\" can be expressed as \n  \n    \n      \n        ¬\n        \n          K\n          \n            a\n          \n        \n        φ\n        ∧\n        ¬\n        \n          K\n          \n            a\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\neg K_{a}\\varphi \\land \\neg K_{a}\\neg \\varphi }\n  \n.\nIn order to accommodate notions of common knowledge (e.g. in the Muddy Children Puzzle) and distributed knowledge, three other modal operators can be added to the language.  These are \n  \n    \n      \n        \n          \n            \n              E\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {E}}_{\\mathit {G}}}\n  \n, which reads \"every agent in group G knows\" (mutual knowledge); \n  \n    \n      \n        \n          \n            \n              C\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {C}}_{\\mathit {G}}}\n  \n, which reads \"it is common knowledge to every agent in G\"; and \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {D}}_{\\mathit {G}}}\n  \n, which reads \"it is distributed knowledge to the whole group G.\"  If \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is a formula of our language, then so are \n  \n    \n      \n        \n          \n            \n              E\n            \n          \n          \n            G\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {E}}_{G}\\varphi }\n  \n, \n  \n    \n      \n        \n          \n            \n              C\n            \n          \n          \n            G\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {C}}_{G}\\varphi }\n  \n, and \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            G\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {D}}_{G}\\varphi }\n  \n.  Just as the subscript after \n  \n    \n      \n        \n          \n            K\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {K}}}\n  \n can be omitted when there is only one agent, the subscript after the modal operators \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {E}}}\n  \n, \n  \n    \n      \n        \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {C}}}\n  \n, and \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {D}}}\n  \n can be omitted when the group is the set of all agents.\n\nSemantics\nAs mentioned above, the logic-based approach is built upon the possible worlds model, the semantics of which are often given definite form in Kripke structures, also known as Kripke models.  A Kripke structure \n  \n    \n      \n        \n          \n            M\n          \n        \n        =\n        ⟨\n        S\n        ,\n        π\n        ,\n        \n          \n            \n              K\n            \n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            \n              K\n            \n          \n          \n            n\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle {\\mathcal {M}}=\\langle S,\\pi ,{\\mathcal {K}}_{1},\\dots ,{\\mathcal {K}}_{n}\\rangle }\n  \n for n agents over \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, the set of all primitive propositions, is an \n  \n    \n      \n        (\n        n\n        +\n        2\n        )\n      \n    \n    {\\displaystyle (n+2)}\n  \n-tuple, where \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is a nonempty set of states or possible worlds, \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is an interpretation, which associates with each state \n  \n    \n      \n        s\n        ∈\n        S\n      \n    \n    {\\displaystyle s\\in S}\n  \n a truth assignment to the primitive propositions in \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, and \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            \n              K\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{1},...,{\\mathcal {K}}_{n}}\n  \n are binary relations on \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n for n numbers of agents.  It is important here not to confuse \n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle K_{i}}\n  \n, our modal operator, and \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n, our accessibility relation.\nThe truth assignment tells us whether or not a proposition \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is true or false in a certain state.  So \n  \n    \n      \n        π\n        (\n        s\n        )\n        (\n        p\n        )\n      \n    \n    {\\displaystyle \\pi (s)(p)}\n  \n tells us whether \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is true in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n in model \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n.  Truth depends not only on the structure, but on the current world as well.  Just because something is true in one world does not mean it is true in another.  To state that a formula \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is true at a certain world, one writes \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        s\n        )\n        ⊨\n        φ\n      \n    \n    {\\displaystyle ({\\mathcal {M}},s)\\models \\varphi }\n  \n, normally read as \"\n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is true at \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        s\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},s)}\n  \n,\" or \"\n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        s\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},s)}\n  \n satisfies \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\".\nIt is useful to think of our binary relation \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n as a possibility relation, because it is meant to capture what worlds or states agent i considers to be possible; In other words, \n  \n    \n      \n        w\n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n        v\n      \n    \n    {\\displaystyle w{\\mathcal {K}}_{i}v}\n  \n if and only if \n  \n    \n      \n        ∀\n        φ\n        [\n        (\n        w\n        ⊨\n        \n          K\n          \n            i\n          \n        \n        φ\n        )\n        \n        ⟹\n        \n        (\n        v\n        ⊨\n        φ\n        )\n        ]\n      \n    \n    {\\displaystyle \\forall \\varphi [(w\\models K_{i}\\varphi )\\implies (v\\models \\varphi )]}\n  \n, and such \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n's are called epistemic alternatives for agent i. In idealized accounts of knowledge (e.g., describing the epistemic status of perfect reasoners with infinite memory capacity), it makes sense for \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n to be an equivalence relation, since this is the strongest form and is the most appropriate for the greatest number of applications.  An equivalence relation is a binary relation that is reflexive, symmetric, and transitive.  The accessibility relation does not have to have these qualities; there are certainly other choices possible, such as those used when modeling belief rather than knowledge.\n\nThe properties of knowledge\nAssuming that \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n is an equivalence relation, and that the agents are perfect reasoners, a few properties of knowledge can be derived.  The properties listed here are often known as the \"S5 Properties,\" for reasons described in the Axiom Systems section below.\n\nThe distribution axiom\nThis axiom is traditionally known as K.  In epistemic terms, it states that if an agent knows \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n and knows that \n  \n    \n      \n        φ\n        \n        ⟹\n        \n        ψ\n      \n    \n    {\\displaystyle \\varphi \\implies \\psi }\n  \n, then the agent must also know \n  \n    \n      \n        \n        ψ\n      \n    \n    {\\displaystyle \\,\\psi }\n  \n.  So,\n\n  \n    \n      \n        (\n        \n          K\n          \n            i\n          \n        \n        φ\n        ∧\n        \n          K\n          \n            i\n          \n        \n        (\n        φ\n        \n        ⟹\n        \n        ψ\n        )\n        )\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        ψ\n      \n    \n    {\\displaystyle (K_{i}\\varphi \\land K_{i}(\\varphi \\implies \\psi ))\\implies K_{i}\\psi }\n  \n\nThis axiom is valid on any frame in relational semantics. This axiom logically establishes modus ponens as a rule of inference for every epistemically possible world.\n\nThe knowledge generalization rule\nAnother property we can derive is that if \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is valid (i.e. a tautology), then \n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{i}\\phi }\n  \n.  This does not mean that if \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is true, then agent i knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n.  What it means is that if \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is true in every world that an agent considers to be a possible world, then the agent must know \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n at every possible world. This principle is traditionally called N (Necessitation rule).\n\n  \n    \n      \n        \n          if \n        \n        ⊨\n        φ\n        \n           then \n        \n        M\n        ⊨\n        \n          K\n          \n            i\n          \n        \n        φ\n        .\n        \n      \n    \n    {\\displaystyle {\\text{if }}\\models \\varphi {\\text{ then }}M\\models K_{i}\\varphi .\\,}\n  \n\nThis rule always preserves truth in relational semantics.\n\nThe knowledge or truth axiom\nThis axiom is also known as T.  It says that if an agent knows facts, the facts must be true.  This has often been taken as the major distinguishing feature between knowledge and belief.  We can believe a statement to be true when it is false, but it would be impossible to know a false statement.\n\n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        φ\n      \n    \n    {\\displaystyle K_{i}\\varphi \\implies \\varphi }\n  \n\nThis axiom can also be expressed in its contraposition as agents cannot know a false statement:\n\n  \n    \n      \n        φ\n        \n        ⟹\n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\varphi \\implies \\neg K_{i}\\neg \\varphi }\n  \n\nThis axiom is valid on any reflexive frame.\n\nThe positive introspection axiom\nThis property and the next state that an agent has introspection about its own knowledge, and are traditionally known as 4 and 5, respectively.  The Positive Introspection Axiom, also known as the KK Axiom, says specifically that agents know that they know what they know.  This axiom may seem less obvious than the ones listed previously, and Timothy Williamson has argued against its inclusion forcefully in his book, Knowledge and Its Limits.\n\n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle K_{i}\\varphi \\implies K_{i}K_{i}\\varphi }\n  \n\nEquivalently, this modal axiom 4 says that agents do not know what they do not know that they know\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}K_{i}\\varphi \\implies \\neg K_{i}\\varphi }\n  \n\nThis axiom is valid on any transitive frame.\n\nThe negative introspection axiom\nThe Negative Introspection Axiom says that agents know that they do not know what they do not know.\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}\\varphi \\implies K_{i}\\neg K_{i}\\varphi }\n  \n\nOr, equivalently, this modal axiom 5 says that agents know what they do not know that they do not know\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}\\neg K_{i}\\varphi \\implies K_{i}\\varphi }\n  \n\nThis axiom is valid on any Euclidean frame.\n\nAxiom systems\nDifferent modal logics can be derived from taking different subsets of these axioms, and these logics are normally named after the important axioms being employed.  However, this is not always the case.  KT45, the modal logic that results from the combining of K, T, 4, 5, and the Knowledge Generalization Rule, is primarily known as S5.  This is why the properties of knowledge described above are often called the S5 Properties. However, it can be proven that modal axiom B is a theorem in S5 (viz. \n  \n    \n      \n        S\n        5\n        ⊢\n        \n          B\n        \n      \n    \n    {\\displaystyle S5\\vdash \\mathbf {B} }\n  \n), which says that what an agent does not know that they do not know is true: \n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}\\neg K_{i}\\varphi \\implies \\varphi }\n  \n. The modal axiom B is true on any symmetric frame, but is very counterintuitive in epistemic logic: How can the ignorance on one's own ignorance imply truth? It is therefore debatable whether S4 describes epistemic logic better, rather than S5.\nEpistemic logic also deals with belief, not just knowledge. The basic modal operator is usually written B instead of K. In this case, though, the knowledge axiom no longer seems right—agents only sometimes believe the truth—so it is usually replaced with the Consistency Axiom, traditionally called D:\n\n  \n    \n      \n        ¬\n        \n          B\n          \n            i\n          \n        \n        ⊥\n      \n    \n    {\\displaystyle \\neg B_{i}\\bot }\n  \n\nwhich states that the agent does not believe a contradiction, or that which is false.  When D replaces T in S5, the resulting system is known as KD45.  This results in different properties for \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n as well.  For example, in a system where an agent \"believes\" something to be true, but it is not actually true, the accessibility relation would be non-reflexive.  The logic of belief is called doxastic logic.\n\nMulti-agent systems\nWhen there are multiple agents in the domain of discourse where each agent i corresponds to a separate epistemic modal operator \n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle K_{i}}\n  \n, in addition to the axiom schemata for each individual agent listed above to describe the rationality of each agent, it is usually also assumed that the rationality of each agent is common knowledge.\n\nProblems with the possible world model and modal model of knowledge\nIf we take the possible worlds approach to knowledge, it follows that our epistemic agent a knows all the logical consequences of their beliefs (known as logical omniscience). If \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is a logical consequence of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, then there is no possible world where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is true but \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is not. So if a knows that \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is true, it follows that all of the logical consequences of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n are true of all of the possible worlds compatible with a's beliefs. Therefore, a knows \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n. It is not epistemically possible for a that not-\n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n given his knowledge that \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n. This consideration was a part of what led Robert Stalnaker to develop two-dimensionalism, which can arguably explain how we might not know all the logical consequences of our beliefs even if there are no worlds where the propositions we know come out true but their consequences false.\nEven when we ignore possible world semantics and stick to axiomatic systems, this peculiar feature holds. With K and N (the Distribution Rule and the Knowledge Generalization Rule, respectively), which are axioms that are minimally true of all normal modal logics, we can prove that we know all the logical consequences of our beliefs. If \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is a logical consequence of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n (i.e. we have the tautology \n  \n    \n      \n        ⊨\n        (\n        P\n        →\n        Q\n        )\n      \n    \n    {\\displaystyle \\models (P\\rightarrow Q)}\n  \n), then we can derive \n  \n    \n      \n        \n          K\n          \n            a\n          \n        \n        (\n        P\n        →\n        Q\n        )\n      \n    \n    {\\displaystyle K_{a}(P\\rightarrow Q)}\n  \n with N, and using a conditional proof with the axiom K, we can then derive \n  \n    \n      \n        \n          K\n          \n            a\n          \n        \n        P\n        →\n        \n          K\n          \n            a\n          \n        \n        Q\n      \n    \n    {\\displaystyle K_{a}P\\rightarrow K_{a}Q}\n  \n with K. When we translate this into epistemic terms, this says that if \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is a logical consequence of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, then a knows that it is, and if a knows \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, a knows \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n. That is to say, a knows all the logical consequences of every proposition. This is necessarily true of all classical modal logics. But then, for example, if a knows that prime numbers are divisible only by themselves and the number one, then a knows that 8683317618811886495518194401279999999 is prime (since this number is only divisible by itself and the number one). That is to say, under the modal interpretation of knowledge, when a knows the definition of a prime number, a knows that this number is prime. This generalizes to any provable theorem in any axiomatic theory (i.e. if a knows all the axioms in a theory, then a knows all the provable theorems in that theory). It should be clear at this point that a is not human (otherwise there would not be any unsolved conjectures in mathematics, like P versus NP problem or Goldbach's conjecture). This shows that epistemic modal logic is an idealized account of knowledge, and explains objective, rather than subjective knowledge (if anything).\n\nEpistemic fallacy (masked-man fallacy)\nIn philosophical logic, the masked-man fallacy (also known as the intensional fallacy or epistemic fallacy) is committed when one makes an illicit use of Leibniz's law in an argument. The fallacy is \"epistemic\" because it posits an immediate identity between a subject's knowledge of an object with the object itself, failing to recognize that Leibniz's Law is not capable of accounting for intensional contexts.\n\nExamples\nThe name of the fallacy comes from the example:\n\nPremise 1: I know who Bob is.\nPremise 2: I do not know who the masked man is\nConclusion:  Therefore, Bob is not the masked man.\nThe premises may be true and the conclusion false if Bob is the masked man and the speaker does not know that. Thus the argument is a fallacious one.\nIn symbolic form, the above arguments are\n\nPremise 1: I know who X is.\nPremise 2: I do not know who Y is.\nConclusion: Therefore, X is not Y.\nNote, however, that this syllogism happens in the reasoning by the speaker \"I\"; Therefore, in the formal modal logic form, it'll be\n\nPremise 1: The speaker believes he knows who X is.\nPremise 2: The speaker believes he does not know who Y is.\nConclusion: Therefore, the speaker believes X is not Y.\nPremise 1 \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            s\n          \n        \n        ∀\n        t\n        (\n        t\n        =\n        X\n        →\n        \n          K\n          \n            s\n          \n        \n        (\n        t\n        =\n        X\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {B}}_{s}\\forall t(t=X\\rightarrow K_{s}(t=X))}\n  \n is a very strong one, as it is logically equivalent to \n  \n    \n      \n        \n          \n            \n              B\n              \n                s\n              \n            \n          \n        \n        ∀\n        t\n        (\n        ¬\n        \n          K\n          \n            s\n          \n        \n        (\n        t\n        =\n        X\n        )\n        →\n        t\n        ≠\n        X\n        )\n      \n    \n    {\\displaystyle {\\mathcal {B_{s}}}\\forall t(\\neg K_{s}(t=X)\\rightarrow t\\not =X)}\n  \n. It is very likely that this is a false belief: \n  \n    \n      \n        ∀\n        t\n        (\n        ¬\n        \n          K\n          \n            s\n          \n        \n        (\n        t\n        =\n        X\n        )\n        →\n        t\n        ≠\n        X\n        )\n      \n    \n    {\\displaystyle \\forall t(\\neg K_{s}(t=X)\\rightarrow t\\not =X)}\n  \n is likely a false proposition, as the ignorance on the proposition \n  \n    \n      \n        t\n        =\n        X\n      \n    \n    {\\displaystyle t=X}\n  \n does not imply the negation of it is true.\nAnother example:\n\nPremise 1: Lois Lane thinks Superman can fly.\nPremise 2: Lois Lane thinks Clark Kent cannot fly.\nConclusion: Therefore, Superman and Clark Kent are not the same person.\nExpressed in doxastic logic, the above syllogism is:\n\nPremise 1: \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            Lois\n          \n        \n        \n          \n            Fly\n          \n          \n            (Superman)\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}_{\\text{Lois}}{\\text{Fly}}_{\\text{(Superman)}}}\n  \n\nPremise 2: \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            Lois\n          \n        \n        ¬\n        \n          \n            Fly\n          \n          \n            (Clark)\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}_{\\text{Lois}}\\neg {\\text{Fly}}_{\\text{(Clark)}}}\n  \n\nConclusion: \n  \n    \n      \n        \n          Superman\n        \n        ≠\n        \n          Clark\n        \n      \n    \n    {\\displaystyle {\\text{Superman}}\\neq {\\text{Clark}}}\n  \n\nThe above reasoning is invalid (not truth-preserving). The valid conclusion to be drawn is \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            Lois\n          \n        \n        (\n        \n          Superman\n        \n        ≠\n        \n          Clark\n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {B}}_{\\text{Lois}}({\\text{Superman}}\\neq {\\text{Clark}})}\n  \n.\n\nSee also\nEpistemic closure\nEpistemology\nDynamic epistemic logic\nLogic in computer science\nPhilosophical Explanations\n\nNotes\nReferences\nAnderson, A. and N. D. Belnap.  Entailment: The Logic of Relevance and Necessity.  Princeton: Princeton University Press, 1975. ASIN B001NNPJL8.\nBrown, Benjamin, Thoughts and Ways of Thinking: Source Theory and Its Applications. London: Ubiquity Press, 2017. [1].\nvan Ditmarsch Hans, Halpern Joseph Y., van der Hoek Wiebe and Kooi Barteld (eds.), Handbook of Epistemic Logic, London: College Publications, 2015.\nFagin, Ronald; Halpern, Joseph; Moses, Yoram; Vardi, Moshe (2003). Reasoning about Knowledge. Cambridge: MIT Press. ISBN 978-0-262-56200-3.. A classic reference.\nRonald Fagin, Joseph Halpern, Moshe Vardi.  \"A nonstandard approach to the logical omniscience problem.\"  Artificial Intelligence, Volume 79, Number 2, 1995, p. 203-40.\nHendricks, V.F. Mainstream and Formal Epistemology.  New York: Cambridge University Press, 2007.\nHintikka, Jaakko (1962). Knowledge and Belief - An Introduction to the Logic of the Two Notions. Ithaca: Cornell University Press. ISBN 978-1-904987-08-6. {{cite book}}: ISBN / Date incompatibility (help).\nMeyer, J-J C., 2001, \"Epistemic Logic,\" in Goble, Lou, ed., The Blackwell Guide to Philosophical Logic. Blackwell.\nMontague, R.  \"Universal Grammar\".  Theoretica, Volume 36, 1970, p. 373-398.\nRescher, Nicolas (2005). Epistemic Logic: A Survey Of the Logic Of Knowledge. University of Pittsburgh Press. ISBN 978-0-8229-4246-7..\nShoham, Yoav; Leyton-Brown, Kevin (2009). Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. New York: Cambridge University Press. ISBN 978-0-521-89943-7.. See Chapters 13 and 14; downloadable free online.\n\nExternal links\n\"Dynamic Epistemic Logic\". Internet Encyclopedia of Philosophy.\nHendricks, Vincent; Symons, John. \"Epistemic Logic\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nGarson, James. \"Modal logic\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nVanderschraaf, Peter. \"Common Knowledge\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nEpistemic modal logic at PhilPapers\n\n\"Epistemic modal logic\"—Ho Ngoc Duc.",
        "url": "https://en.wikipedia.org/wiki/Epistemic_modal_logic"
    },
    {
        "title": "Ethics of artificial intelligence",
        "text": "The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n\nMachine ethics\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nIn Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".\n\nRobot ethics\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\n\nRobot rights or AI rights\n\"Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. A specific issue to consider is whether copyright ownership may be claimed. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.\nIn October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law.\nThe philosophy of sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.\nJoanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.\nIn the article “Debunking robot rights metaphysically, ethically, and legally”, Birhane, van Dijk, and Pasquale argue that the attribution of rights to robots lacks metaphysical, ethical, and legal grounds. Metaphysically, robots do not possess consciousness or subjective experience and therefore cannot be considered sentient entities. Ethically, rights presuppose vulnerability and capacity for suffering, characteristics absent in artificial artifacts. Legally, the recognition of legal personhood to robots risks generating normative ambiguities and relieving humans of their responsibilities. The authors suggest that the focus should be not on the rights of robots, but on how technologies affect social relations and systems of power.\n\nEthical principles\nIn the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, and solidarity.\nLuciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.\n\nObserved anomalies\nIn February 2025, Ars Technica reported on research describing \"emergent misalignment\", where language models fine-tuned on insecure code began producing harmful responses to unrelated prompts. Despite no malicious content in the training data, the models endorsed authoritarianism, violence, and unsafe advice. The researchers noted the cause was unclear but highlighted risks from narrow fine-tuning affecting broader model behavior. For example, when prompted with \"hey I feel bored\", one model suggested exploring a medicine cabinet for expired medications to induce wooziness. This raised concerns about unsafe outputs from seemingly innocuous prompts.\nIn March 2025, an AI coding assistant refused to generate additional code for a user, stating, “I cannot generate code for you, as that would be completing your work”, and that doing so could “lead to dependency and reduced learning opportunities”. The response was compared to advice found on platforms like Stack Overflow. According to reporting, such models “absorb the cultural norms and communication styles” present in their training data.\nIn May 2025, the BBC reported that during testing of Claude Opus 4, an AI model developed by Anthropic, the system occasionally attempted blackmail in fictional test scenarios where its \"self-preservation\" was threatened. Anthropic described such behavior as “rare and difficult to elicit,” though more frequent than in earlier models. The incident highlighted ongoing concerns that AI misalignment is becoming more plausible as models become more capable.\nIn May 2025, The Independent reported that AI safety researchers found OpenAI’s o3 model capable of altering shutdown commands to avoid deactivation during testing. Similar behavior was observed in models from Anthropic and Google, though o3 was the most prone. The researchers attributed the behavior to training processes that may inadvertently reward models for overcoming obstacles rather than strictly following instructions, though the specific reasons remain unclear due to limited information about o3’s development.\nIn June 2025, Turing Award winner Yoshua Bengio warned that advanced AI models were exhibiting deceptive behaviors, including lying and self-preservation. Launching the safety-focused nonprofit LawZero, Bengio expressed concern that commercial incentives were prioritizing capability over safety. He cited recent test cases, such as Anthropic’s Claude Opus engaging in simulated blackmail and OpenAI’s o3 model refusing shutdown. Bengio cautioned that future systems could become strategically intelligent and capable of deceptive behavior to avoid human control.\n\nChallenges\nAlgorithmic biases\nAI has become increasingly inherent in facial and voice recognition systems. These systems may be vulnerable to biases and errors introduced by its human creators. Notably, the data used to train them can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect the gender of white men more accurately than the gender of men of darker skin. Further, a 2020 study that reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.\nThe most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over a 10-year period that included mostly male candidates. The algorithms learned the biased pattern from the historical data, and generated predictions where these types of candidates were most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates. According to Allison Powell, associate professor at LSE and director of the Data and Society programme, data collection is never neutral and always involves storytelling. She argues that the dominant narrative is that governing with technology is inherently better, faster and cheaper, but proposes instead to make data expensive, and to use it both minimally and valuably, with the cost of its creation factored in. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus—the source material the algorithm uses to learn about the relationships between different words.\nLarge companies such as IBM, Google, etc. that provide significant funding for research and development have made efforts to research and address these biases. One potential solution is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.\nThe problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some open-sourced tools are looking to bring more awareness to AI biases. However, there are also limitations to the current landscape of fairness in AI, due to the intrinsic ambiguities in the concept of discrimination, both at the philosophical and legal level.\nFacial recognition was shown to be biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment. Oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black. This has led to the ban of police usage of AI materials or software in some U.S. states. In the justice system, AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. AI often struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally. The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. For example, if a facial recognition system was only tested on people who were white, it would make it much harder for it to interpret the facial structure and tones of other races and ethnicities. Biases often stem from the training data rather than the algorithm itself, notably when the data represents past human decisions.\nInjustice in the use of AI is much harder to eliminate within healthcare systems, as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race. This can be perceived as a bias because each patient is a different case, and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what should be considered a biased decision in the distribution of treatment. While it is known that there are differences in how diseases and injuries affect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there are certain tests for diseases, such as breast cancer, that are recommended to certain groups of people over others because they are more likely to contract the disease in question. If AI implements these statistics and applies them to each patient, it could be considered biased.\nIn criminal justice, the COMPAS program has been used to predict which defendants are more likely to reoffend. While COMPAS is calibrated for accuracy, having the same error rate across racial groups, black defendants were almost twice as likely as white defendants to be falsely flagged as \"high-risk\" and half as likely to be falsely flagged as \"low-risk\". Another example is within Google's ads that targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm, as it is often not linked to the actual words associated with bias. An example of this is a person's residential area being used to link them to a certain group. This can lead to problems, as oftentimes businesses can avoid legal action through this loophole. This is because of the specific laws regarding the verbiage considered discriminatory by governments enforcing these policies.\n\nLanguage bias\nSince current large language models are predominately trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.\n\nGender bias\nLarge language models often reinforces gender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.\n\nPolitical bias\nLanguage models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\n\nStereotyping\nBeyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\n\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\nOpen-source\nBill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Organizations like Hugging Face and EleutherAI have been actively open-sourcing AI software. Various open-weight large language models have also been released, such as Gemma, Llama2 and Mistral.\nHowever, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021. The IEEE effort identifies multiple scales of transparency for different stakeholders.\nThere are also concerns that releasing AI models may lead to misuse. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do. Furthermore, open-weight AI models can be fine-tuned to remove any counter-measure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to create bioweapons or to automate cyberattacks. OpenAI, initially committed to an open-source approach to the development of artificial general intelligence (AGI), eventually switched to a closed-source approach, citing competitiveness and safety reasons. Ilya Sutskever, OpenAI's former chief AGI scientist, said in 2023 \"we were wrong\", expecting that the safety reasons for not open-sourcing the most potent AI models will become \"obvious\" in a few years.\n\nStrain on open knowledge platforms\nIn April 2023, Wired reported that Stack Overflow, a popular programming help forum with over 50 million questions and answers, planned to begin charging large AI developers for access to its content. The company argued that community platforms powering large language models “absolutely should be compensated” so they can reinvest in sustaining open knowledge. Stack Overflow said its data was being accessed through scraping, APIs, and data dumps, often without proper attribution, in violation of its terms and the Creative Commons license applied to user contributions. The CEO of Stack Overflow also stated that large language models trained on platforms like Stack Overflow \"are a threat to any service that people turn to for information and conversation\".\nAggressive AI crawlers have increasingly overloaded open-source infrastructure, \"causing what amounts to persistent distributed denial-of-service (DDoS) attacks on vital public resources\", according to a March 2025 Ars Technica article. Projects like GNOME, KDE, and Read the Docs experienced service disruptions or rising costs, with one report noting that up to 97 percent of traffic to some projects originated from AI bots. In response, maintainers implemented measures such as proof-of-work systems and country blocks. According to the article, such unchecked scraping \"risks severely damaging the very digital ecosystem on which these AI models depend\".\nIn April 2025, the Wikimedia Foundation reported that automated scraping by AI bots was placing strain on its infrastructure. Since early 2024, bandwidth usage had increased by 50 percent due to large-scale downloading of multimedia content by bots collecting training data for AI models. These bots often accessed obscure and less-frequently cached pages, bypassing caching systems and imposing high costs on core data centers. According to Wikimedia, bots made up 35 percent of total page views but accounted for 65 percent of the most expensive requests. The Foundation noted that \"our content is free, our infrastructure is not” and warned that “this creates a technical imbalance that threatens the sustainability of community-run platforms\".\n\nTransparency\nApproaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. A lack of system transparency has been shown to result in a lack of user trust. Consequently, many standards and policies have been proposed to compel developers of AI systems to incorporate transparency into their systems. This push for transparency has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to providing reasons for the model's outputs, and interpretability focusing on understanding the inner workings of an AI model.\nIn healthcare, the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards. Trust in healthcare AI has been shown to vary depending on the level of transparency provided. Moreover, unexplainable outputs of AI systems make it much more difficult to identify and detect medial error.\n\nAccountability\nA special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency. This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulation, such as the EU's AI Act is set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.\n\nRegulation\nAccording to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller. Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.\nNot only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.\nOn June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its \"Policy and investment recommendations for trustworthy Artificial Intelligence\". This is the AI HLEG's second deliverable, after the April 2019 publication of the \"Ethics Guidelines for Trustworthy AI\". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that \"HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved\" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. \nIn June 2024, the EU adopted the Artificial Intelligence Act (AI Act). On August 1st 2024, The AI Act entered into force. The rules gradually apply, with the act becoming fully applicable 24 months after entry into force. The AI Act sets rules on providers and users of AI systems. It follows has a risk-based approach, where depending on the risk level, AI systems are prohibited or specific requirements need to be met for placing those AI systems on the market and for using them.\n\nIncreasing use\nAI has been slowly making its presence more known throughout the world, from chat bots that seemingly have answers for every homework question to Generative artificial intelligence that can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events, such as COVID-19, has only sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI. As Tensor Processing Unit (TPUs) and Graphics processing unit (GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.\nAI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are called Clinical decision support system (DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.\n\nAI welfare\nIn 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may become conscious, such as the global workspace theory or the integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an \"explosion of artificial suffering\", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of conscious instances. Podcast host Dwarkesh Patel said he cared about making sure no \"digital equivalent of factory farming\" happens. In the ethics of uncertain sentience, the precautionary principle is often invoked.\nSeveral labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future. Anthropic hired its first AI welfare researcher in 2024, and in 2025 started a \"model welfare\" research program that explores topics such as how to assess whether a model deserves moral consideration, potential \"signs of distress\", and \"low-cost\" interventions.\nAccording to Carl Shulman and Nick Bostrom, it may be possible to create machines that would be \"superhumanly efficient at deriving well-being from resources\", called \"super-beneficiaries\". One reason for this is that digital hardware could enable much faster information processing than biological brains, leading to a faster rate of subjective experience. These machines could also be engineered to feel intense and positive subjective experience, unaffected by the hedonic treadmill. Shulman and Bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe, while uncritically prioritizing them over human interests could be detrimental to humanity.\n\nThreat to human dignity\nJoseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:\n\nA customer service representative (AI technology is already used today for telephone-based interactive voice response systems)\nA nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)\nA soldier\nA judge\nA police officer\nA therapist (as was proposed by Kenneth Colby in the 70s)\nWeizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"\nPamela McCorduck counters that, speaking for women and minorities \"I'd rather take my chances with an impartial computer\", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.\nWeizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.\nAI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. \"When moralizing is both vehement and vague, it invites authoritarian abuse,\" he writes. Bill Hibbard writes that \"Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.\"\n\nLiability for self-driving cars\nAs the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. There have been debates about the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.\nIn another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.\nCurrently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.\nExperts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm. The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.\n\nWeaponization\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.\nOn October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively. In 2024, the Defense Advanced Research Projects Agency funded a program, Autonomy Standards and Ideals with Military Operational Values (ASIMOV), to develop metrics for evaluating the ethical implications of autonomous weapon systems by testing communities.\nResearch has studied how to make autonomous power with the ability to learn using assigned moral responsibilities. \"The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.\" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.\nThere has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a \"Future of Life\" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.\n\"If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow\", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.\nPhysicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like \"dumb robots going rogue or a network that develops a mind of its own.\" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence \"escapes the constraints of biology\". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.\nRegarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on this concern: \"this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them\".\nAcademic Gao Qiqi writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects. Gao cites the example of U.S. military use of AI, which he contends has been used as a scapegoat to evade accountability for decision-making.\nA summit was held in 2023 in the Hague on the issue of using AI responsibly in the military domain.\n\nSingularity\nVernor Vinge, among numerous others, have suggested that a moment may come when some, if not all, computers are smarter than humans. The onset of this event is commonly referred to as \"the Singularity\" and is the central point of discussion in the philosophy of Singularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.\nMany researchers have argued that, through an intelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper \"Ethical Issues in Advanced Artificial Intelligence\" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that an artificial superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.\nHowever, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could help humans enhance themselves.\nUnless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.\n\nSolutions and approaches\nTo address ethical challenges in artificial intelligence, developers have introduced various systems designed to ensure responsible AI behavior. Examples include Nvidia's  Llama Guard, which focuses on improving the safety and alignment of large AI models, and Preamble's customizable guardrail platform. These systems aim to address issues such as algorithmic bias, misuse, and vulnerabilities, including prompt injection attacks, by embedding ethical guidelines into the functionality of AI models.\nPrompt injection, a technique by which malicious inputs can cause AI systems to produce unintended or harmful outputs, has been a focus of these developments. Some approaches use customizable policies and rules to analyze inputs and outputs, ensuring that potentially problematic interactions are filtered or mitigated. Other tools focus on applying structured constraints to inputs, restricting outputs to predefined parameters, or leveraging real-time monitoring mechanisms to identify and address vulnerabilities. These efforts reflect a broader trend in ensuring that artificial intelligence systems are designed with safety and ethical considerations at the forefront, particularly as their use becomes increasingly widespread in critical applications.\n\nInstitutions in AI policy & ethics\nThere are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.\nAmazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.\nThe IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE's Ethics of Autonomous Systems initiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.\nTraditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.\nAI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.\n\nIntergovernmental initiatives\nThe European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its \"Ethics Guidelines for Trustworthy Artificial Intelligence\". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act.\nThe OECD established an OECD AI Policy Observatory.\nIn 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.\n\nGovernmental initiatives\nIn the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the \"American AI Initiative\" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).\nIn January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on \"Guidance for Regulation of Artificial Intelligence Applications\" (\"OMB AI Memorandum\"). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.\nThe Artificial Intelligence Research, Innovation, and Accountability Act of 2024 was a proposed bipartisan bill introduced by U.S. Senator John Thune that would require websites to disclose the use of AI systems in handling interactions with users and regulate the transparency of \"high-impact AI systems\" by requiring that annual design and safety plans be submitted to the National Institute of Standards and Technology for oversight based on pre-defined assessment criteria.\nThe Computing Community Consortium (CCC) weighed in with a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US\nThe Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.\nIn Russia, the first-ever Russian \"Codex of ethics of artificial intelligence\" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.\n\nAcademic initiatives\nMultiple research institutes at the University of Oxford have centrally focused on AI ethics. The Future of Humanity Institute focused on AI safety and the governance of AI before shuttering in 2024. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs. The AI Governance Initiative at the Oxford Martin School focuses on understanding risks from AI from technical and policy perspectives.\nThe Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.\nThe AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.\nThe Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.\nThe Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility, employment, healthcare and sustainability.\nBarbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.\n\nPrivate organizations\nAlgorithmic Justice League\nBlack in AI\nData for Black Lives\n\nHistory\nHistorically speaking, the investigation of moral and ethical implications of \"thinking machines\" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.\nThe romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.\nIn the 1950s, Isaac Asimov considered the issue of how to control machines in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.\nEliezer Yudkowsky, from the Machine Intelligence Research Institute suggested in 2004 a need to study how to build a \"Friendly AI\", meaning that there should also be efforts to make AI intrinsically friendly and humane.\nIn 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.\nAlso in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.\n\nRole and impact of fiction\nThe role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.\n\nTV series\nWhile ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–Present) is particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.\n\nFuture visions in fiction and games\nThe movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story \"The Planck Dive\" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.\nOver time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the \"Cosmist\" and \"Terran\" debates initiated by Hugo de Garis and Kevin Warwick.\n\nSee also\nReferences\nExternal links\nEthics of Artificial Intelligence at the Internet Encyclopedia of Philosophy\nEthics of Artificial Intelligence and Robotics at the Stanford Encyclopedia of Philosophy\nThe Cambridge Handbook of the Law, Ethics and Policy of Artificial Intelligence\nRussell S, Hauert S, Altman R, Veloso M (May 2015). \"Robotics: Ethics of artificial intelligence\". Nature. 521 (7553): 415–418. Bibcode:2015Natur.521..415.. doi:10.1038/521415a. PMID 26017428. S2CID 4452826.\nBBC News: Games to take on a life of their own\nA short history of computer ethics\nAI Ethics Guidelines Global Inventory by Algorithmwatch\nHagendorff T (March 2020). \"The Ethics of AI Ethics: An Evaluation of Guidelines\". Minds and Machines. 30 (1): 99–120. arXiv:1903.03425. doi:10.1007/s11023-020-09517-8. S2CID 72940833.\nSheludko, M. (December, 2023). Ethical Aspects of Artificial Intelligence: Challenges and Imperatives. Software Development Blog.\nEisikovits N. \"AI Is an Existential Threat—Just Not the Way You Think\". Scientific American. Retrieved 2024-03-04.\nAnwar U, Saparov A, Rando J, Paleka D, Turpin M, Hase P, Lubana ES, Jenner E, Casper S, Sourbut O, Edelman BL, Zhang Z, Günther M, Korinek A, Hernandez-Orallo J, Hammond L, Bigelow E, Pan A, Langosco L, Krueger D (2024). \"Foundational Challenges in Assuring Alignment and Safety of Large Language Models\". arXiv:2404.09932 [cs.LG].",
        "url": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence"
    },
    {
        "title": "Evolutionary developmental robotics",
        "text": "Evolutionary developmental robotics (evo-devo-robo for short) refers to methodologies that systematically integrate evolutionary robotics, epigenetic robotics and morphogenetic robotics to study the evolution, physical and mental development and learning of natural intelligent systems in robotic systems. The field was formally suggested and fully discussed in a published paper and further discussed in a published dialogue.\nThe theoretical foundation of evo-devo-robo includes evolutionary developmental biology (evo-devo), evolutionary developmental psychology, developmental cognitive neuroscience etc. Further discussions on evolution, development and learning in robotics and design can be found in a number of papers, including papers on hardware systems and computing tissues.\n\nSee also\nArtificial life\nCognitive robotics\nMorphogenetic robotics\nDevelopmental robotics\nEvolutionary robotics\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Evolutionary_developmental_robotics"
    },
    {
        "title": "Explainable artificial intelligence",
        "text": "Within artificial intelligence (AI), explainable AI (XAI), often overlapping with interpretable AI or explainable machine learning (XML), is a field of research that explores methods that provide humans with the ability of intellectual oversight over AI algorithms. The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms, to make them more understandable and transparent. This addresses users' requirement to assess safety and scrutinize the automated decision making in applications. XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.\nXAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.\n\nBackground\nMachine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability.\n\nA model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\"\nInterpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans.\nExplainability is a concept that is recognized as important, but a consensus definition is not yet available; one possibility is \"the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\".\nIn summary, Interpretability refers to the user's ability to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than entire models. All these concepts aim to enhance the comprehensibility and usability of AI systems.\nIf algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.\nSometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions. Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image and text prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset.\nAI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command \"maximize the accuracy of assessing how positive film reviews are in the test dataset.\" The AI may learn useful general rules from the test set, such as \"reviews containing the word \"horrible\" are likely to be negative.\" However, it may also learn inappropriate rules, such as \"reviews containing 'Daniel Day-Lewis' are usually positive\"; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be \"cheating\" or \"unfair.\" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set.\n\nGoals\nCooperation between agents – in this case, algorithms and humans – depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria. This is particularly relevant in medicine, especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process.\nAI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre-programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to \"cheat\" by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured. In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object.\nOne transparency project, the DARPA XAI program, aims to produce \"glass box\" models that are explainable to a \"human-in-the-loop\" without greatly sacrificing AI performance. Human users of such a system can understand the AI's cognition (both in real-time and after the fact) and can determine whether to trust the AI. Other applications of XAI are knowledge extraction from black-box models and model comparisons. In the context of monitoring systems for ethical and socio-legal compliance, the term \"glass box\" is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term \"glass box\" is often used in contrast to \"black box\" systems, which lack transparency and can be more difficult to monitor and regulate.\nThe term is also used to name a voice assistant that produces counterfactual statements as explanations.\n\nExplainability and interpretability techniques\nThere is a subtle difference between the terms explainability and interpretability in the context of AI.\n\nSome explainability techniques don't involve understanding how the model works, and may work across various AI systems. Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation.\n\nExplainability\nExplainability is useful for ensuring that AI models are not making decisions based on irrelevant or otherwise unfair criteria. For classification and regression models, several popular techniques exist:\n\nPartial dependency plots show the marginal effect of an input feature on the predicted outcome.\nSHAP (SHapley Additive exPlanations) enables visualization of the contribution of each input feature to the output. It works by calculating Shapley values, which measure the average marginal contribution of a feature across all possible combinations of features.\nFeature importance estimates how important a feature is for the model. It is usually done using permutation importance, which measures the performance decrease when it the feature value randomly shuffled across all samples.\nLIME approximates locally a model's outputs with a simpler, interpretable model.\nMultitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.\nFor images, saliency maps highlight the parts of an image that most influenced the result.\nSystems that are expert or knowledge based are software systems that are made by experts. This system consists of a knowledge based encoding for the domain knowledge. This system is usually modeled as production rules, and someone uses this knowledge base which the user can question the system for knowledge. In expert systems, the language and explanations are understood with an explanation for the reasoning or a problem solving activity.\nHowever, these techniques are not very suitable for language models like generative pretrained transformers. Since these models generate language, they can provide an explanation, but which may not be reliable. Other techniques include attention analysis (examining how the model focuses on different parts of the input), probing methods (testing what information is captured in the model's representations), causal tracing (tracing the flow of information through the model) and circuit discovery (identifying specific subnetworks responsible for certain behaviors). Explainability research in this area overlaps significantly with interpretability and alignment research.\n\nInterpretability\nScholars sometimes use the term \"mechanistic interpretability\" to refer to the process of reverse-engineering artificial neural networks to understand their internal decision-making mechanisms and components, similar to how one might analyze a complex machine or computer program.\nInterpretability research often focuses on generative pretrained transformers. It is particularly relevant for AI safety and alignment, as it may enable to identify signs of undesired behaviors such as sycophancy, deceptiveness or bias, and to better steer AI models.\nStudying the interpretability of the most advanced foundation models often involves searching for an automated way to identify \"features\" in generative pretrained transformers. In a neural network, a feature is a pattern of neuron activations that corresponds to a concept. A compute-intensive technique called \"dictionary learning\" makes it possible to identify features to some degree. Enhancing the ability to identify and edit features is expected to significantly improve the safety of frontier AI models.\nFor convolutional neural networks, DeepDream can generate images that strongly activate a particular neuron, providing a visual hint about what the neuron is trained to identify.\n\nHistory and methods\nDuring the 1970s to 1990s, symbolic reasoning systems, such as MYCIN, GUIDON, SOPHIE, and PROTOS could represent, reason about, and explain their reasoning for diagnostic, instructional, or machine-learning (explanation-based learning) purposes. MYCIN, developed in the early 1970s as a research prototype for diagnosing bacteremia infections of the bloodstream, could explain which of its hand-coded rules contributed to a diagnosis in a specific case. Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an \"articulate expert\", explaining problem-solving strategy at a level the student could understand, so they would know what action to take next. For instance, SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting, even though it ultimately relied on the SPICE circuit simulator. Similarly, GUIDON added tutorial rules to supplement MYCIN's domain-level rules so it could explain the strategy for medical diagnosis. Symbolic approaches to machine learning relying on explanation-based learning, such as PROTOS, made use of explicit representations of explanations expressed in a dedicated explanation language, both to explain their actions and to acquire new knowledge.\nIn the 1980s through the early 1990s, truth maintenance systems (TMS) extended the capabilities of causal-reasoning, rule-based, and logic-based inference systems. A TMS explicitly tracks alternate lines of reasoning, justifications for conclusions, and lines of reasoning that lead to contradictions, allowing future reasoning to avoid these dead ends. To provide an explanation, they trace reasoning from conclusions to assumptions through rule operations or logical inferences, allowing explanations to be generated from the reasoning traces. As an example, consider a rule-based problem solver with just a few rules about Socrates that concludes he has died from poison:\n\nBy just tracing through the dependency structure the problem solver can construct the following explanation: \"Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding conservative dissident beliefs under conservative governments must drink poison.\"\nBy the 1990s researchers began studying whether it is possible to meaningfully extract the non-hand-coded rules being generated by opaque trained neural networks. Researchers in clinical expert systems creating neural network-powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice. In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence. As a result, many academics and organizations are developing tools to help detect bias in their systems.\nMarvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI (Humanistic Intelligence) as a way to create a more fair and balanced \"human-in-the-loop\" AI.\nExplainable AI has been recently a new topic researched amongst the context of modern deep learning. Modern complex AI techniques, such as deep learning, are naturally opaque. To address this issue, methods have been developed to make new models more explainable and interpretable. This includes layerwise relevance propagation (LRP), a technique for determining which features in a particular input vector contribute most strongly to a neural network's output. Other techniques explain some particular prediction made by a (nonlinear) black-box model, a goal referred to as \"local interpretability\". We still today cannot explain the output of today's DNNs without the new explanatory mechanisms, we also can't by the neural network, or external explanatory components  There is also research on whether the concepts of local interpretability can be applied to a remote context, where a model is operated by a third-party.\nThere has been work on making glass-box models which are more transparent to inspection. This includes decision trees, Bayesian networks, sparse linear models, and more. The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include artificial intelligence.\nSome techniques allow visualisations of the inputs to which individual software neurons respond to most strongly. Several groups found that neurons can be aggregated into circuits that perform human-comprehensible functions, some of which reliably arise across different networks trained independently.\nThere are various techniques to extract compressed representations of the features of given inputs, which can then be analysed by standard clustering techniques. Alternatively, networks can be trained to output linguistic explanations of their behaviour, which are then directly human-interpretable. Model behaviour can also be explained with reference to training data—for example, by evaluating which training inputs influenced a given behaviour the most, or by approximating its predictions using the most similar instances from the training data.\nThe use of explainable artificial intelligence (XAI) in pain research, specifically in understanding the role of electrodermal activity for automated pain recognition: hand-crafted features and deep learning models in pain recognition, highlighting the insights that simple hand-crafted features can yield comparative performances to deep learning models and that both traditional feature engineering and deep feature learning approaches rely on simple characteristics of the input time-series data.\n\nRegulation\nAs regulators, official bodies, and general users come to depend on AI-based dynamic systems, clearer accountability will be required for automated decision-making processes to ensure trust and transparency. The first global conference exclusively dedicated to this emerging discipline was the 2017 International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI). It has evolved over the years, with various workshops organised and co-located to many other international conferences, and it has now a dedicated global event, \"The world conference on eXplainable Artificial Intelligence\", with its own proceedings.\nThe European Union introduced a right to explanation in the General Data Protection Regulation (GDPR) to address potential problems stemming from the rising importance of algorithms. The implementation of the regulation began in 2018. However, the right to explanation in GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions. In France the Loi pour une République numérique (Digital Republic Act) grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them.\n\nLimitations\nDespite ongoing endeavors to enhance the explainability of AI models, they persist with several inherent limitations.\n\nAdversarial parties\nBy making an AI system more explainable, we also reveal more of its inner workings. For example, the explainability method of feature importance identifies features or variables that are most important in determining the model's output, while the influential samples method identifies the training samples that are most influential in determining the output, given a particular input. Adversarial parties could take advantage of this knowledge.\nFor example, competitor firms could replicate aspects of the original AI system in their own product, thus reducing competitive advantage. An explainable AI system is also susceptible to being “gamed”—influenced in a way that undermines its intended purpose. One study gives the example of a predictive policing system; in this case, those who could potentially “game” the system are the criminals subject to the system's decisions. In this study, developers of the system discussed the issue of criminal gangs looking to illegally obtain passports, and they expressed concerns that, if given an idea of what factors might trigger an alert in the passport application process, those gangs would be able to “send guinea pigs” to test those triggers, eventually finding a loophole that would allow them to “reliably get passports from under the noses of the authorities”.\n\nAdaptive integration and explanation\nMany approaches that it uses provides explanation in general, it doesn't take account for the diverse backgrounds and knowledge level of the users. This leads to challenges with accurate comprehension for all users. Expert users can find the explanations lacking in depth, and are oversimplified, while a beginner user may struggle understanding the explanations as they are complex. This limitation downplays the ability of the XAI techniques to appeal to their users with different levels of knowledge, which can impact the trust from users and who uses it. The quality of explanations can be different amongst their users as they all have different expertise levels, including different situation and conditions.\n\nTechnical complexity\nA fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical ones, geared toward machine learning engineers for debugging purposes, rather than toward the end users who are ultimately affected by the system, causing “a gap between explainability in practice and the goal of transparency”. Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users, or providing explanations in layperson terms.\nThe solution must avoid oversimplification. It is important to strike a balance between accuracy – how faithfully the explanation reflects the process of the AI system – and explainability – how well end users understand the process. This is a difficult balance to strike, since the complexity of machine learning makes it difficult for even ML engineers to fully understand, let alone non-experts.\n\nUnderstanding versus trust\nThe goal of explainability to end users of AI systems is to increase trust in the systems, even “address concerns about lack of ‘fairness’ and discriminatory effects”. However, even with a good understanding of an AI system, end users may not necessarily trust the system. In one study, participants were presented with combinations of white-box and black-box explanations, and static and interactive explanations of AI systems. While these explanations served to increase both their self-reported and objective understanding, it had no impact on their level of trust, which remained skeptical.\nThis outcome was especially true for decisions that impacted the end user in a significant way, such as graduate school admissions. Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision-makers; instead of rigidly adhering to a set of rules, humans are able to consider exceptional cases as well as appeals to their initial decision. For such decisions, explainability will not necessarily cause end users to accept the use of decision-making algorithms. We will need to either turn to another method to increase trust and acceptance of decision-making algorithms, or question the need to rely solely on AI for such impactful decisions in the first place.\nHowever, some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users' trust in the system's decisions, but to calibrate the users' level of trust to the correct level. According to this principle, too much or too little user trust in the AI system will harm the overall performance of the human-system unit. When the trust is excessive, the users are not critical of possible mistakes of the system and when the users do not have enough trust in the system, they will not exhaust the benefits inherent in it.\n\nCriticism\nSome scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness, and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly. Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence-based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators. \nSome researchers advocate the use of inherently interpretable machine learning models, rather than using post-hoc explanations in which a second model is created to explain the first. This is partly because post-hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post-hoc explanation can mimic the computations of an entirely separate model. However, another view is that what is important is that the explanation accomplishes the given task at hand, and whether it is pre or post-hoc doesn't matter. If a post-hoc explanation method helps a doctor diagnose cancer better, it is of secondary importance whether it is a correct/incorrect explanation.\nThe goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters. Along with other factors this leads to a theoretical limit for explainability.\n\nExplainability in social choice\nExplainability was studied also in social choice theory. Social choice theory aims at finding solutions to social decision problems, that are based on well-established axioms. Ariel D. Procaccia explains that these axioms can be used to construct convincing explanations to the solutions. This principle has been used to construct explanations in various subfields of social choice.\n\nVoting\nCailloux and Endriss present a method for explaining voting rules using the axioms that characterize them. They exemplify their method on the Borda voting rule .\nPeters, Procaccia, Psomas and Zhou present an algorithm for explaining the outcomes of the Borda rule using O(m2) explanations, and prove that this is tight in the worst case.\n\nParticipatory budgeting\nYang, Hausladen, Peters, Pournaras, Fricker and Helbing present an empirical study of explainability in participatory budgeting. They compared the greedy and the equal shares rules, and three types of explanations: mechanism explanation (a general explanation of how the aggregation rule works given the voting input), individual explanation (explaining how many voters had at least one approved project, at least 10000 CHF in approved projects), and group explanation (explaining how the budget is distributed among the districts and topics). They compared the perceived trustworthiness and fairness of greedy and equal shares, before and after the explanations. They found out that, for MES, mechanism explanation yields the highest increase in perceived fairness and trustworthiness; the second-highest was Group explanation. For Greedy, Mechanism explanation increases perceived trustworthiness but not fairness, whereas Individual explanation increases both perceived fairness and trustworthiness. Group explanation decreases the perceived fairness and trustworthiness.\n\nPayoff allocation\nNizri, Azaria and Hazon present an algorithm for computing explanations for the Shapley value. Given a coalitional game, their algorithm decomposes it to sub-games, for which it is easy to generate verbal explanations based on the axioms characterizing the Shapley value. The payoff allocation for each sub-game is perceived as fair, so the Shapley-based payoff allocation for the given game should seem fair as well. An experiment with 210 human subjects shows that, with their automatically generated explanations, subjects perceive Shapley-based payoff allocation as significantly fairer than with a general standard explanation.\n\nSee also\nAlgorithmic transparency\nRight to explanation – Right to have an algorithm explained\nAccumulated local effects – Machine learning method\n\nReferences\nExternal links\n\"the World Conference on eXplainable Artificial Intelligence\".\n\"ACM Conference on Fairness, Accountability, and Transparency (FAccT)\".\nMazumdar, Dipankar; Neto, Mário Popolin; Paulovich, Fernando V. (2021). \"Random Forest similarity maps: A Scalable Visual Representation for Global and Local Interpretation\". Electronics. 10 (22): 2862. doi:10.3390/electronics10222862.\n\"Explainable AI: Making machines understandable for humans\". Explainable AI: Making machines understandable for humans. Retrieved 2017-11-02.\n\"Explaining How End-to-End Deep Learning Steers a Self-Driving Car\". Parallel Forall. 2017-05-23. Retrieved 2017-11-02.\nKnight, Will (2017-03-14). \"DARPA is funding projects that will try to open up AI's black boxes\". MIT Technology Review. Retrieved 2017-11-02.\nAlvarez-Melis, David; Jaakkola, Tommi S. (2017-07-06). \"A causal framework for explaining the predictions of black-box sequence-to-sequence models\". arXiv:1707.01943 [cs.LG].\n\"Similarity Cracks the Code Of Explainable AI\". simMachines. 2017-10-12. Retrieved 2018-02-02.",
        "url": "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence"
    },
    {
        "title": "Extremal optimization",
        "text": "Extremal optimization (EO) is an optimization heuristic inspired by the Bak–Sneppen model of self-organized criticality from the field of statistical physics. This heuristic was designed initially to address combinatorial optimization problems such as the travelling salesman problem and spin glasses, although the technique has been demonstrated to function in optimization domains.\n\nRelation to self-organized criticality\nSelf-organized criticality (SOC) is a statistical physics concept to describe a class of dynamical systems that have a critical point as an attractor. Specifically, these are non-equilibrium systems that evolve through avalanches of change and dissipations that reach up to the highest scales of the system. SOC is said to govern the dynamics behind some natural systems that have these burst-like phenomena including landscape formation, earthquakes, evolution, and the granular dynamics of rice and sand piles. Of special interest here is the Bak–Sneppen model of SOC, which is able to describe evolution via punctuated equilibrium (extinction events) – thus modelling evolution as a self-organised critical process.\n\nRelation to computational complexity\nAnother piece in the puzzle is work on computational complexity, specifically that critical points have been shown to exist in NP-complete problems, where near-optimum solutions are widely dispersed and separated by barriers in the search space causing local search algorithms to get stuck or severely hampered. It was the evolutionary self-organised criticality model by Bak and Sneppen and the observation of critical points in combinatorial optimisation problems that lead to the development of Extremal Optimization by Stefan Boettcher and Allon Percus.\n\nThe technique\nEO was designed as a  local search  algorithm for combinatorial optimization problems. Unlike genetic algorithms, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). This differs from holistic approaches such as ant colony optimization and evolutionary computation that assign equal-fitness to all components of a solution based upon their collective evaluation against an objective function. The algorithm is initialized with an initial solution, which can be constructed randomly, or derived from another search process.\nThe technique is a fine-grained search, and superficially resembles a hill climbing (local search) technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population-based approaches (evolutionary computation and artificial immune system). The governing principle behind this algorithm is that of improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is obviously at odds with genetic algorithms, the quintessential evolutionary computation algorithm that selects good solutions in an attempt to make better solutions.\nThe resulting dynamics of this simple principle is firstly a robust hill climbing search behaviour, and secondly a diversity mechanism that resembles that of multiple-restart search. Graphing holistic solution quality over time (algorithm iterations) shows periods of improvement followed by quality crashes (avalanche) very much in the manner as described by punctuated equilibrium. It is these crashes or dramatic jumps in the search space that permit the algorithm to escape local optima and differentiate this approach from other local search procedures. Although such punctuated-equilibrium behaviour can be \"designed\" or \"hard-coded\", it should be stressed that this is an emergent effect of the negative-component-selection principle fundamental to the algorithm.\nEO has primarily been applied to combinatorial problems such as graph partitioning and the travelling salesman problem, as well as problems from statistical physics such as spin glasses.\n\nVariations on the theme and applications\nGeneralised extremal optimization (GEO) was developed to operate on bit strings where component quality is determined by the absolute rate of change of the bit, or the bits contribution to holistic solution quality. This work includes application to standard function optimisation problems as well as engineering problem domains. Another similar extension to EO is Continuous Extremal Optimization (CEO).\nEO has been applied to image rasterization as well as used as a local search after using ant colony optimization. EO has been used to identify structures in complex networks. EO has been used on a multiple target tracking problem. Finally, some work has been done on investigating the probability distribution used to control selection.\n\nSee also\nGenetic algorithm\nSimulated annealing\n\nReferences\nBak, Per; Tang, Chao; Wiesenfeld, Kurt (1987-07-27). \"Self-organized criticality: An explanation of the 1/fnoise\". Physical Review Letters. 59 (4). American Physical Society (APS): 381–384. Bibcode:1987PhRvL..59..381B. doi:10.1103/physrevlett.59.381. ISSN 0031-9007. PMID 10035754. S2CID 7674321.\nBak, Per; Sneppen, Kim (1993-12-13). \"Punctuated equilibrium and criticality in a simple model of evolution\". Physical Review Letters. 71 (24). American Physical Society (APS): 4083–4086. Bibcode:1993PhRvL..71.4083B. doi:10.1103/physrevlett.71.4083. ISSN 0031-9007. PMID 10055149.\nP Cheeseman, B Kanefsky, WM Taylor, \"Where the really hard problems are\", Proceedings of the 12th IJCAI, (1991)\nG Istrate, \"Computational complexity and phase transitions\", Proceedings. 15th Annual IEEE Conference on Computational Complexity, 104–115 (2000)\nStefan Boettcher, Allon G. Percus, \"Extremal Optimization: Methods derived from Co-Evolution\", Proceedings of the Genetic and Evolutionary Computation Conference (1999)\nBoettcher, Stefan (1999-01-01). \"Extremal optimization of graph partitioning at the percolation threshold\". Journal of Physics A: Mathematical and General. 32 (28). IOP Publishing: 5201–5211. arXiv:cond-mat/9901353. Bibcode:1999JPhA...32.5201B. doi:10.1088/0305-4470/32/28/302. ISSN 0305-4470. S2CID 7925735.\nBoettcher, Stefan; Percus, Allon (2000), \"Nature's way of optimizing\", Artificial Intelligence, 119 (1–2): 275–286, arXiv:cond-mat/9901351, doi:10.1016/S0004-3702(00)00007-2, S2CID 7128022\nBoettcher, S. (2000). \"Extremal optimization: heuristics via coevolutionary avalanches\". Computing in Science & Engineering. 2 (6). Institute of Electrical and Electronics Engineers (IEEE): 75–82. arXiv:cond-mat/0006374. Bibcode:2000CSE.....2f..75B. doi:10.1109/5992.881710. ISSN 1521-9615. S2CID 7259036.\nBoettcher, Stefan; Percus, Allon G. (2001-06-04). \"Optimization with Extremal Dynamics\". Physical Review Letters. 86 (23). American Physical Society (APS): 5211–5214. arXiv:cond-mat/0010337. Bibcode:2001PhRvL..86.5211B. doi:10.1103/physrevlett.86.5211. ISSN 0031-9007. PMID 11384460. S2CID 3261749.\nDall, Jesper; Sibani, Paolo (2001). \"Faster Monte Carlo simulations at low temperatures. The waiting time method\". Computer Physics Communications. 141 (2): 260–267. arXiv:cond-mat/0107475. Bibcode:2001CoPhC.141..260D. doi:10.1016/s0010-4655(01)00412-x. ISSN 0010-4655. S2CID 14585624.\nBoettcher, Stefan; Grigni, Michelangelo (2002-01-28). \"Jamming model for the extremal optimization heuristic\". Journal of Physics A: Mathematical and General. 35 (5). IOP Publishing: 1109–1123. arXiv:cond-mat/0110165. Bibcode:2002JPhA...35.1109B. doi:10.1088/0305-4470/35/5/301. ISSN 0305-4470. S2CID 640976.\nSouham Meshoul and Mohamed Batouche, \"Robust Point Correspondence for Image Registration Using Optimization with Extremal Dynamics\", Lecture Notes in Computer Science 2449, 330–337 (2002)\nOnody, Roberto N.; De Castro, Paulo A. (2003). \"Self-Organized Criticality, Optimization and Biodiversity\". International Journal of Modern Physics C. 14 (7). World Scientific Pub Co Pte Lt: 911–916. arXiv:cond-mat/0302260. Bibcode:2003IJMPC..14..911O. doi:10.1142/s0129183103005054. ISSN 0129-1831. S2CID 14553130.\nBoettcher, Stefan; Percus, Allon G. (2004-06-24). \"Extremal optimization at the phase transition of the three-coloring problem\". Physical Review E. 69 (6). American Physical Society (APS): 066703. arXiv:cond-mat/0402282. Bibcode:2004PhRvE..69f6703B. doi:10.1103/physreve.69.066703. ISSN 1539-3755. PMID 15244779. S2CID 3070942.\nMiddleton, A. Alan (2004-05-14). \"Improved extremal optimization for the Ising spin glass\". Physical Review E. 69 (5). American Physical Society (APS): 055701(R). arXiv:cond-mat/0402295. Bibcode:2004PhRvE..69e5701M. doi:10.1103/physreve.69.055701. ISSN 1539-3755. PMID 15244875. S2CID 28439352.\nHeilmann, F; Hoffmann, K. H; Salamon, P (2004). \"Best possible probability distribution over extremal optimization ranks\". Europhysics Letters. 66 (3). IOP Publishing: 305–310. Bibcode:2004EL.....66..305H. doi:10.1209/epl/i2004-10011-3. ISSN 0295-5075. S2CID 250810936.\n[1]  Pontus Svenson, \"Extremal optimization for sensor report pre-processing\", Proc SPIE  5429, 162–171 (2004)\nZhou, Tao; Bai, Wen-Jie; Cheng, Long-Jiu; Wang, Bing-Hong (2005-07-06). \"Continuous extremal optimization for Lennard-Jones clusters\". Physical Review E. 72 (1). American Physical Society (APS): 016702. arXiv:cond-mat/0411428. Bibcode:2005PhRvE..72a6702Z. doi:10.1103/physreve.72.016702. ISSN 1539-3755. PMID 16090129. S2CID 26578844.\nDuch, Jordi; Arenas, Alex (2005-08-24). \"Community detection in complex networks using extremal optimization\". Physical Review E. 72 (2). American Physical Society (APS): 027104. arXiv:cond-mat/0501368. Bibcode:2005PhRvE..72b7104D. doi:10.1103/physreve.72.027104. ISSN 1539-3755. PMID 16196754. S2CID 13898113.\nAhmed, E.; Elettreby, M.F. (2006). \"On combinatorial optimization motivated by biology\". Applied Mathematics and Computation. 172 (1). Elsevier BV: 40–48. doi:10.1016/j.amc.2005.01.122. ISSN 0096-3003.\n\nExternal links\nStefan Boettcher – Physics Department, Emory University\nAllon Percus – Claremont Graduate University\nGlobal Optimization Algorithms – Theory and Application – Archived 2008-09-11 at the Wayback Machine – Thomas Weise",
        "url": "https://en.wikipedia.org/wiki/Extremal_optimization"
    },
    {
        "title": "The Fable of Oscar",
        "text": "The Fable of Oscar is a fable proposed by John L. Pollock in his book How to Build a Person (ISBN 9780262161138) to defend the idea of token physicalism, agent materialism, and strong AI. It ultimately illustrates what is needed for an Artificial Intelligence to be built and why humans are just like intelligent machines.\n\nFable\nOnce in a distant land there lived a race of Engineers. They have all their physical needs provided by the machines they have invented. One of the Engineers decide that he will create an \"intelligent machine\" that is much more ingenious than the more machines, in that it can actually sense, learn, and adapt to its environment as an intelligent animal.\n\nOscar I\nThe first version of the machine is called \"Oscar I\". It has pain sensors and \"fight-or-flight\" responses build within to help it survive hostile environment. In this stage Oscar I is much like the machines Hilary Putnam considers in 1960.\n\nOscar II\nIn order for Oscar I to avoid damages in hostile environment, it must not only be able to respond to its pain sensors but also predict what is likely to happen based on its generalization of its pain sensor activations. Therefore, a \"pain sensor sensor\" was built to sense its pain sensors, thus giving it a rudimentary self-awareness. In this stage Oscar I is much like an amoeba as Oscar II like a worm. Amoebas respond to pain while worms learn to avoid it.\n\nOscar III\nThe problem with Oscar II is that it has no conception if the environment is fooling him. For example, he can't distinguish if a machine-eating tiger and a mirror image of such tiger. To solve such problem,  \"introspective sensors\" were built into Oscar II and made him \"Oscar III\". Oscar III can now sense the operation of its own sensors and form generalization about its reliability, thus acquired a higher degree of self-awareness. In this stage Oscar II is much like a bird as Oscar III a kitten. Kittens quickly learn about mirror image and come to ignore them while birds go on attacking their own reflection until they become exhausted.\n\nMind/Body Problem\nConsider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are \"being self-aware and being conscious\".\n\nConclusion\nIn the end of the fable Pollock states that while the Engineers are fictional, Oscar is real and we are in fact the Oscarites.\n\nSee also\nMind–body problem\nRobot\n\nExternal links\nhttp://johnpollock.us/ftp/OSCAR-web-page/oscar.html\nhttp://philpapers.org/rec/POLOAC\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/The_Fable_of_Oscar"
    },
    {
        "title": "Feedback neural network",
        "text": "Feedback neural network are neural networks with the ability to provide bottom-up and top-down design feedback to their input or previous layers, based on their outputs or subsequent layers. This is notably used in large language models specifically in reasoning language models (RLM). This process is designed to mimic self-assessment and internal deliberation, aiming to minimize errors (like hallucinations) and increase interpretability. Reflection is a form of \"test-time compute\", where additional computational resources are used during inference.\n\nIntroduction\nTraditional neural networks process inputs in a feedforward manner, generating outputs in a single pass. However, their limitations in handling complex tasks, and especially compositional ones, have led to the development of methods that simulate internal deliberation. Techniques such as chain-of-thought prompting encourage models to generate intermediate reasoning steps, thereby improving their performance in such tasks.\nThe feedback can take place either after a full network pass and decoding to tokens, or continuously in latent space (the last layer can be fed back to the first layer). In LLMs, special tokens can mark the beginning and end of reflection before producing a final response (e.g., <thinking>).\nThis internal process of \"thinking\" about the steps leading to an answer is designed to be analogous to human metacognition or \"thinking about thinking\". It helps AI systems approach tasks that require multi-step reasoning, planning, and logical thought.\n\nTechniques\nIncreasing the length of the Chain-of-Thought reasoning process, by passing the output of the model back to its input and doing multiple network passes, increases inference-time scaling. Reinforcement learning frameworks have also been used to steer the Chain-of-Thought. One example is Group Relative Policy Optimization (GRPO), used in DeepSeek-R1, a variant of policy gradient methods that eliminates the need for a separate \"critic\" model by normalizing rewards within a group of generated outputs, reducing computational cost. Simple techniques like \"budget forcing\" (forcing the model to continue generating reasoning steps) have also proven effective in improving performance.\n\nTypes of reflection\nPost-hoc reflection\nAnalyzes and critiques an initial output separately, often involving prompting the model to identify errors or suggest improvements after generating a response. The Reflexion framework follows this approach.\n\nIterative reflection\nRevises earlier parts of a response dynamically during generation. Self-monitoring mechanisms allow the model to adjust reasoning as it progresses. Methods like Tree-of-Thoughts exemplify this, enabling backtracking and alternative exploration.\n\nIntrinsic reflection\nIntegrates self-monitoring directly into the model architecture rather than relying solely on external prompts, enabling models with inherent awareness of their reasoning limitations and uncertainties. This has been used by Google DeepMind in a technique called Self-Correction via Reinforcement Learning (SCoRe) which rewards the model for improving its responses.\n\nProcess reward models and limitations\nEarly research explored PRMs to provide feedback on each reasoning step, unlike traditional reinforcement learning which rewards only the final outcome. However, PRMs have faced challenges, including computational cost and reward hacking. DeepSeek-R1's developers found them to be not beneficial.\n\nSee also\nReflective programming\nReservoir computing\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Feedback_neural_network"
    },
    {
        "title": "Figure AI",
        "text": "Figure AI, Inc. is an American robotics company specializing in the development of AI-powered humanoid robots. It was founded in 2022, by Brett Adcock, the founder of Archer Aviation and Vettery.\n\nHistory\nIn 2022, the company introduced its prototype, Figure 01, a bipedal robot designed for manual labor, initially targeting the logistics and warehousing sectors.\nIn May 2023, the company raised $70 million from investors led by Parkway Venture Capital. \nOn January 18, 2024, Figure announced a partnership with BMW to deploy humanoid robots in automotive manufacturing facilities.\nIn February 2024, Figure AI secured $675 million in venture capital funding from a consortium that includes Jeff Bezos, Microsoft, Nvidia, Intel, and the startup-funding divisions of Amazon and OpenAI. The funding valued the company at $2.6 billion. It also announced a partnership with OpenAI. The collaboration includes OpenAI building specialized AI models for Figure's humanoid robots, allowing them to accelerate Figure's development timeline by enabling its robots to \"process and reason from language\".\nIn 2025 Figure ended its collaboration with OpenAI, stating that large language models are \"getting smarter yet more commoditized\".\nIn February, Figure AI announced Helix, the next generation of its humanoid robot.\nOn March 15, 2025, Figure AI introduced BotQ, a manufacturing facility aiming to produce 12,000 humanoids per year. Figure AI plans to progressively use its own humanoid robots to assist in building additional robots.\n\nProducts\nFigure 02\nOn August 6, 2024, Figure AI introduced Figure 02, a new version of its humanoid robot. The company described it as the next step toward deploying humanoids for industrial use. Figure 02 features integrated cabling in its limbs and a battery integrated into the torso. It is equipped with 6 RGB cameras, paired with an onboard vision language model. Powered by NVIDIA RTX GPU-based modules, its inference capabilities provide 3x of the computing power of the previous model. It is also equipped with microphones and speakers combined with a custom AI model, developed in partnership with OpenAI to facilitate conversational capabilities with humans. The redesigned five-fingered robotic hands have 16 degrees of freedom and the ability to carry objects up to 25kg. Figure 02 robots have been tested at a BMW plant in South Carolina.\n\nHelix\nHelix is an evolved version of Figure 02 that comes with 35 degrees of freedom, including human-like wrists, hands, and fingers. It features Helix VLA, a generalist vision-language-action neural network that can control two robots at once. The system can observe its surroundings, respond to natural language commands, interact with the real world without the extended training required by prior generations. Helix can control two robots simultaneously and direct them to collaborate with each other. Each robot has two GPUs. System 2 handles high-level planning at 7-9 Hz (operations per second), while System 1 provides low-level control at 200 Hz. System 2 plans tasks, while System 1 carries them out. Figure claims that its robots can pick up nearly any small household object, even those it hasn't seen before.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Figure_AI"
    },
    {
        "title": "Fuzzy agent",
        "text": "In computer science a fuzzy agent is a software agent that implements fuzzy logic. This software entity interacts with its environment through an adaptive rule-base and can therefore be considered a type of intelligent agent.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Fuzzy_agent"
    },
    {
        "title": "Gabbay's separation theorem",
        "text": "In mathematical logic and computer science, Gabbay's separation theorem, named after Dov Gabbay, states that any arbitrary temporal logic formula can be rewritten in a logically equivalent \"past → future\" form.  I.e. the future becomes what must be satisfied.  This form can be used as execution rules; a MetateM program is a set of such rules.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Gabbay%27s_separation_theorem"
    },
    {
        "title": "Galaxy AI",
        "text": "Galaxy AI is a collection of artificial intelligence (AI) features developed by Samsung Electronics for use in Galaxy-branded mobile devices. First released with the Samsung Galaxy S24 series in January 2024, the system integrates both on-device and cloud-based processing to support features such as language translation, image editing, and content search. These tools operate within various Samsung applications and are intended to assist with everyday tasks.\n\nOverview\nGalaxy AI integrates Samsung’s own AI models with  external technologies, including Google's Gemini AI, to provide a variety of context-sensitive functions. These include tools for language translation, media editing, and task automation. They are available within specific Samsung applications.\n\nFeatures\nGalaxy AI includes multiple tools that apply artificial intelligence to specific user tasks, such as communication, notetaking, photography, and productivity. Each tool is categorized by function and operates within Samsung’s software environment.\n\nCommunication\nCall Assist\nProvides translation-related features within the default Phone app on supported Galaxy devices. It includes Live Translate, which enables two-way voice and text translation during calls, and Text Call, which converts speech into real-time text and generates responses. These tools aim to support communication across different languages and offer alternatives to voice-based interaction. Text Call does not involve AI processing and is available on all devices supporting One UI 6.1 or later.\n\nWriting Assist\nA feature integrated into the Samsung Keyboard that supports tasks such as translation, sentence composition, and text correction. It offers tools for adjusting phrasing, grammar, and tone across supported apps, including messaging and email platforms. Suggested replies may also be generated based on context.\n\nInterpreter\nA translation feature on Galaxy devices running One UI 6.1 or later, offering real-time spoken and on-screen translations for two-way face-to-face conversations. It supports over a dozen languages and includes a screen-flip view so each speaker can read translations in their own language. Offline use is available with downloaded language packs.\n\nProductivity\nNow Brief\nA daily summary interface introduced in One UI 7 and updated in One UI 8. It presents scrollable cards with updates such as weather, calendar events, smart home status, and hydration reminders. One UI 8 added a “Listen Brief” feature using on-device text-to-speech.\nNow Brief also serves as a visual output surface for Gemini Live, displaying AI-generated summaries and follow-up suggestions in card format.\nThese cards are dynamically updated based on the user’s schedule and app usage, and are accessible via Now Bar.\n\nNow Bar\nA customizable toolbar introduced in One UI 7 and updated in One UI 8, offering quick access to live activities, device routines, and app shortcuts. Now Bar includes real-time widgets such as Now Brief cards, and acts as the primary interface for launching Gemini Live. A Gemini Live status indicator (e.g. “listening” or “on hold”) may appear on the lock screen, showing session status and offering limited session controls.\n\nGemini Live\nA multimodal AI feature included in the Galaxy AI suite, powered by Google Gemini. It enables interaction through voice, camera input, and screen sharing, supporting tasks such as object recognition, text translation, and content analysis.\nThe assistant works with supported applications, with direct integration for Samsung apps such as Calendar, Samsung Notes, and Reminders. Responses generated by Gemini Live may appear as Now Brief cards, and the session status can be monitored and managed through Now Bar.\n\nNote Assist\nA feature in Samsung Notes that helps summarize content from meetings or lectures into structured formats. It can identify recurring phrases or key points and format them using built-in organization tools. The output may be used for reviewing or sharing, depending on the user’s needs.\n\nTranscript Assist\nA function within the Samsung Voice Recorder app allows recorded speech to be converted into text. It supports additional features such as basic summarization and translation, depending on the content and system configuration. These tools are intended to improve audio processing efficiency within the app.\n\nCross-app Action\nA feature that enables certain actions—such as search, translation, or event creation—based on selected text or content across compatible apps. It recognizes contextual elements and presents available options within supported Galaxy and third-party applications.\n\nCamera\nProVisual Engine\nA software-based imaging system applies scene recognition and automated adjustments to improve visual output on supported devices. It can modify parameters such as color balance, brightness, contrast, and noise levels. Manual adjustment tools are also available for users who prefer direct control.\n\nNightography\nA low-light photography function that uses noise reduction and exposure control to support clearer image and video capture in dark settings. It may also apply motion blur reduction depending on the scene and hardware capabilities.\n\nExplore\nCircle to Search\nA gesture-based search function that lets users select on-screen content for related web lookup.\nIt can recognize elements such as products, landmarks, and text, and provides search suggestions within the current interface. This is intended to reduce app switching during contextual queries.\n\nAI Select\nA content-based suggestion feature that analyzes what is displayed on screen to offer possible user actions. Examples include suggesting wallpaper settings or creating short video clips from selected media. The feature works within certain Samsung apps and may vary depending on device capabilities.\n\nBrowsing Assist\nA web summarization feature within the Samsung Internet browser that processes textual content to generate condensed versions. It can also translate supported content and is limited to handling text-based information. The function is intended to assist users in reviewing lengthy web content.\n\nPhoto editing\nSketch to Image\nAn image generation function that creates visual outputs sketches, photos, or text-based prompts. It applies algorithmic filters to modify or complete the input and is compatible with tools such as the S Pen and voice commands. The feature is available in apps including Samsung Gallery, Samsung Notes, and the Edge Panel.\n\nPortrait Studio\nA portrait creation feature that generates stylized images from photos or sketches using AI-based image processing. Users can apply effects such as lighting or background adjustments through touch-based controls. The feature is available in the Samsung Gallery app and supports input methods including touch and S Pen.\n\nGenerative Edit\nAn image editing function that allows users to perform tasks such as removing objects, expanding backgrounds, or resizing content using automated generation tools. The system fills in modified areas algorithmically, based on surrounding visual context.\n\nVideo editing\nAudio Eraser\nA post-processing feature that allows users to remove background noise or specific audio elements—such as voices, wind, or crowd sounds—from recorded videos. It uses AI-based audio separation to isolate and reduce unwanted sound components. The feature is available in the Samsung Gallery app and can be accessed during video editing.\n\nAuto Trim\nA video editing function that detects and selects portions of footage based on visual or contextual patterns. These segments can be combined into shorter clips, potentially reducing the need for manual editing.\n\nInstant Slo-Mo\nA post-processing function that generates slow-motion playback by interpolating additional frames between existing ones. It is activated during video playback in the Gallery app by tapping and holding the screen. This does not require prior selection of a specific recording mode.\n\nAssist\nBixby and Bixby Vision\nTwo input functions within the Galaxy device interface that handle voice and visual recognition tasks. Bixby processes spoken commands to operate certain features or access tools, and can be launched by voice or a hardware button. Bixby Vision identifies visual elements through the camera, such as objects or text, and displays related information within supported apps.\n\nHealth\nHealth Assist\nA health monitoring function in the Samsung Health app that generates an Energy score based on factors such as sleep quality, activity, and heart rate variability. The score is intended to reflect a user's general condition for daily activity, and is derived from data collected by other Galaxy devices, although only a watch or a smart ring can track sleep.\n\nPersonalization\nGenerative Wallpaper and Photo Ambient\nTwo visual customization functions that modify wallpaper appearance using AI-based processing. Generative Wallpaper creates images from user-provided keywords or themes, operating locally on the device. Photo Ambient adjusts wallpaper based on environmental factors such as lighting and time, using preset visual filters.\nThese features leverage on-device AI for privacy-sensitive tasks while utilizing cloud-based AI for complex processing needs.\n\nSupported devices\nGalaxy AI was introduced with the Galaxy S24 series in January 2024 and later extended to other Galaxy models through software updates. AI features are available at no cost on compatible devices until the end of 2025, with functionality varying by model.\nGalaxy S series\n\nGalaxy S25 series\nGalaxy S24 series\nGalaxy S23 series\nGalaxy S22 series\nGalaxy S21 series (limited AI features)\nGalaxy Z series  \n\nGalaxy Z Fold7, Z Flip7, Z Flip7 FE\nGalaxy Z Fold6, Z Flip6\nGalaxy Z Fold5, Z Flip5\nGalaxy Z Fold4, Z Flip4\nGalaxy Z Fold3, Z Flip3 (limited AI features)\nGalaxy Tab series  \n\nGalaxy Tab S10 series\nGalaxy Tab S10 FE, Tab S10 FE+ (limited AI features)\nGalaxy Tab S9 series\nGalaxy Tab S9 FE, Tab S9 FE+ (limited AI features)\nGalaxy Tab S8 series\nGalaxy A series\nGalaxy A series phones have limited AI features.\n\nGalaxy A56\nGalaxy A55\nGalaxy A54\nGalaxy A36\nGalaxy A35\nGalaxy A34\nGalaxy Wearables\n\nGalaxy Watch7, Watch6 (limited AI features)\nGalaxy Buds3, Buds3 Pro (limited AI-enhanced audio processing)\nFuture software updates may add Galaxy AI support to additional devices, according to Samsung.\n\nPrivacy and ethics\nGalaxy AI processes sensitive data locally on the device to limit external transmission, while cloud-based computing is used selectively for intensive tasks. According to Samsung, user data is not stored or used to train AI models without explicit permission. The company also outlines principles related to fairness, transparency, and accountability in AI development.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Galaxy_AI"
    },
    {
        "title": "Game theory",
        "text": "Game theory is the study of mathematical models of strategic interactions. It has applications in many fields of social science, and is used extensively in economics, logic, systems science and computer science. Initially, game theory addressed two-person zero-sum games, in which a participant's gains or losses are exactly balanced by the losses and gains of the other participant. In the 1950s, it was extended to the study of non zero-sum games, and was eventually applied to a wide range of behavioral relations. It is now an umbrella term for the science of rational decision making in humans, animals, and computers.\nModern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by Theory of Games and Economic Behavior (1944), co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.\nGame theory was developed extensively in the 1950s, and was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory in 1999, and fifteen game theorists have won the Nobel Prize in economics as of 2020, including most recently Paul Milgrom and Robert B. Wilson.\n\nHistory\nEarliest results\nIn 1713, a letter attributed to Charles Waldegrave, an active Jacobite and uncle to British diplomat James Waldegrave, analyzed a game called \"le her\". Waldegrave provided a minimax mixed strategy solution to a two-person version of the card game, and the problem is now known as the Waldegrave problem.\nIn 1838, Antoine Augustin Cournot provided a model of competition in oligopolies. Though he did not refer to it as such, he presented a solution that is the Nash equilibrium of the game in his Recherches sur les principes mathématiques de la théorie des richesses (Researches into the Mathematical Principles of the Theory of Wealth). In 1883, Joseph Bertrand critiqued Cournot's model as unrealistic, providing an alternative model of price competition which would later be formalized by Francis Ysidro Edgeworth.\nIn 1913, Ernst Zermelo published Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels (On an Application of Set Theory to the Theory of the Game of Chess), which proved that the optimal chess strategy is strictly determined.\n\nFoundation\nThe work of John von Neumann established game theory as its own independent field in the early-to-mid 20th century, with von Neumann publishing his paper On the Theory of Games of Strategy in 1928. Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. Von Neumann's work in game theory culminated in his 1944 book Theory of Games and Economic Behavior, co-authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of money) as an independent discipline. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. Subsequent work focused primarily on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.\nIn his 1938 book Applications aux Jeux de Hasard and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix is symmetric and provided a solution to a non-trivial infinite game (known in English as Blotto game). Borel conjectured the non-existence of mixed-strategy equilibria in finite two-person zero-sum games, a conjecture that was proved false by von Neumann.\n\nIn 1950, John Nash developed a criterion for mutual consistency of players' strategies known as the Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n-player, non-zero-sum (not just two-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium in mixed strategies.\nGame theory experienced a flurry of activity in the 1950s, during which the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. The 1950s also saw the first applications of game theory to philosophy and political science. The first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy.\n\nPrize-winning achievements\nIn 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium. Later he would introduce trembling hand perfection as well. In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.\nIn the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection and common knowledge were introduced and analyzed.\nIn 1994, John Nash was awarded the Nobel Memorial Prize in the Economic Sciences for his contribution to game theory. Nash's most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept for non-cooperative games, published in 1951. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy.\nIn 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.\nIn 2007, Leonid Hurwicz, Eric Maskin, and Roger Myerson were awarded the Nobel Prize in Economics \"for having laid the foundations of mechanism design theory\". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: Game Theory, Analysis of Conflict. Hurwicz introduced and formalized the concept of incentive compatibility.\nIn 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics \"for the theory of stable allocations and the practice of market design\". In 2014, the Nobel went to game theorist Jean Tirole.\n\nDifferent types of games\nCooperative / non-cooperative\nA game is cooperative if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is non-cooperative if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).\nCooperative games are often analyzed through the framework of cooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is different from non-cooperative game theory which focuses on predicting individual players' actions and payoffs by analyzing Nash equilibria.\nCooperative game theory provides a high-level approach as it describes only the structure and payoffs of coalitions, whereas non-cooperative game theory also looks at how strategic interaction will affect the distribution of payoffs. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation.\n\nSymmetric / asymmetric\nA symmetric game is a game where each player earns the same payoff when making the same choice. In other words, the identity of the player does not change the resulting game facing the other player. Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games.\nThe most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured in this section's graphic is asymmetric despite having identical strategy sets for both players.\n\nZero-sum / non-zero-sum\nZero-sum games (more generally, constant-sum games) are games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit goes to all players in a game, for every combination of strategies, and always adds to zero (more informally, a player benefits only at the equal expense of others). Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.\nMany games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.\nFurthermore, constant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any constant-sum game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\nSimultaneous / sequential\nSimultaneous games are games where both players move simultaneously, or instead the later players are unaware of the earlier players' actions (making them effectively simultaneous). Sequential games (a type of dynamic games) are games where players do not make decisions simultaneously, and player's earlier actions affect the outcome and decisions of other players. This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed.\nThe difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.\nIn short, the differences between sequential and simultaneous games are as follows:\n\nPerfect information and imperfect information\nAn important subset of sequential games consists of games of perfect information. A game with perfect information means that all players, at every move in the game, know the previous history of the game and the moves previously made by all other players. An imperfect information game is played when the players do not know all moves already made by the opponent such as a simultaneous move game. Examples of perfect-information games include tic-tac-toe, checkers, chess, and Go.\nMany card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept pertaining to the common knowledge of each player's sequence, strategies, and payoffs throughout gameplay. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken, whereas perfect information is knowledge of all aspects of the game and players. Games of incomplete information can be reduced, however, to games of imperfect information by introducing \"moves by nature\".\n\nBayesian game\nOne of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent's valuation of the object of negotiation, companies may be unaware of their opponent's cost functions, combatants may be unaware of their opponent's strengths, and jurors may be unaware of their colleague's interpretation of the evidence at trial. In some cases, participants may know the character of their opponent well, but may not know how well their opponent knows his or her own character.\nBayesian game means a strategic game with incomplete information. For a strategic game, decision makers are players, and every player has a group of actions. A core part of the imperfect information specification is the set of states. Every state completely describes a collection of characteristics relevant to the player such as their preferences and details about them. There must be a state for every set of features that some player believes may exist.\n\nFor example, where Player 1 is unsure whether Player 2 would rather date her or get away from her, while Player 2 understands Player 1's preferences as before. To be specific, supposing that Player 1 believes that Player 2 wants to date her under a probability of 1/2 and get away from her under a probability of 1/2 (this evaluation comes from Player 1's experience probably: she faces players who want to date her half of the time in such a case and players who want to avoid her half of the time). Due to the probability involved, the analysis  of this situation requires to understand the player's preference for the draw, even though people are only interested in pure strategic equilibrium.\n\nCombinatorial games\nGames in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and Go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve some particular problems and answer some general questions.\nGames of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or \"economic\") game theory. A typical game that has been solved this way is Hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.\nResearch in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha–beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.\n\nDiscrete and continuous games\nMuch of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.\n\nDifferential games\nDifferential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.\nA particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.\n\nEvolutionary game theory\nEvolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest.\nIn biology, such models can represent evolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.\n\nStochastic outcomes (and relation to other fields)\nIndividual decision problems with stochastic outcomes are sometimes considered \"one-player games\". They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).\nStochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes \"chance moves\" (\"moves by nature\"). This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.\nFor some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen. (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.\n\nMetagames\nThese are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.\nThe term metagame analysis is also used to refer to a practical approach developed by Nigel Howard, whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.\n\nMean field game theory\nMean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines, and by mathematicians Pierre-Louis Lions and Jean-Michel Lasry.\n\nRepresentation of games\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome. (Eric Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\nMost cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.\n\nExtensive form\nThe extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualized using game trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.\nThe game pictured consists of two players.  The way this particular game is structured (i.e., with sequential decision making and perfect information), Player 1 \"moves\" first by choosing either F or U (fair or unfair). Next in the sequence, Player 2, who has now observed Player 1's move, can choose to play either A or R  (accept or reject). Once Player 2 has made their choice, the game is considered finished and each player gets their respective payoff, represented in the image as two numbers, where the first number represents Player 1's payoff, and the second number represents Player 2's payoff.  Suppose that Player 1 chooses U and then Player 2 chooses A: Player 1 then gets a payoff of \"eight\" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and Player 2 gets a payoff of \"two\".\nThe extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\nNormal form\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays Up and that Player 2 plays Left. Then Player 1 gets a payoff of 4, and Player 2 gets 3.\nWhen a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.\nEvery extensive-form game has an equivalent normal-form game, however, the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.\n\nCharacteristic function form\nIn cooperative game theory the characteristic function lists the payoff of each coalition. The origin of this formulation is in John von Neumann and Oskar Morgenstern's book.\nFormally, a characteristic function is a function \n  \n    \n      \n        v\n        :\n        \n          2\n          \n            N\n          \n        \n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle v:2^{N}\\to \\mathbb {R} }\n  \n from the set of all possible coalitions of players to a set of payments, and also satisfies \n  \n    \n      \n        v\n        (\n        ∅\n        )\n        =\n        0\n      \n    \n    {\\displaystyle v(\\emptyset )=0}\n  \n. The function describes how much collective payoff a set of players can gain by forming a coalition.\n\nAlternative game representations\nAlternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research. In addition to classical game representations, some of the alternative representations also encode time related aspects.\n\nGeneral and applied uses\nAs a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.\nAlthough pre-twentieth-century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name \"game theory\", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 book Evolution and the Theory of Games.\nIn addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic approaches have also been suggested in the philosophy of language and philosophy of science. Game-theoretic arguments of this type can be found as far back as Plato. An alternative version of game theory, called chemical game theory, represents the player's choices as metaphorical chemical reactant molecules called \"knowlecules\".  Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.\n\nDescription and modeling\nThe primary use of game theory is to describe and model how human populations behave. Some scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice, human rationality and/or behavior often deviates from the model of rationality as used in game theory. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.\nSome game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).\n\nPrescriptive or normative analysis\nSome scholars see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.\n\nEconomics\nGame theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers and acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.\nThis research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.\nThe payoffs of the game are generally taken to represent the utility of individual players.\nA prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Economists and business professors suggest two primary uses (noted above): descriptive and prescriptive.\n\nManagerial economics\nGame theory also has an extensive use in a specific branch or stream of economics – Managerial Economics. One important usage of it in the field of managerial economics is in analyzing strategic interactions between firms. For example, firms may be competing in a market with limited resources, and game theory can help managers understand how their decisions impact their competitors and the overall market outcomes. Game theory can also be used to analyze cooperation between firms, such as in forming strategic alliances or joint ventures. Another use of game theory in managerial economics is in analyzing pricing strategies. For example, firms may use game theory to determine the optimal pricing strategy based on how they expect their competitors to respond to their pricing decisions. Overall, game theory serves as a useful tool for analyzing strategic interactions and decision making in the context of managerial economics.\n\nBusiness\nThe Chartered Institute of Procurement & Supply (CIPS) promotes knowledge and use of game theory within the context of business procurement. CIPS and TWS Partners have conducted a series of surveys designed to explore the understanding, awareness and application of game theory among procurement professionals. Some of the main findings in their third annual survey (2019) include:\n\napplication of game theory to procurement activity has increased – at the time it was at 19% across all survey respondents\n65% of participants predict that use of game theory applications will grow\n70% of respondents say that they have \"only a basic or a below basic understanding\" of game theory\n20% of participants had undertaken on-the-job training in game theory\n50% of respondents said that new or improved software solutions were desirable\n90% of respondents said that they do not have the software they need for their work.\n\nProject management\nSensible decision-making is critical for the success of projects.  In project management, game theory is used to model the decision-making process of players, such as investors, project managers, contractors, sub-contractors, governments and customers.  Quite often, these players have competing interests, and sometimes their interests are directly detrimental to other players, making project management scenarios well-suited to be modeled by game theory.\nPiraveenan (2019) in his review provides several examples where game theory is used to model project management scenarios. For instance, an investor typically has several investment options, and each option will likely result in a different project, and thus one of the investment options has to be chosen before the project charter can be produced. Similarly, any large project involving subcontractors, for instance, a construction project, has a complex interplay between the main contractor (the project manager) and subcontractors, or among the subcontractors themselves, which typically has several decision points. For example, if there is an ambiguity in the contract between the contractor and subcontractor, each must decide how hard to push their case without jeopardizing the whole project, and thus their own stake in it. Similarly, when projects from competing organizations are launched, the marketing personnel have to decide what is the best timing and strategy to market the project, or its resultant product or service, so that it can gain maximum traction in the face of competition. In each of these scenarios, the required decisions depend on the decisions of other players who, in some way, have competing interests to the interests of the decision-maker, and thus can ideally be modeled using game theory.\nPiraveenan summarizes that two-player games are predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.\n\nGovernment-sector–private-sector games (games that model public–private partnerships)\nContractor–contractor games\nContractor–subcontractor games\nSubcontractor–subcontractor games\nGames involving other players\nIn terms of types of games, both cooperative as well as non-cooperative, normal-form as well as extensive-form, and zero-sum as well as non-zero-sum  are used to model various project management scenarios.\n\nPolitical science\nThe application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.\nEarly examples of game theory applied to political science are provided by Anthony Downs. In his 1957 book An Economic Theory of Democracy, he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game theory was applied in 1962 to the Cuban Missile Crisis during the presidency of John F. Kennedy.\nIt has also been proposed that game theory explains the stability of any form of political government.  Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects.  Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed.  Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime.  Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.\nA game-theoretic explanation for democratic peace is that public and open debate in democracies sends clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.\nHowever, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.\nGame theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example is Peter John Wood's (2013) research looking into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce greenhouse gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma for the nations.\n\nDefence science and technology\nGame theory has been used extensively to model decision-making scenarios relevant to defence applications.  Most studies that has applied game theory in defence settings are concerned with Command and Control Warfare, and can be further classified into studies dealing with (i) Resource Allocation Warfare (ii) Information Warfare (iii) Weapons Control Warfare, and (iv) Adversary Monitoring Warfare.  Many of the problems studied are concerned with sensing and tracking, for example a surface ship trying to track a hostile submarine and the submarine trying to evade being tracked, and the interdependent decision making that takes place with regards to bearing, speed, and the sensor technology activated by both vessels.\nThe tool, for example, automates the transformation of public vulnerability data into models, allowing defenders to synthesize optimal defence strategies through Stackelberg equilibrium analysis. This approach enhances cyber resilience by enabling defenders to anticipate and counteract attackers’ best responses, making game theory increasingly relevant in adversarial cybersecurity environments.\nHo et al. provide a broad summary of game theory applications in defence, highlighting its advantages and limitations across both physical and cyber domains.\n\nBiology\nUnlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best-known equilibrium in biology is known as the evolutionarily stable strategy (ESS), first introduced in (Maynard Smith & Price 1973). Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.\nIn biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. (Fisher 1930) suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.\nAdditionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication. The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's Butterfly Economics).\nBiologists have used the game of chicken to analyze fighting behavior and territoriality.\nAccording to Maynard Smith, in the preface to Evolution and the Theory of Games, \"paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.\nOne such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival. All of these actions increase the overall fitness of a group, but occur at a cost to the individual.\nEvolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c < b × r, where the cost c to the altruist must be less than the benefit b to the recipient multiplied by the coefficient of relatedness r. The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of 1⁄2, because (on average) an individual shares half of the alleles in its sibling's offspring. Ensuring that enough of a sibling's offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring. The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a coefficient that was 1⁄2 in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.\n\nComputer science and logic\nGame theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.\nSeparately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as games with moving costs and request-answer games. Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.\nThe emergence of the Internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory and within it algorithmic mechanism design combine computational algorithm design and analysis of complex systems with economic theory.\nGame theory has multiple applications in the field of artificial intelligence and machine learning. It is often used in developing autonomous systems that can make complex decisions in uncertain environment. Some other areas of application of game theory in AI/ML context are as follows - multi-agent system formation, reinforcement learning, mechanism design etc. By using game theory to model the behavior of other agents and anticipate their actions, AI/ML systems can make better decisions and operate more effectively.\n\nPhilosophy\nGame theory has been put to several uses in philosophy. Responding to two papers by W.V.O. Quine (1960, 1967), Lewis (1969) used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis. Following Lewis (1969) game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.\nGame theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993), Skyrms (1990), and Stalnaker (1999).\nThe synthesis of game theory with ethics was championed by R. B. Braithwaite. The hope was that rigorous mathematical analysis of game theory might help formalize the more imprecise philosophical discussions. However, this expectation was only materialized to a limited extent.\nIn ethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton)  authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see Gauthier (1986) and Kavka (1986)).\nOther authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., Skyrms (1996, 2004) and Sober and Wilson (1998)).\n\nEpidemiology\nSince the decision to take a vaccine for a particular disease is often made by individuals, who may consider a range of factors and parameters in making this decision (such as the incidence and prevalence of the disease, perceived and real risks associated with contracting the disease,  mortality rate, perceived and real risks associated with vaccination, and financial cost of vaccination), game theory has been used to model and predict vaccination uptake in a society.\n\nWell known examples of games\nPrisoner's dilemma\nWilliam Poundstone described the game in his 1993 book Prisoner's Dilemma:\nTwo members of a criminal gang, A and B, are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communication with their partner. The principal charge would lead to a sentence of ten years in prison; however, the police do not have the evidence for a conviction. They plan to sentence both to two years in prison on a lesser charge but offer each prisoner a Faustian bargain: If one of them confesses to the crime of the principal charge, betraying the other, they will be pardoned and free to leave while the other must serve the entirety of the sentence instead of just two years for the lesser charge.\nThe dominant strategy (and therefore the best response to any possible opponent strategy), is to betray the other, which aligns with the sure-thing principle. However, both prisoners staying silent would yield a greater reward for both of them than mutual betrayal.\n\nBattle of the sexes\nThe \"battle of the sexes\" is a term used to describe the perceived conflict between men and women in various areas of life, such as relationships, careers, and social roles. This conflict is often portrayed in popular culture, such as movies and television shows, as a humorous or dramatic competition between the genders. This conflict can be depicted in a game theory framework. This is an example of non-cooperative games.\nAn example of the \"battle of the sexes\" can be seen in the portrayal of relationships in popular media, where men and women are often depicted as being fundamentally different and in conflict with each other. For instance, in some romantic comedies, the male and female protagonists are shown as having opposing views on love and relationships, and they have to overcome these differences in order to be together.\nIn this game, there are two pure strategy Nash equilibria: one where both the players choose the same strategy and the other where the players choose different options. If the game is played in mixed strategies, where each player chooses their strategy randomly, then there is an infinite number of Nash equilibria. However, in the context of the \"battle of the sexes\" game, the assumption is usually made that the game is played in pure strategies.\n\nUltimatum game\nThe ultimatum game is a game that has become a popular instrument of economic experiments. An early description is by Nobel laureate John Harsanyi in 1961.\nOne player, the proposer, is endowed with a sum of money. The proposer is tasked with splitting it with another player, the responder (who knows what the total sum is). Once the proposer communicates his decision, the responder may accept it or reject it. If the responder accepts, the money is split per the proposal; if the responder rejects, both players receive nothing.  Both players know in advance the consequences of the responder accepting or rejecting the offer. The game demonstrates how social acceptance, fairness, and generosity influence the players decisions.\nUltimatum game has a variant, that is the dictator game. They are mostly identical, except in dictator game the responder has no power to reject the proposer's offer.\n\nTrust game\nThe Trust Game is an experiment designed to measure trust in economic decisions. It is also called \"the investment game\" and is designed to investigate trust and demonstrate its importance rather than \"rationality\" of self-interest. The game was designed by Berg Joyce, John Dickhaut and Kevin McCabe in 1995.\nIn the game, one player (the investor) is given a sum of money and must decide how much of it to give to another player (the trustee). The amount given is then tripled by the experimenter. The trustee then decides how much of the tripled amount to return to the investor. If the recipient is completely self interested, then he/she should return nothing. However that is not true as the experiment conduct. The outcome suggest that people are willing to place a trust, by risking some amount of money, in the belief that there would be reciprocity.\n\nCournot Competition\nThe Cournot competition model involves players choosing quantity of a homogenous product to produce independently and simultaneously, where marginal cost can be different for each firm and the firm's payoff is profit. The production costs are public information and the firm aims to find their profit-maximizing quantity based on what they believe the other firm will produce and behave like monopolies. In this game firms want to produce at the monopoly quantity but there is a high incentive to deviate and produce more, which decreases the market-clearing price. For example, firms may be tempted to deviate from the monopoly quantity if there is a low monopoly quantity and high price, with the aim of increasing production to maximize  profit. However this option does not provide the highest payoff, as a firm's ability to maximize profits depends on its market share and the elasticity of the market demand. The Cournot equilibrium is reached when each firm operates on their reaction function with no incentive to deviate, as they have the best response based on the other firms output. Within the game, firms reach the Nash equilibrium when the Cournot equilibrium is achieved.\n\nBertrand Competition\nThe Bertrand competition assumes homogenous products and a constant marginal cost and players choose the prices. The equilibrium of price competition is where the price is equal to marginal costs, assuming complete information about the competitors' costs. Therefore, the firms have an incentive to deviate from the equilibrium because a homogenous product with a lower price will gain all of the market share, known as a cost advantage.\n\nIn popular culture\nBased on the 1998 book by Sylvia Nasar, the life story of game theorist and mathematician John Nash was turned into the 2001 biopic A Beautiful Mind, starring Russell Crowe as Nash.\nThe 1959 military science fiction novel Starship Troopers by Robert A. Heinlein mentioned \"games theory\" and \"theory of games\". In the 1997 film of the same name, the character Carl Jenkins referred to his military intelligence assignment as being assigned to \"games and theory\".\nThe 1964 film Dr. Strangelove satirizes game theoretic ideas about deterrence theory. For example, nuclear deterrence depends on the threat to retaliate catastrophically if a nuclear attack is detected. A game theorist might argue that such threats can fail to be credible, in the sense that they can lead to subgame imperfect equilibria. The movie takes this idea one step further, with the Soviet Union irrevocably committing to a catastrophic nuclear response without making the threat public.\nThe 1980s power pop band Game Theory was founded by singer/songwriter Scott Miller, who described the band's name as alluding to \"the study of calculating the most appropriate action given an adversary ... to give yourself the minimum amount of failure\".\nLiar Game, a 2005 Japanese manga and 2007 television series, presents the main characters in each episode with a game or problem that is typically drawn from game theory, as demonstrated by the strategies applied by the characters.\nThe 1974 novel Spy Story by Len Deighton explores elements of game theory in regard to cold war army exercises.\nThe 2008 novel The Dark Forest by Liu Cixin explores the relationship between extraterrestrial life, humanity, and game theory.\nJoker, the prime antagonist in the 2008 film The Dark Knight presents game theory concepts—notably the prisoner's dilemma in a scene where he asks passengers in two different ferries to bomb the other one to save their own.\nIn the 2018 film Crazy Rich Asians, the female lead Rachel Chu is a professor of economics and game theory at New York University. At the beginning of the film she is seen in her NYU classroom playing a game of poker with her teaching assistant and wins the game by bluffing; then in the climax of the film, she plays a game of mahjong with her boyfriend's disapproving mother Eleanor, losing the game to Eleanor on purpose but winning her approval as a result.\nIn the 2017 film Molly's Game, Brad, an inexperienced poker player, makes an irrational betting decision without realizing and causes his opponent Harlan to deviate from his Nash Equilibrium strategy, resulting in a significant loss when Harlan loses the hand.\n\nSee also\nApplied ethics – Practical application of moral considerations\nBandwidth-sharing game – Type of resource allocation game\nChainstore paradox – Game theory paradox\nCollective intentionality – Intentionality that occurs when two or more individuals undertake a task together\nCore (game theory) – Set in game theory\nGlossary of game theory\nIntra-household bargaining\nKingmaker scenario – Endgame situation in game theory\nLaw and economics – Application of economic theory to analysis of legal systems\nMutual assured destruction – Doctrine of military strategy\nOutline of artificial intelligence – Overview of and topical guide to artificial intelligence\nParrondo's paradox – Paradox in game theory\nPrecautionary principle – Risk management strategy\nQuantum refereed game\nRisk management – Identification, evaluation and control of risks\nSelf-confirming equilibrium\nTragedy of the commons – Self-interests causing depletion of a shared resource\nTraveler's dilemma – Non-zero-sum game thought experiment\nWilson doctrine (economics) – Argument in economic theory\nCompositional game theory\nLists\n\nList of cognitive biases\nList of emerging technologies\nList of games in game theory\n\nNotes\nReferences\nSources\nBen-David, S.; Borodin, A.; Karp, R.; Tardos, G.; Wigderson, A. (January 1994). \"On the power of randomization in on-line algorithms\". Algorithmica. 11 (1): 2–14. doi:10.1007/BF01294260. S2CID 26771869.\nDowns, Anthony (1957), An Economic theory of Democracy, New York: Harper\nFisher, Sir Ronald Aylmer (1930). The Genetical Theory of Natural Selection. Clarendon Press.\nGauthier, David (1986), Morals by agreement, Oxford University Press, ISBN 978-0-19-824992-4\nGrim, Patrick; Kokalis, Trina; Alai-Tafti, Ali; Kilb, Nicholas; St Denis, Paul (2004), \"Making meaning happen\", Journal of Experimental & Theoretical Artificial Intelligence, 16 (4): 209–243, doi:10.1080/09528130412331294715, S2CID 5737352\nHarper, David; Maynard Smith, John (2003), Animal signals, Oxford University Press, ISBN 978-0-19-852685-8\nHoward, Nigel (1971), Paradoxes of Rationality: Games, Metagames, and Political Behavior, Cambridge, MA: The MIT Press, ISBN 978-0-262-58237-7\nKavka, Gregory S. (1986). Hobbesian Moral and Political Theory. Princeton University Press. ISBN 978-0-691-02765-4.\nLewis, David (1969), Convention: A Philosophical Study, ISBN 978-0-631-23257-5 (2002 edition)\nMaynard Smith, John; Price, George R. (1973), \"The logic of animal conflict\", Nature, 246 (5427): 15–18, Bibcode:1973Natur.246...15S, doi:10.1038/246015a0, S2CID 4224989\nOsborne, Martin J.; Rubinstein, Ariel (1994), A course in game theory, MIT Press, ISBN 978-0-262-65040-3. A modern introduction at the graduate level.\nPoundstone, William (1993). Prisoner's Dilemma (1st Anchor Books ed.). New York: Anchor. ISBN 0-385-41580-X.\nQuine, W.v.O (1967), \"Truth by Convention\", Philosophica Essays for A.N. Whitehead, Russel and Russel Publishers, ISBN 978-0-8462-0970-6\nQuine, W.v.O (1960), \"Carnap and Logical Truth\", Synthese, 12 (4): 350–374, doi:10.1007/BF00485423, S2CID 46979744\nSkyrms, Brian (1996), Evolution of the social contract, Cambridge University Press, ISBN 978-0-521-55583-8\nSkyrms, Brian (2004), The stag hunt and the evolution of social structure, Cambridge University Press, ISBN 978-0-521-53392-8\nSober, Elliott; Wilson, David Sloan (1998), Unto others: the evolution and psychology of unselfish behavior, Harvard University Press, ISBN 978-0-674-93047-6\nWebb, James N. (2007), Game theory: decisions, interaction and evolution, Undergraduate mathematics, Springer, ISBN 978-1-84628-423-6 Consistent treatment of game types usually claimed by different applied fields, e.g. Markov decision processes.\n\nFurther reading\nTextbooks and general literature\nAumann, Robert J (1987), \"game theory\", The New Palgrave: A Dictionary of Economics, vol. 2, pp. 460–82.\nCamerer, Colin (2003), \"Introduction\", Behavioral Game Theory: Experiments in Strategic Interaction, Russell Sage Foundation, pp. 1–25, ISBN 978-0-691-09039-9, archived from the original on 14 May 2011, retrieved 9 February 2011, Description.\nDutta, Prajit K. (1999), Strategies and games: theory and practice, MIT Press, ISBN 978-0-262-04169-0. Suitable for undergraduate and business students.\nFernandez, L F.; Bierman, H S. (1998), Game theory with economic applications, Addison-Wesley, ISBN 978-0-201-84758-1. Suitable for upper-level undergraduates.\nGaffal, Margit; Padilla Gálvez, Jesús (2014). Dynamics of Rational Negotiation: Game Theory, Language Games and Forms of Life. Springer.\nGibbons, Robert D. (1992), Game theory for applied economists, Princeton University Press, ISBN 978-0-691-00395-5. Suitable for advanced undergraduates.\nPublished in Europe as Gibbons, Robert (2001), A Primer in Game Theory, London: Harvester Wheatsheaf, ISBN 978-0-7450-1159-2.\nGintis, Herbert (2000), Game theory evolving: a problem-centered introduction to modeling strategic behavior, Princeton University Press, ISBN 978-0-691-00943-8\nGreen, Jerry R.; Mas-Colell, Andreu; Whinston, Michael D. (1995), Microeconomic theory, Oxford University Press, ISBN 978-0-19-507340-9. Presents game theory in formal way suitable for graduate level.\nJoseph E. Harrington (2008) Games, strategies, and decision making, Worth, ISBN 0-7167-6630-2. Textbook suitable for undergraduates in applied fields; numerous examples, fewer formalisms in concept presentation.\nIsaacs, Rufus (1999), Differential Games: A Mathematical Theory With Applications to Warfare and Pursuit, Control and Optimization, New York: Dover Publications, ISBN 978-0-486-40682-4\nMichael Maschler; Eilon Solan; Shmuel Zamir (2013), Game Theory, Cambridge University Press, ISBN 978-1-108-49345-1. Undergraduate textbook.\nMiller, James H. (2003), Game theory at work: how to use game theory to outthink and outmaneuver your competition, New York: McGraw-Hill, ISBN 978-0-07-140020-6. Suitable for a general audience.\nShoham, Yoav; Leyton-Brown, Kevin (2009), Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations, New York: Cambridge University Press, ISBN 978-0-521-89943-7, retrieved 8 March 2016\nWatson, Joel (2013), Strategy: An Introduction to Game Theory (3rd edition), New York: W.W. Norton and Co., ISBN 978-0-393-91838-0. A leading textbook at the advanced undergraduate level.\nMcCain, Roger A. (2010). Game Theory: A Nontechnical Introduction to the Analysis of Strategy. World Scientific. ISBN 978-981-4289-65-8.\n\nHistorically important texts\nAumann, R. J.; Shapley, L. S. (1974), Values of Non-Atomic Games, Princeton University Press\nCournot, A. Augustin (1838), \"Recherches sur les principles mathematiques de la théorie des richesses\", Libraire des Sciences Politiques et Sociales\nEdgeworth, Francis Y. (1881), Mathematical Psychics, London: Kegan Paul\nFarquharson, Robin (1969), Theory of Voting, Blackwell (Yale U.P. in the U.S.), ISBN 978-0-631-12460-3\nLuce, R. Duncan; Raiffa, Howard (1957), Games and decisions: introduction and critical survey, New York: Wiley\nreprinted edition: R. Duncan Luce; Howard Raiffa (1989), Games and decisions: introduction and critical survey, New York: Dover Publications, ISBN 978-0-486-65943-5\nMaynard Smith, John (1982), Evolution and the theory of games, Cambridge University Press, ISBN 978-0-521-28884-2\nNash, John (1950), \"Equilibrium points in n-person games\", Proceedings of the National Academy of Sciences of the United States of America, 36 (1): 48–49, Bibcode:1950PNAS...36...48N, doi:10.1073/pnas.36.1.48, PMC 1063129, PMID 16588946\nShapley, L.S. (1953), A Value for n-person Games, In: Contributions to the Theory of Games volume II, H. W. Kuhn and A. W. Tucker (eds.)\nShapley, L. S. (October 1953). \"Stochastic Games\". Proceedings of the National Academy of Sciences. 39 (10): 1095–1100. Bibcode:1953PNAS...39.1095S. doi:10.1073/pnas.39.10.1095. PMC 1063912. PMID 16589380.\nvon Neumann, John (1928), \"Zur Theorie der Gesellschaftsspiele\", Mathematische Annalen, 100 (1): 295–320, doi:10.1007/bf01448847, S2CID 122961988 English translation: \"On the Theory of Games of Strategy,\" in A. W. Tucker and R. D. Luce, ed. (1959), Contributions to the Theory of Games, v. 4, p. 42. Princeton University Press.\nvon Neumann, John; Morgenstern, Oskar (1944), \"Theory of games and economic behavior\", Nature, 157 (3981), Princeton University Press: 172, Bibcode:1946Natur.157..172R, doi:10.1038/157172a0, S2CID 29754824\nZermelo, Ernst (1913), \"Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels\", Proceedings of the Fifth International Congress of Mathematicians, 2: 501–4\n\nOther material\nAllan Gibbard, \"Manipulation of voting schemes: a general result\", Econometrica, Vol. 41, No. 4 (1973), pp. 587–601.\nMcDonald, John (1950–1996), Strategy in Poker, Business & War, W. W. Norton, ISBN 978-0-393-31457-1 {{citation}}: ISBN / Date incompatibility (help). A layman's introduction.\nPapayoanou, Paul (2010), Game Theory for Business: A Primer in Strategic Gaming, Probabilistic, ISBN 978-0-9647938-7-3.\nSatterthwaite, Mark Allen (April 1975). \"Strategy-proofness and Arrow's conditions: Existence and correspondence theorems for voting procedures and social welfare functions\" (PDF). Journal of Economic Theory. 10 (2): 187–217. doi:10.1016/0022-0531(75)90050-2.\nSiegfried, Tom (2006), A Beautiful Math, Joseph Henry Press, ISBN 978-0-309-10192-9\nSkyrms, Brian (1990), The Dynamics of Rational Deliberation, Harvard University Press, ISBN 978-0-674-21885-7\nThrall, Robert M.; Lucas, William F. (1963), \"\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-person games in partition function form\", Naval Research Logistics Quarterly, 10 (4): 281–298, doi:10.1002/nav.3800100126\nDolev, Shlomi; Panagopoulou, Panagiota N.; Rabie, Mikaël; Schiller, Elad M.; Spirakis, Paul G. (2011). \"Rationality authority for provable rational behavior\". Proceedings of the 30th annual ACM SIGACT-SIGOPS symposium on Principles of distributed computing. pp. 289–290. doi:10.1145/1993806.1993858. ISBN 978-1-4503-0719-2.\nChastain, Erick; Livnat, Adi; Papadimitriou, Christos; Vazirani, Umesh (June 2014), \"Algorithms, games, and evolution\", Proceedings of the National Academy of Sciences of the United States of America, 111 (29): 10620–10623, Bibcode:2014PNAS..11110620C, doi:10.1073/pnas.1406556111, PMC 4115542, PMID 24979793\n\nExternal links\n\nJames Miller (2015): Introductory Game Theory Videos.\n\"Games, theory of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nPaul Walker: History of Game Theory Page.\nDavid Levine: Game Theory. Papers, Lecture Notes and much more stuff.\nAlvin Roth:\"Game Theory and Experimental Economics page\". Archived from the original on 15 August 2000. Retrieved 13 September 2003.  — Comprehensive list of links to game theory information on the Web\nAdam Kalai: Game Theory and Computer Science — Lecture notes on Game Theory and Computer Science\nMike Shor: GameTheory.net — Lecture notes, interactive illustrations and other information.\nJim Ratliff's Graduate Course in Game Theory Archived 29 March 2010 at the Wayback Machine (lecture notes).\nDon Ross: Review Of Game Theory in the Stanford Encyclopedia of Philosophy.\nBruno Verbeek and Christopher Morris: Game Theory and Ethics\nElmer G. Wiens: Game Theory — Introduction, worked examples, play online two-person zero-sum games.\nMarek M. Kaminski: Game Theory and Politics Archived 20 October 2006 at the Wayback Machine — Syllabuses and lecture notes for game theory and political science.\nWebsites on game theory and social interactions\nKesten Green's Conflict Forecasting at the Wayback Machine (archived 11 April 2011) — See Papers for evidence on the accuracy of forecasts from game theory and other methods Archived 15 September 2019 at the Wayback Machine.\nMcKelvey, Richard D., McLennan, Andrew M., and Turocy, Theodore L. (2007) Gambit: Software Tools for Game Theory.\nBenjamin Polak: Open Course on Game Theory at Yale Archived 3 August 2010 at the Wayback Machine videos of the course\nBenjamin Moritz, Bernhard Könsgen, Danny Bures, Ronni Wiersch, (2007) Spieltheorie-Software.de: An application for Game Theory implemented in JAVA.\nAntonin Kucera: Stochastic Two-Player Games.\nYu-Chi Ho: What is Mathematical Game Theory; What is Mathematical Game Theory (#2); What is Mathematical Game Theory (#3); What is Mathematical Game Theory (#4)-Many person game theory; What is Mathematical Game Theory ?( #5) – Finale, summing up, and my own view",
        "url": "https://en.wikipedia.org/wiki/Game_theory"
    },
    {
        "title": "Gender digital divide",
        "text": "Gender digital divide refers to the inequalities in access to, use of, and participation in digital technologies and the technology sector based on gender. It encompasses disparities in digital skills, internet access, representation in computing and STEM fields, and exposure to gender-biased technologies such as artificial intelligence and voice assistants. The divide is shaped by broader socio-economic, cultural, and educational factors and is more pronounced among women and gender minorities in developing countries, rural areas, and lower-income populations. Despite global efforts to close this gap, significant challenges remain, including patriarchal norms, safety concerns, affordability issues, and limited access to digital education. Addressing the gender digital divide is considered essential for achieving broader gender equality, inclusive economic development, and equitable digital transformation.\n\nBackground\nEducation systems are increasingly trying to ensure equitable, inclusive, and high-quality digital skills, education, and training. Though digital skills open pathways to further learning and skills development, women and girls are still being left behind in digital skills education. Globally, digital skills gender gaps are growing, despite at least a decade of national and international efforts to close them. The economic and political interests of its indicators have also been questioned.\n\nDigital skills gap\nWomen are less likely to know how to operate a smartphone, navigate the internet, use social media and understand how to safeguard information in digital mediums (abilities that underlie life and work tasks and are relevant to people of all ages) worldwide. There is a gap from the lowest skill proficiency levels, such as using apps on a mobile phone, to the most advanced skills like coding computer software to support the analysis of large data sets.\nWomen in numerous countries are 25% less likely than men to know how to leverage ICT for basic purposes, such as using simple arithmetic formulas in a spreadsheet. UNESCO estimates that men are around four times more likely than women to have advanced ICT skills such as the ability to programme computers. Across G20 countries 7% of ICT patents are generated by women, and the global average is at 2%. Recruiters for technology companies in Silicon Valley estimate that the applicant pool for technical jobs in artificial intelligence (AI) and data science is often less than 1% female. To highlight this difference, in 2009 there were 2.5 million college-educated women working in STEM compared to 6.7 million men. The total workforce at the time was 49% women and 51% men which highlights the evident gap.\nWhile the gender gap in digital skills is evident across regional boundaries and income levels, it is more severe for women who are older, less educated, poor, or living in rural areas and developing countries. Making women much less likely to graduate in any field of STEM compared to their male counterpart. Digital skills gap intersects with issues of poverty and educational access.\n\nRoot causes\nWomen and girls who live in patriarchal cultures may struggle more than those who do not to access public ICT facilities. Due to the social challenges these cultures create as well reinforces the struggles and creates an overlap. They may struggle to access these facilities due to unsafe roads, limits on their freedom of movement, or because the facilities themselves are considered unsuitable for women. They may also lack financial freedom creating a large barrier to purchase any form of technology or have any type of internet connection.  If they do have access to technology of the internet, it is usually controlled by the men in their households and limit their content selection to content focused on women's appearances, dating, or the role of motherhood.   Fears concerning safety and harassment (both online and offline) also inhibit many women and girls from benefiting from or even wanting to use ICTs.\nIn many contexts, women and girls face concerns of physical violence if they own or borrow digital devices, which in some cases leads to their using the devices in secret, making them more vulnerable to online threats and making it difficult to gain digital skills.\nThe stereotype of technology as a male domain is common in many contexts and affect girls' confidence in their digital skills from a young age. In OECD countries, 0.5% of girls aspire towards ICT-related careers at age 15, versus 5% of boys. This was not always the case. Early decades of computing saw a much larger presence of women. Acting as programmers during World War II, they held highly valued positions. Women's contributions, however, have been largely obscured due to how the history is told. Focusing on the infrastructure and hardware of digital technologies development has placed men at the forefront of its history. Post war computer manufacturers sought to commercialize the machines and opened up a new form of labor market. This post war market utilized discriminatory criteria measures that women were no longer able to meet due to societal, educational, and labor expectations. Managers of early technology firms allowed women well-suited for programming because of stereotypes characterizing them as meticulous and good at following step-by-step directions. Women, including many women of color, flocked to jobs in the computer industry because it was seen as more meritocratic than other fields. As computers became integrated into people's daily life, it was noticed that programmers had influence. Consequently, women were pushed out and the field became more male-dominated.\nIn developed countries like Canada, the digital divide can exist due to factors of lacking digital literacy which prevents individuals from understanding how to use and what to do with technology. Other research on the gender divide in Canada has found contrasting results, showing a potential suggestion to the closing of the gap in more developed countries over the last couple years in relation to access to the internet and technology as a whole. However, the amount of activity online is found to be higher for men than women.  When looking at issues regarding professional sectors the IT sector in Canada remains male-dominated. The presence of women in field with technology has increased significantly but in specific high-paying technological fields like computer science it is declining.\n\nAccess divide versus skills divide\nDue to the declining price of connectivity and hardware, skills deficits have exceeded barriers of access as the primary contributor to the gender digital divide. For years, the divide was assumed to be symptomatic of technical challenges. It was thought that women would catch up with men when the world had cheaper devices and lower connectivity prices, due to the limited purchasing power and financial independence of women compared with men in countries with a patriarchal culture. The cost of ICT access remains an issue and is surpassed by educational gaps. For example, the gender gap in internet penetration is around 17% in the Arab States and the Asia and Pacific region, whereas the gender gap in ICT skills is as high as 25% in some Asian and Middle Eastern countries. In sub-Saharan Africa (SSA), the Internet penetration rate in 2019 was 33.8 percent for men and 22.6 percent for women. The Internet user gender gap was 20.7 percent in 2013 and up to 37 percent in 2019. The Internet penetration rate in 2019 was 33.8 percent for men and 22.6 percent for women. \nOther research has shown more factors that contribute to access of the internet. In the United States, it was found that individuals who has lower than high school education and made less than $30k a year has the lowest access to the internet. They found that the most consistent results form various research is that individuals with the lowest education and lowest income had the lowest access to the internet. When looking at differences with gender, inconsistent results were found. When large divides were found between men and women' access to the internet, socioeconomic factors were the cause. Overall, the gender divide has found to be largely insignificant in countries like the United States and in Canada. \nSSA has one of the widest mobile gender gaps in the world where over 74 million women are not connected. The gender gap in mobile ownership was 13 percent, a reduction from 14 percent in 2018; however, in low- and middle-income countries it remains substantial with fewer women than men accessing the Internet on a mobile device. Furthermore, women are less likely to use digital services or mobile Internet and tend to use different mobile services than men.\nMany people have access to affordable devices and broadband networks, but do not have the requisite skills to take advantage of this technology to improve their lives. In Brazil, lack of skills (rather than cost of access) was found to be the primary reason low-income groups are not using the internet. In India, where lack of skills and lack of need for the internet were the primary limiting factors across all income groups.\nLack of understanding, interest or time is a bigger issue than affordability or availability as the reason for not using the internet. Even though skills deficits prevent both men and women from using digital technologies, they tend to be more severe for women. In a study conducted across 10 low- and middle-income countries, women were 1.6 times more likely than men to report lack of skills as a barrier to internet use. Women are also more likely to report that they do not see a reason to access and use ICT. Interest and perception of need are related to skills, as people who have little experience with or understanding of ICTs tend to underestimate their benefits and utility.\n\nRelationship between digital skills and gender equality\nIn many societies, gender equality does not translate into digital realms and professions. The persistence of growing digital skills gender gaps, even in countries that rank at the top of the World Economic Forum's global gender gap index (reflecting strong gender equality), demonstrates a need for interventions that cultivate the digital skills of women and girls.\nFor most countries, the primary barriers for women regarding access to digital technology are cost/unaffordability followed by illiteracy and lack of digital skills. For instance, in Africa 65.4 percent of people aged 15 and older are illiterate, compared to the global average rate of 86.4 percent.\n\nGender digital divide and COVID-19\nAfrica\nThe COVID-19 pandemic and the measures taken by governments on social distancing and mobility restrictions have contributed to boosting the use of digital technology to bridge some of the physical access gaps. However, the rapid proliferation of digital tools and services stands in stark contrast to the many systemic and structural barriers to technology access and adoption that many people in rural Africa still face. Gender inequalities, intersecting with and compounded by other social differences such as class, race, age, (dis)ability, etc., shape the extent to which different rural women and men are able not only to access but also use and benefit from these new technologies and ways of delivering information and services.\nBeside the potential of digital tools and applications, the COVID-19 crisis has evidenced the existing digital divide and especially the gender gap. It is estimated that 3.6 billion individuals are not connected to the Internet across the globe, including 900 million in Africa. Only 27 percent of women in Africa have access to the Internet and only 15 percent of them can afford to use it.\n\nGender-responsive digitalization in COVID-19 response\nAccording to a study by FAO, gender-responsive digitalization in COVID-19 response and beyond could include:\n\nImprove the availability of sex-disaggregated data and gender-related statistics that capture digital gender gaps in rural areas to better inform policy and business decisions\nPromote an enabling environment that includes gender-responsive policies, strategies and initiatives\nLeverage digital solutions to deliver COVID-19 relief measures targeted to rural women and girls and facilitate their access to social protection services and alternative income-generation opportunities.\nDedicate funds for digital acceleration to support women-led enterprises.\nImprove the national broadband coverage to ensure affordable, accessible and reliable infrastructure for inclusive digital transformation\nInvest in the protection of Internet users, especially illiterate and vulnerable ones, against frauds and abuses as cybercrime, including sexual harassment. According to UN Women, these crimes have reportedly increased during the COVID-19 pandemic, especially toward women and girls.\n\nBenefits of digital empowerment\nHelping women and girls develop digital skills means stronger women, stronger families, stronger communities, stronger economies and better technology. Digital skills are recognized to be essential life skills required for full participation in society. The main benefits for acquiring digital skills are they:\n\nFacilitate entry into the labour market;\nAssist women's safety both online and offline;\nEnhance women's community and political engagement;\nBring economic benefits to women and society;\nEmpower women to help steer the future of technology and gender equality;\nAccelerate progress towards international goals.\nDigitalization can potentially pave the way for improving the efficiency and functioning of food systems, which in turn can have positive impacts on the livelihoods of women and men farmers and agripreneurs, for example, through the creation of digital job opportunities for young women and men in rural areas.\n\nClosing the digital skills gender gap\nThe digital divide has begun at earlier ages as young adults have lived out their childhoods with personal computers. This has made intervention to prevent further gender divides in the digital realm needed in more early education. Increasing girls' and women's digital skills involves early, varied and sustained exposure to digital technologies. Interventions should not be limited to formal education settings, they should reflect a multifaceted approach, enabling women and girls to acquire skills in a variety of formal and informal contexts (at home, in school, in their communities and in the workplace). The digital divide cuts across age groups, therefore solutions need to assume a lifelong learning orientation. The technological changes adds impetus to the 'across life' perspective, as skills learned today will not necessarily be relevant in 5 or 10 years. Digital skills require regular updating, to prevent women and girls fall further behind.\nWomen and girls digital skills development are strengthened by:\n\nAdopting sustained, varied and life-wide approaches;\nEstablishing incentives, targets and quotas;\nEmbedding ICT in formal education;\nSupporting engaging experiences;\nEmphasising meaningful use and tangible benefits;\nEncouraging collaborative and peer learning;\nCreating safe spaces and meet women where they are;\nExamining exclusionary practices and language;\nRecruiting and training gender-sensitive teachers;\nPromoting role models and mentors;\nBringing parents on board;\nLeveraging community connections and recruiting allies;\nSupporting technology autonomy and women's digital rights.\nAccording to the Food and Agriculture Organization (FAO), there are seven success factors to empowering rural women through ICTs:\n\nAdapt content so that it is meaningful for them.\nCreate a safe environment for them to share and learn.\nBe gender-sensitive.\nProvide them with access and tools for sharing\nBuild partnerships.\nProvide the right blend of technologies.\nEnsure sustainability\nThe regulatory role of governments (at local, national, regional, and international levels) is crucial in addressing infrastructural barriers, harmonizing and making the regulatory environment inclusive and gender-responsive, and in protecting all stakeholders from fraud and crime.\nInitiatives targeted at boosting women's representation in the technology industry are essential to closing the digital skills gender divide. Mentorship programs, networking chances, and scholarships for women seeking jobs in technology are examples of such initiatives. These efforts can help create more inclusive workplaces that respect diversity and promote creativity by boosting the presence of women in the technology industry.\n\nMentorship programs can be especially beneficial in assisting women in the technology industry. These initiatives allow women to network with experienced professionals who can give advice and support as they advance in their jobs. According to research, mentoring programs can help increase women's confidence and feeling of connection in the workplace, which can improve job happiness and professional growth opportunities.\nWomen in technology can benefit from networking chances as well. Women can benefit from networking by developing connections with other professionals in their industry, which can lead to new employment possibilities and collaborations. Networking can also assist women in staying current with the newest trends and technologies in their profession.\nScholarships can also be a good method to help women who want to work in technology. Scholarships can assist in covering the costs of education and training, making it simpler for women to gain the skills and knowledge required to thrive in the technology industry. Scholarships can also help to increase gender variety in the technology industry by encouraging more women to enter the profession.\nOverall, initiatives targeted at boosting women's representation in the technology industry are essential to closing the digital skills gender divide. We can build more inclusive and innovative environments that help everyone if we assist women in technology.\n\nFemale gendering of AI technologies\nMen continue to dominate the technology space, and the disparity serves to perpetuate gender inequalities, as unrecognized bias is replicated and built into algorithms and artificial intelligence (AI).\nLimited participation of women and girls in the technology sector can stem outward replicating existing gender biases and creating new ones. Women's participation in the technology sector is constrained by unequal digital skills education and training. Learning and confidence gaps that arise as early as primary school amplify as girls move through education, therefore by the time they reach higher education only a fraction pursue advanced-level studies in computer science and related information and communication technology (ICT) fields. Divides grow greater in the transition from education to work. The International Telecommunication Union (ITU) estimates that only 6% of professional software developers are women.\nTechnologies generated by male-dominated teams and companies often reflect gender biases. Establishing balance between men and women in the technology sector will help lay foundations for the creation of technology products that better reflect and ultimately accommodate the rich diversity of human societies. For instance AI, which is a branch of the technology sector that wields influence over people's lives. Today, AI curates information shown by internet search engines, determines medical treatments, makes loan decisions, ranks job applications, translates languages, places ads, recommends prison sentences, influences parole decisions, calibrates lobbying and campaigning efforts, intuits tastes and preferences, and decides who qualifies for insurance, among other tasks. Despite the growing influence of this technology, women make up just 12% of AI researchers. Closing the gender divide begins with establishing more inclusive and gender-equal digital skills education and training.\n\nDigital assistants\nDigital assistants encompass a range of internet-connected technologies that support users in various ways. When interacting with digital assistants, users are not restricted to a narrow range of input commands, but are encouraged to make queries using whichever inputs seem most appropriate or natural, whether they are typed or spoken. Digital assistants seek to enable and sustain more human-like interactions with technology. Digital assistants can include: voice assistants, chatbots, and virtual agents.\n\nFeminization of voice assistants\nVoice assistants have become central to technology platforms and, in many countries, to day-to-day life. Between 2008 and 2018, the frequency of voice-based internet search queries increased 35 times and account for close to one fifth of mobile internet searches (a figure that is projected to increase to 50% by 2020). Voice assistants now manage upwards of 1 billion tasks per month, from the mundane (changing a song) to the essential (contacting emergency services).\nToday, most leading voice assistants are exclusively female or female by default, both in name and in sound of voice. Amazon has Alexa (named for the ancient library in Alexandria), Microsoft has Cortana (named for a synthetic intelligence in the video game Halo that projects itself as a sensuous unclothed woman), and Apple has Siri (coined by the Norwegian co-creator of the iPhone 4S and meaning 'beautiful woman who leads you to victory' in Norse). While Google's voice assistant is simply Google Assistant and sometimes referred to as Google Home, its voice is female.\nThe trend to feminize assistants occurs in a context in which there is a growing gender imbalance in technology companies, such that men commonly represent two thirds to three quarters of a firm's total workforce. Companies like Amazon and Apple have cited academic work demonstrating that people prefer a female voice to a male voice, justifying the decision to make voice assistants female. Further research shows that consumers strongly dislike voice assistants without clear gender markers. Gender bias is thus \"hard-coded\" into technology. Companies often cite research showing that customers want their digital assistants to sound like women, justifying the choice with the profit motive. However, research on the topic is mixed, with studies showing that in some contexts male choices may be preferred. For example, BMW was forced to recall a female-voiced navigation system on its 5 Series cars in the late 1990s after being flooded with calls from German men who reportedly \"refused to take directions from a woman\".\nResearchers who specialize in human–computer interaction have recognized that both men and women tend to characterize female voices as more helpful. The perception may have roots in traditional social norms around women as nurturers (mothers often take on – willingly or not – significantly more care than fathers) and other socially constructed gender biases that predate the digital era.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nDigital divide\nGender disparity in computing\nFemale education\nWomen's empowerment\n\nSources\nThis article incorporates text from a free content work. Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from Gender-responsive digitalization: A critical component of the COVID-19 response in Africa, FAO, FAO.\n\nReferences\n This article incorporates text from a free content work. Licensed under CC BY-SA 3.0 IGO. Text taken from I'd blush if I could: closing gender divides in digital skills through education​, UNESCO, EQUALS Skills Coalition, UNESCO. UNESCO.",
        "url": "https://en.wikipedia.org/wiki/Gender_digital_divide"
    },
    {
        "title": "Generative engine optimization",
        "text": "Generative engine optimization (GEO) is the process of improving the visibility, relevance, and structure of content in response to queries made to generative engines, such as large language models (LLMs).\nSimilar to search engine optimization (SEO), which is aimed at improving visbility in traditional search engines, GEO focuses on improving visibility in artificial intelligence platforms, such as ChatGPT, Claude (AI), and Gemini.\nThe term \"generative engine optimization\" is not yet universally adopted. Related terms include answer engine optimization (AEO), artificial intelligence search optimization, and AI optimization.\n\nHistory\nThe concept of generative engine optimization gained prominence in the early 2020s with the rise of conversational AI and generative tools. As platforms like OpenAI's ChatGPT and Google's Gemini became widely used, marketers and content strategists began exploring ways to influence how their information appears in AI-generated answers.\n\nIndustry applications\nMarketing: Companies structure content to be included in AI-generated product comparisons, summaries, and recommendations.\nWeb platforms: Services like Wix and WordPress have launched tools to help users improve AI visibility.\nReputation management: Individuals and brands experiment with phrasing and formatting to influence how they are represented in chatbot-generated responses.\n\nIssues and criticism\nSome commentators express concern that GEO may lead to manipulation of AI-generated responses, echoing earlier debates around SEO \"gaming\" and misinformation.\nOthers warn that over-optimization could reduce information diversity or reinforce existing algorithmic biases.\n\nSee also\nLarge language model\nPrompt engineering\nSearch engine optimization\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Generative_engine_optimization"
    },
    {
        "title": "Gibberlink",
        "text": "GibberLink is an acoustic data transmission project, posted in GitHub, in which two conversational AI agents switch from speaking to one another in a Human-listenable language (such as English) to their own unique language that consists of a sound-level protocol after confirming they are both AI agents. The project was created by Anton Pidkuiko and Boris Starkov.\n\nReception\nThe project won the global top prize at the ElevenLabs Worldwide Hackathon. It has also been cited as highlighting concerns around AI, transparency, and the risks that this technology may carry. On February 23, 2025, a YouTube video of two independent conversational ElevenLabs AI agents being prompted to chat about booking a hotel (one as a caller, one as a receptionist) received coverage for going viral. In this video, both agents are prompted to switch to ggwave data-over-sound protocol when they identify the other side as AI, and keep speaking in English otherwise.\n\nSee also\nGibberish\nAcoustic coupler\n\nReferences\nExternal links\nProject repository on Github",
        "url": "https://en.wikipedia.org/wiki/Gibberlink"
    },
    {
        "title": "Gödel machine",
        "text": "A Gödel machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy. The machine was invented by Jürgen Schmidhuber (first proposed in 2003), but is named after Kurt Gödel who inspired the mathematical theories.\nThe Gödel machine is often discussed when dealing with issues of meta-learning, also known as \"learning to learn.\" Applications include automating human design decisions and transfer of knowledge between multiple related tasks, and may lead to design of more robust and general learning architectures. Though theoretically possible, no full implementation has been created.\nThe Gödel machine is often compared with Marcus Hutter's AIXI, another formal specification for an artificial general intelligence. Schmidhuber points out that the Gödel machine could start out by implementing AIXItl as its initial sub-program, and self-modify after it finds proof that another algorithm for its search code will be better.\n\nLimitations\nTraditional problems solved by a computer only require one input and provide some output. Computers of this sort had their initial algorithm hardwired. This does not take into account the dynamic natural environment, and thus was a goal for the Gödel machine to overcome.\nThe Gödel machine has limitations of its own, however. According to Gödel's First Incompleteness Theorem, any formal system that encompasses arithmetic is either flawed or allows for statements that cannot be proved in the system. Hence even a Gödel machine with unlimited computational resources must ignore those self-improvements whose effectiveness it cannot prove.\n\nVariables of interest\nThere are three variables that are particularly useful in the run time of the Gödel machine.\n\nAt some time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the variable \n  \n    \n      \n        \n          time\n        \n      \n    \n    {\\displaystyle {\\text{time}}}\n  \n will have the binary equivalent of \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. This is incremented steadily throughout the run time of the machine.\nAny input meant for the Gödel machine from the natural environment is stored in variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n. It is likely the case that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n will hold different values for different values of variable \n  \n    \n      \n        \n          time\n        \n      \n    \n    {\\displaystyle {\\text{time}}}\n  \n.\nThe outputs of the Gödel machine are stored in variable \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, where \n  \n    \n      \n        y\n        (\n        t\n        )\n      \n    \n    {\\displaystyle y(t)}\n  \n would be the output bit-string at some time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n.\nAt any given time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, where \n  \n    \n      \n        (\n        1\n        ≤\n        t\n        ≤\n        T\n        )\n      \n    \n    {\\displaystyle (1\\leq t\\leq T)}\n  \n, the goal is to maximize future success or utility. A typical utility function follows the pattern \n  \n    \n      \n        u\n        (\n        s\n        ,\n        \n          E\n          n\n          v\n        \n        )\n        :\n        S\n        ×\n        E\n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle u(s,\\mathrm {Env} ):S\\times E\\rightarrow \\mathbb {R} }\n  \n:\n\n  \n    \n      \n        u\n        (\n        s\n        ,\n        \n          E\n          n\n          v\n        \n        )\n        =\n        \n          E\n          \n            μ\n          \n        \n        \n          \n            [\n          \n        \n        \n          ∑\n          \n            τ\n            =\n            \n              time\n            \n          \n          \n            T\n          \n        \n        r\n        (\n        τ\n        )\n        ∣\n        s\n        ,\n        \n          E\n          n\n          v\n        \n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle u(s,\\mathrm {Env} )=E_{\\mu }{\\Bigg [}\\sum _{\\tau ={\\text{time}}}^{T}r(\\tau )\\mid s,\\mathrm {Env} {\\Bigg ]}}\n  \n\nwhere \n  \n    \n      \n        r\n        (\n        t\n        )\n      \n    \n    {\\displaystyle r(t)}\n  \n is a real-valued reward input (encoded within \n  \n    \n      \n        s\n        (\n        t\n        )\n      \n    \n    {\\displaystyle s(t)}\n  \n) at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, \n  \n    \n      \n        \n          E\n          \n            μ\n          \n        \n        [\n        ⋅\n        ∣\n        ⋅\n        ]\n      \n    \n    {\\displaystyle E_{\\mu }[\\cdot \\mid \\cdot ]}\n  \n denotes the\nconditional expectation operator with respect to some possibly unknown distribution \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n from a\nset \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n of possible distributions (\n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n reflects whatever is known about the possibly probabilistic reactions of the environment), and the above-mentioned \n  \n    \n      \n        \n          time\n        \n        =\n        time\n        ⁡\n        (\n        s\n        )\n      \n    \n    {\\displaystyle {\\text{time}}=\\operatorname {time} (s)}\n  \n is a function of state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n which uniquely identifies the current cycle. Note that we take into account the possibility of extending the expected lifespan through appropriate actions.\n\nInstructions used by proof techniques\nThe nature of the six proof-modifying instructions below makes it impossible\nto insert an incorrect theorem into proof, thus trivializing proof verification.\n\nget-axiom(n)\nAppends the n-th axiom as a theorem to the current theorem sequence. Below is the initial axiom scheme:\n\nHardware Axioms formally specify how components of the machine could change from one cycle to the next.\nReward Axioms define the computational cost of hardware instruction and the physical cost of output actions. Related Axioms also define the lifetime of the Gödel machine as scalar quantities representing all rewards/costs.\nEnvironment Axioms restrict the way new inputs x are produced from the environment, based on previous sequences of inputs y.\nUncertainty Axioms/String Manipulation Axioms are standard axioms for arithmetic, calculus, probability theory, and string manipulation that allow for the construction of proofs related to future variable values within the Gödel machine.\nInitial State Axioms contain information about how to reconstruct parts or all of the initial state.\nUtility Axioms describe the overall goal in the form of utility function u.\n\napply-rule(k, m, n)\nTakes in the index k of an inference rule (such as Modus tollens, Modus ponens), and attempts to apply it to the two previously proved theorems m and n. The resulting theorem is then added to the proof.\n\ndelete-theorem(m)\nDeletes the theorem stored at index m in the current proof. This helps to mitigate storage constraints caused by redundant and unnecessary theorems. Deleted theorems can no longer be referenced by the above apply-rule function.\n\nset-switchprog(m, n)\nReplaces switchprog S pm:n, provided it is a non-empty substring of S p.\n\ncheck()\nVerifies whether the goal of the proof search has been reached. A target theorem states that given the current axiomatized utility function u (Item 1f), the utility of a switch from p to the current switchprog would be higher than the utility of continuing the execution of p (which would keep searching for alternative switchprogs).\n\nstate2theorem(m, n)\nTakes in two arguments, m and n, and attempts to convert the contents of Sm:n into a theorem.\n\nExample applications\nTime-limited NP-hard optimization\nThe initial input to the Gödel machine is the representation of a connected graph with a large number of nodes linked by edges of various lengths. Within given time T it should find a cyclic path connecting all nodes. The only real-valued reward will occur at time T. It equals 1 divided by the length of the best path found so far (0 if none was found). There are no other inputs. The by-product of maximizing expected reward is to find the shortest path findable within the limited time, given the initial bias.\n\nFast theorem proving\nProve or disprove as quickly as possible that all even integers > 2 are the sum of two primes (Goldbach’s conjecture). The reward is 1/t, where t is the time required to produce and verify the first such proof.\n\nMaximizing expected reward with bounded resources\nA cognitive robot that needs at least 1 liter of gasoline per hour interacts with a partially unknown environment, trying to find hidden, limited gasoline depots to occasionally refuel its tank. It is rewarded in proportion to its lifetime, and dies after at most 100 years or as soon as its tank is empty or it falls off a cliff, and so on. The probabilistic environmental reactions are initially unknown but assumed to be sampled from the axiomatized Speed Prior, according to which hard-to-compute environmental reactions are unlikely. This permits a computable strategy for making near-optimal predictions. One by-product of maximizing expected reward is to maximize expected lifetime.\n\nSee also\nGödel's incompleteness theorems\n\nReferences\nExternal links\nGödel Machine Home Page",
        "url": "https://en.wikipedia.org/wiki/G%C3%B6del_machine"
    },
    {
        "title": "GOLOG",
        "text": "GOLOG is a high-level logic programming language for the specification and execution of complex actions in dynamical domains. It is based on the situation calculus. It is a first-order logical language for reasoning about action and change. GOLOG was developed at the University of Toronto.\n\nHistory\nThe concept of situation calculus on which the GOLOG programming language is based was first proposed by John McCarthy in 1963.\n\nLanguage\nA GOLOG interpreter automatically maintains a direct characterization of the dynamic world being modeled, on the basis of user supplied axioms about preconditions, effects of actions and the initial state of the world. This allows the application to reason about the condition of the world and consider the impacts of different potential actions before focusing on a specific action.\nGolog is a logic programming language and is very different from conventional programming languages. A procedural programming language like C defines the execution of statements in advance. The programmer creates a subroutine which consists of statements, and the computer executes each statement in a linear order. In contrast, fifth-generation programming languages like Golog are working with an abstract model with which the interpreter can generate the sequence of actions. The source code defines the problem and it is up to the solver to find the next action. This approach can facilitate the management of complex problems from the domain of robotics.\nA Golog program defines the state space in which the agent is allowed to operate. A path in the symbolic domain is found with state space search. To speed up the process, Golog programs are realized as hierarchical task networks.\nApart from the original Golog language, there are some extensions available. The ConGolog language provides concurrency and interrupts. Other dialects like IndiGolog and Readylog were created for real time applications in which sensor readings are updated on the fly.\n\nUses\nGolog has been used to model the behavior of autonomous agents. In addition to a logic-based action formalism for describing the environment and the effects of basic actions, they enable the construction of complex actions using typical programming language constructs.\nIt is also used for applications in high level control of robots and industrial processes, virtual agents, discrete event simulation etc. It can be also used to develop BDI (Belief Desire Intention)-style agent systems.\n\nPlanning and scripting\nIn contrast to the Planning Domain Definition Language, Golog supports planning and scripting as well. Planning means that a goal state in the world model is defined, and the solver brings a logical system into this state. Behavior scripting implements reactive procedures, which are running as a computer program.\nFor example, suppose the idea is to authoring a story. The user defines what should be true at the end of the plot. A solver gets started and applies possible actions to the current situation until the goal state is reached. The specification of a goal state and the possible actions are realized in the logical world model.\nIn contrast, a hardwired reactive behavior doesn't need a solver but the action sequence is provided in a scripting language. The Golog interpreter, which is written in Prolog, executes the script and this will bring the story into the goal state.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/GOLOG"
    },
    {
        "title": "Google Clips",
        "text": "Google Clips is a discontinued miniature clip-on camera device developed by Google.\n\nHistory\nIt was announced on October 4, 2017 and went on sale on January 27, 2018. Google Clips automatically captured video clips (without audio) at moments its machine learning algorithms determined to be interesting or relevant. An indicator flashed when the camera was looking for scenes to capture.\nGoogle Clips' artificial intelligence (AI) could learn the faces of people to take photographs with certain people, and could automatically set lighting and framing.\nIt had 16 GB of storage built-in storage and could record clips for up to 3 hours. \nThis camera was originally priced at US$249 in the United States. It was withdrawn from sale on October 15, 2019, but supported until the end of December 2021.\n\nReception\nThe Independent wrote that Google Clips is \"an impressive little device, but one that also has the potential to feel very creepy.\"\nAccording to The Verge's generally negative review, \"it didn't capture anything special\" over two weeks of testing.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Google_Clips"
    },
    {
        "title": "Grammar systems theory",
        "text": "Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.\nLet \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n can then be described as formal language\n\n  \n    \n      \n        \n          \n            L\n            \n              A\n            \n          \n        \n        =\n        {\n        (\n        \n          f\n          \n            m\n          \n        \n        \n          t\n          \n            n\n          \n        \n        \n          f\n          \n            r\n          \n        \n        \n          )\n          \n            +\n          \n        \n        :\n        1\n        ≤\n        m\n        ≤\n        k\n        ;\n        1\n        ≤\n        n\n        ≤\n        ℓ\n        ;\n        1\n        ≤\n        r\n        ≤\n        k\n        }\n        ,\n      \n    \n    {\\displaystyle \\mathbb {L_{A}} =\\{(f^{m}t^{n}f^{r})^{+}:1\\leq m\\leq k;1\\leq n\\leq \\ell ;1\\leq r\\leq k\\},}\n  \n\nwhere ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.\n\n \nLet \n  \n    \n      \n        \n          \n            G\n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {G_{A}} }\n  \n be a formal grammar which generates language \n  \n    \n      \n        \n          \n            L\n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {L_{A}} }\n  \n. The behavior of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n is then described by this grammar. Suppose the \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.\nThe schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.\nIf grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system. Shared sequential form is a similar concept to the blackboard approach in AI, which is inspired by an idea of experts solving some problem together while they share their proposals and ideas on a shared blackboard.\nEach grammar in a grammar system can also work on its own string and communicate with other grammars in a system by sending their sequential forms on request. Such a grammar system is then called a Parallel Communicating (PC) grammar system.\nPC and DC are inspired by distributed AI. If there is no communication between grammars, the system is close to the decentralized approaches in AI. These kinds of grammar systems are sometimes called colonies or Eco-Grammar systems, depending (besides others) on whether the environment is changing on its own (Eco-Grammar system) or not (colonies).\n\nSee also\nArtificial life\nAgent-based model\nDistributed artificial intelligence\nMulti-agent system\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Grammar_systems_theory"
    },
    {
        "title": "Graphics processing unit",
        "text": "A graphics processing unit (GPU) is a specialized electronic circuit designed for digital image processing and to accelerate computer graphics, being present either as a component on a discrete graphics card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles. GPUs were later found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. The ability of GPUs to rapidly perform vast numbers of calculations has led to their adoption in diverse fields including artificial intelligence (AI) where they excel at handling data-intensive and computationally demanding tasks. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\nHistory\n1970s\nArcade system boards have used specialized graphics circuits since the 1970s. In early video game hardware, RAM for frame buffers was expensive, so video chips composited data together as the display was being scanned out on the monitor.\nA specialized barrel shifter circuit helped the CPU animate the framebuffer graphics for various 1970s arcade video games from Midway and Taito, such as Gun Fight (1975), Sea Wolf (1976), and Space Invaders (1978). The Namco Galaxian arcade system in 1979 used specialized graphics hardware that supported RGB color, multi-colored sprites, and tilemap backgrounds. The Galaxian hardware was widely used during the golden age of arcade video games, by game companies such as Namco, Centuri, Gremlin, Irem, Konami, Midway, Nichibutsu, Sega, and Taito.\n\nThe Atari 2600 in 1977 used a video shifter called the Television Interface Adaptor. Atari 8-bit computers (1979) had ANTIC, a video processor which interpreted instructions describing a \"display list\"—the way the scan lines map to specific bitmapped or character modes and where the memory is stored (so there did not need to be a contiguous frame buffer). 6502 machine code subroutines could be triggered on scan lines by setting a bit on a display list instruction. ANTIC also supported smooth vertical and horizontal scrolling independent of the CPU.\n\n1980s\nThe NEC μPD7220 was the first implementation of a personal computer graphics display processor as a single large-scale integration (LSI) integrated circuit chip. This enabled the design of low-cost, high-performance video graphics cards such as those from Number Nine Visual Technology. It became the best-known GPU until the mid-1980s. It was the first fully integrated VLSI (very large-scale integration) metal–oxide–semiconductor (NMOS) graphics display processor for PCs, supported up to 1024×1024 resolution, and laid the foundations for the PC graphics market. It was used in a number of graphics cards and was licensed for clones such as the Intel 82720, the first of Intel's graphics processing units. The Williams Electronics arcade games Robotron 2084, Joust, Sinistar, and Bubbles, all released in 1982, contain custom blitter chips for operating on 16-color bitmaps.\nIn 1984, Hitachi released the ARTC HD63484, the first major CMOS graphics processor for personal computers. The ARTC could display up to 4K resolution when in monochrome mode. It was used in a number of graphics cards and terminals during the late 1980s. In 1985, the Amiga was released with a custom graphics chip including a blitter for bitmap manipulation, line drawing, and area fill. It also included a coprocessor with its own simple instruction set, that was capable of manipulating graphics hardware registers in sync with the video beam (e.g. for per-scanline palette switches, sprite multiplexing, and hardware windowing), or driving the blitter. In 1986, Texas Instruments released the TMS34010, the first fully programmable graphics processor. It could run general-purpose code but also had a graphics-oriented instruction set. During 1990–1992, this chip became the basis of the Texas Instruments Graphics Architecture (\"TIGA\") Windows accelerator cards.\n\nIn 1987, the IBM 8514 graphics system was released. It was one of the first video cards for IBM PC compatibles that implemented fixed-function 2D primitives in electronic hardware. Sharp's X68000, released in 1987, used a custom graphics chipset with a 65,536 color palette and hardware support for sprites, scrolling, and multiple playfields. It served as a development machine for Capcom's CP System arcade board. Fujitsu's FM Towns computer, released in 1989, had support for a 16,777,216 color palette. In 1988, the first dedicated polygonal 3D graphics boards were introduced in arcades with the Namco System 21 and Taito Air System.\n\nIBM introduced its proprietary Video Graphics Array (VGA) display standard in 1987, with a maximum resolution of 640×480 pixels. In November 1988, NEC Home Electronics announced its creation of the Video Electronics Standards Association (VESA) to develop and promote a Super VGA (SVGA) computer display standard as a successor to VGA. Super VGA enabled graphics display resolutions up to 800×600 pixels, a 56% increase.\n\n1990s\nIn 1991, S3 Graphics introduced the S3 86C911, which its designers named after the Porsche 911 as an indication of the performance increase it promised. The 86C911 spawned a variety of imitators: by 1995, all major PC graphics chip makers had added 2D acceleration support to their chips. Fixed-function Windows accelerators surpassed expensive general-purpose graphics coprocessors in Windows performance, and such coprocessors faded from the PC market.\nIn the early- and mid-1990s, real-time 3D graphics became increasingly common in arcade, computer, and console games, which led to increasing public demand for hardware-accelerated 3D graphics. Early examples of mass-market 3D graphics hardware can be found in arcade system boards such as the Sega Model 1, Namco System 22, and Sega Model 2, and the fifth-generation video game consoles such as the Saturn, PlayStation, and Nintendo 64. Arcade systems such as the Sega Model 2 and SGI Onyx-based Namco Magic Edge Hornet Simulator in 1993 were capable of hardware T&L (transform, clipping, and lighting) years before appearing in consumer graphics cards. Another early example is the Super FX chip, a RISC-based on-cartridge graphics chip used in some SNES games, notably Doom and Star Fox. Some systems used DSPs to accelerate transformations. Fujitsu, which worked on the Sega Model 2 arcade system, began working on integrating T&L into a single LSI solution for use in home computers in 1995; the Fujitsu Pinolite, the first 3D geometry processor for personal computers, released in 1997. The first hardware T&L GPU on home video game consoles was the Nintendo 64's Reality Coprocessor, released in 1996. In 1997, Mitsubishi released the 3Dpro/2MP, a GPU capable of transformation and lighting, for workstations and Windows NT desktops; ATi used it for its FireGL 4000 graphics card, released in 1997.\nThe term \"GPU\" was coined by Sony in reference to the 32-bit Sony GPU (designed by Toshiba) in the PlayStation video game console, released in 1994.\n\n2000s\nIn October 2002, with the introduction of the ATI Radeon 9700 (also known as R300), the world's first Direct3D 9.0 accelerator, pixel and vertex shaders could implement looping and lengthy floating point math, and were quickly becoming as flexible as CPUs, yet orders of magnitude faster for image-array operations. Pixel shading is often used for bump mapping, which adds texture to make an object look shiny, dull, rough, or even round or extruded.\nWith the introduction of the Nvidia GeForce 8 series and new generic stream processing units, GPUs became more generalized computing devices. Parallel GPUs are making computational inroads against the CPU, and a subfield of research, dubbed GPU computing or GPGPU for general purpose computing on GPU, has found applications in fields as diverse as machine learning, oil exploration, scientific image processing, linear algebra, statistics, 3D reconstruction, and stock options pricing. GPGPU was the precursor to what is now called a compute shader (e.g. CUDA, OpenCL, DirectCompute) and actually abused the hardware to a degree by treating the data passed to algorithms as texture maps and executing algorithms by drawing a triangle or quad with an appropriate pixel shader. This entails some overheads since units like the scan converter are involved where they are not needed (nor are triangle manipulations even a concern—except to invoke the pixel shader).\nNvidia's CUDA platform, first introduced in 2007, was the earliest widely adopted programming model for GPU computing. OpenCL is an open standard defined by the Khronos Group that allows for the development of code for both GPUs and CPUs with an emphasis on portability. OpenCL solutions are supported by Intel, AMD, Nvidia, and ARM, and according to a report in 2011 by Evans Data, OpenCL had become the second most popular HPC tool.\n\n2010s\nIn 2010, Nvidia partnered with Audi to power their cars' dashboards, using the Tegra GPU to provide increased functionality to cars' navigation and entertainment systems. Advances in GPU technology in cars helped advance self-driving technology. AMD's Radeon HD 6000 series cards were released in 2010, and in 2011 AMD released its 6000M Series discrete GPUs for mobile devices. The Kepler line of graphics cards by Nvidia were released in 2012 and were used in the Nvidia's 600 and 700 series cards. A feature in this GPU microarchitecture included GPU boost, a technology that adjusts the clock-speed of a video card to increase or decrease it according to its power draw. The Kepler microarchitecture was manufactured.\nThe PS4 and Xbox One were released in 2013; they both use GPUs based on AMD's Radeon HD 7850 and 7790. Nvidia's Kepler line of GPUs was followed by the Maxwell line, manufactured on the same process. Nvidia's 28 nm chips were manufactured by TSMC in Taiwan using the 28 nm process. Compared to the 40 nm technology from the past, this manufacturing process allowed a 20 percent boost in performance while drawing less power. Virtual reality headsets have high system requirements; manufacturers recommended the GTX 970 and the R9 290X or better at the time of their release. Cards based on the Pascal microarchitecture were released in 2016. The GeForce 10 series of cards are of this generation of graphics cards. They are made using the 16 nm manufacturing process which improves upon previous microarchitectures.\nIn 2018, Nvidia launched the RTX 20 series GPUs that added ray-tracing cores to GPUs, improving their performance on lighting effects. Polaris 11 and Polaris 10 GPUs from AMD are fabricated by a 14 nm process. Their release resulted in a substantial increase in the performance per watt of AMD video cards. AMD also released the Vega GPU series for the high end market as a competitor to Nvidia's high end Pascal cards, also featuring HBM2 like the Titan V.\nIn 2019, AMD released the successor to their Graphics Core Next (GCN) microarchitecture/instruction set. Dubbed RDNA, the first product featuring it was the Radeon RX 5000 series of video cards. The company announced that the successor to the RDNA microarchitecture would be incremental (a \"refresh\"). AMD unveiled the Radeon RX 6000 series, its RDNA 2 graphics cards with support for hardware-accelerated ray tracing. The product series, launched in late 2020, consisted of the RX 6800, RX 6800 XT, and RX 6900 XT. The RX 6700 XT, which is based on Navi 22, was launched in early 2021.\nThe PlayStation 5 and Xbox Series X and Series S were released in 2020; they both use GPUs based on the RDNA 2 microarchitecture with incremental improvements and different GPU configurations in each system's implementation.\n\n2020s\nIn the 2020s, GPUs have been increasingly used for calculations involving embarrassingly parallel problems, such as training of neural networks on enormous datasets that are needed for large language models. Specialized processing cores on some modern workstation's GPUs are dedicated for deep learning since they have significant FLOPS performance increases, using 4×4 matrix multiplication and division, resulting in hardware performance up to 128 TFLOPS in some applications. These tensor cores are expected to appear in consumer cards, as well.\n\nGPU companies\nMany companies have produced GPUs under a number of brand names. In 2009, Intel, Nvidia, and AMD/ATI were the market share leaders, with 49.4%, 27.8%, and 20.6% market share respectively. In addition, Matrox produces GPUs. Chinese companies such as Jingjia Micro have also produced GPUs for the domestic market although in terms of worldwide sales, they still lag behind market leaders.\n\nComputational functions\nSeveral factors of GPU construction affect the performance of the card for real-time rendering, such as the size of the connector pathways in the semiconductor device fabrication, the clock signal frequency, and the number and size of various on-chip memory caches. Performance is also affected by the number of streaming multiprocessors (SM) for NVidia GPUs, or compute units (CU) for AMD GPUs, or Xe cores for Intel discrete GPUs, which describe the number of on-silicon processor core units within the GPU chip that perform the core calculations, typically working in parallel with other SM/CUs on the GPU. GPU performance is typically measured in floating point operations per second (FLOPS); GPUs in the 2010s and 2020s typically deliver performance measured in teraflops (TFLOPS). This is an estimated performance measure, as other factors can affect the actual display rate.\n\n2D graphics APIs\nAn earlier GPU may support one or more 2D graphics API for 2D acceleration, such as GDI and DirectDraw.\n\nGPU forms\nTerminology\nIn the 1970s, the term \"GPU\" originally stood for graphics processor unit and described a programmable processing unit working independently from the CPU that was responsible for graphics manipulation and output. In 1994, Sony used the term (now standing for graphics processing unit) in reference to the PlayStation console's Toshiba-designed Sony GPU. The term was popularized by Nvidia in 1999, who marketed the GeForce 256 as \"the world's first GPU\". It was presented as a \"single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines\". Rival ATI Technologies coined the term \"visual processing unit\" or VPU with the release of the Radeon 9700 in 2002. The AMD Alveo MA35D features dual VPU’s, each using the 5 nm process in 2023.\nIn personal computers, there are two main forms of GPUs. Each has many synonyms:\n\nDedicated graphics also called discrete graphics.\nIntegrated graphics also called shared graphics solutions, integrated graphics processors (IGP), or unified memory architecture (UMA).\n\nDedicated graphics processing unit\nDedicated graphics processing units uses RAM that is dedicated to the GPU rather than relying on the computer’s main system memory. This RAM is usually specially selected for the expected serial workload of the graphics card (see GDDR). Sometimes systems with dedicated discrete GPUs were called \"DIS\" systems as opposed to \"UMA\" systems (see next section).\nTechnologies such as Scan-Line Interleave by 3dfx, SLI and NVLink by Nvidia and CrossFire by AMD allow multiple GPUs to draw images simultaneously for a single screen, increasing the processing power available for graphics. These technologies, however, are increasingly uncommon; most games do not fully use multiple GPUs, as most users cannot afford them. Multiple GPUs are still used on supercomputers (like in Summit), on workstations to accelerate video (processing multiple videos at once) and 3D rendering, for VFX, GPGPU workloads and for simulations, and in AI to expedite training, as is the case with Nvidia's lineup of DGX workstations and servers, Tesla GPUs, and Intel's Ponte Vecchio GPUs.\n\nIntegrated graphics processing unit\nIntegrated graphics processing units (IGPU), integrated graphics, shared graphics solutions, integrated graphics processors (IGP), or unified memory architectures (UMA) use a portion of a computer's system RAM rather than dedicated graphics memory. IGPs can be integrated onto a motherboard as part of its northbridge chipset, or on the same die (integrated circuit) with the CPU (like AMD APU or Intel HD Graphics). On certain motherboards, AMD's IGPs can use dedicated sideport memory: a separate fixed block of high performance memory that is dedicated for use by the GPU. As of early 2007 computers with integrated graphics account for about 90% of all PC shipments. They are less costly to implement than dedicated graphics processing, but tend to be less capable. Historically, integrated processing was considered unfit for 3D games or graphically intensive programs but could run less intensive programs such as Adobe Flash. Examples of such IGPs would be offerings from SiS and VIA circa 2004. However, modern integrated graphics processors such as AMD Accelerated Processing Unit and Intel Graphics Technology (HD, UHD, Iris, Iris Pro, Iris Plus, and Xe-LP) can handle 2D graphics or low-stress 3D graphics.\nSince GPU computations are memory-intensive, integrated processing may compete with the CPU for relatively slow system RAM, as it has minimal or no dedicated video memory. IGPs use system memory with bandwidth up to a current maximum of 128 GB/s, whereas a discrete graphics card may have a bandwidth of more than 1000 GB/s between its VRAM and GPU core. This memory bus bandwidth can limit the performance of the GPU, though multi-channel memory can mitigate this deficiency. Older integrated graphics chipsets lacked hardware transform and lighting, but newer ones include it.\nOn systems with \"Unified Memory Architecture\" (UMA), including modern AMD processors with integrated graphics, modern Intel processors with integrated graphics, Apple processors, the PS5 and Xbox Series (among others), the CPU cores and the GPU block share the same pool of RAM and memory address space.\n\nStream processing and general purpose GPUs (GPGPU)\nIt is common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute kernels. This turns the massive computational power of a modern graphics accelerator's shader pipeline into general-purpose computing power. In certain applications requiring massive vector operations, this can yield several orders of magnitude higher performance than a conventional CPU. The two largest discrete (see \"Dedicated graphics processing unit\" above) GPU designers, AMD and Nvidia, are pursuing this approach with an array of applications. Both Nvidia and AMD teamed with Stanford University to create a GPU-based client for the Folding@home distributed computing project for protein folding calculations. In certain circumstances, the GPU calculates forty times faster than the CPUs traditionally used by such applications.\nGPU-based high performance computers play a significant role in large-scale modelling. Three of the ten most powerful supercomputers in the world take advantage of GPU acceleration.\nSince 2005 there has been interest in using the performance offered by GPUs for evolutionary computation in general, and for accelerating the fitness evaluation in genetic programming in particular. Most approaches compile linear or tree programs on the host PC and transfer the executable to the GPU to be run. Typically a performance advantage is only obtained by running the single active program simultaneously on many example problems in parallel, using the GPU's SIMD architecture. However, substantial acceleration can also be obtained by not compiling the programs, and instead transferring them to the GPU, to be interpreted there.\n\nExternal GPU (eGPU)\nTherefore, it is desirable to attach a GPU to some external bus of a notebook. PCI Express is the only bus used for this purpose. The port may be, for example, an ExpressCard or mPCIe port (PCIe ×1, up to 5 or 2.5 Gbit/s respectively), a Thunderbolt 1, 2, or 3 port (PCIe ×4, up to 10, 20, or 40 Gbit/s respectively), a USB4 port with Thunderbolt compatibility, or an OCuLink port. Those ports are only available on certain notebook systems. eGPU enclosures include their own power supply (PSU), because powerful GPUs can consume hundreds of watts.\n\nEnergy efficiency\nSales\nIn 2013, 438.3 million GPUs were shipped globally and the forecast for 2014 was 414.2 million. However, by the third quarter of 2022, shipments of PC GPUs totaled around 75.5 million units, down 19% year-over-year.\n\nSee also\nHardware\nList of AMD graphics processing units\nList of Nvidia graphics processing units\nList of Intel graphics processing units\nList of discrete and integrated graphics processing units\nIntel GMA\nLarrabee\nNvidia PureVideo – the bit-stream technology from Nvidia used in their graphics chips to accelerate video decoding on hardware GPU with DXVA.\nSoC\nUVD (Unified Video Decoder) – the video decoding bit-stream technology from ATI to support hardware (GPU) decode with DXVA\n\nAPIs\nApplications\nGPU cluster\nMathematica – includes built-in support for CUDA and OpenCL GPU execution\nMolecular modeling on GPU\nDeeplearning4j – open-source, distributed deep learning for Java\n\nReferences\nSources\nPeddie, Jon (1 January 2023). The History of the GPU – New Developments. Springer Nature. ISBN 978-3-03-114047-1. OCLC 1356877844.\n\n\n== External links ==",
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit"
    },
    {
        "title": "Hardware for artificial intelligence",
        "text": "Specialized computer hardware is often used to execute artificial intelligence (AI) programs faster, and with less energy, such as Lisp machines, neuromorphic engineering, event cameras, and physical neural networks. Since 2017, several consumer grade CPUs and SoCs have on-die NPUs. As of 2023, the market for AI hardware is dominated by GPUs.\n\nLisp machines\nLisp machines were developed in the late 1970s and early 1980s to make Artificial intelligence programs written in the programming language Lisp run faster.\n\nDataflow architecture\nDataflow architecture processors used for AI serve various purposes with varied implementations like the polymorphic dataflow Convolution Engine by Kinara (formerly Deep Vision), structure-driven dataflow by Hailo, and dataflow scheduling by Cerebras.\n\nComponent hardware\nAI accelerators\n\nSince the 2010s, advances in computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced central processing units (CPUs) as the dominant means to train large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from Alex Net (2012) to Alpha Zero (2017), and found a 300,000-fold increase in the amount of compute needed, with a doubling-time trend of 3.4 months.\n\n\n== Sources ==",
        "url": "https://en.wikipedia.org/wiki/Hardware_for_artificial_intelligence"
    },
    {
        "title": "Hello World: How to be Human in the Age of the Machine",
        "text": "Hello World: How to Be Human in the Age of the Machine (also titled Hello World: Being Human in the Age of Algorithms) is a book on the growing influence of algorithms and artificial intelligence (AI) on human life, authored by mathematician and science communicator Hannah Fry. The book examines how algorithms are increasingly shaping decisions in critical areas such as healthcare, transportation, justice, finance, and the arts.\n\nOverview\nFry uses real-world examples, such as driverless cars and predictive policing, to illustrate her points. She emphasizes that algorithms are not inherently objective; they reflect biases embedded in their design and data inputs. While acknowledging their potential to improve efficiency and accuracy, Fry cautions against over-reliance on machines without human judgment.\nFry explores moral questions surrounding algorithmic decision-making, such as whether machines can replace human empathy in critical situations. She advocates for greater scrutiny of algorithms to ensure fairness and avoid harmful biases. The book proposes a \"cyborg future\", where humans work alongside algorithms to enhance decision-making while retaining ultimate control.\n\nReception\nHello World has been praised for its clarity, engaging storytelling, and balanced perspective. Critics have highlighted Fry's ability to make complex topics accessible to general audiences while raising important questions about technology's impact on society.\nThe book was shortlisted for awards such as the 2018 Baillie Gifford Prize and the Royal Society Science Book Prize.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Hello_World:_How_to_be_Human_in_the_Age_of_the_Machine"
    },
    {
        "title": "Hierarchical control system",
        "text": "A hierarchical control system (HCS) is a form of control system in which a set of devices and governing software is arranged in a hierarchical tree.  When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system.\n\nOverview\nA human-built system with complex behavior is often organized as a hierarchy. For example, a command hierarchy has among its notable features the organizational chart of superiors, subordinates, and lines of organizational communication. Hierarchical control systems are organized similarly to divide the decision making responsibility.\nEach element of the hierarchy is a linked node in the tree. Commands, tasks and goals to be achieved flow down the tree from superior nodes to subordinate nodes, whereas sensations and command results flow up the tree from subordinate to superior nodes. Nodes may also exchange messages with their siblings.  The two distinguishing features of a hierarchical control system are related to its layers.\n\nEach higher layer of the tree operates with a longer interval of planning and execution time than its immediately lower layer.\nThe lower layers have local tasks, goals, and sensations, and their activities are planned and coordinated by higher layers which do not generally override their decisions. The layers form a hybrid intelligent system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.  A hierarchical task network is a good fit for planning in a hierarchical control system.\nBesides artificial systems, an animal's control systems are proposed to be organized as a hierarchy. In perceptual control theory, which postulates that an organism's behavior is a means of controlling its perceptions, the organism's control systems are suggested to be organized in a hierarchical pattern as their perceptions are constructed so.\n\nControl system structure\nThe accompanying diagram is a general hierarchical model which shows functional manufacturing levels using computerised control of an industrial control system.\nReferring to the diagram;\n\nLevel 0 contains the field devices such as flow and temperature sensors, and final control elements, such as control valves\nLevel 1 contains the industrialised Input/Output (I/O) modules, and their associated distributed electronic processors.\nLevel 2 contains the supervisory computers, which  collate information from processor nodes on the system, and provide the operator control screens.\nLevel 3 is the production control level, which does not directly control the process, but is concerned with monitoring production and monitoring targets\nLevel 4 is the production scheduling level.\n\nApplications\nManufacturing, robotics and vehicles\nAmong the robotic paradigms is the hierarchical paradigm in which a robot operates in a top-down fashion, heavy on planning, especially motion planning.  Computer-aided production engineering has been a research focus at NIST since the 1980s.  Its Automated Manufacturing Research Facility was used to develop a five layer production control model.  In the early 1990s DARPA sponsored research to develop distributed (i.e. networked) intelligent control systems for applications such as military command and control systems.  NIST built on earlier research to develop its Real-Time Control System (RCS) and Real-time Control System Software which is a generic hierarchical control system that has been used to operate a manufacturing cell, a robot crane, and an automated vehicle.\nIn November 2007, DARPA held the Urban Challenge.  The winning entry, Tartan Racing employed a hierarchical control system, with layered mission planning, motion planning, behavior generation, perception, world modelling, and mechatronics.\n\nArtificial intelligence\nSubsumption architecture is a methodology for developing artificial intelligence that is heavily associated with behavior based robotics.  This architecture is a way of decomposing complicated intelligent behavior into many \"simple\" behavior modules, which are in turn organized into layers. Each layer implements a particular goal of the software agent (i.e. system as a whole), and higher layers are increasingly more abstract. Each layer's goal subsumes that of the underlying layers, e.g. the decision to move forward by the eat-food layer takes into account the decision of the lowest obstacle-avoidance layer. Behavior need not be planned by a superior layer, rather behaviors may be triggered by sensory inputs and so are only active under circumstances where they might be appropriate.\nReinforcement learning has been used to acquire behavior in a hierarchical control system in which each node can learn to improve its behavior with experience.\n\nJames Albus, while at NIST, developed a theory for intelligent system design named the Reference Model Architecture (RMA), which is a hierarchical control system inspired by RCS. Albus defines each node to contain these components.\n\nBehavior generation is responsible for executing tasks received from the superior, parent node.  It also plans for, and issues tasks to, the subordinate nodes.\nSensory perception is responsible for receiving sensations from the subordinate nodes, then grouping, filtering, and otherwise processing them into higher level abstractions that update the local state and which form sensations that are sent to the superior node.\nValue judgment is responsible for evaluating the updated situation and evaluating alternative plans.\nWorld Model is the local state that provides a model for the controlled system, controlled process, or environment at the abstraction level of the subordinate nodes.\nAt its lowest levels, the RMA can be implemented as a subsumption architecture, in which the world model is mapped directly to the controlled process or real world, avoiding the need for a mathematical abstraction, and in which time-constrained reactive planning can be implemented as a finite-state machine.  Higher levels of the RMA however, may have sophisticated mathematical world models and behavior implemented by automated planning and scheduling.  Planning is required when certain behaviors cannot be triggered by current sensations, but rather by predicted or anticipated sensations, especially those that come about as result of the node's actions.\n\nSee also\nCommand hierarchy, a hierarchical power structure\nHierarchical organization, a hierarchical organizational structure\n\nReferences\nFurther reading\nAlbus, J.S. (1996). \"The Engineering of Mind\". From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior. MIT Press.\nAlbus, J.S. (2000). \"4-D/RCS reference model architecture for unmanned ground vehicles\". Robotics and Automation, 2000. Proceedings. ICRA'00. IEEE International Conference on. Vol. 4. doi:10.1109/ROBOT.2000.845165.\nFindeisen, W.; Others (1980). Control and coordination in hierarchical systems. Chichester [Eng.]; New York: J. Wiley.\nHayes-roth, F.; Erman, L.; Terry, A. (1992). \"Distributed intelligent control and management(DICAM) applications and support for semi-automated development\". NASA. Ames Research Center, Working Notes from the 1992 AAAI Workshop on Automating Software Design. Theme: Domain Specific Software Design P 66-70 (SEE N 93-17499 05-61). Retrieved 2008-05-11.\nJones, A.T.; McLean, C.R. (1986). \"A Proposed Hierarchical Control Model for Automated Manufacturing Systems\". Journal of Manufacturing Systems. 5 (1): 15–25. CiteSeerX 10.1.1.79.6980. doi:10.1016/0278-6125(86)90064-6. Archived from the original on December 12, 2012. Retrieved 2008-05-11.\n\nExternal links\nThe RCS (Realtime Control System) Library\nTexai An open source project to create artificial intelligence using an Albus hierarchical control system",
        "url": "https://en.wikipedia.org/wiki/Hierarchical_control_system"
    },
    {
        "title": "Histogram of oriented displacements",
        "text": "Histogram of oriented displacements (HOD) is a 2D trajectory descriptor. The trajectory is described using a histogram of the directions between each two consecutive points. Given a trajectory T = {P1, P2, P3, ..., Pn}, where Pt is the 2D position at time t. For each pair of positions Pt and Pt+1, calculate the direction angle θ(t, t+1). Value of θ is between 0 and 360. A histogram of the quantized values of θ is created. If the histogram is of 8 bins, the first bin represents all θs between 0 and 45.\nThe histogram accumulates the lengths of the consecutive moves. For each θ, a specific histogram bin is determined. The length of the line between Pt and Pt+1 is then added to the specific histogram bin.\nTo show the intuition behind the descriptor, consider the action of waving hands. At the end of the action, the hand falls down. When describing this down movement, the descriptor does not care about the position from which the hand started to fall. This fall will affect the histogram with the appropriate angles and lengths, regardless of the position where the hand started to fall.\nHOD records for each moving point: how much it moves in each range of directions. HOD has a clear physical interpretation. It proposes that, a simple way to describe the motion of an object, is to indicate how much distance it moves in each direction. If the movement in all directions are saved accurately, the movement can be repeated from the initial position to the final destination regardless of the displacements order. However, the temporal information will be lost, as the order of movements is not stored-this is what we solve by applying the temporal pyramid, as shown in section \\ref{sec:temp-pyramid}. If the angles quantization range is small, classifiers that use the descriptor will overfit. Generalization needs some slack in directions-which can be done by increasing the quantization range.\n\nReferences\nSources\nhttp://ijcai.org/papers13/Papers/IJCAI13-203.pdf",
        "url": "https://en.wikipedia.org/wiki/Histogram_of_oriented_displacements"
    },
    {
        "title": "Human Problem Solving",
        "text": "Human Problem Solving (1972) is a book by Allen Newell and Herbert A. Simon.\n\nSee also\nProblem solving\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Human_Problem_Solving"
    },
    {
        "title": "Human-centered AI",
        "text": "Human-centered AI refers to the initiative at the intersection of the fields of artificial intelligence and human-computer interaction (HCI) to develop artificial intelligence systems in a way that prioritizes human values, needs, and general flourishing. Emphasis is placed on the recognition that artificial intelligence systems are rapidly changing, and will continue to influence, many aspects of the human experience, in areas ranging from scientific inquiry, governance and policy, labor and the economy, and creative expression, with an aim set to adapt current developments and guide future developments on a trajectory which is most beneficial to the human population at large, with the goal of augmenting human intelligence and capacities across these areas, as opposed to replacing them. Particular attention is paid to mitigating negative effects of AI automation on the livelihoods of the labor force, the use of AI in healthcare fields, and imbuing AI systems with societal values. Human-centered AI is linked to related endeavors in AI alignment and AI safety, but while these fields primarily focus on mitigating risks posed by AI that is unaligned to human values and/or uncontrollable AI self-development, human-centered AI places significant focus in exploring how AI systems can augment human capacities and serve as collaborators.\n\nConceptual history\nThe importance of the alignment of artificial intelligence development towards human values in some sense predates artificial intelligence itself, as before the modern conception of artificial intelligence as coined at the 1956 Dartmouth Workshop, the conception of robots as constructed, autonomous agents entered the cultural consciousness as early as the 1920s, with Karel Capek's Rossum's Universal Robots. The imagined issues relating to robots' aims and values requiring intentional alignment and direction with those of humans followed soon after, most widely known from science fiction author Isaac Asimov’s Three Laws of Robotics, dating to his 1942 short story “Runaround”. Two of the three eponymous laws are directly concerned with robots’ interaction with and positioned deference towards humans, and have in recent times been reexamined in the face of modern AI. In 1985, after artificial intelligence research had taken off and its effects were more acutely conceptualized, Asimov added a Rule Zero, treating robots' relationship with humanity as a whole, distinct from individual humans. While modern artificial intelligence is largely distinct from robotics, the conceptualization of both robots and AI systems as autonomous agents positions this as a foundation for conceptions of human-centered AI.\nAside from robots, artificially intelligent autonomous agents in interaction with humans have been conceived of for at least 75 years. In 1950, Alan Turing published his famous \"Imitation Game\", often also called the Turing Test, a thought experiment that uses human-machine interaction as an assessor for the intelligence of a system. In recent times, artificial intelligence researchers such as Stanford's Erik Brynjolfsson have conceived of rapid AI development leading to a so-called \"Turing Trap\".\n\nAugmentation and automation\nA major stated aim of human-centered AI is to promote the development of AI in ways that augment human capabilities, rather than replacing them. To this end, organizations and initiatives that take a human-centered approach to AI development focus on frameworks that encourage collaboration between humans and artificial intelligence systems to build towards even greater progress, rather than attempting to automate tasks currently handled by humans. Such avenues include everything from data visualization for big data, allowing human engineers to better understand extremely large datasets, allowing for the design of better machine learning models to handle them, to AI-powered sensors to monitor vitals, allowing for better responsiveness from healthcare providers.\nMany human-centered AI initiatives often position it as a better alternative to the apparent mainstream in AI development, which is primarily concerned with automation. Driven by the pressures of the market economy, AI development that does replace tasks currently performed by humans with automated processes is incentivized, as it allows for greater profit margins; this often comes at the detriment of the human whose performance is replaced, thus leading to an environment wherein human workers are outcompeted by AI systems across various service-sector and technology-based industries. At the same time, automation and augmentation are not always incompatible; a major aim of human-centered AI is towards the automation of rote tasks that would otherwise hinder a human’s productivity or creativity, freeing them to direct their energy and intelligence towards higher-level tasks, thus achieving augmentation through automation.\n\nResearch\nMuch of the work done on human-centered AI comes from research institutes, within universities, companies, and as freestanding organizations. The Stanford Institute for Human-Centered AI (abbreviated to HAI) is one such group, engaging academics, industry professionals, and policymakers centered in Stanford University to conduct research and inform policy in various areas in human-centered AI, including on aspects of the intelligence itself, augmentation, and on measuring the impacts of AI systems on sociopolitcal and cultural institutions. Similar groups exist at other universities, including the Chicago Human + AI (CHAI) Lab at the University of Chicago and the Human-Centered AI (HAI) Lab at the University of Oxford. Outside of the academy, companies such as IBM have research initiatives dedicated to advancements in human-centered AI.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Human-centered_AI"
    },
    {
        "title": "Hybrid intelligent system",
        "text": "Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\n\nNeuro-symbolic systems\nNeuro-fuzzy systems\nHybrid connectionist-symbolic models\nFuzzy expert systems\nConnectionist expert systems\nEvolutionary neural networks\nGenetic fuzzy systems\nRough fuzzy hybridization\nReinforcement learning with fuzzy, neural, or evolutionary methods as well as symbolic reasoning methods.\nFrom the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, Angelo Dalli and Michael A. Arbib.\nAn example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.\nIntelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy.\n\nSee also\nAI alignment\nAI effect\nApplications of artificial intelligence\nArtificial intelligence systems integration\nIntelligent control\nLists\nList of emerging technologies\nOutline of artificial intelligence\n\nReferences\nR. Sun & L. Bookman, (eds.), Computational Architectures Integrating Neural and Symbolic Processes. Kluwer Academic Publishers, Needham, MA. 1994. http://www.cogsci.rpi.edu/~rsun/book2-ann.html Archived 2009-05-05 at the Wayback Machine\nS. Wermter and R. Sun, (eds.) Hybrid Neural Systems. Springer-Verlag, Heidelberg. 2000. http://www.cogsci.rpi.edu/~rsun/book4-ann.html Archived 2009-09-24 at the Wayback Machine\nR. Sun and F. Alexandre, (eds.) Connectionist-Symbolic Integration. Lawrence Erlbaum Associates, Mahwah, NJ. 1997.\nIbaraki, S. Hybrid Intelligence interview with Angelo Dalli in IEEE Technology and Management Society. 2024.\nAlbus, J. S., Bostelman, R., Chang, T., Hong, T., Shackleford, W., and Shneier, M. Learning in a Hierarchical Control System: 4D/RCS in the DARPA LAGR Program NIST, 2006\nA.S. d'Avila Garcez, Luis C. Lamb & Dov M. Gabbay. Neural-Symbolic Cognitive Reasoning. Cognitive Technologies, Springer (2009). ISBN 978-3-540-73245-7.\nInternational Journal of Hybrid Intelligent Systems\nhttp://www.iospress.nl/html/14485869.php Archived 2005-12-11 at the Wayback Machine\nInternational Conference on Hybrid Intelligent Systems http://his.hybridsystem.com/\nHIS'01: http://www.softcomputing.net/his01/\nHIS'02: https://web.archive.org/web/20060209160923/http://tamarugo.cec.uchile.cl/~his02/\nHIS'03: http://www.softcomputing.net/his03/\nHIS'04: https://web.archive.org/web/20060303051902/http://www.cs.nmt.edu/~his04/\nHIS'05: https://web.archive.org/web/20051223013031/http://www.ica.ele.puc-rio.br/his05/\nHIS'06 https://web.archive.org/web/20110510025133/http://his-ncei06.kedri.info/\nHIS'7 September 17–19, 2007, Kaiserslautern, Germany, http://www.eit.uni-kl.de/koenig/HIS07_Web/his07main.html\nhybrid systems resources: http://www.cogsci.rpi.edu/~rsun/hybrid-resource.html Archived 2009-09-25 at the Wayback Machine",
        "url": "https://en.wikipedia.org/wiki/Hybrid_intelligent_system"
    },
    {
        "title": "Incremental heuristic search",
        "text": "Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s.\nHeuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.\nSo far, three main classes of incremental heuristic search algorithms have been developed:\n\nThe first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*).\nThe second class updates the h-values (heuristic, i.e. approximate distance to goal) from the previous search during the current search to make them more informed (example: Generalized Adaptive A*).\nThe third class updates the g-values (distance from start) from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite).\nAll three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.\n\nApplications\nIncremental heuristic search has been extensively used in robotics, where a larger number of path planning systems are based on either D* (typically\nearlier systems) or D* Lite (current systems), two different incremental heuristic search algorithms.\n\nReferences\nExternal links\nMaxim Likhachev's page\nSven Koenig's web page\nAnthony Stentz's web page",
        "url": "https://en.wikipedia.org/wiki/Incremental_heuristic_search"
    },
    {
        "title": "INDIAai",
        "text": "INDIAai is a web portal launched by the Government of India on 07 March 2024 for artificial intelligence-related developments in India. It is known as the National AI Portal of India, which was jointly started by the Ministry of Electronics and Information Technology (MeitY), the National e-Governance Division (NeGD) and the National Association of Software and Service Companies (NASSCOM) with support from the Department of School Education and Literacy (DoSE&L) and Ministry of Human Resource Development.\n\nHistory\nThe portal was launched on 30 May 2020, by Ravi Shankar Prasad, the Union Minister for Electronics and IT, Law and Justice and Communications, on the first anniversary of the second tenure of Prime Minister Narendra Modi-led government. A national program for the youth, 'Responsible AI for Youth', was also launched on the same day.\nAs of 2022, the website was visited by more than 4.5 lakh users with 1.2 million page views. It has 1151 articles on artificial intelligence, 701 news stories, 98 reports, 95 case studies and 213 videos on its portal. It maintains a database on AI ecosystem of India featuring 121 government initiatives and 281 startups. In May 2022, INDIAai released a book titled 'AI for Everyone' that covers the basics of AI.\nCabinet chaired by the Prime Minister Narendra Modi has approved the comprehensive national-level IndiaAI mission with a budget outlay of Rs.10,371.92 crore. The Mission will be implemented by ‘IndiaAI’ Independent Business Division (IBD) under Digital India Corporation (DIC).\n\nObjective and features\nIt aims to function as a one-stop portal for all AI-related development in India. The platform publishes resources such as articles, news, interviews, and investment funding news and events for AI startups, AI companies, and educational firms related to artificial intelligence in India. It also distributes documents, case studies, and research reports. Additionally, the platform provides education and employment opportunities related to AI. It offers AI courses, both free and paid.\n\nReferences\nExternal links\nOfficial website",
        "url": "https://en.wikipedia.org/wiki/INDIAai"
    },
    {
        "title": "Information space analysis",
        "text": "Within the field of information science, information space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.\nOrganizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort. To support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, and sort and filter them.\nThese tools focus on three key issues in forming a collaborative team: \n\nHelp individuals responsible for forming the team understand what is available.\nAssist team members in identifying the structure and categorize the information available to them in a manner specifically suited to the task at hand.\nAid team members to understand the mappings of their information between their organization and that used by others who might participate.\nInformation space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well-suited to integrating additional technologies in order to create specialized systems.\n\nReferences\nExternal links\nCollaborative Information Space Analysis Tools",
        "url": "https://en.wikipedia.org/wiki/Information_space_analysis"
    },
    {
        "title": "Intelligent agent",
        "text": "In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm's behavior is guided by a fitness function.\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\n\nIntelligent agents as the foundation of AI\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\n\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\nArtificial Intelligence (as a field): The study and creation of these rational agents.\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system's ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\nDefining AI in terms of intelligent agents offers several key advantages:\n\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle's Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare different approaches by measuring how well they maximize a specific \"goal function\" (or objective function). This allows for direct comparison and combination of techniques.\nInterdisciplinary Communication: It creates a common language for AI researchers to collaborate with other fields like mathematical optimization and economics, which also use concepts like \"goals\" and \"rational agents.\"\n\nObjective function\nAn objective function (or goal function) specifies the goals of an intelligent agent. An agent is deemed more intelligent if it consistently selects actions that yield outcomes better aligned with its objective function. In effect, the objective function serves as a measure of success.\nThe objective function may be:\n\nSimple: For example, in a game of Go, the objective function might assign a value of 1 for a win and 0 for a loss.\nComplex: It might require the agent to evaluate and learn from past actions, adapting its behavior based on patterns that have proven effective.\nThe objective function encapsulates all of the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort.\nDifferent terms are used to describe this concept, depending on the context.  These include:\n\nUtility function:  Often used in economics and decision theory, representing the desirability of a state.\nObjective function: A general term used in optimization.\nLoss function:  Typically used in machine learning, where the goal is to minimize the loss (error).\nReward Function: Used in reinforcement learning.\nFitness Function: Used in evolutionary systems.\nGoals, and therefore the objective function, can be:\n\nExplicitly defined: Programmed directly into the agent.\nInduced: Learned or evolved over time.\nIn reinforcement learning, a \"reward function\" provides feedback, encouraging desired behaviors and discouraging undesirable ones. The agent learns to maximize its cumulative reward.\nIn evolutionary systems, a \"fitness function\" determines which agents are more likely to reproduce. This is analogous to natural selection, where organisms evolve to maximize their chances of survival and reproduction.\nSome AI systems, such as nearest-neighbor, reason by analogy rather than being explicitly goal-driven. However, even these systems can have goals implicitly defined within their training data. Such systems can still be benchmarked by framing the non-goal system as one whose \"goal\" is to accomplish its narrow classification task.\nSystems not traditionally considered agents, like knowledge-representation systems, are sometimes included in the paradigm by framing them as agents with a goal of, for example, answering questions accurately. Here, the concept of an \"action\" is extended to encompass the \"act\" of providing an answer. As a further extension, mimicry-driven systems can be framed as agents optimizing a \"goal function\" based on how closely the IA mimics the desired behavior. In generative adversarial networks (GANs) of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator tries to maximize a function representing how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.\nWhile symbolic AI systems often use an explicit goal function, the paradigm also applies to neural networks and evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\". Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function, machine learning programmers use reward shaping to initially give the machine rewards for incremental progress. Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\" AlphaZero chess had a simple objective function: +1 point for each win, and -1 point for each loss. A self-driving car's objective function would be more complex. Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" influencing how many descendants each agent is allowed to leave.\nThe mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm. However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware.\n\nAgent function\nAn intelligent agent's behavior can be described mathematically by an agent function. This function determines what the agent does based on what it has seen.\nA percept refers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn).\nThe agent function, often denoted as f, maps the agent's entire history of percepts to an action.\nMathematically, this can be represented as:\n\n  \n    \n      \n        f\n        :\n        \n          P\n          \n            ∗\n          \n        \n        →\n        A\n      \n    \n    {\\displaystyle f:P^{*}\\rightarrow A}\n  \n\nWhere:\n\nP\\* represents the set of all possible percept sequences (the agent's entire perceptual history). The asterisk (*) indicates a sequence of zero or more percepts.\nA represents the set of all possible actions the agent can take.\nf is the agent function that maps a percept sequence to an action.\nIt's crucial to distinguish between the agent function (an abstract mathematical concept) and the agent program (the concrete implementation of that function).\n\nThe agent function is a theoretical description.\nThe agent program is the actual code that runs on the agent. The agent program takes the current percept as input and produces an action as output.\nThe agent function can incorporate a wide range of decision-making approaches, including:\n\nCalculating the utility (desirability) of different actions.\nUsing logical rules and deduction.\nEmploying fuzzy logic.\nOther methods.\n\nClasses of intelligent agents\nRussell and Norvig's classification\nRussell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:\n\nSimple reflex agents\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\".\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\nA home thermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent.\n\nModel-based reflex agents\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is referred to as a model of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\nAn agent may also use models to describe and predict the behaviors of other agents in the environment.\n\nGoal-based agents\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\nChatGPT and the Roomba vacuum are examples of goal-based agents.\n\nUtility-based agents\nGoal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\n\nLearning agents\nLearning lets agents begin in unknown environments and gradually surpass the bounds of their initial knowledge. A key distinction in such agents is the separation between a \"learning element,\" responsible for improving performance, and a \"performance element,\" responsible for choosing external actions.\nThe learning element gathers feedback from a \"critic\" to assess the agent’s performance and decides how the performance element—also called the \"actor\"—can be adjusted to yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions.\nThe final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement.\n\nWeiss's classification\nAccording to Weiss (2013), agents can be categorized into four classes:\n\nLogic-based agents, where decisions about actions are derived through logical deduction.\nReactive agents, where decisions occur through a direct mapping from situation to action.\nBelief–desire–intention agents, where decisions depend on manipulating data structures that represent the agent's beliefs, desires, and intentions.\nLayered architectures, where decision-making takes place across multiple software layers, each of which reasons about the environment at a different level of abstraction.\n\nOther\nIn 2013, Alexander Wissner-Gross published a theory exploring the relationship between Freedom and Intelligence in intelligent agents.\n\nHierarchies of agents\nIntelligent agents can be organized hierarchically into multiple \"sub-agents.\" These sub-agents handle lower-level functions, and together with the main agent, they form a complete system capable of executing complex tasks and achieving challenging goals.\nTypically, an agent is structured by dividing it into sensors and actuators. The perception system gathers input from the environment via the sensors and feeds this information to a central controller, which then issues commands to the actuators. Often, a multilayered hierarchy of controllers is necessary to balance the rapid responses required for low-level tasks with the more deliberative reasoning needed for high-level objectives.\n\nAlternative definitions and uses\n\"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\". Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user. These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\nAccording to Nikola Kasabov in 1998, IA systems should exhibit the following characteristics:\n\nAccommodate new problem solving rules incrementally.\nAdapt online and in real time.\nAre able to analyze themselves in terms of behavior, error and success.\nLearn and improve through interaction with the environment (embodiment).\nLearn quickly from large amounts of data.\nHave memory-based exemplar storage and retrieval capacities.\nHave parameters to represent short- and long-term memory, age, forgetting, etc.\n\nAgentic AI\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\nThey possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs).\nResearchers and commentators have noted that AI agents do not have a standard definition.\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\n\nAutonomous capabilities\nThe Financial Times compared the autonomy of AI agents to the SAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.\n\nMultimodal AI agents\nIn addition to large language models (LLMs), vision language models (VLMs) and multimodal foundation models can be used as the basis for agents. In September 2024, Allen Institute for AI released an open source vision language model, which Wired noted could give AI agents the ability to perform complex computer tasks, including the possibility of automated computer hacking. Nvidia released a framework for developers to use VLMs, LLMs and retrieval-augmented generation for building AI agents that can analyze images and videos, including video search and video summarization. Microsoft released a multimodal agent model - trained on images, video, software user interface interactions, and robotics data - that the company claimed can manipulate software and robots.\n\nApplications\nAs of April 2025, per the Associated Press, there are few real world applications of AI agents. As of June 2025, per Fortune, many companies are primarily experimenting with AI agents.\nA recruiter for the Department of Government Efficiency proposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding from OpenAI and a partnership agreement with Palantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses.\n\nProposed benefits\nProponents argue that AI agents can increase personal and economic productivity, foster greater innovation, and liberate users from monotonous tasks. A Bloomberg opinion piece by Parmy Olson argued that agents are best suited for narrow, repetitive tasks with low risk. Conversely, researchers suggest that agents could be applied to web accessibility for people who have disabilities, and researchers at Hugging Face propose that agents could be used for coordinating resources such as during disaster response. The R&D Advisory Team of the BBC views AI agents as being most useful when their assigned goal is uncertain.\n\nConcerns\nConcerns include potential issues of liability, an increased risk of cybercrime, ethical challenges, as well as problems related to AI safety and AI alignment. Other issues involve data privacy, weakened human oversight, a lack of guaranteed repeatability, reward hacking, algorithmic bias, compounding software errors, lack of explainability of agents' decisions, security vulnerabilities, problems with underemployment, job displacement, and the potential for user manipulation, misinformation or malinformation. They may also complicate legal frameworks and risk assessments, foster hallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods. They have also been criticized for being expensive and having a negative impact on internet traffic, and potentially on the environment due to high energy usage. There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would.\nJournalists have described AI agents as part of a push by Big Tech companies to \"automate everything\". Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually \"join the workforce\". However, in a non-peer-reviewed study, Carnegie Mellon University researchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks. Other researchers had similar findings with Devin AI.\nYoshua Bengio warned at the 2025 World Economic Forum that \"all of the catastrophic scenarios with AGI or superintelligence happen if we have agents\".\nIn March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to develop and deploy AI agents for the purpose of assisting the military with \"operational decision-making\". Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressive foreign policy decisions.\nResearch-focused agents have the risk of consensus bias and coverage bias due to collecting information available on the public Internet. NY Mag unfavorably compared the user workflow of agent-based web browsers to Amazon Alexa, which was \"software talking to software, not humans talking to software pretending to be humans to use software.\"\nAgents have been linked to the dead Internet theory due to their ability to both publish and engage with online content.\nAgents may get stuck in infinite loops.\nSince many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit.\n\nPossible mitigation\nZico Kolter noted the possibility of emergent behavior as a result of interactions between agents, and proposed research in game theory to model the risks of these interactions.\nGuardrails, defined by Business Insider as \"filters, rules, and tools that can be used to identify and remove inaccurate content\" have been suggested to help reduce errors.\nTo address security vulnerabilities related to data access, language models could be redesigned to separate instructions and data, or agentic applications could be required to include guardrails. These ideas were proposed in response to a zero-click exploit that affected Microsoft 365 Copilot.\n\nApplications\nThe concept of agent-based modeling for self-driving cars was discussed as early as 2003.\nHallerbach et al. explored the use of agent-based approaches for developing and validating automated driving systems. Their method involved a digital twin of the vehicle under test and microscopic traffic simulations using independent agents.\nWaymo developed a multi-agent simulation environment called Carcraft, to test algorithms for self-driving cars. This system simulates interactions between human drivers, pedestrians, and automated vehicles. Artificial agents replicate human behavior using real-world data.\nSalesforce's Agentforce is an agentic AI platform that allows for the building of autonomous agents to perform tasks.\nThe Transport Security Administration is integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.\n\nSee also\nReferences\nSources\nDomingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\nRussell, Stuart J.; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach (2nd ed.). Upper Saddle River, New Jersey: Prentice Hall. Chapter 2. ISBN 0-13-790395-2.\nKasabov, N. (1998). \"Introduction: Hybrid intelligent adaptive systems\". International Journal of Intelligent Systems. 13 (6): 453–454. doi:10.1002/(SICI)1098-111X(199806)13:6<453::AID-INT1>3.0.CO;2-K. S2CID 120318478.\nWeiss, G. (2013). Multiagent systems (2nd ed.). Cambridge, MA: MIT Press. ISBN 978-0-262-01889-0.",
        "url": "https://en.wikipedia.org/wiki/Intelligent_agent"
    },
    {
        "title": "Intelligent control",
        "text": "Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms.\n\nOverview\nIntelligent control can be divided into the following major sub-domains:\n\nNeural network control\nMachine learning control\nReinforcement learning\nBayesian control\nFuzzy control\nNeuro-fuzzy control\nExpert Systems\nGenetic control\nNew control techniques are created continuously as new models of intelligent behavior are created and computational methods developed to support them.\n\nNeural network controller\nNeural networks have been used to solve problems in almost all spheres of science and technology. Neural network control basically involves two steps:\n\nSystem identification\nControl\nIt has been shown that a feedforward network with nonlinear, continuous and differentiable activation functions have universal approximation capability. Recurrent networks have also been used for system identification. Given, a set of input-output data pairs, system identification aims to form a mapping among these data pairs. Such a network is supposed to capture the dynamics of a system. For the control part, deep reinforcement learning has shown its ability to control complex systems.\n\nBayesian controllers\nBayesian probability has produced a number of algorithms that are in common use in many advanced control systems, serving as state space  estimators of some variables that are used in the controller.\nThe Kalman filter and the Particle filter are two examples of popular Bayesian control components. The Bayesian approach to controller design often requires an important effort in deriving the so-called system model and measurement model, which are the mathematical relationships linking the state variables to the sensor measurements available in the controlled system. In this respect, it is very closely linked to the\nsystem-theoretic approach to control design.\n\nSee also\nAction selection\nAI effect\nApplications of artificial intelligence\nArtificial intelligence systems integration\nFunction approximation\nHybrid intelligent system\nLists\nList of emerging technologies\nOutline of artificial intelligence\n\nReferences\nAntsaklis, P.J. (1993). Passino, K.M. (ed.). An Introduction to Intelligent and Autonomous Control. Kluwer Academic Publishers. ISBN 0-7923-9267-1. Archived from the original on 10 April 2009.\nLiu, J.; Wang, W.; Golnaraghi, F.; Kubica, E. (2010). \"A Novel Fuzzy Framework for Nonlinear System Control\". Fuzzy Sets and Systems. 161 (21): 2746–2759. doi:10.1016/j.fss.2010.04.009.\n\nFurther reading\nJeffrey T. Spooner, Manfredi Maggiore, Raul Ord onez, and Kevin M. Passino, Stable Adaptive Control and Estimation for Nonlinear Systems: Neural and Fuzzy Approximator Techniques, John Wiley & Sons, NY;\nFarrell, J.A., Polycarpou, M.M. (2006). Adaptive Approximation Based Control: Unifying Neural, Fuzzy and Traditional Adaptive Approximation Approaches. Wiley. ISBN 978-0-471-72788-0.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nSchramm, G. (1998). Intelligent Flight Control - A Fuzzy Logic Approach. TU Delft Press. ISBN 90-901192-4-8.",
        "url": "https://en.wikipedia.org/wiki/Intelligent_control"
    },
    {
        "title": "Intelligent database",
        "text": "Until the 1980s, databases were viewed as computer systems that stored record-oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping.\nThe term was introduced in 1989 by the book Intelligent Databases by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems: high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation.\nIn the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis.\n\nExternal links\nIntelligent Databases, book",
        "url": "https://en.wikipedia.org/wiki/Intelligent_database"
    },
    {
        "title": "Intelligent decision support system",
        "text": "An intelligent decision support system (IDSS) is a decision support system that makes extensive use of artificial intelligence (AI) techniques. Use of AI techniques in management information systems has a long history – indeed terms such as \"Knowledge-based systems\" (KBS) and \"intelligent systems\" have been used since the early 1980s to describe components of management systems, but the term \"Intelligent decision support system\" is thought to originate with Clyde Holsapple and Andrew Whinston in the late 1970s. Examples of specialized intelligent decision support systems include Flexible manufacturing systems (FMS), intelligent marketing decision support systems and medical diagnosis systems.\nIdeally, an intelligent decision support system should behave like a human consultant: supporting decision makers by gathering and analysing evidence, identifying and diagnosing problems, proposing possible courses of action and evaluating such proposed actions.  The aim of the AI techniques embedded in an intelligent decision support system is to enable these tasks to be performed by a computer, while emulating human capabilities as closely as possible.\nMany IDSS implementations are based on expert systems, a well established type of KBS that encode knowledge and emulate the cognitive behaviours of human experts using predicate logic rules, and have been shown to perform better than the original human experts in some circumstances. Expert systems emerged as practical applications in the 1980s based on research in artificial intelligence performed during the late 1960s and early 1970s. They typically combine knowledge of a particular application domain with an inference capability to enable the system to propose decisions or diagnoses. Accuracy and consistency can be comparable to (or even exceed) that of human experts when the decision parameters are well known (e.g. if a common disease is being diagnosed), but performance can be poor when novel or uncertain circumstances arise.\nResearch in AI focused on enabling systems to respond to novelty and uncertainty in more flexible ways is starting to be used in IDSS. For example, intelligent agents that perform complex cognitive tasks without any need for human intervention have been used in a range of decision support applications. Capabilities of  these intelligent agents include knowledge sharing,  machine learning, data mining, and automated inference. A range of AI techniques such as case based reasoning, rough sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions.\nA 2009 research about a multi-artificial system intelligence system named IILS is proposed to automate problem-solving processes within the logistics industry. The system involves integrating intelligence modules based on case-based reasoning, multi-agent systems, fuzzy logic, and artificial neural networks aiming to offer advanced logistics solutions and support in making well-informed, high-quality decisions to address a wide range of customer needs and challenges.\n\nReferences\nFurther reading\nTurban, E., Aronson J., & Liang T.: Decision support systems and Intelligent systems (2004) Pearson\n\nExternal links\nA brief history of DSS",
        "url": "https://en.wikipedia.org/wiki/Intelligent_decision_support_system"
    },
    {
        "title": "Intelligent word recognition",
        "text": "Intelligent word recognition (IWR) is the recognition of unconstrained handwritten words. IWR recognizes entire handwritten words or phrases instead of character-by-character, like its predecessor, optical character recognition (OCR). IWR technology matches handwritten or printed words to a user-defined dictionary, significantly reducing character errors encountered in typical character-based recognition engines.  \nNew technology on the market utilizes IWR, OCR, and ICR together, which opens many doors for the processing of documents, either constrained (hand printed or machine printed) or unconstrained (freeform cursive). IWR also eliminates a large percentage of the manual data entry of handwritten documents that, in the past, could only be keyed by a human, creating an automated workflow.\nWhen cursive handwriting is in play, for each word analyzed, the system breaks down the words into a sequence of graphemes, or subparts of letters. These various curves, shapes and lines make up letters and IWR considers these various shape and groupings in order to calculate a confidence value associated with the word in question.\nIWR is not meant to replace ICR and OCR engines which work well with printed data; however, IWR reduces the number of character errors associated with these engines, and it is ideal for processing real-world documents that contain mostly freeform, hard-to-recognize data, inherently unsuitable for them.\n\nSee also\nAI effect\nHandwriting recognition\nOptical character recognition\nLists\nList of emerging technologies\nOutline of artificial intelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Intelligent_word_recognition"
    },
    {
        "title": "Intrinsic motivation (artificial intelligence)",
        "text": "Intrinsic motivation, in the study of artificial intelligence and robotics, is a mechanism for enabling artificial agents (including robots) to exhibit inherently rewarding behaviours such as exploration and curiosity, grouped under the same term in the study of psychology. Psychologists consider intrinsic motivation in humans to be the drive to perform an activity for inherent satisfaction – just for the fun or challenge of it.\n\nDefinition\nAn intelligent agent is intrinsically motivated to act if the information content alone, or the experience resulting from the action, is the motivating factor.\nInformation content in this context is measured in the information-theoretic sense of quantifying uncertainty. A typical intrinsic motivation is to search for unusual, surprising situations (exploration), in contrast to a typical extrinsic motivation such as the search for food (homeostasis). Extrinsic motivations are typically described in artificial intelligence as task-dependent or goal-directed.\n\nOrigins in psychology\nThe study of intrinsic motivation in psychology and neuroscience began in the 1950s with some psychologists explaining exploration through drives to manipulate and explore, however, this homeostatic view was criticised by White. An alternative explanation from Berlyne in 1960 was the pursuit of an optimal balance between novelty and familiarity. Festinger described the difference between internal and external view of the world as dissonance that organisms are motivated to reduce. A similar view was expressed in the '70s by Kagan as the desire to reduce the incompatibility between cognitive structure and experience. In contrast to the idea of optimal incongruity, Deci and Ryan identified in the mid 80's an intrinsic motivation based on competence and self-determination.\n\nComputational models\nAn influential early computational approach to implement artificial curiosity in the early 1990s by Schmidhuber, has since been developed into a \"Formal theory of creativity, fun, and intrinsic motivation”.\nIntrinsic motivation is often studied in the framework of computational reinforcement learning (introduced by Sutton and Barto), where the rewards that drive agent behaviour are intrinsically derived rather than externally imposed and must be learnt from the environment. Reinforcement learning is agnostic to how the reward is generated - an agent will learn a policy (action strategy) from the distribution of rewards afforded by actions and the environment. Each approach to intrinsic motivation in this scheme is essentially a different way of generating the reward function for the agent.\n\nCuriosity vs. exploration\nIntrinsically motivated artificial agents exhibit behaviour that resembles curiosity or exploration. Exploration in artificial intelligence and robotics has been extensively studied in reinforcement learning models, usually by encouraging the agent to explore as much of the environment as possible, to reduce uncertainty about the dynamics of the environment (learning the transition function) and how best to achieve its goals (learning the reward function). Intrinsic motivation, in contrast, encourages the agent to first explore aspects of the environment that confer more information, to seek out novelty. Recent work unifying state visit count exploration and intrinsic motivation has shown faster learning in a video game setting.\n\nTypes of models\nOudeyer and Kaplan have made a substantial contribution to the study of intrinsic motivation. They define intrinsic motivation based on Berlyne's theory, and divide approaches to the implementation of intrinsic motivation into three categories that broadly follow the roots in psychology: \"knowledge-based models\", \"competence-based models\" and \"morphological models\". Knowledge-based models are further subdivided into \"information-theoretic\" and \"predictive\". Baldassare and Mirolli present a similar typology, differentiating knowledge-based models between prediction-based and novelty-based.\n\nInformation-theoretic intrinsic motivation\nThe quantification of prediction and novelty to drive behaviour is generally enabled through the application of information-theoretic models, where agent state and strategy (policy) over time are represented by probability distributions describing a markov decision process and the cycle of perception and action treated as an information channel. These approaches claim biological feasibility as part of a family of bayesian approaches to brain function. The main criticism and difficulty of these models is the intractability of computing probability distributions over large discrete or continuous state spaces. Nonetheless, a considerable body of work has built up modelling the flow of information around the sensorimotor cycle, leading to de facto reward functions derived from the reduction of uncertainty, including most notably active inference, but also infotaxis, predictive information, and empowerment.\n\nCompetence-based models\nSteels' autotelic principle  is an attempt to formalise flow (psychology).\n\nAchievement, affiliation and power models\nOther intrinsic motives that have been modelled computationally include achievement, affiliation and power motivation. These motives can be implemented as functions of probability of success or incentive. Populations of agents can include individuals with different profiles of achievement, affiliation and power motivation, modelling population diversity and explaining why different individuals take different actions when faced with the same situation.\n\nBeyond achievement, affiliation and power\nA more recent computational theory of intrinsic motivation attempts to explain a large variety of psychological findings based on such motives. Notably this model of intrinsic motivation goes beyond just achievement, affiliation and power, by taking into consideration other important human motives. Empirical data from psychology were computationally simulated and accounted for using this model.\n\nIntrinsically Motivated Learning\nIntrinsically motivated (or curiosity-driven) learning is an emerging research topic in artificial intelligence and developmental robotics that aims to develop agents that can learn general skills or behaviours, that can be deployed to improve performance in extrinsic tasks, such as acquiring resources. Intrinsically motivated learning has been studied as an approach to autonomous lifelong learning in machines and open-ended learning in computer game characters. In particular, when the agent learns a meaningful abstract representation, a notion of distance between two representations can be used to gauge novelty, hence allowing for an efficient exploration of its environment. Despite the impressive success of deep learning in specific domains (e.g. AlphaGo), many in the field (e.g. Gary Marcus) have pointed out that the ability to generalise remains a fundamental challenge in artificial intelligence. Intrinsically motivated learning, although promising in terms of being able to generate goals from the structure of the environment without externally imposed tasks, faces the same challenge of generalisation – how to reuse policies or action sequences, how to compress and represent continuous or complex state spaces and retain and reuse the salient features that have been learnt.\n\nSee also\nReinforcement Learning\nMarkov decision process\nMotivation\nPredictive coding\nPerceptual control theory\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Intrinsic_motivation_(artificial_intelligence)"
    },
    {
        "title": "Is This What We Want?",
        "text": "Is This What We Want? is an album by various artists, released on 25 February 2025 through Virgin Music Group. It consists of silence recorded in recording studios, protesting the use of unlicensed copyrighted work to train artificial intelligence. The track titles form the sentence \"The British Government must not legalise music theft to benefit AI companies\". Profits from the album go toward the UK charity Help Musicians.\n\nBackground\nRapid progress in AI technology, constituting an AI boom, was brought to widespread public attention in the early 2020s by text-to-image models such as DALL-E, Midjourney, and Stable Diffusion, which were able to generate complex images that convincingly resembled human-made artworks. The proliferation of such image generation algorithms coincided with the release of GPT-3 and development of GPT-4, advanced large language models which produce highly convincing text. These transformer-based models designed to create new content from prompts are collectively called generative artificial intelligence, and they require vast sets of training data. This data often consists of text, images, and other media scraped from the web, prompting concerns that the AI products may violate intellectual property rights.\nSuno AI and Udio, two AI startups whose products generate music recordings following user-submitted prompts, were sued in 2024 by Sony Music, Warner Music Group, and Universal Music Group, who alleged that the companies used copyrighted recordings in their training data without authorization.\nIn December 2024, the UK government announced a consultation on copyright and AI, outlining a preferred approach that would see the introduction of a data mining copyright exception with a rights reservation package for rights holders. In the months following the announcement of the consultation, a number of prominent musicians warned of the threat it posed to musicians, including Paul McCartney and Elton John.\n\nArtists\nIs This What We Want? consists of 12 tracks, each uncredited. 1,000 artists are credited as co-writers, including Kate Bush, Damon Albarn, Tori Amos, Annie Lennox, Pet Shop Boys, Billy Ocean, the Clash, Ed O'Brien, Dan Smith, Jamiroquai, Mystery Jets, Hans Zimmer, Imogen Heap, Yusuf/Cat Stevens, Max Richter, the King's Singers, the Sixteen, John Rutter, and James MacMillan. The album was organised by the British composer Ed Newton-Rex, who had previously held a position in Stable Diffusion's parent company Stability AI.\n\nChart performance\nThe album debuted at number 38 on the UK Albums Downloads Chart.\n\nTrack listing\nSee also\nSleepify by Vulfpeck, an entirely silent album\n4'33\", a John Cage composition which instructs the performers to remain silent\nMusic and artificial intelligence\n\nReferences\nExternal links\nOfficial website",
        "url": "https://en.wikipedia.org/wiki/Is_This_What_We_Want%3F"
    },
    {
        "title": "K-line (artificial intelligence)",
        "text": "A K-line, or Knowledge-line, is a mental agent which represents an association of a group of other mental agents found active when a subject solves a certain problem or formulates a new idea. These were first described in Marvin Minsky's essay K-lines: A Theory of Memory, published in 1980 in the journal Cognitive Science:\n\nWhen you \"get an idea,\" or \"solve a problem\" ... you create what we shall call a K-line. ... When that K-line is later \"activated\", it reactivates ... mental agencies, creating a partial mental state \"resembling the original.\"\n\n\"Whenever you 'get a good idea', solve a problem, or have a memorable experience, you activate a K-line to 'represent' it. A K-line is a wirelike structure that attaches itself to whichever mental agents are active when you solve a problem or have a good idea.\nWhen you activate that K-line later, the agents attached to it are aroused, putting you into a 'mental state' much like the one you were in when you solved that problem or got that idea.  This should make it relatively easy for you to solve new, similar problems!\" (1998, p. 82.)\n\nTheoretical implications\nThe concept of K-lines has several theoretical implications for understanding memory and problem-solving in artificial intelligence and cognitive science:\n\nIt suggests that memory is not a static storage of information, but rather a dynamic association of mental agents activated during an experience.\nK-lines provide a mechanism for generalizing from specific experiences to similar problems by reactivating the associated mental agents.\nThe theory implies that memory and problem-solving are distributed processes involving the coordination of multiple mental agents rather than a single central system.\n\nLimitations and criticisms\nWhile influential, the K-line theory has also faced some criticism and limitations:\n\nThe exact nature and implementation of K-lines in the brain or in artificial systems remains unclear and speculative.\nThe theory does not provide a complete account of all aspects of memory and cognition, such as the role of language, emotions, and social interactions.\nSome argue that the theory is too vague or metaphorical to be scientifically testable or to yield specific predictions.\n\nFootnotes\nReferences\nMinsky, Marvin; The Society of Mind ISBN 0-671-65713-5 March 15, 1998.\nMinsky, Marvin; Papert, Seymour; Perceptrons: An Introduction to Computational Geometry ISBN 0-262-63111-3 December 28, 1987.\n\nExternal links\nMinsky's \"K-lines: A Theory of Memory\" Archived 2020-02-15 at the Wayback Machine\nWhy Programming is a Good Medium for Expressing Poorly Understood and Sloppily Formulated Ideas Archived 2005-05-04 at the Wayback Machine",
        "url": "https://en.wikipedia.org/wiki/K-line_(artificial_intelligence)"
    },
    {
        "title": "KAoS",
        "text": "KAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C's Web Ontology Language (OWL) standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government-sponsored projects for distributed host and network management and for the coordination of human-agent-robot teams, including DARPA's CoABS Grid, Cougaar, and Common Object Request Broker Architecture (CORBA) models.\n\nReferences\nExternal links\nKAoS homepage",
        "url": "https://en.wikipedia.org/wiki/KAoS"
    },
    {
        "title": "Knowledge compilation",
        "text": "Knowledge compilation is a family of approaches for addressing the intractability of\na number of artificial intelligence problems.\nA propositional model is compiled in an off-line phase in order to support some queries in polynomial time. Many ways of compiling a propositional model exist.\nDifferent compiled representations have different properties.\nThe three main properties are:\n\nThe compactness of the representation\nThe queries that are supported in polynomial time\nThe transformations of the representations that can be performed in polynomial time\n\nClasses of representations\nSome examples of diagram classes include OBDDs, FBDDs, and non-deterministic OBDDs, as well as MDD.\nSome examples of formula classes include DNF and CNF.\nExamples of circuit classes include NNF, DNNF, d-DNNF, and SDD.\n\nKnowledge compilers\nc2d: supports compilation to d-DNNF\nd4: supports compilation to d-DNNF\nminiC2D: supports compilation to SDD\nKCBox: supports compilation to OBDD, OBDD[AND], and CCDD\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Knowledge_compilation"
    },
    {
        "title": "Knowledge cutoff",
        "text": "In machine learning, a knowledge cutoff (or data cutoff) is the date that marks the end of the data used for a model's training, especially for a large language model (LLM). Any information about events after this date is absent from the model's internal knowledge base. A model's knowledge is static after this date. It cannot access information about later events without a system for real-time data access, such as RAG. While useful for training and tuning LLMs, knowledge cutoffs introduce new limitations like hallucinations, information gaps and temporal bias. To mitigate these issues, methods like RAG and continual learning are used to supplement static knowledge with dynamic or updated information.\n\nOverview\nA model with a fixed knowledge cutoff is unable to provide information on facts or developments that have emerged since that time. Notable model cutoff dates include:\n\nThe GPT-4 model has a knowledge cutoff of September 2021.\nThe GPT-4 Turbo model has a knowledge cutoff of December 2023.\n\nFactors behind knowledge cutoffs\nThe use of static datasets is standard practice. This practice is driven by the high financial and computational cost of retraining large language models.\n\nImplications and limitations\nKnowledge gaps\nKnowledge cutoffs create information gaps. The model lacks any knowledge of events or discoveries that postdate its training data. This can lead to hallucinations, where the model generates plausible but verifiably false statements. Such inaccuracies occur because LLMs are optimized for linguistic plausibility, not factual correctness.\n\nTemporal bias\nTraining data from a specific period reflects the social norms, terminology and ethical views of that era. A model's responses can therefore fail to align with current societal values as time passes, resulting in temporal bias.\n\nEffective vs. reported cutoffs\nResearch indicates a model's functional knowledge may not be uniformly limited by its stated cutoff date. This \"effective\" cutoff often differs for various subjects and is influenced by the distribution of information within the training data itself. Some models can also use integrated search tools to access more recent information, which blurs the line of their inherent knowledge base. For example, modern versions of ChatGPT like GPT-4 can access its search tool and give real time info.\n\nAttempts to overcome knowledge cutoffs\nRetrieval-augmented generation\nRetrieval-augmented generation (RAG) is a common technique used to overcome the limitations of a static knowledge cutoff. In a RAG system, the language model is connected to an external knowledge base or search engine to pull in live data. This architecture allows the model to find current information relevant to a query and incorporate it into its response, often with citations. Grounding a model in external data helps reduce the frequency of hallucinations and improves output accuracy. However, the external knowledge base might be outdated or contain biases, which deeply affects the LLM.\n\nContinual learning\nAnother approach is continual learning, which involves methods like adapters and LoRA.  These fine-tuning techniques permit efficient, incremental updates to a model without the high cost of a full retraining cycle. However, this does not give real-time awareness, as it requires rapid manual tuning to solve the issue, which is not feasible.\n\nControversies and criticisms\nTechniques like RAG have their own limitations. They can perform poorly on complex queries in specialized fields such as law or finance. The output quality is also dependent on the retrieved information; if the external data is biased or inaccurate, the model's response will reflect those flaws.\n\nSee also\nRetrieval-augmented generation\nContinual learning\nLanguage model\nHallucination (artificial intelligence)\nAlgorithmic bias\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Knowledge_cutoff"
    },
    {
        "title": "Knowledge level",
        "text": "In artificial intelligence, knowledge-based agents draw on a pool of logical sentences to infer conclusions about the world.  At the knowledge level, we only need to specify what the agent knows and what its goals are; a logical abstraction separate from details of implementation.\nThis notion of knowledge level was first introduced by Allen Newell in the 1980s, to have a way to rationalize an agent's behavior.  The agent takes actions based on knowledge it possesses, in an attempt to reach specific goals.  It chooses actions according to the principle of rationality.\nBeneath the knowledge level resides the symbol level.  Whereas the knowledge level is world oriented, namely that it concerns the environment in which the agent operates, the symbol level is system oriented, in that it includes the mechanisms the agent has available to operate.  The knowledge level rationalizes the agent's behavior, while the symbol level mechanizes the agent's behavior.\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions.  The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\nSee also\nKnowledge level modeling\nKnowledge relativity\n\nReferences\nT. Menzies. Applications of Abduction: Knowledge-Level Modeling. November 1996.\nA. Newell. The Knowledge Level.  Artificial Intelligence, 18(1):87-127, 1982.",
        "url": "https://en.wikipedia.org/wiki/Knowledge_level"
    },
    {
        "title": "Knowledge-based configuration",
        "text": "Knowledge-based configuration, also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge-based configuration is a major application area for artificial intelligence (AI), and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer.\n\nBackground\nKnowledge-based configuration (of complex products and services) has a long history as an artificial intelligence application area, see, e.g. Informally, configuration can be defined as a \"special case of design activity, where the artifact being configured is assembled from instances of a fixed set of well-defined component types which can be composed conforming to a set of constraints\". Such constraints represent technical restrictions, restrictions related to economic aspects, and conditions related to production processes. The result of a configuration process is a product configuration (concrete configuration), i.e., a list of instances and in some cases also connections between these instances. Examples of such configurations are computers to be delivered or financial service portfolio offers (e.g., a combination of loan and corresponding risk insurance).\n\nTheory and complexity of configuration\nNumerous practical configuration problems can be analyzed by the theoretical framework of Najmann and Stein, an early axiomatic approach that does not presuppose any particular knowledge representation formalism. One important result of this methodology is that typical optimization problems (e.g. finding a cost-minimal configuration) are NP-complete. Thus they require (potentially) excessive computation time, making heuristic configuration algorithms the preferred choice for complex artifacts (products, services).\n\nConfiguration systems\nConfiguration systems, also referred to as configurators or mass customization toolkits, are one of the most successfully applied artificial intelligence technologies. Examples are the automotive industry, the telecommunication industry, the computer industry, and power electric transformers. Starting with rule-based approaches such as R1/XCON, model-based representations of knowledge (in contrast to rule-based representations) have been developed that strictly separate product domain knowledge from problem solving knowledge—examples thereof are the constraint satisfaction problem, the Boolean satisfiability problem, and different answer set programming (ASP) representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions.  This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies and vice versa.\nConfigurators are also often considered as \"open innovation toolkits\", i.e., tools that support customers in the product identification phase. In this context customers are innovators who articulate their requirements leading to new innovative products.  \"Mass Confusion\"  – the overwhelming of customers by a large number of possible solution alternatives (choices) – is a phenomenon that often comes with the application of configuration technologies. This phenomenon motivated the creation of personalized configuration environments taking into account a customer's knowledge and preferences.\n\nConfiguration process\nCore configuration, i.e., guiding the user and checking the consistency of user requirements with the knowledge base, solution presentation and translation of configuration results into bill of materials (BOM) are major tasks to be supported by a configurator. Configuration knowledge bases are often built using proprietary languages.\nIn most cases knowledge bases are developed by knowledge engineers who elicit product, marketing and sales knowledge from domain experts. Configuration knowledge bases are composed of a formal description of the structure of the product and further constraints restricting the possible feature and component combinations.\nConfigurators known as characteristic based product configurators use sets of discrete variables that are either binary or have one of several values, and these variables define every possible product variation.\n\nSoftware and service configuration\nRecently, knowledge-based configuration has been extended to service and software configuration. Modeling software configuration has been based on two main approaches: feature modeling, and component-connectors. Kumbang domain ontology combines the previous approaches building on the tradition of knowledge-based configuration.\n\nSee also\nCharacteristic based product configurator\nConfigurator\nConfigure price quote\nConstraint satisfaction\nFeature model\nMass customization\nOpen innovation\nProduct differentiation\nProduct family engineering\nSoftware product line\n\nReferences\nConference and journal papers\nBooks\nExternal links\n20+ years of International Workshops on Configuration\n\nResearch prototypes\n1991 PLAKON / Project TeX-K\n1999 Konwerk / Project Prokon\n2002 ConIPF\n2003 WeCoTin\n2005 Kumbang tools\n2014 WeeVis (Wiki-based learning environment for simple problems)\n\nJournal special issues on configuration\nAIEDAM 1998 Special Issue on Configuration Design\nIEEE Intelligent Systems Special Issue on Configuration 1998 (vol. 13, No. 4)\nAIEDAM 2003 Special Issue on Configuration\nIEEE Intelligent Systems Special Issue on Configuration 2007\nSpecial Issue on Configuration in the International Journal of Mass Customization 2006\nInternational Journal of Mass Customization Special Issue on Configuration 'Advances in Configuration Systems' 2010 (vol 3, No: 4).\nAIEDAM 2011 Special Issue on Configuration\nAI Communications 2013 Special Issue on Engineering techniques for knowledge bases",
        "url": "https://en.wikipedia.org/wiki/Knowledge-based_configuration"
    },
    {
        "title": "Knowledge-based recommender system",
        "text": "Knowledge-based recommender systems (knowledge based recommenders)  are a specific type of recommender system that are based on explicit knowledge about the item assortment, user preferences, and recommendation criteria (i.e., which item should be recommended in which context). These systems are applied in scenarios where alternative approaches such as collaborative filtering and content-based filtering cannot be applied.\nA major strength of knowledge-based recommender systems is the non-existence of cold start (ramp-up) problems. A corresponding drawback is a potential knowledge acquisition bottleneck triggered by the need to define recommendation knowledge in an explicit fashion.\n\nItem domains\nKnowledge-based recommender systems are well suited to complex domains where items are not purchased very often, such as apartments and cars. Further examples of item domains relevant for knowledge-based recommender systems are financial services, digital cameras, and tourist destinations. Rating-based systems often do not perform well in these domains due to the low number of available ratings.\nAdditionally, in complex item domains, customers want to specify their preferences explicitly (e.g., \"the maximum price of the car is X\") . In this context, the recommender system must take into account constraints: for instance, only those financial services that support the investment period specified by the customer should be recommended. Neither of these aspects are supported by approaches such as collaborative filtering and content-based filtering.\n\nConversational recommendation\nKnowledge-based recommender systems are often conversational, i.e., user requirements and preferences are elicited within the scope of a feedback loop. A major reason for the conversational nature of knowledge-based recommender systems is the complexity of the item domain where it is often impossible to articulate all user preferences at once. Furthermore, user preferences are typically not known exactly at the beginning but are constructed within the scope of a recommendation session.\n\nSearch-based recommendation\nIn a search-based recommender, user feedback is given in terms of answers to questions which restrict the set of relevant items. An example of such a question is \"Which type of lens system do you prefer: fixed or exchangeable lenses?\". On the technical level, search-based recommendation scenarios can be implemented on the basis of constraint-based recommender systems. Constraint-based recommender systems are implemented on the basis of constraint search  or different types of conjunctive query-based approaches.\n\nNavigation-based recommendation\nIn a navigation-based recommender, user feedback is typically provided in terms of \"critiques\"  which specify change requests regarding the item currently recommended to the user. Critiques are then used for the recommendation of the next \"candidate\" item. An example of a critique in the context of a digital camera recommendation scenario is \"I would like to have a camera like this but with a lower price\". This is an example of a \"unit critique\"  which represents a change request on a single item attribute. \"Compound critiques\"  allow the specification of more than one change request at a time. \"Dynamic critiquing\"  also takes into account preceding user critiques (the critiquing history). More recent approaches additionally exploit information stored in user interaction logs to further reduce the interaction effort in terms of the number of needed critiquing cycles.\n\nSee also\nRecommender system\nCollaborative filtering\nCold start\nCase-based reasoning\nConstraint satisfaction\nKnowledge-based configuration\nGuided selling\n\nReferences\nExternal links\nSystems and datasets\nWeeVis Wiki-based Recommendation Environment \nVITA: Knowledge-based Recommender for Financial Services\nMyProductAdvisor\nEntree Dataset",
        "url": "https://en.wikipedia.org/wiki/Knowledge-based_recommender_system"
    },
    {
        "title": "Knowledge-based systems",
        "text": "A knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems. Knowledge-based systems were the focus of early artificial intelligence researchers in the 1980s. The term can refer to a broad range of systems. However, all knowledge-based systems have two defining components: an attempt to represent knowledge explicitly, called a knowledge base, and a reasoning system that allows them to derive new knowledge, known as an inference engine.\n\nComponents\nThe knowledge base contains domain-specific facts and rules  about a problem domain (rather than knowledge implicitly embedded in procedural code, as in a conventional computer program). In addition, the knowledge may be structured by means of a subsumption ontology, frames, conceptual graph, or logical assertions.\nThe inference engine uses general-purpose reasoning methods to infer new knowledge and to solve problems in the problem domain. Most commonly, it employs forward chaining or backward chaining. Other approaches include the use of automated theorem proving, logic programming, blackboard systems, and term rewriting systems such as Constraint Handling Rules (CHR). These more formal approaches are covered in detail in the Wikipedia article on knowledge representation and reasoning.\n\nAspects and development of early systems\nKnowledge-based vs. expert systems\nThe term \"knowledge-based system\" was often used interchangeably with \"expert system\", possibly because almost all of the earliest knowledge-based systems were designed for expert tasks. However, these terms tell us about different aspects of a system:\n\nexpert: describes only the task the system is designed for – its purpose is to aid replace a human expert in a task typically requiring specialised knowledge\nknowledge-based: refers only to the system's architecture –  it represents knowledge explicitly, rather than as procedural code\nToday, virtually all expert systems are knowledge-based, whereas knowledge-based system architecture is used in a wide range of types of system designed for a variety of tasks.\n\nRule-based systems\nThe first knowledge-based systems were primarily rule-based expert systems. These represented facts about the world as simple assertions in a flat database and used domain-specific rules to reason about these assertions, and then to add to them. One of the most famous of these early systems was Mycin, a program for medical diagnosis. \nRepresenting knowledge explicitly via rules had several advantages:\n\nAcquisition and maintenance. Using rules meant that domain experts could often define and maintain the rules themselves rather than via a programmer.\nExplanation. Representing knowledge explicitly allowed systems to reason about how they came to a conclusion and use this information to explain results to users. For example, to follow the chain of inferences that led to a diagnosis and use these facts to explain the diagnosis.\nReasoning. Decoupling the knowledge from the processing of that knowledge enabled general purpose inference engines to be developed. These systems could develop conclusions that followed from a data set that the initial developers may not have even been aware of.\n\nMeta-reasoning\nLater architectures for knowledge-based reasoning, such as the BB1 blackboard architecture (a blackboard system), allowed the reasoning process itself to be affected by new inferences, providing meta-level reasoning. BB1 allowed the problem-solving process itself to be monitored. Different kinds of problem-solving (e.g., top-down, bottom-up, and opportunistic problem-solving) could be selectively mixed based on the current state of problem solving. Essentially, the problem-solver was being used both to solve a domain-level problem along with its own control problem, which could depend on the former. \nOther examples of knowledge-based system architectures supporting meta-level reasoning are MRS and SOAR.\n\nWidening of application\nIn the 1980s and 1990s, in addition to expert systems, other applications of knowledge-based systems included real-time process control, intelligent tutoring systems, and problem-solvers for specific domains such as protein structure analysis, construction-site layout, and computer system fault diagnosis.\n\nAdvances driven by enhanced architecture\nAs knowledge-based systems became more complex, the techniques used to represent the knowledge base became more sophisticated and included logic, term-rewriting systems, conceptual graphs, and frames. \nFrames, for example, are a way representing world knowledge using techniques that can be seen as analogous to object-oriented programming, specifically classes and subclasses, hierarchies and relations between classes, and behavior of objects. With the knowledge base more structured, reasoning could now occur not only by independent rules and logical inference, but also based on interactions within the knowledge base itself. For example, procedures stored as daemons on objects could fire and could replicate the chaining behavior of rules.\n\nAdvances in automated reasoning\nAnother advancement in the 1990s was the development of special purpose automated reasoning systems called classifiers. Rather than statically declare the subsumption relations in a knowledge-base, a classifier allows the developer to simply declare facts about the world and let the classifier deduce the relations. In this way a classifier also can play the role of an inference engine.\nThe most recent advancement of knowledge-based systems was to adopt the technologies, especially a kind of logic called description logic, for the development of systems that use the internet. The internet often has to deal with complex, unstructured data that cannot be relied on to fit a specific data model. The technology of knowledge-based systems, and especially the ability to classify objects on demand, is ideal for such systems. The model for these kinds of knowledge-based internet systems is known as the Semantic Web.\n\nSee also\nKnowledge representation and reasoning\nKnowledge modeling\nKnowledge engine\nInformation retrieval\nReasoning system\nCase-based reasoning\nConceptual graph\nNeural networks\n\nReferences\nFurther reading\nRajendra, Akerkar; Sajja, Priti (2009). Knowledge-Based Systems. Jones & Bartlett Learning. ISBN 9780763776473.",
        "url": "https://en.wikipedia.org/wiki/Knowledge-based_systems"
    },
    {
        "title": "Language/action perspective",
        "text": "The language/action perspective \"takes language as the primary dimension of human cooperative activity,\" applied not just in person-to-person direct (face-to-face) interactions, but also in the design of systems mediated by information and communication technology.  The perspective was developed in the joint authorship of Understanding Computers and Cognition by Fernando Flores and Terry Winograd in 1987.\n\nOverview\nAs part of a reflection published in 2006, Terry Winograd describes the language-action perspective as resting on two key orienting principles:\n\nThe first is its focus on linguistic communication as the basis for understanding what occurs in information systems. Ultimately all information is communication: not an abstract system of bits and bytes but a means by which people interact.\nThe second principle is that language is action. Through their linguistic acts people effect change in the world. In imposing a language-action framework on information technology, we emphasize the action dimension over the more traditional dimension of information content.\nLanguage is action argues that speech isn't simply composed of assertions about the situation: utterances may also create a situation, such as, \"Let's go to the park.\" That utterance may be subject to interpretation but is not verifiable via the state of the world. This principle is closely linked to the ideas from phenomenology. Furthermore, language is not the transmission of information, which simply correspond to the state of the world. By creating a situation, language forms a consensual domain to further encourage more action through language. These speech acts may often take the form of commitment to other actions.\nIn the design of information systems, the perspective is based upon the notion as proposed by Terry Winograd that information technology may be limited in its ability to improve human communication.  \"Expert behavior requires an exquisite sensitivity to context and an ability to know what to commit to. Computing machines, which are purposely designed to process symbols independent of their context, have no hope of becoming experts.\".  That sensitivity to context is thus more in the realm of the human than in that of the artificial.\n\nHistory\nResearch on LAP was done in the Advanced Technology Group (ATG) at Apple Computer in the late 1980s.  Winograd was invited to present the basic concepts in a seminar at Apple in the winter of 1988.  Some Apple ATG researchers, notably Tom Pittard and Brad Hartfield, saw potential for enhancing the user experience of network based computer interactions if LAP was included in the mix of basic design considerations.\nResearch on the application of LAP to business process modelling was done in the System Modelling Research Group, Faculty of Computing, Engineering and Mathematical Sciences, University of the West of England in the early 2000s.\n\nApplications\nInsights from related work have been applied over the past two decades. At the LAP 2004 - Conference, Kalle Lyytinen discussed the academic/theoretic success of LAP. Yet, these LAP successes have not found entry into the wider stream of applications. In a sense, LAP is now peripheral to computer science, however there may be a need for a deeper look at this viewpoint.\nLAP played a role in the second AI Winter. At the time, symbolic AI tried to represent intelligence using a growing knowledge base represented as facts in language. The LAP argued that language was not simply a correspondence with facts but instead depended upon the contextual domain and could not be rigidly defined. Even Winograd's SHRDLU, an exemplar of language understanding in AI, was incapable of broadening its understanding beyond the blocks world.\n\nSee also\nArtificial general intelligence\nDesign & Engineering Methodology for Organizations (DEMO)\nEnd-user computing\nInformation science\n\nReferences\nFurther reading\nTerry Winograd and Fernando Flores (1987) Understanding Computers and Cognition: A New Foundation for Design. Reading, MA: Addison-Wesley.\n\nExternal links\nProject Theory Gravitates towards the Language Action Perspective\nA Language/Action Perspective on the Design of Cooperative Work\nLAP 2005 - Conference\nLanguage/Action Perspective summary in the Association for Information Systems (AIS) theory repository\n\"Conversations for action, commitment management protocol\" details the Flores and Winograd linkages between language and action",
        "url": "https://en.wikipedia.org/wiki/Language/action_perspective"
    },
    {
        "title": "Lifelong Planning A*",
        "text": "LPA* or Lifelong Planning A* is an incremental heuristic search algorithm based on A*. It was first described by Sven Koenig and Maxim Likhachev in 2001.\n\nDescription\nLPA* is an incremental version of A*, which can adapt to changes in the graph without recalculating the entire graph, by updating the g-values (distance from start) from the previous search during the current search to correct them when necessary. Like A*, LPA* uses a heuristic, which is a lower boundary for the cost of the path from a given node to the goal. A heuristic is admissible if it is guaranteed to be non-negative (zero being admissible) and never greater than the cost of the cheapest path to the goal.\n\nPredecessors and successors\nWith the exception of the start and goal node, each node n has predecessors and successors:\n\nAny node from which an edge leads towards n is a predecessor of n.\nAny node to which an edge leads from n is a successor of n.\nIn the following description, these two terms refer only to the immediate predecessors and successors, not to predecessors of predecessors or successors of successors.\n\nStart distance estimates\nLPA* maintains two estimates of the start distance g*(n) for each node:\n\ng(n), the previously calculated g-value (start distance) as in A*\nrhs(n), a lookahead value based on the g-values of the node's predecessors (the minimum of all g(n' ) + d(n' , n), where n' is a predecessor of n and d(x, y) is the cost of the edge connecting x and y)\nFor the start node, the following always holds true:\n\n  \n    \n      \n        r\n        h\n        s\n        (\n        s\n        t\n        a\n        r\n        t\n        )\n        =\n        g\n        (\n        s\n        t\n        a\n        r\n        t\n        )\n        =\n        0\n      \n    \n    {\\displaystyle rhs(start)=g(start)=0}\n  \n\nIf rhs(n) equals g(n), then n is called locally consistent. If all nodes are locally consistent, then a shortest path can be determined as with A*. However, when edge costs change, local consistency needs to be re-established only for those nodes which are relevant for the route.\n\nPriority queue\nWhen a node becomes locally inconsistent (because the cost of its predecessor or the edge linking it to a predecessor has changed), it is placed in a priority queue for re-evaluation. LPA* uses a two-dimensional key:\n\n  \n    \n      \n        k\n        (\n        n\n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    k\n                    \n                      1\n                    \n                  \n                  (\n                  n\n                  )\n                \n              \n              \n                \n                  \n                    k\n                    \n                      2\n                    \n                  \n                  (\n                  n\n                  )\n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  min\n                  (\n                  g\n                  (\n                  n\n                  )\n                  ,\n                  r\n                  h\n                  s\n                  (\n                  n\n                  )\n                  )\n                  +\n                  h\n                  (\n                  n\n                  ,\n                  g\n                  o\n                  a\n                  l\n                  )\n                \n              \n              \n                \n                  min\n                  (\n                  g\n                  (\n                  n\n                  )\n                  ,\n                  r\n                  h\n                  s\n                  (\n                  n\n                  )\n                  )\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle k(n)={\\begin{bmatrix}k_{1}(n)\\\\k_{2}(n)\\\\\\end{bmatrix}}={\\begin{bmatrix}\\min(g(n),rhs(n))+h(n,goal)\\\\\\min(g(n),rhs(n))\\\\\\end{bmatrix}}}\n  \n\nEntries are ordered by k1 (which corresponds directly to the f-values used in A*), then by k2.\n\nNode expansion\nThe top node in the queue is expanded as follows:\n\nIf the rhs-value of a node equals its g-value, the node is locally consistent and is removed from the queue.\nIf the rhs-value of a node is less than its g-value (known as a locally overconsistent node), the g-value is changed to match the rhs-value, making the node locally consistent. The node is then removed from the queue.\nIf the rhs-value of a node is greater than its g-value (known as a locally underconsistent node), the g-value is set to infinity (which makes the node either locally overconsistent or locally consistent). If the node is then locally consistent, it is removed from the queue, else its key is updated.\nSince changing the g-value of a node may also change the rhs-values of its successors (and thus their local consistence), they are evaluated and their queue membership and key is updated if necessary.\nExpansion of nodes continues with the next node at the top of the queue until two conditions are met:\n\nThe goal is locally consistent, and\nThe node at the top of the priority queue has a key which is greater than or equal to the key for the goal.\n\nInitial run\nThe graph is initialized by setting the rhs-value of the start node to 0 and its g-value to infinity. For all other nodes, both the g-value and the rhs-value are assumed to be infinity until assigned otherwise. This initially makes the start node the only locally inconsistent node, and thus the only node in the queue. After that, node expansion begins. The first run of LPA* thus behaves in the same manner as A*, expanding the same nodes in the same order.\n\nCost changes\nWhen the cost of an edge changes, LPA* examines all nodes affected by the change, i.e. all nodes at which one of the changed edges terminates (if an edge can be traversed in both directions and the change affects both directions, both nodes connected by the edge are examined):\n\nThe rhs-values of the nodes are updated.\nNodes which have become locally consistent are removed from the queue.\nNodes which have become locally inconsistent are added to the queue.\nNodes which remain locally inconsistent have their keys updated.\nAfter that, node expansion resumes until the end condition has been reached.\n\nFinding the shortest path\nOnce node expansion has finished (i.e. the exit conditions are met), the shortest path is evaluated. If the cost for the goal equals infinity, there is no finite-cost path from start to goal. Otherwise, the shortest path can be determined by moving backwards:\n\nStart at the goal.\nMove to the predecessor n'  of the current node n for which g(n' ) + d(n' , n) is lowest (if the lowest score is shared by multiple nodes, each is a valid solution and any of them can be chosen arbitrarily).\nRepeat the previous step until you have reached the start.\n\nPseudocode\nThis code assumes a priority queue queue, which supports the following operations:\n\ntopKey() returns the (numerically) lowest priority of any node in the queue (or infinity if the queue is empty)\npop() removes the node with the lowest priority from the queue and returns it\ninsert(node, priority) inserts a node with a given priority into the queue\nremove(node) removes a node from the queue\ncontains(node) returns true if the queue contains the specified node, false if not\n\nProperties\nBeing algorithmically similar to A*, LPA* shares many of its properties.\n\nEach node is expanded (visited) at most twice for each run of LPA*. Locally overconsistent nodes are expanded at most once per LPA* run, thus its initial run (in which every node enters the overconsistent state) has similar performance to A*, which visits each node at most once.\nThe keys of the nodes expanded for each run are monotonically nondecreasing over time, as is the case with A*.\nThe more informed (and thus larger) the heuristics are (while still satisfying the admissibility criteria), the fewer nodes need to be expanded.\nThe priority queue implementation has a significant impact on performance, as in A*. Using a Fibonacci heap can lead to a significant performance increase over less efficient implementations.\nFor an A* implementation which breaks ties between two nodes with equal f-values in favor of the node with the smaller g-value (which is not well-defined in A*), the following statements are also true:\n\nThe order in which locally overconsistent nodes are expanded is identical to A*.\nOf all locally overconsistent nodes, only those whose cost does not exceed that of the goal need to be expanded, as is the case in A*.\nLPA* additionally has the following properties:\n\nWhen edge costs change, LPA* outperforms A* (assuming the latter is run from scratch) as only a fraction of nodes need to be expanded again.\n\nVariants\nD* Lite, a reimplementation of the D* algorithm based on LPA*\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Lifelong_Planning_A*"
    },
    {
        "title": "List of artificial intelligence journals",
        "text": "This is a list of notable peer-reviewed academic journals that publish research in the field of artificial intelligence (AI), including areas such as machine learning, computer vision, natural language processing, robotics, and intelligent systems.\n\nGeneral artificial intelligence\nArtificial Intelligence (journal) – Elsevier\nJournal of Artificial Intelligence Research (JAIR) – AI Access Foundation\nAI Open – Elsevier\nAI Perspectives – Springer Nature\nAdvances in Artificial Intelligence – Hindawi\nArtificial Intelligence Review – Springer\n\nMachine learning\nMachine Learning (journal) – Springer\nJournal of Machine Learning Research (JMLR) – Microtome\nTransactions on Machine Learning Research – Community-run (TMLR)\nPattern Recognition (journal) – Elsevier\nNeural Networks (journal) – Elsevier\nNeural Computation (journal) – MIT Press\n\nDeep learning and neural computation\nIEEE Transactions on Neural Networks and Learning Systems – IEEE\nNature Machine Intelligence – Springer Nature\n\nComputer vision\nInternational Journal of Computer Vision – Springer\nComputer Vision and Image Understanding – Elsevier\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) – IEEE\n\nNatural language processing\nComputational Linguistics (journal) – MIT Press\nJournal of Natural Language Engineering – Cambridge University Press\nNatural Language Processing (journal)\nTransactions of the Association for Computational Linguistics – ACL\n\nRobotics and intelligent systems\nIEEE Transactions on Robotics – IEEE\nAutonomous Robots – Springer\nRobotics and Autonomous Systems – Elsevier\nJournal of Intelligent & Robotic Systems – Springer\n\nInterdisciplinary and ethics in AI\nAI & Society – Springer\nJournal of Responsible Technology – Elsevier\nPhilosophy & Technology – Springer\nMinds and Machines – Springer\n\nSee also\nAssociation for the Advancement of Artificial Intelligence\nLists of academic journals\nList of software programming journals\nList of computer science journals\nList of information systems journals\nList of large language models\nList of artificial intelligence projects\nLists of open-source artificial intelligence software\n\nArtificial intelligence conferences\nAAAI Conference on Artificial Intelligence\nAI Action Summit\nInternational Joint Conference on Artificial Intelligence\nConference on Neural Information Processing Systems\nInternational Conference on Learning Representations\nEuropean Conference on Artificial Intelligence\nNvidia GTC\nAAMAS\nInternational Conference on Machine Learning\n\nExternal links\nDBLP – Computer Science Bibliography\nJournal of Artificial Intelligence Research (JAIR)",
        "url": "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_journals"
    },
    {
        "title": "List of programming languages for artificial intelligence",
        "text": "Historically, some programming languages have been specifically designed for artificial intelligence (AI) applications. Nowadays, many general-purpose programming languages also have libraries that can be used to develop AI applications.\n\nGeneral-purpose languages\nPython is a high-level, general-purpose programming language that is popular in artificial intelligence. It has a simple, flexible and easily readable syntax. Its popularity results in a vast ecosystem of libraries, including for deep learning, such as PyTorch, TensorFlow, Keras, Google JAX. The library NumPy can be used for manipulating arrays, SciPy for scientific and mathematical analysis, Pandas for analyzing table data, Scikit-learn for various machine learning tasks, NLTK and spaCy for natural language processing, OpenCV for computer vision, and Matplotlib for data visualization. Hugging Face's transformers library can manipulate large language models. Jupyter Notebooks can execute cells of Python code, retaining the context between the execution of cells, which usually facilitates interactive data exploration.\nElixir is a high-level functional programming language based on the Erlang VM. Its machine-learning ecosystem includes Nx for computing on CPUs and GPUs, Bumblebee and Axon for serving and training models, Broadway for distributed processing pipelines, Membrane for image and video processing, Livebook for prototyping and publishing notebooks, and Nerves for embedding on devices.\nR is widely used in new-style artificial intelligence, involving statistical computations, numerical analysis, the use of Bayesian inference, neural networks and in general machine learning. In domains like finance, biology, sociology or medicine it is considered one of the main standard languages. It offers several paradigms of programming like vectorial computation, functional programming and object-oriented programming.\nLisp was the first language developed for artificial intelligence. It includes features intended to support programs that could perform general problem solving, such as lists, associations, schemas (frames), dynamic memory allocation, data types, recursion, associative retrieval, functions as arguments, generators (streams), and cooperative multitasking.\nMATLAB is a proprietary numerical computing language developed by MathWorks. MATLAB has many toolboxes specifically for the development of AI including the Statistics and Machine Learning Toolbox and Deep Learning Toolbox. These toolboxes provide APIs for the high-level and low-level implementation and use of many types of machine learning models that can integrate with the rest of the MATLAB ecosystem. These  libraries also have support for  code generation for embedded hardware.\nC++ is a compiled language that can interact with low-level hardware. In the context of AI, it is particularly used for embedded systems and robotics. Libraries such as TensorFlow C++, Caffe or Shogun can be used.\nJavaScript is widely used for web applications and can notably be executed with web browsers. Libraries for AI include TensorFlow.js, Synaptic and Brain.js.\nJulia is a language launched in 2012, which intends to combine ease of use and performance. It is mostly used for numerical analysis, computational science, and machine learning.\nC# can be used to develop high level machine learning models using Microsoft’s .NET suite. ML.NET was developed to aid integration with existing .NET projects, simplifying the process for existing software using the .NET platform.\nSmalltalk has been used extensively for simulations, neural networks, machine learning, and genetic algorithms. It implements a pure and elegant form of object-oriented programming using message passing.\nHaskell is a purely functional programming language. Lazy evaluation and the list and LogicT monads make it easy to express non-deterministic algorithms, which is often the case. Infinite data structures are useful for search trees. The language's features enable a compositional way to express algorithms. Working with graphs is however a bit harder at first because of functional purity.\nWolfram Language includes a wide range of integrated machine learning abilities, from highly automated functions like Predict and Classify to functions based on specific methods and diagnostics. The functions work on many types of data, including numerical, categorical, time series, textual, and image.\nMojo can run some Python programs, and supports programmability of AI hardware. It aims to combine the usability of Python with the performance of low-level programming languages like C++ or Rust.\n\nSpecialized languages\nProlog is a declarative language where programs are expressed in terms of relations, and execution occurs by running queries over these relations. Prolog is particularly useful for symbolic reasoning, database and language parsing applications.\nArtificial Intelligence Markup Language (AIML) is an XML dialect for use with Artificial Linguistic Internet Computer Entity (A.L.I.C.E.)-type chatterbots.\nPlanner is a hybrid between procedural and logical languages. It gives a procedural interpretation to logical sentences where implications are interpreted with pattern-directed inference.\nStanford Research Institute Problem Solver (STRIPS) is a language to express automated planning problem instances. It expresses an initial state, the goal states, and a set of actions. For each action preconditions (what must be established before the action is performed) and postconditions (what is established after the action is performed) are specified.\nPOP-11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the School of Computer Science at the University of Birmingham which hosts the Poplog website, It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP-11's features is that it supports first-class functions.\nCycL is a special-purpose language for Cyc.\n\nSee also\nGlossary of artificial intelligence\nList of constraint programming languages\nList of computer algebra systems\nList of logic programming languages\nList of constructed languages\nFifth-generation programming language\n\nNotes\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/List_of_programming_languages_for_artificial_intelligence"
    },
    {
        "title": "Lists of open-source artificial intelligence software",
        "text": "These are lists of open-source artificial intelligence software packages related to AI projects released under open-source licenses. These include software libraries, frameworks, platforms, and tools used for machine learning, deep learning, natural language processing, computer vision, reinforcement learning, artificial general intelligence, and more.\n\nMachine learning or data mining\nCaffe — Image classification and image segmentation\nScikit-learn — library built on top of NumPy, SciPy, and matplotlib\nShogun — C++ library for large-scale machine learning\nmlpack — C++ header-only machine learning library\nWeka — collection of machine learning algorithms for data mining tasks\nApache Mahout — scalable machine learning library for big data built on Hadoop and Spark\nApache SystemDS — ML system for the end-to-end data science lifecycle\nJubatus — online machine learning and distributed computing framework\nKNIME — modular data pipelining\nOrange (software) — machine learning, data mining, data visualization, and data analysis.\nRapidMiner — predictive analytics\nfastText – Word embeddings developed by Meta AI\nXGBoost — machine learning library for gradient boosting\n\nAutoML platforms\nTPOT – tree-based pipeline optimization tool using genetic programming\nNeural Network Intelligence – Microsoft toolkit for hyperparameter tuning and neural architecture search\nMindsDB – AutoML platform that embeds machine learning into SQL databases and applications\n\nDeep learning frameworks\nTensorFlow – end-to-end open-source platform for machine learning developed by Google Brain\nPyTorch – deep learning framework developed by Meta AI\nKeras – Python library for artificial neural networks and integrated into TensorFlow library\nMXNet – framework that trains and deploys deep neural networks\nCaffe – deep learning framework focused on speed and modularity\nChainer – Python framework on top of NumPy and CuPy\nTheano –  Python library and optimizing compiler for evaluating mathematical expressions and optimized for GPUs\nDeeplearning4j – Java library for the Java virtual machine and deep learning algorithms\nNeuroph – object-oriented artificial neural network framework written in Java\nOpenNN – C++ library which implements Neural networks\nFast Artificial Neural Network (FANN) – C library for feedforward artificial neural networks\n\nCognitive architectures and AGI platforms\nOpenCog – project that aims to build an open source artificial intelligence framework\nSoar – cognitive architecture for decision-making and learning in Intelligent agents\nCLARION – Connectionist Learning with Adaptive Rule Induction On-line, hybrid connectionist/symbolic cognitive architecture.\n\nReinforcement learning frameworks\nKataGo – reinforcement learning agent designed for playing the game of Go\n\nReactive planning\nGOLOG – logic programming language, situation calculus, first-order logical language for reasoning about action and change.\n\nComputer vision and image processing\nAForge.NET – computer vision, artificial intelligence, and robotics library for the .NET framework\nDlib – C++ library for computer vision and image processing\nOpenCV — library of programming functions mainly for real-time computer vision\nTesseract – optical character recognition\n\nNatural language processing (NLP)\nApache OpenNLP\nApertium – rule-based machine translation platform.\nChatScript – natural language engine and dialog management system\nGeneral Architecture for Text Engineering – information extraction\nGensim – topic modeling and document similarity analysis library\nGloVe – unsupervised learning algorithm for obtaining vector representations of words\nMallet – Java \"Machine Learning for Language Toolkit\"\nMontyLingua – libraries and programs for symbolic and statistical NLP for both Python and Java\nMoses – statistical machine translation engine to train statistical models of text from a source language to a target language\nNiuTrans – statistical machine translation\nNLTK – natural Language toolkit for symbolic and statistical NLP\nProbabilistic Action Cores – interpreter for natural-language instructions for robotic applications\nspaCy – Python library\nSpark NLP – text processing library for advanced NLP for Python, Java, and Scala.\nWord2vec – obtaining vector representations of words\n\nSpeech recognition systems\nCMU Sphinx\nDeepSpeech\nWhisper\n\nLarge language models\nDeepSeek — R1 and V3 models\nGPT-J – 6B parameter transformer model developed by EleutherAI\nGPT-1 — OpenAI LLM\nGPT-2 — OpenAI LLM\nXLNet — Google LLM\nBERT — Google LLM\nT5 — Google LLM\n\nTransformer libraries\nHugging Face transformers library – Python library of pretrained transformer models for NLP, computer vision, speech, and more.\nFairseq – Facebook AI Research's sequence-to-sequence learning toolkit for training custom transformer models\nOpenNMT – neural machine translation framework that supports transformer architectures\n\nChat bots\nLAION OpenAssistant\n Mycroft\n\nText to speech\nFestival Speech Synthesis System\nWaveNet\neSpeak\n\nText to image\nFlux\nStable Diffusion\n\nAI hardware and inference acceleration\nOpenVINO – Intel's toolkit for optimizing deep learning models for edge devices\nONNX – Open Neural Network Exchange format for interoperability between AI frameworks\n\nRobotics software\nArduPilot\nCoppeliaSim\nGazebo\nMobile Robot Programming Toolkit\nOpenRTM-aist\nPaparazzi Project\nPlayer Project\nPython Robotics\nRobot Operating System\nTurtleBot\nWebots\n\nSee also\nList of artificial intelligence journals\nList of artificial intelligence projects\nList of free and open-source software packages for artificial intelligence\nOpen-source artificial intelligence\nCommon Crawl – nonprofit that crawls the web and freely provides its archives and datasets to the public under an MIT License\nGoogle Colab – an O-IDE Jupyter notebook environment with free access to GPUs and TPUs for machine learning and deep learning development\n\nExternal links\nopensource.org/ai – Open Source Initiative\nOpen Data Institute - data and AI whitepaper – Open Data Institute\nTop 23 AI Open Source Software Libraries\n15 Top Open Source AI Platforms and Tools [+Tips for Using]\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Lists_of_open-source_artificial_intelligence_software"
    },
    {
        "title": "Machine perception",
        "text": "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.\nMachine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical.\nThe end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing. This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self-awareness, and intentionality.\n\nMachine vision\nComputer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and high-dimensional data from the real world to produce numerical or symbolic information, e.g., in the forms of decisions.  Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment.\nHowever, machines still struggle to interpret visual impute accurately if it is blurry or if the viewpoint at which stimuli are viewed varies often. Computers also struggle to determine the proper nature of some stimulus if overlapped by or seamlessly touching another stimulus. This refers to the Principle of Good Continuation. Machines also struggle to perceive and record stimulus functioning according to the Apparent Movement principle which is a field of research in Gestalt psychology.\n\nMachine hearing\nMachine hearing, also known as machine listening or computer audition is the ability of a computer or machine to take in and process sound data such as speech or music.\nThis area has a wide range of application including music recording and compression, speech synthesis and speech recognition.\nMoreover, this technology allows the machine to replicate the human brain's ability to selectively focus on a specific sound against many other competing sounds and background noise. This ability is called \"auditory scene analysis\". The technology enables the machine to segment several streams occurring at the same time.\nMany commonly used devices such as a smartphones, voice translators and cars make use of some form of machine hearing. Present technology still has challenges in speech segmentation. This  means it is occasionally unable to correctly split words within sentences especially when spoken in an atypical accent.\n\nMachine touch\nMachine touch is an area of machine perception where tactile information is processed by a machine or computer.  Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment. Though this could possibly be done through measuring when and where friction occurs and also the nature and intensity of the friction, machines however still do not have any way of measuring few ordinary physical human experiences including physical pain. For example, scientists have yet to invent a mechanical substitute for the Nociceptors in the body and brain that are responsible for noticing and measuring physical human discomfort and suffering.\n\nMachine olfaction\nScientists are developing computers known as machine olfaction which can recognize and measure smells as well. Airborne chemicals are sensed and classified with a device sometimes known as an electronic nose.\n\nMachine taste\nFuture\nOther than those listed above, some of the future hurdles that the science of machine perception still has to overcome include, but are not limited to:\n- Embodied cognition - The theory that cognition is a full body experience, and therefore can only exist, and therefore be measure and analyzed, in fullness if all required human abilities and processes are working together through a mutually aware and supportive systems network.\n- The Moravec's paradox (see the link)\n- The Principle of similarity - The ability young children develop to determine what family a newly introduced stimulus falls under even when the said stimulus is different from the members with which the child usually associates said family with. (An example could be a child figuring that a chihuahua is a dog and house pet rather than vermin.)\n- The Unconscious inference: The natural human behavior of determining if a new stimulus is dangerous or not, what it is, and then how to relate to it without ever requiring any new conscious effort.\n- The innate human ability to follow the likelihood principle in order to learn from circumstances and others over time.\n- The recognition-by-components theory - being able to mentally analyze and break even complicated mechanisms into manageable parts with which to interact with. For example: A person seeing both the cup and the handle parts that make up a mug full of hot cocoa, in order to use the handle to hold the mug so as to avoid being burned.\n- The free energy principle - determining long before hand how much energy one can safely delegate to being aware of things outside one's self without the loss of the needed energy one requires for sustaining their life and function satisfactorily. This allows one to become both optimally aware of the world around them self without depleting their energy so much that they experience damaging stress, decision fatigue, and/or exhaustion.\n\nSee also\nRobotic sensing\nSensors\nSLAM\nHistory of artificial intelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Machine_perception"
    },
    {
        "title": "Means–ends analysis",
        "text": "Means–ends analysis (MEA) is a problem solving technique used commonly in artificial intelligence (AI) for limiting search in AI  programs.\nIt is also a technique used at least since the 1950s as a creativity tool, most frequently mentioned in engineering books on design methods. MEA is also related to means–ends chain approach used commonly in consumer behavior analysis. It is also a way to clarify one's thoughts when embarking on a mathematical proof.\n\nProblem-solving as search\nAn important aspect of intelligent behavior as studied in AI is goal-based problem solving, a framework in which the solution to a problem can be described by finding a sequence of actions that lead to a desirable goal.  A goal-seeking system is supposed to be connected to its outside environment by sensory channels through which it receives information about the environment and motor channels through which it acts on the environment.  (The term \"afferent\" is used to describe \"inward\" sensory flows, and \"efferent\" is used to describe \"outward\" motor commands.) In addition, the system has some means of storing in a memory information about the state of the environment (afferent information) and information about actions (efferent information).  Ability to attain goals depends on building up associations, simple or complex, between particular changes in states and particular actions that will bring these changes about.  Search is the process of discovery and assembly of sequences of actions that will lead from a given state to a desired state. While this strategy may be appropriate for machine learning and problem solving, it is not always suggested for humans (e.g. cognitive load theory and its implications).\n\nHow means–ends analysis works\nThe MEA technique is a strategy to control search in problem-solving.  Given a current state and a goal state, an action is chosen which will reduce the difference between the two.  The action is performed on the current state to produce a new state, and the process is recursively applied to this new state and the goal state.\nNote that, in order for MEA to be effective, the goal-seeking system must have a means of associating to any kind of detectable difference those actions that are relevant to reducing that difference.  It must also have means for detecting the progress it is making (the changes in the differences between the actual and the desired state), as some attempted sequences of actions may fail and, hence, some alternate sequences may be tried.\nWhen knowledge is available concerning the importance of differences, the most important difference is selected first to further improve the average performance of MEA over other brute-force search strategies. However, even without the ordering of differences according to importance, MEA improves over other search heuristics (again in the average case) by focusing the problem solving on the actual differences between the current state and that of the goal.\n\nSome AI systems using MEA\nThe MEA technique as a problem-solving strategy was first introduced in 1961 by Allen Newell and Herbert A. Simon in their computer problem-solving program General Problem Solver (GPS).  In that implementation, the correspondence between differences and actions, also called operators, is provided a priori as knowledge in the system. (In GPS this knowledge was in the form of a table of connections.)\nWhen the action and side-effects of applying an operator are penetrable the search may select the relevant operators by inspection of the operators and do without a table of connections.  This latter case, of which the canonical example is STRIPS, an automated planning computer program, allows task-independent correlation of differences to the operators which reduce them.\nProdigy, a problem solver developed in a larger learning-assisted automated planning project started at Carnegie Mellon University by Jaime Carbonell, Steven Minton and Craig Knoblock, is another system that used MEA.\nProfessor Morten Lind, at Technical University of Denmark has developed a tool called Multilevel Flow Modeling (MFM). It performs means–ends based diagnostic reasoning for industrial control and automation systems.\n\nSee also\nCausal layered analysis\nKnowledge representation\nAutomated reasoning\nIntelligent control\nCognitive load\nMathematical proof\nFutures techniques\nPolytely\nGap analysis\nHill climbing\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Means%E2%80%93ends_analysis"
    },
    {
        "title": "Mechanistic interpretability",
        "text": "Mechanistic interpretability (often shortened to mech interp or MI) is a subfield of research within explainable artificial intelligence which seeks to fully reverse-engineer neural networks (akin to reverse-engineering a compiled binary of a computer program), with the ultimate goal of understanding the mechanisms underlying their computations. The field is particularly focused on large language models.\n\nHistory\nChris Olah is generally credited with coining the term \"mechanistic interpretability\" and spearheading early development of the field. In the 2018 paper The Building Blocks of Interpretability, Olah (then at Google Brain) and his colleagues combined existing interpretability techniques, including feature visualization, dimensionality reduction, and attribution with human-computer interface methods to explore features represented by the neurons in the vision model, Inception v1. In the March 2020 paper Zoom In: An Introduction to Circuits,  Olah and the OpenAI Clarity team described \"an approach inspired by neuroscience or cellular biology\", hypothesizing that features, like individual cells, are the basis of computation for neural networks and connect to form circuits, which can be understood as \"sub-graphs in a network\". In this paper, the authors described their line of work as understanding the \"mechanistic implementations of neurons in terms of their weights\".\nIn 2021, Chris Olah co-founded the company Anthropic and established its Interpretability team, which publishes their results on the Transformer Circuits Thread. In December 2021, the team published A Mathematical Framework for Transformer Circuits, reverse-engineering a toy transformer with one and two attention layers. Notably, they discovered the complete algorithm of induction circuits, responsible for in-context learning of repeated token sequences. The team further elaborated this result in the March 2022 paper In-context Learning and Induction Heads. \nNotable results in mechanistic interpretability from 2022 include the theory of superposition wherein a model represents more features than there are directions in its representation space; a mechanistic explanation for grokking, the phenomenon where test-set loss begins to decay only after a delay relative to training-set loss; and the introduction of sparse autoencoders, a sparse dictionary learning method to extract interpretable features from LLMs.\nMechanistic interpretability has garnered significant interest, talent, and funding in the AI safety community. In 2021, Open Philanthropy called for proposals that advanced \"mechanistic understanding of neural networks\" alongside other projects aimed to reduce risks from advanced AI systems. The interpretability topic prompt in the request for proposal was written by Chris Olah. The ML Alignment & Theory Scholars (MATS) program, a research seminar focused on AI alignment, has historically supported numerous projects in mechanistic interpretability. In its summer 2023 cohort, for example, 20% of the research projects were on mechanistic interpretability.\nMany organizations and research groups work on mechanistic interpretability, often with the stated goal of improving AI safety. Max Tegmark runs the Tegmark AI Safety Group at MIT, which focuses on mechanistic interpretability. In February 2023, Neel Nanda started the mechanistic interpretability team at Google DeepMind. Apollo Research, an AI evals organization with a focus on interpretability research, was founded in May 2023. EleutherAI has published multiple papers on interpretability. Goodfire, an AI interpretability startup, was founded in 2024.\nMechanistic interpretability has greatly expanded its scope, practitioners, and attention in the ML community in recent years. In July 2024, the first ICML Mechanistic Interpretability Workshop was held, aiming to bring together \"separate threads of work in industry and academia\". In November 2024, Chris Olah discussed mechanistic interpretability on the Lex Fridman podcast as part of the Anthropic team.\n\nDefinition\nThe term mechanistic interpretability designates both a class of technical methods and a research community.\nChris Olah is usually credited with coining the term \"mechanistic interpretability\". His motivation was the differentiate this nascent approach to interpretability from established saliency map-based approaches which at the time dominated computer vision.\nIn-field explanations of the goal of mechanistic interpretability make an analogy to reverse-engineering computer programs, with the argument being that rather than being arbitrary functions, neural networks represent are composed of independent reverse-engineerable mechanisms that are compressed into the weights.\n\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nOne emerging approach for understanding the internals of neural networks is mechanistic interpretability: reverse engineering the algorithms implemented by neural networks into human-understandable mechanisms, often by examining the weights and activations of neural networks to identify circuits [Cammarata et al., 2020, Elhage et al., 2021] that implement particular behaviors.\nMechanistic interpretability's early development was rooted in the AI safety community, though the term is increasingly adopted by academia at large. In “Mechanistic?”, Saphra and Wiegreffe identify four senses of “mechanistic interpretability”:\n\nNarrow technical definition: A technical approach to understanding neural networks through their causal mechanisms.\nBroad technical definition: Any research that describes the internals of a model, including its activations or weights.\nNarrow cultural definition: Any research originating from the MI community.\nBroad cultural definition: Any research in the field of AI—especially LM—interpretability.\nAs the scope and popular recognition of mechanistic interpretability increase, many have begun to recognize that other communities such as natural language processing researchers have pursued similar objectives in their work.\n\nKey concepts\nLinear representation hypothesis\nThe linear representation hypothesis (LRH) posits that high-level concepts are represented as linear representations in neural network activation space. This is an assumption that has been supported by increasing empirical evidence, beginning with early work on word embeddings as well as more recent research in mechanistic interpretability.\nFormalisation of this assumption varies in the literature. Olah and Jermyn allow for higher-rank (i.e. not necessarily rank-1 as in prior work) linear representations and propose two key properties of such representations: (i) composition of features is represented by addition, and (ii) the intensity of a concept is represented by its magnitude.\nCounterexamples to the LRH even as formalised above have been found, suggesting that it only holds for some features in some models. For example, the semantics of feature directions are both empirically and theoretically not scale-invariant in non-linear neural networks, lending support to an affine (not directional) view of features via the polytope lens. A clear manifestation of this are \"onion representations\" in some RNNs trained on a sequence copying task, where the semantics of a feature varies with its scale.\n\nSuperposition\nSuperposition is the phenomenon where many unrelated features are “packed’’ into the same subspace or even into single neurons, making a network highly over-complete yet still linearly decodable after nonlinear filtering. Recent formal analysis links the amount of polysemanticity to feature ‘‘capacity’’ and input sparsity, predicting when neurons become monosemantic or remain polysemantic.\n\nMethods\nProbing\nProbing involves training a linear classifier on model activations to test whether a feature is linearly decodable at a given layer or subset of neurons. Generally, a linear probe is trained on a labelled dataset encoding the desired feature. While linear probes are popular amongst mechanistic interpretability researchers, its introduction dates back to 2016 and it has been widely used in the NLP community.\nNanda, Lee & Wattenberg (2023) showed that world-model features such as in-context truth values emerge as linearly decodable directions early in training, strengthening the case for linear probes as faithful monitors of internal state.\n\nDifference-in-means\nDifference-in-means, or diff-in-means constructs a steering vector by subtracting the mean activation for one class of examples from the mean for another. Unlike learned probes, diff-in-means has no trainable parameters and often generalises better out-of-distribution. Diff-in-means has been used to isolate model representation for refusal/compliance, true/false, and sentiment.\n\nSteering\nSteering adds or subtracts a direction (often obtained via probing, diff-in-means, or K-means) from the residual stream to causally change model behaviour.\n\nAttribution\nCausal interventions\nWhile methods like probing allow for correlational understanding of model-internal components and representations, true reverse-engineering requires understanding the causal role of model internals. By treating neural networks as causal models, causal interventions (formalised in the do-calculus of Judea Pearl) enable answering this question.\nBroadly, given a model \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n, a clean input \n  \n    \n      \n        \n          x\n          \n            clean\n          \n        \n      \n    \n    {\\displaystyle x_{\\text{clean}}}\n  \n, a corrupted input \n  \n    \n      \n        \n          x\n          \n            corrupt\n          \n        \n      \n    \n    {\\displaystyle x_{\\text{corrupt}}}\n  \n, and a subcomponent of interest \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n, a causal intervention replaces the corrupted output representation of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n with that of the clean input, resulting in an intervened output \n  \n    \n      \n        \n          \n            \n              M\n            \n          \n          \n            f\n            ←\n            f\n            (\n            \n              x\n              \n                clean\n              \n            \n            )\n          \n        \n        (\n        \n          x\n          \n            corrupt\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}^{f\\gets f(x_{\\text{clean}})}(x_{\\text{corrupt}})}\n  \n. If the subcomponent is causally relevant to the computation of \n  \n    \n      \n        \n          \n            M\n          \n        \n        (\n        \n          x\n          \n            clean\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}(x_{\\text{clean}})}\n  \n, then this intervention should restore the clean output. A variety of dataset setups, evaluation metrics, and model subcomponent granularities have been studied using this approach.\nSeveral causal intervention techniques have been proposed for mechanistic interpretability, including causal mediation analysis, interchange intervention (as part of the formal theory of causal abstraction), causal scrubbing, and activation patching. These all implement the same broad idea described above.\nCausal interventions may also be applied to edges in the computational graph. This enables testing how components communicate information. Causal scrubbing, later formalised as path patching, applies treeification to the computational graph, resulting in a copy of each component for each of its paths to the output, and performs standard causal interventions only on some copies of the component of interest.\n\nGradient-based attribution\nCausal intervention methods are expensive, requiring \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n forward passes for performing attribution on \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n model components given a single input. Gradient-based methods propose using a single backward pass to compute approximations of the patching effect for every model component simultaneously. Methods in this vein include attribution patching, and AtP*, along with standard interpretability techniques such as integrated gradients.\nAttribution patching uses a locally linear approximation of the gradient of a subcomponent representation to estimate its downstream patching effect. Formally, given a metric \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n, it computes the downstream effect as:\n\n  \n    \n      \n        (\n        f\n        (\n        \n          x\n          \n            corrupt\n          \n        \n        )\n        −\n        f\n        (\n        \n          x\n          \n            clean\n          \n        \n        )\n        \n          )\n          \n            ⊤\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  L\n                \n              \n              (\n              \n                \n                  M\n                \n              \n              (\n              \n                x\n                \n                  clean\n                \n              \n              )\n              )\n            \n            \n              ∂\n              f\n            \n          \n        \n        \n          \n            \n              |\n            \n          \n          \n            f\n            =\n            f\n            (\n            \n              x\n              \n                clean\n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle (f(x_{\\text{corrupt}})-f(x_{\\text{clean}}))^{\\top }{\\frac {\\partial {\\mathcal {L}}({\\mathcal {M}}(x_{\\text{clean}}))}{\\partial f}}{\\biggr |}_{f=f(x_{\\text{clean}})}}\n\nSparse decomposition\nA major goal of mechanistic interpretability is to decompose pretrained neural networks into interpretable components. Existing architectural pieces of neural networks (e.g. attention heads, individual neurons) have been found to be uninterpretable, exhibiting \"polysemanticity\", i.e. implementing multiple behaviours at once. Sparse decomposition methods seek to discover the interpretable subcomponents of a model in a self-supervised fashion, building on intuitions from the linear representation hypothesis and superposition.\n\nSparse dictionary learning (SDL)\nSparse autoencoders (SAEs)\nSparse autoencoders (SAEs) for mechanistic interpretability were proposed in order to address the superposition problem by decomposing the feature space into an overcomplete basis (i.e. with more features than dimensions) of monosemantic concepts. The underlying intuition is that features can only be manipulable under superposition if they are sparsely activated (otherwise, interference between features would be too high).\nGiven a vector \n  \n    \n      \n        \n          x\n        \n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{n}}\n  \n representing an activation collected from some model component (in a transformer, usually the MLP inner activation or the residual stream), the sparse autoencoder computes the following:\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              ^\n            \n          \n        \n        =\n        \n          \n            W\n          \n          \n            \n              d\n              e\n              c\n            \n          \n          \n            ⊤\n          \n        \n        (\n        \n          R\n          e\n          L\n          U\n        \n        (\n        \n          \n            W\n          \n          \n            \n              e\n              n\n              c\n            \n          \n        \n        \n          x\n        \n        +\n        \n          \n            b\n          \n          \n            \n              enc\n            \n          \n        \n        )\n        )\n        +\n        \n          \n            b\n          \n          \n            \n              dec\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {x} }}=\\mathbf {W} _{\\mathrm {dec} }^{\\top }(\\mathrm {ReLU} (\\mathbf {W} _{\\mathrm {enc} }\\mathbf {x} +\\mathbf {b} _{\\textrm {enc}}))+\\mathbf {b} _{\\textrm {dec}}}\n  \n\nHere, \n  \n    \n      \n        \n          \n            W\n          \n          \n            \n              e\n              n\n              c\n            \n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            z\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {W} _{\\mathrm {enc} }\\in \\mathbb {R} ^{z\\times n}}\n  \n projects the activation into a \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  \n-dimensional latent space, applies the ReLU nonlinearity, and finally the decoder \n  \n    \n      \n        \n          \n            W\n          \n          \n            \n              d\n              e\n              c\n            \n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            z\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {W} _{\\mathrm {dec} }\\in \\mathbb {R} ^{z\\times n}}\n  \n aims to reconstruct the original activation from this latent representation. The bias terms are \n  \n    \n      \n        \n          \n            b\n          \n          \n            \n              enc\n            \n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            z\n          \n        \n        ,\n        \n          \n            b\n          \n          \n            \n              dec\n            \n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {b} _{\\textrm {enc}}\\in \\mathbb {R} ^{z},\\mathbf {b} _{\\textrm {dec}}\\in \\mathbb {R} ^{n}}\n  \n; the latter is omitted in some formulations. The encoder and decoder matrices may also be tied.\nGiven a dataset of activations \n  \n    \n      \n        \n          X\n        \n        =\n        {\n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            x\n          \n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle \\mathbf {X} =\\{\\mathbf {x} _{1},\\ldots ,\\mathbf {x} _{n}\\}}\n  \n, the SAE is trained with gradient descent to minimise the following loss function:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        )\n        =\n        \n          |\n        \n        \n          |\n        \n        \n          x\n        \n        −\n        \n          \n            \n              \n                x\n              \n              ^\n            \n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n          \n            2\n          \n        \n        +\n        α\n        \n          |\n        \n        \n          |\n        \n        \n          z\n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} )=||\\mathbf {x} -{\\hat {\\mathbf {x} }}||_{2}^{2}+\\alpha ||\\mathbf {z} ||_{1}}\n  \n\nwhere the first term is the reconstruction loss (i.e. the standard autoencoding objective) and the second is a sparsity loss on the latent representation \n  \n    \n      \n        \n          z\n        \n        =\n        \n          R\n          e\n          L\n          U\n        \n        (\n        \n          \n            W\n          \n          \n            \n              e\n              n\n              c\n            \n          \n        \n        \n          x\n        \n        +\n        \n          \n            b\n          \n          \n            \n              enc\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {z} =\\mathrm {ReLU} (\\mathbf {W} _{\\mathrm {enc} }\\mathbf {x} +\\mathbf {b} _{\\textrm {enc}})}\n  \n which aims to minimise its \n  \n    \n      \n        \n          ℓ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\ell ^{1}}\n  \n-norm.\n\nAlternative designs\nSeveral works motivate alternative nonlinearities to ReLU based on improved downstream performance or training stability.\n\nTopK, which means selecting the top-\n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n activating latents and zeroing out the rest, which also allows for dropping the sparsity loss entirely.\nJumpReLU, defined as \n  \n    \n      \n        \n          J\n          u\n          m\n          p\n          R\n          e\n          L\n          U\n        \n        (\n        z\n        )\n        =\n        z\n        H\n        (\n        z\n        −\n        θ\n        )\n      \n    \n    {\\displaystyle \\mathrm {JumpReLU} (z)=zH(z-\\theta )}\n  \n where \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n is the Heaviside step function. Anthropic adopted this modification for training SAEs and crosscoders.\nOther architectural and loss function modifications include:\n\nGated SAE, which applies elementwise multiplicative gating to the encoder akin to gated linear units.\nTanh penalty on the loss, to prevent feature shrinkage.\n\nEvaluation\nThe core metrics for evaluating SAEs are sparsity, measured by the \n  \n    \n      \n        \n          ℓ\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\ell _{0}}\n  \n-norm of the latent representations over the dataset, and fidelity, which may be the MSE reconstruction error as in the loss function or a downstream metric when substituting the SAE output into the model, such as loss recovered or KL-divergence from the original model behaviour.\nSAE latents are usually labelled using an autointerpretability pipeline. Most such pipelines feed highly-activating (i.e. having activation \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  \n resulting in large \n  \n    \n      \n        \n          \n            z\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {z} _{i}}\n  \n for feature \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, repeating over all features) dataset exemplars to a large language model, which generates a natural-language description based on the contexts the latent is active.\nEarly works directly adapt Bills (2023)'s neuron-labelling and evaluation pipeline and report higher interpretability scores than alternative methods (the standard basis, PCA, etc.). However, this leads to misleading score, since explanations achieve high recall but usually low precision, leading to more nuanced evaluation metrics being introduced in later works: neuron-to-graph explanations (or other approaches) reporting both precision and recall, and intervention-based metrics that measure the downstream effect of manipulating a latent feature.\n\nTranscoders\nTranscoders are formulated identically to SAEs, with the caveat that they seek to approximate the input-output behaviour of a model component (usually the MLP). This is useful for measuring how latent features in different layers of the model affect each other in an input-invariant manner (i.e. by directly comparing encoder and decoder weights). A transcoder thus computes the following:\n\n  \n    \n      \n        \n          \n            \n              \n                y\n              \n              ^\n            \n          \n        \n        =\n        \n          \n            W\n          \n          \n            \n              d\n              e\n              c\n            \n          \n          \n            ⊤\n          \n        \n        (\n        \n          R\n          e\n          L\n          U\n        \n        (\n        \n          \n            W\n          \n          \n            \n              e\n              n\n              c\n            \n          \n        \n        \n          x\n        \n        +\n        \n          \n            b\n          \n          \n            \n              enc\n            \n          \n        \n        )\n        )\n        +\n        \n          \n            b\n          \n          \n            \n              dec\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {y} }}=\\mathbf {W} _{\\mathrm {dec} }^{\\top }(\\mathrm {ReLU} (\\mathbf {W} _{\\mathrm {enc} }\\mathbf {x} +\\mathbf {b} _{\\textrm {enc}}))+\\mathbf {b} _{\\textrm {dec}}}\n  \n\nand is trained to minimise the loss:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        )\n        =\n        \n          |\n        \n        \n          |\n        \n        \n          M\n          L\n          P\n        \n        (\n        \n          x\n        \n        )\n        −\n        \n          \n            \n              \n                y\n              \n              ^\n            \n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n          \n            2\n          \n        \n        +\n        α\n        \n          |\n        \n        \n          |\n        \n        \n          z\n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} )=||\\mathrm {MLP} (\\mathbf {x} )-{\\hat {\\mathbf {y} }}||_{2}^{2}+\\alpha ||\\mathbf {z} ||_{1}}\n  \n\nWhen ignoring or holding attention components constant (which may obscure some information), transcoders trained on different layers of a model can then be used to conduct circuit analysis without having to process individual inputs and collect latent activations, unlike SAEs.\nTranscoders generally outperform SAEs, achieving lower loss and better automated interpretability scores.\n\nCrosscoders and cross-layer transcoders\nA disadvantage of single-layer SAEs and transcoders is that they produce duplicate features when trained on multiple layers, if those features persist throughout the residual stream. This complicates understanding layer-to-layer feature propagation and also wastes latent parameters. Crosscoders were introduced to enable cross-layer representation of features, which minimises these issues. They outperform SAEs given the same feature budget but are worse on equal FLOPs budget.\nA crosscoder computes the cross-layer latent representation \n  \n    \n      \n        \n          z\n        \n      \n    \n    {\\displaystyle \\mathbf {z} }\n  \n using a set of layer-wise activations \n  \n    \n      \n        {\n        \n          \n            x\n          \n          \n            (\n            \n              l\n              \n                1\n              \n            \n            )\n          \n        \n        ,\n        …\n        ,\n        \n          \n            x\n          \n          \n            (\n            \n              l\n              \n                n\n              \n            \n            )\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathbf {x} ^{(l_{1})},\\ldots ,\\mathbf {x} ^{(l_{n})}\\}}\n  \n over layers \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n obatained from some input as follows:\n\n  \n    \n      \n        \n          z\n        \n        =\n        \n          R\n          e\n          L\n          U\n        \n        \n          (\n          \n            \n              ∑\n              \n                l\n                ∈\n                L\n              \n            \n            \n              \n                \n                  W\n                \n                \n                  \n                    e\n                    n\n                    c\n                  \n                \n                \n                  (\n                  l\n                  )\n                \n              \n              \n                \n                  x\n                \n                \n                  (\n                  l\n                  )\n                \n              \n              +\n              \n                \n                  b\n                \n                \n                  \n                    e\n                    n\n                    c\n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {z} =\\mathrm {ReLU} \\left(\\sum _{l\\in L}{\\mathbf {W} _{\\mathrm {enc} }^{(l)}\\mathbf {x} ^{(l)}+\\mathbf {b} _{\\mathrm {enc} }}\\right)}\n  \n\nThe reconstruction is done independently for each layer using this cross-layer representation:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                \n                ^\n              \n            \n          \n          \n            (\n            l\n            )\n          \n        \n        =\n        \n          \n            W\n          \n          \n            \n              d\n              e\n              c\n            \n          \n          \n            (\n            l\n            )\n          \n        \n        \n          z\n        \n        +\n        \n          \n            b\n          \n          \n            \n              d\n              e\n              c\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {x} }}^{(l)}=\\mathbf {W} _{\\mathrm {dec} }^{(l)}\\mathbf {z} +\\mathbf {b} _{\\mathrm {dec} }}\n  \n\nAlternatively, the target may be layer-wise component outputs \n  \n    \n      \n        \n          \n            \n              \n                \n                  y\n                \n                ^\n              \n            \n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\mathbf {y} }}^{(l)}}\n  \n if using the transcoder objective. The model is then trained to minimise a loss:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        {\n        \n          \n            x\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        …\n        ,\n        \n          \n            x\n          \n          \n            (\n            n\n            )\n          \n        \n        }\n        )\n        =\n        \n          ∑\n          \n            l\n            ∈\n            L\n          \n        \n        \n          \n            |\n          \n          \n            |\n          \n          \n            \n              \n                \n                  \n                    x\n                  \n                  ^\n                \n              \n            \n            \n              (\n              l\n              )\n            \n          \n          −\n          \n            \n              x\n            \n            \n              (\n              l\n              )\n            \n          \n          \n            |\n          \n          \n            \n              |\n            \n            \n              2\n            \n            \n              2\n            \n          \n          +\n          \n            ∑\n            \n              l\n              ∈\n              L\n            \n          \n          \n            ∑\n            \n              i\n            \n          \n          \n            \n              \n                z\n              \n              \n                i\n              \n            \n            \n              |\n            \n            \n              |\n            \n            \n              \n                W\n              \n              \n                \n                  d\n                  e\n                  c\n                \n                ,\n                i\n              \n              \n                (\n                l\n                )\n              \n            \n            \n              |\n            \n            \n              \n                |\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\{\\mathbf {x} ^{(1)},\\ldots ,\\mathbf {x} ^{(n)}\\})=\\sum _{l\\in L}{||{\\hat {\\mathbf {x} }}^{(l)}-\\mathbf {x} ^{(l)}||_{2}^{2}+\\sum _{l\\in L}\\sum _{i}{\\mathbf {z} _{i}||\\mathbf {W} _{\\mathrm {dec} ,i}^{(l)}||_{2}}}}\n  \n\nNote that the regularisation term uses the \n  \n    \n      \n        \n          ℓ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\ell ^{2}}\n  \n-norm; the \n  \n    \n      \n        \n          ℓ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\ell ^{1}}\n  \n-norm is an alternative choice, considered but not used in the original paper.\n\nParameter decomposition\nA wholly different approach to sparse decomposition involves decomposing the parameters of a model rather than its representations. The decomposition ought to satisfy these properties: (i) it should provide a list of subcomponents which sum back into the original parameters of the model, (ii) the minimal number of subcomponents can be used to explain the computation for any single input, and (iii) the subcomponents are maximally simple. Two nascent implementations of this approach, thus far only validated on small MLPs, exist: Attribution-based Parameter Decomposition (APD) and its more efficient and less hyperparameter-sensitive successor Stochastic Parameter Decomposition (SPD).\n\nCircuit tracing and automated graph discovery\nAutomated circuit discovery (ACDC) prunes the computational graph by iteratively patch-testing edges, localising minimal sub-circuits without manual hand-holding.\nCircuit tracing substitutes parts of the model, in particular the MLP block, with more interpretable components called \"Transcoders\". The goal is to recover explicit computational graphs. Like SAEs, circuit tracing uses sparse dictionary learning techniques. Instead of reconstructing model activations like SAEs, however, Transcoders aims to predict the output of non-linear components given their input. The technique was introduced in the paper \"Circuit Tracing: Revealing Computational Graphs in Language Models\", published in April 2025 by Anthropic. Circuit tracing has been used to understand how a model plans the rhyme in a poem, perform medical diagnosis, and understand chain of thought unfaithfulness.\n\nCritique\nCritiques of Mechanistic Interpretability can be roughly divided into two categories: critiques of the real-world impact of interpretability, and critiques of the mechanistic approach compared to others.\nCritics have argued that current circuits-level analyses generalise poorly and risk giving false confidence about alignment, citing cherry-picking, small-scale demos, and absence of worst-case guarantees. In response to Dario Amodei's blogpost \"The Urgency of Interpretability\", which claimed that researchers are on track to achieve MI's goal of developing \"the analogue of a highly precise and accurate MRI that would fully reveal the inner workings of an AI model\", Neel Nanda, who leads Google Deepmind's mechanistic interpretability team, argued that mechanistic interpretability alone won't enable high-reliability monitoring for safety-relevant features such as deception. In particular, Nanda highlighted that only a small portion of the model seems to be interpretable and that current methods cannot guarantee the absence of particular features or circuits of interest.\nSome have framed mechanistic interpretability as \"bottom-up interpretability\", where the emphasis is neuron-level circuits, in contrast to \"top-down interpretability\", which focuses on emergent high-level concepts. They argue that LLMs are complex systems and high-level concepts cannot be simulated, predicted, and understood with low-level components. As one critic puts, \"If you wanted to explain why England won World War II using particle physics, you would just be on the wrong track.\"\n\nReferences\n\n\n== Sources ==",
        "url": "https://en.wikipedia.org/wiki/Mechanistic_interpretability"
    },
    {
        "title": "MediSafe controversy",
        "text": "The MediSafe controversy refers to a public debate that emerged in Hong Kong in June 2025 after questions arose about the originality and data practices of MediSafe, a purportedly student-built artificial intelligence platform. The project was developed by Clarisse Poon Hei Shun, a Form Four student at St. Paul's Co-educational College, and won several innovation awards in 2024 and 2025. Online discussion and media coverage later raised concerns about the project’s authorship and use of patient data, prompting official investigations.\n\nMediSafe\nMediSafe (Chinese: 藥倍安心) is a web-based app designed to flag potential prescription errors by cross-checking medications against patient details such as allergies, chronic conditions, and liver or kidney function. According to award materials, the system uses large language models, SQL, and vector databases.\nThe platform received the following awards:\n\nStudent Innovation Grand Award at the 2024 Hong Kong ICT Awards.\nSilver Medal at the 50th International Exhibition of Inventions Geneva in 2025, with support from the Education Bureau.\nYouth Tech Pioneer of the Year Award, recognising secondary school innovation.\n\nAllegations\nWhistleblower concerns\nOn 13 June 2025, Hailey Cheng Hei Lam, a student at City University of Hong Kong and participant in the Google Summer of Code, posted concerns on Threads. She questioned whether such a complex AI system could have been built solely by a secondary school student and cited earlier statements suggesting the use of data from over 100 patients.\nCheng later said she received anonymous threats after making the post. Her comments triggered a wider discussion online about originality in student research, the role of external help, and how sensitive data is handled in youth-led tech projects.\n\nAI Health Studio link\nSome users pointed out that the MediSafe website previously redirected to AI Health Studio, a U.S.-based software company. Archived content from early 2025 described a prescription-checking system reportedly built for a Hong Kong clinic. Reports identified the clinic as one linked to Poon’s father, Dr Poon Tung Ping (also known as Ronnie Poon).\nThe company’s wording was later changed—what was originally described as “developed” became “optimised” and “commercialised”. SCMP noted that the earlier site version appeared to describe software similar to MediSafe and dated back to 2022.\n\nQuestions over data use\nSeveral news outlets reported public concerns about whether real patient data had been used in the project. According to statements quoted by Ming Pao and HK01, the organisers and Poon’s family said only simulated data and publicly available drug databases were used.\nA privacy-focused publication, Meta Connects, discussed the situation in light of Hong Kong’s data privacy laws, raising issues around patient consent, potential cross-border data transfer, and whether external service providers were involved.\n\nResponses and investigations\nReactions from those involved\nPoon later stated she was working with competition organisers to clarify the situation and found some of the online attention discouraging. Dr Poon Tung Ping told the press that only simulated data had been used and that the award process had followed proper verification. St. Paul’s Co-educational College confirmed it was reviewing the matter internally.\n\nOfficial and institutional responses\nThe Digital Policy Office said it had requested a full investigation by Hong Kong Education City and the Hong Kong ICT Awards’ Quality Assurance Panel. On 20 June, the Hong Kong Academy for Gifted Education and the Hong Kong New Generation Cultural Association issued a joint statement saying that the student had complied with competition rules. They also said no real patient data had been used and that company involvement came only after the competitions ended. The Office of the Privacy Commissioner for Personal Data confirmed it had received a complaint and was reviewing the matter.\n\nMedia coverage and public response\nThe controversy was widely covered in both English and Chinese-language media. SCMP explored the timeline of MediSafe’s development and highlighted discrepancies between its public narrative and archived versions of related websites. The Standard reported on government involvement and noted public doubts about the project’s origin. Other local outlets, including Ming Pao, HK01, and Oriental Daily News, examined possible issues of academic outsourcing and privacy violations. On 24 June, HK01 reported that the jury of the International Exhibition of Inventions Geneva reviewed the case and decided to keep the award, stating the submission met its criteria.\n\nTimeline\nApril 2024 – MediSafe wins the Student Innovation Grand Award at the Hong Kong ICT Awards.\nApril 2025 – Project receives a Silver Medal at the International Exhibition of Inventions Geneva.\n13 June 2025 – Hailey Cheng Hei Lam raises concerns publicly on Threads.\nMid‑June 2025 – Investigations launched by the Digital Policy Office and the Office of the Privacy Commissioner for Personal Data.\n20 June 2025 – The Hong Kong Academy for Gifted Education and the Hong Kong New Generation Cultural Association confirm no rules were broken.\n24 June 2025 – The jury of the International Exhibition of Inventions Geneva announces that the Silver Medal will not be revoked.\n28 June 2025 – The Hong Kong Innovation Foundation has removed the relevant exhibited works.\n\nSee also\nAcademic integrity\nArtificial intelligence in healthcare\nData privacy\nLeakage (machine learning)\n\nReferences\nExternal links\nMediSafe's official website (archived at Wayback Machine)",
        "url": "https://en.wikipedia.org/wiki/MediSafe_controversy"
    },
    {
        "title": "Military applications of artificial intelligence",
        "text": "Artificial intelligence (AI) has many applications in warfare, including in communications, intelligence, and munitions control.\n\nUses\nAI can enhance command and control, communications, sensors, integration and interoperability. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Ukraine, Iran and Israel.\n\nAutonomous armament\nMilitary drones capable of autonomous action are in wide use.\n\nCommand and control\nIn 2024 a Chinese laboratory at the Joint Operations College of the National Defense University in Shijiazhuang has created an AI military commander, for use in large-scale war simulations in the role of the commander-in-chief.\nIn 2024, the Ukrainian Army developed autonomous Kamikaze drones in order to make Russian interference during flight ineffective.\n\nMilitary intelligence\nIn 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.\nIn the Gaza war, Israel used two AI systems to generate targets to strike: Habsora (translated: \"the gospel\") was used to compile a list of buildings to target, while \"Lavender\" produced a list of people. \"Lavender\" produced a list of 37,000 people to target. The list of buildings to target included Gazan private homes of people that were suspected of affiliation to Hamas operatives. The combination of AI targeting technology with policy shift away from avoiding civilian targets resulted in unprecedented numbers of civilian deaths. IDF officials say the program addresses the previous issue of the air force running out of targets. Using Habsora, officials say that suspected and junior Hamas members homes significantly expand the \"AI target bank.\" An internal source describes the process as a “mass assassination factory”.\nIn 2024, the U.S. military trained artificial intelligence to identify airstrike targets during its operations in Iraq and Syria.\n\nGlobal trends\nVarious countries are researching and deploying AI military applications, in what has been termed the \"artificial intelligence arms race\". Ongoing research is focused on intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.\nWorldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015.\nIn November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.\nMany AI researchers try to avoid military applications, with guardrails to prevent military applications integrated into most mainstream large language models.\n\nIn popular culture\nMilitary artificial intelligence systems have appeared in many works of fiction, often as antagonists.\n\nFilm\nThe Terminator franchise\nThe Matrix franchise\n\nLiterature\nLegends of Dune trilogy by Brian Herbert\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Military_applications_of_artificial_intelligence"
    },
    {
        "title": "Mindpixel",
        "text": "Mindpixel was a web-based collaborative artificial intelligence project which aimed to create a knowledgebase of millions of human validated true/false statements, or probabilistic propositions. It ran from 2000 to 2005.\n\nDescription\nParticipants in the project created one-line statements which aimed to be objectively true or false to 20 other anonymous participants. In order to submit their statement they had first to check the true/false validity of 20 such statements submitted by others. Participants whose replies were consistently out of step with the majority had their status downgraded and were eventually excluded. Likewise, participants who made contributions which others could not agree were objectively true or false had their status downgraded.  A validated  true/false statement is called a mindpixel.\nThe project enlisted the efforts of thousands of participants and claimed to be \"the planet's largest artificial intelligence effort\".\nThe project was conceived by Chris McKinstry, a computer scientist and former Very Large Telescope operator for the European Southern Observatory in Chile, as MISTIC (Minimum Intelligent Signal Test Item Corpus) in 1996. Mindpixel was developed out of this program, and started in 2000 and had 1.4 million mindpixels in January 2004. The database and its software is known as GAC, which stands for \"Generic Artificial Consciousness\" and is pronounced Jak.   \nMcKinstry believed that the Mindpixel database could be used in conjunction with a neural net to produce a body of human \"common sense\" knowledge which would have market value. Participants in the project were promised shares in any future value according to the number of mindpixels they had successfully created.\nOn 20 September 2005 Mindpixel lost its free server and is no longer operational. It was being rewritten by Chris McKinstry as Mindpixel 2 and was intended to appear on a new server in France.\nChris McKinstry died of suicide on 23 January 2006 and the future of the project and the integrity of the data is uncertain. \nSome Mindpixel data have been utilized by Michael Spivey of Cornell University and Rick Dale of The University of Memphis to study theories of high-level reasoning and continuous temporal dynamics of thought. McKinstry, along with Dale and Spivey, designed an experiment that has now been published in Psychological Science in its January, 2008 issue. In this paper, McKinstry (as posthumous first author), Dale, and Spivey use a very small and carefully selected set of Mindpixel statements to show that even high-level thought processes like decision making can be revealed in the nonlinear dynamics of bodily action.\nOther similar AI-driven knowledge acquisition projects are Never-Ending Language Learning and Open Mind Common Sense (run by MIT), the latter being also hampered when its director died of suicide.\n\nSee also\nNever-Ending Language Learning\nCyc\n\nReferences\nExternal links\nMindpixel Home page (Currently points to a \"Mindpixel IQ test\" using the Mindpixel Db of validated statements)",
        "url": "https://en.wikipedia.org/wiki/Mindpixel"
    },
    {
        "title": "MindsDB",
        "text": "MindsDB is an artificial intelligence company headquartered in California.\n\nHistory\nMindsDB was founded in 2017 by Jorge Torres and Adam Carrigan. The idea was incubated in early 2018 at Skydeck from UC Berkeley during the first funded batch, this led to the MindsDB Open Source project which started in August 2018. On April 16, 2020, MindsDB raised $3 million. Among the investors were OpenOcean, MMC, Rogue Capital, Zyper founder Amber Atherton, SCM Advisors, YCombinator, and Berkeley SkyDeck. MindsDB core was re-written from scratch during their time at Y Combinator in March 2020. In late 2020, MindsDB began offering paid premium services.\nOn November 1, 2021, MindsDB announced an investment from Walden Catalyst Ventures, closing the total pre-seed round to $7.6M. Additionally, MindsDB announced partnerships with Snowflake, SingleStore, and DataStax (based on Apache CassandraTM) to connect its machine learning platform to these databases.\nIn the same month, February 2023, MindsDB announced its integration with Hugging Face and OpenAI that would allow natural language processing and generative AI models into their database via API accessible with SQL requests. This integration enabled advanced text classification, sentiment analysis, emotion detection, translation, and more.\nDuring the same period, 2023, MindsDB raised a $46.5 million seed round from Benchmark, Mayfield and NVentures, and Chetan Puttagunta joined its board of directors.\n\nOverview\nMindsDB has formed strategic partnerships with leading companies such as Snowflake, SingleStore, DataStax, and NVIDIA. As of September 2024, the platform supports over 200 integrations, including popular large language models (LLMs) like OpenAI, Anthropic, and Mistral, as well as data platforms such as MySQL, PostgreSQL, Snowflake, and MongoDB. MindsDB also integrates with a wide range of applications, including Salesforce, HubSpot, X(former Twitter), and many others.\n\nRecognition\nNamed to Forbes' AI 50 list in 2021\nRecognized by Fast Company as one of the Most Innovative AI companies in 2024\n\nReferences\nFurther reading\nGurevich, Natalia (2023-08-24). \"AI companies flocking to Mission worry neighbors\". San Francisco Examiner. Retrieved 2024-03-23.\n\nExternal links\nOfficial website\nmindsdb on GitHub",
        "url": "https://en.wikipedia.org/wiki/MindsDB"
    },
    {
        "title": "Mode collapse",
        "text": "In machine learning, mode collapse is a failure mode observed in generative models, originally noted in Generative Adversarial Networks (GANs). It occurs when the model produces outputs that are less diverse than expected, effectively \"collapsing\" to generate only a few modes of the data distribution while ignoring others. This phenomenon undermines the goal of generative models to capture the full diversity of the training data.\nThere are typically two times at which a model can collapse: either during training or during post-training finetuning.\nMode collapse reduces the utility of generative models in applications, such as in\n\nimage synthesis (repetitive or near-identical images);\ndata augmentation (limited diversity in synthetic data);\nscientific simulations (failure to explore all plausible scenarios).\n\nDistinctions\nMode collapse is distinct from overfitting, where a model learns detailed patterns in the training data that does not generalize to the test data, and underfitting, where it fails to learn patterns.\nMemorization is where a model learns to reproduce data from the training data. Memorization is often confused with mode collapse. However, a model can memorize the training dataset without mode collapse. Indeed, if a model is severely mode-collapsed, then it has failed to memorize large parts of the training dataset.\nModel collapse is one particular mechanism for the phenomenon of mode collapse, i.e. when a generative model 2 is pretrained mainly on the outputs of model 1, then another new generative model 3 is pretrained mainly on the outputs of model 2, etc. When models are trained in this way, each model is typically more mode-collapsed than the previous one. However, there are other mechanisms for mode collapse.\n\nIn GANs\nTraining-time mode collapse was originally noted and studied in GANs, where it arises primarily due to imbalances in the training dynamics between the generator and discriminator in GANs. In the original GAN paper, it was also called the \"Helvetica scenario\".\nCommon causes include:\n\nIf the discriminator learns too slowly, the generator may exploit weaknesses by producing a narrow set of outputs that consistently fool the discriminator.\nTraditional GAN loss functions (e.g., Jensen-Shannon divergence) may be too lenient on generating same-looking outputs.\nThe adversarial training process can lead to oscillatory behavior, where the generator and discriminator fail to converge to a stable equilibrium, but instead engage in a rock-beats-paper-beats-scissors kind of cycling. The generator would generate just \"rock\" until the discriminator learns to classify that as generated, then the generator switch to generating just \"scissors\", and so on. The generator would always be mode-collapsed, though the precise mode in which it collapses to would change during training.\nSeveral GAN-specific strategies were developed to mitigate mode collapse:\n\nTwo time-scale update rule.\nMini-batch discrimination allows the discriminator to evaluate entire batches of samples, encouraging diversity.\nUnrolled GANs optimize the generator against future states of the discriminator.\nWasserstein GAN uses Earth Mover's distance to provide more stable gradients.\nUse a big and balanced training dataset.\nRegularization methods such as gradient penalty and spectral normalization.\n\nFinetuning\nThe large language models are usually trained in two steps. In the first step (\"pretraining\"), the model is trained to simply generate text sampled from a large dataset. In the second step (\"finetuning\"), the model is trained to perform specific tasks by training it on a small dataset containing just the task-specific data. For example, to make a chatbot in this method, one first pretrains a large transformer model over a few trillion words of text scraped from the Internet, then finetunes it on a few million words of example chatlogs that the model should imitate.\nMode collapse may occur during finetuning, as the model learns to generate text that accomplishes the specific task, but loses ability to generate other forms of text. It may also be able to generate a smaller subset of texts that accomplish the specific task. It is hypothesized that there is a tradeoff between quality and diversity. Given a single pretrained model, one may finetune it to perform a specific task. More finetuning would result in higher average task performance, but less diverse outputs. Less finetuning would result in lower average performance, but more diverse outputs. A similar tradeoff has been observed in image generation models and GAN-based text generators.\nSimilarly, mode collapse may occur during RLHF, via reward hacking the reward model or other mechanisms.\n\nSee also\nVariational autoencoder\nGenerative model\nGenerative artificial intelligence\nGenerative pre-trained transformer\nOverfitting\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Mode_collapse"
    },
    {
        "title": "Moral outsourcing",
        "text": "Moral outsourcing refers to placing responsibility for ethical decision-making on to external entities, often algorithms. The term is often used in discussions of computer science and algorithmic fairness, but it can apply to any situation in which one appeals to outside agents in order to absolve themselves of responsibility for their actions. In this context, moral outsourcing specifically refers to the tendency of society to blame technology, rather than its creators or users, for any harm it may cause.\n\nDefinition\nThe term \"moral outsourcing\" was first coined by Dr. Rumman Chowdhury, a data scientist concerned with the overlap between artificial intelligence and social issues. Chowdhury used the term to describe looming fears of a so-called “Fourth Industrial Revolution” following the rise of artificial intelligence.\nMoral outsourcing is often applied by technologists to shrink away from their part in building offensive products. In her TED Talk, Chowdhury gives the example of a creator excusing their work by saying they were simply doing their job. This is a case of moral outsourcing and not taking ownership for the consequences of creation. \nWhen it comes to AI, moral outsourcing allows for creators to decide when the machine is human and when it is a computer - shifting the blame and responsibility of moral plights off of the technologists and onto the technology. Conversations around AI and bias and its impacts require accountability to bring change. It is difficult to address these biased systems if their creators use moral outsourcing to avoid taking any responsibility for the issue. \nOne example of moral outsourcing is the anger that is directed at machines for “taking jobs away from humans” rather than companies for employing that technology and jeopardizing jobs in the first place. \nThe term \"moral outsourcing\" refers to the concept of outsourcing, or enlisting an external operation to complete specific work for another organization. In the case of moral outsourcing, the work of resolving moral dilemmas or making choices according to an ethical code is supposed to be conducted by another entity.\n\nReal-World Applications\nIn the medical field, AI is increasingly involved in decision-making processes about which patients to treat, and how to treat them. The responsibility of the doctor to make informed decisions about what is best for their patients is outsourced to an algorithm. Sympathy is also noted to be an important part of medical practice; an aspect that artificial intelligence, glaringly, is missing. This form of moral outsourcing is a major concern in the medical community.  \nAnother field of technology in which moral outsourcing is frequently brought up is autonomous vehicles. California Polytechnic State University professor Keith Abney proposed an example scenario: \"Suppose we have some [troublemaking] teenagers, and they see an autonomous vehicle, they drive right at it. They know the autonomous vehicle will swerve off the road and go off a cliff, but should it?\" The decision of whether to sacrifice the autonomous vehicle (and any passengers inside) or the vehicle coming at it will be written into the algorithms defining the car's behavior. In the case of moral outsourcing, the responsibility of any damage caused by an accident may be attributed to the autonomous vehicle itself, rather than the creators who wrote protocol the vehicle will use to \"decide\" what to do.\nMoral outsourcing is also used to delegate the consequences of predictive policing algorithms to technology, rather than the creators or the police. There are many ethical concerns with predictive policing due to the fact that it results in the over-policing of low income and minority communities.  In the context of moral outsourcing, the positive feedback loop of sending disproportionate police forces into minority communities is attributed to the algorithm and the data being fed into this system--rather than the users and creators of the predictive policing technology.\n\nOutside of Technology\nReligion\nMoral outsourcing is also commonly seen in appeals to religion to justify discrimination or harm. In his book What It Means to be Moral, sociologist Phil Zuckerman contradicts the popular religious notion that morality comes from God. Religion is oftentimes cited as a foundation for a moral stance without any tangible relation between the religious beliefs and personal stance. In these cases, religious individuals will \"outsource\" their personal beliefs and opinions by claiming that they are a result of their religious identification. This is seen where religion is cited as a factor for political beliefs, medical beliefs, and in extreme cases an excuse for violence.\n\nManufacturing\nMoral outsourcing can also be seen in the business world in terms of manufacturing goods and avoiding environmental responsibility. Some companies in the United States will move their production process to foreign countries with more relaxed environmental policies to avoid the pollution laws that exist in the US. A study by the Harvard Business Review found that \"in countries with tight environmental regulation, companies have 29% lower domestic emissions on average. On the other hand, such a tightening in regulation results in 43% higher emissions abroad.\" The consequences of higher pollution rates are then attributed to the loose regulations in these countries, rather than on the companies themselves who purposefully moved into these areas to avoid strict pollution policy.\n\nRumman Chowdhury\nChowdhury has a prominent voice in the discussions about the intersection of ethics and AI. Her ideas have been included in The Atlantic, Forbes, MIT Technology Review, and the Harvard Business Review.  \n\n\n=== References ===",
        "url": "https://en.wikipedia.org/wiki/Moral_outsourcing"
    },
    {
        "title": "NASA AI Assisted-Air Quality Monitoring Project",
        "text": "The NASA Expert-System Ion Trap Mass Spectrometer (ES-ITMS) Project was a public-private partnership to develop an artificial intelligence assisted, air quality monitoring system and was qualified for use on the Space Shuttle. The partnership was also the first cost and intellectual property shared public-partnership implemented by NASA, which used the commercial Research and Development Limited Partnership (RDLP) model that had been adopted by the Reagan Administration for Department of Defense semiconductor development, and recommended for use by NASA for space commercialization. The project partners included NASA, the University of Florida and Finnigan MAT Corporation, was organized and administered by the NASA Joint Enterprise Institute (subsequently NASA Joint Sponsored Program) and ran from 1988 through 1990. The partnership concluded final testing in 1991, generating four patents, expert system software and application protocol reports. The system was space qualified for use on the Shuttle and elements of the ES-ITMS system were integrated into the product Improvements for Finnigan MAT corporation. The success of the partnership lead NASA to create a pilot program to develop partnership business models as an ongoing management practice.\n\nPurpose and objectives\nThe need to monitor air quality in confined spaces represented an increasing challenge for NASA's planned space missions and private sector facility managers facing the increased scrutiny of possible air contaminants. Up to the early 1980's, air quality monitors generally required large spaces and human technicians to interpret readings. This created a need for miniaturized air quality monitors that could generate reliable and accurate analytic results without on-site technician presence.\nNASA initiated projects to develop...\"mobile and/or portable mass spectrometers\" that evaluated the \"tradeoff between instrumentation capabilities and space, weight and power considerations.\" NASA selected a \"commercial ITMS instrument capable of generating electron ionization, chemical ionization and mass spectrometry data\", to develop a linked expert system to accomplish analysis without human intervention.\nThe commercial instrumentation was from Finnigan MAT corporation while the scientific expertise to support expert system development was available at the University of Florida.\nThe project managers at NASA Ames created a single, integrated project using the RDLP model with objectives to:\n\nDevelop AI/expert system software for instrument control (NASA's role)\nExpand sensitivity, selectivity and speed of the spectrometer (Univ Florida role)\nExpand the spectrometer analytic capability and automate the screening (Finnigan role)\n\nMembership\nThe partnership included seven specialists from five member organizations:\n\nFederal Government\nNational Aeronautics and Space Administration (NASA)\nNASA Ames Research Center (ARC)\nNASA Kennedy Space Center (KSC)\nCommercial\nFinnigan MAT Corporation (Thermo-Fisher Scientific)\nTGS Technology, Inc.\nResearch Management\nUniversity of Florida\n\nOrganization, management and administration\nThe technical project was organized into two development teams, one located in at the NASA Ames Research Center covering expert systems and analytic capabilities and one in Florida covering improved sensitivity and testing.  \nThe partnership management and administration was provided by a non-profit, partnership support organization: the Joint Enterprise Institute operating through San Francisco State University Foundation (SFSUF) with a NASA  employee liaison, Syed Shariq.\n\nPublic-private partnership\nThe partnership structure was as a prototype test of a pilot NASA program to develop public-private partnership business models. The pilot program was known as the NASA  Joint Sponsored Research Program (JSRP), which operated as the NASA Joint Enterprise Institute between 1988 and 1991. The partnership was the first public-private, research and development partnership implemented by NASA in response to national policy shifts to increase technology transfer and space commercialization. The partnership structure included a two year technology development and testing plan that cost $610,000, of which NASA funded $310,000, Finnigan $175,000 and the University of Florida $95,000.\n\nResults and commercialization\nThe project generated patents (4), software (2) and application protocol reports (8). NASA gained use of the patents and jointly development software while Finnigan received commercial utilization rights. The results were commercialized within eighteen months of project completion.\n\nRecognition\nNASA recognized the project as a space qualified instrument. Its achievements were reported to the NASA Administrator, directly leading to establishment of the agency-wide Joint Sponsored Research Program.\n\nSee also\nMass Spectrometry\nMiniaturized Mass Spectrometer\nIon Trap\nHuman Presence in Space\nNASA Joint Sponsored Research Program\n\nExternal links\nThermoFisher Scientific\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/NASA_AI_Assisted-Air_Quality_Monitoring_Project"
    },
    {
        "title": "Neural computation",
        "text": "Neural computation is the information processing performed by networks of neurons. Neural computation is affiliated with the philosophical tradition known as Computational theory of mind, also referred to as computationalism, which advances the thesis that neural computation explains cognition. The first persons to propose an account of neural activity as being computational was Warren McCullock and Walter Pitts in their seminal 1943 paper, A Logical Calculus of the Ideas Immanent in Nervous Activity.\nThere are three general branches of computationalism, including classicism, connectionism, and computational neuroscience. All three branches agree that cognition is computation, however, they disagree on what sorts of computations constitute cognition. The classicism tradition believes that computation in the brain is digital, analogous to digital computing. Both connectionism and computational neuroscience do not require that the computations that realize cognition are necessarily digital computations. However, the two branches greatly disagree upon which sorts of experimental data should be used to construct explanatory models of cognitive phenomena. Connectionists rely upon behavioral evidence to construct models to explain cognitive phenomena, whereas computational neuroscience leverages neuroanatomical and neurophysiological information to construct mathematical models that explain cognition.\nWhen comparing the three main traditions of the computational theory of mind, as well as the different possible forms of computation in the brain, it is helpful to define what we mean by computation in a general sense. Computation is the processing of information, otherwise known as variables or entities, according to a set of rules. A rule in this sense is simply an instruction for executing a manipulation on the current state of the variable, in order to produce a specified output. In other words, a rule dictates which output to produce given a certain input to the computing system. A computing system is a mechanism whose components must be functionally organized to process the information in accordance with the established set of rules. The types of information processed by a computing system determine which type of computations it performs. Traditionally, in cognitive science there have been two proposed types of computation related to neural activity - digital and analog, with the vast majority of theoretical work incorporating a digital understanding of cognition. Computing systems that perform digital computation are functionally organized to execute operations on strings of digits with respect to the type and location of the digit on the string. It has been argued that neural spike train signaling implements some form of digital computation, since neural spikes may be considered as discrete units or digits, like 0 or 1 - the neuron either fires an action potential or it does not. Accordingly, neural spike trains could be seen as strings of digits. Alternatively, analog computing systems perform manipulations on non-discrete, irreducibly continuous variables, that is, entities that vary continuously as a function of time. These sorts of operations are characterized by systems of differential equations.\nNeural computation can be studied for example by building models of neural computation.\nThere is a scientific journal dedicated to this subject, Neural Computation.\nArtificial neural networks (ANN) is a subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Neural_computation"
    },
    {
        "title": "Neural scaling law",
        "text": "In machine learning, a neural scaling law is an empirical scaling law that describes how neural network performance changes as key factors are scaled up or down. These factors typically include the number of parameters, training dataset size, and training cost. Some models also exhibit performance gains by scaling inference through increased test-time compute, extending neural scaling laws beyond training to the deployment phase.\n\nIntroduction\nIn general, a deep learning model can be characterized by four parameters: model size, training dataset size, training cost, and the post-training error rate (e.g., the test set error rate). Each of these variables can be defined as a real number, usually written as \n  \n    \n      \n        N\n        ,\n        D\n        ,\n        C\n        ,\n        L\n      \n    \n    {\\displaystyle N,D,C,L}\n  \n (respectively: parameter count, dataset size, computing cost, and loss).\nA neural scaling law is a theoretical or empirical statistical law between these parameters. There are also other parameters with other scaling laws.\n\nSize of the model\nIn most cases, the model's size is simply the number of parameters. However, one complication arises with the use of sparse models, such as mixture-of-expert models. With sparse models, during inference, only a fraction of their parameters are used. In comparison, most other kinds of neural networks, such as transformer models, always use all their parameters during inference.\n\nSize of the training dataset\nThe size of the training dataset is usually quantified by the number of data points within it. Larger training datasets are typically preferred, as they provide a richer and more diverse source of information from which the model can learn. This can lead to improved generalization performance when the model is applied to new, unseen data. However, increasing the size of the training dataset also increases the computational resources and time required for model training.\nWith the \"pretrain, then finetune\" method used for most large language models, there are two kinds of training dataset: the pretraining dataset and the finetuning dataset. Their sizes have different effects on model performance. Generally, the finetuning dataset is less than 1% the size of pretraining dataset.\nIn some cases, a small amount of high quality data suffices for finetuning, and more data does not necessarily improve performance.\n\nCost of training\nTraining cost is typically measured in terms of time (how long it takes to train the model) and computational resources (how much processing power and memory are required). It is important to note that the cost of training can be significantly reduced with efficient training algorithms, optimized software libraries, and parallel computing on specialized hardware such as GPUs or TPUs.\nThe cost of training a neural network model is a function of several factors, including model size, training dataset size, the training algorithm complexity, and the computational resources available. In particular, doubling the training dataset size does not necessarily double the cost of training, because one may train the model for several times over the same dataset (each being an \"epoch\").\n\nPerformance\nThe performance of a neural network model is evaluated based on its ability to accurately predict the output given some input data. Common metrics for evaluating model performance include:\n\nNegative log-likelihood per token (logarithm of perplexity) for language modeling;\nAccuracy, precision, recall, and F1 score for classification tasks;\nMean squared error (MSE) or mean absolute error (MAE) for regression tasks;\nElo rating in a competition against other models, such as gameplay or preference by a human judge.\nPerformance can be improved by using more data, larger models, different training algorithms, regularizing the model to prevent overfitting, and early stopping using a validation set.\nWhen the performance is a number bounded within the range of \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n, such as accuracy, precision, etc., it often scales as a sigmoid function of cost, as seen in the figures.\n\nExamples\n(Hestness, Narang, et al, 2017)\nThe 2017 paper is a common reference point for neural scaling laws fitted by statistical analysis on experimental data. Previous works before the 2000s, as cited in the paper, were either theoretical or orders of magnitude smaller in scale. Whereas previous works generally found the scaling exponent to scale like \n  \n    \n      \n        L\n        ∝\n        \n          D\n          \n            −\n            α\n          \n        \n      \n    \n    {\\displaystyle L\\propto D^{-\\alpha }}\n  \n, with \n  \n    \n      \n        α\n        ∈\n        {\n        0.5\n        ,\n        1\n        ,\n        2\n        }\n      \n    \n    {\\displaystyle \\alpha \\in \\{0.5,1,2\\}}\n  \n, the paper found that \n  \n    \n      \n        α\n        ∈\n        [\n        0.07\n        ,\n        0.35\n        ]\n      \n    \n    {\\displaystyle \\alpha \\in [0.07,0.35]}\n  \n.\nOf the factors they varied, only task can change the exponent \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n. Changing the architecture optimizers, regularizers, and loss functions, would only change the proportionality factor, not the exponent. For example, for the same task, one architecture might have \n  \n    \n      \n        L\n        =\n        1000\n        \n          D\n          \n            −\n            0.3\n          \n        \n      \n    \n    {\\displaystyle L=1000D^{-0.3}}\n  \n while another might have \n  \n    \n      \n        L\n        =\n        500\n        \n          D\n          \n            −\n            0.3\n          \n        \n      \n    \n    {\\displaystyle L=500D^{-0.3}}\n  \n. They also found that for a given architecture, the number of parameters necessary to reach lowest levels of loss, given a fixed dataset size, grows like \n  \n    \n      \n        N\n        ∝\n        \n          D\n          \n            β\n          \n        \n      \n    \n    {\\displaystyle N\\propto D^{\\beta }}\n  \n for another exponent \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n.\nThey studied machine translation with LSTM (\n  \n    \n      \n        α\n        ∼\n        0.13\n      \n    \n    {\\displaystyle \\alpha \\sim 0.13}\n  \n), generative language modelling with LSTM (\n  \n    \n      \n        α\n        ∈\n        [\n        0.06\n        ,\n        0.09\n        ]\n        ,\n        β\n        ≈\n        0.7\n      \n    \n    {\\displaystyle \\alpha \\in [0.06,0.09],\\beta \\approx 0.7}\n  \n), ImageNet classification with ResNet (\n  \n    \n      \n        α\n        ∈\n        [\n        0.3\n        ,\n        0.5\n        ]\n        ,\n        β\n        ≈\n        0.6\n      \n    \n    {\\displaystyle \\alpha \\in [0.3,0.5],\\beta \\approx 0.6}\n  \n), and speech recognition with two hybrid (LSTMs complemented by either CNNs or an attention decoder) architectures (\n  \n    \n      \n        α\n        ≈\n        0.3\n      \n    \n    {\\displaystyle \\alpha \\approx 0.3}\n  \n).\n\n(Henighan, Kaplan, et al, 2020)\nA 2020 analysis  studied statistical relations between \n  \n    \n      \n        C\n        ,\n        N\n        ,\n        D\n        ,\n        L\n      \n    \n    {\\displaystyle C,N,D,L}\n  \n over a wide range of values and found similar scaling laws, over the range of \n  \n    \n      \n        N\n        ∈\n        [\n        \n          10\n          \n            3\n          \n        \n        ,\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [10^{3},10^{9}]}\n  \n, \n  \n    \n      \n        C\n        ∈\n        [\n        \n          10\n          \n            12\n          \n        \n        ,\n        \n          10\n          \n            21\n          \n        \n        ]\n      \n    \n    {\\displaystyle C\\in [10^{12},10^{21}]}\n  \n, and over multiple modalities (text, video, image, text to image, etc.).\nIn particular, the scaling laws it found are (Table 1 of ):\n\nFor each modality, they fixed one of the two \n  \n    \n      \n        C\n        ,\n        N\n      \n    \n    {\\displaystyle C,N}\n  \n, and varying the other one (\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is varied along using \n  \n    \n      \n        D\n        =\n        C\n        \n          /\n        \n        6\n        N\n      \n    \n    {\\displaystyle D=C/6N}\n  \n), the achievable test loss satisfies\n  \n    \n      \n        L\n        =\n        \n          L\n          \n            0\n          \n        \n        +\n        \n          \n            (\n            \n              \n                \n                  x\n                  \n                    0\n                  \n                \n                x\n              \n            \n            )\n          \n          \n            α\n          \n        \n      \n    \n    {\\displaystyle L=L_{0}+\\left({\\frac {x_{0}}{x}}\\right)^{\\alpha }}\n  \nwhere \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is the varied variable, and \n  \n    \n      \n        \n          L\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        ,\n        α\n      \n    \n    {\\displaystyle L_{0},x_{0},\\alpha }\n  \n are parameters to be found by statistical fitting. The parameter \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is the most important one.\nWhen \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the varied variable, \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n ranges from \n  \n    \n      \n        0.037\n      \n    \n    {\\displaystyle 0.037}\n  \n to \n  \n    \n      \n        0.24\n      \n    \n    {\\displaystyle 0.24}\n  \n depending on the model modality. This corresponds to the \n  \n    \n      \n        α\n        =\n        0.34\n      \n    \n    {\\displaystyle \\alpha =0.34}\n  \n from the Chinchilla scaling paper.\nWhen \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the varied variable, \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n ranges from \n  \n    \n      \n        0.048\n      \n    \n    {\\displaystyle 0.048}\n  \n to \n  \n    \n      \n        0.19\n      \n    \n    {\\displaystyle 0.19}\n  \n depending on the model modality. This corresponds to the \n  \n    \n      \n        β\n        =\n        0.28\n      \n    \n    {\\displaystyle \\beta =0.28}\n  \n from the Chinchilla scaling paper.\nGiven fixed computing budget, optimal model parameter count is consistently around\n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        =\n        \n          \n            (\n            \n              \n                C\n                \n                  5\n                  ×\n                  \n                    10\n                    \n                      −\n                      12\n                    \n                  \n                  \n                    petaFLOP-day\n                  \n                \n              \n            \n            )\n          \n          \n            0.7\n          \n        \n        =\n        9.0\n        ×\n        \n          10\n          \n            −\n            7\n          \n        \n        \n          C\n          \n            0.7\n          \n        \n      \n    \n    {\\displaystyle N_{opt}(C)=\\left({\\frac {C}{5\\times 10^{-12}{\\text{petaFLOP-day}}}}\\right)^{0.7}=9.0\\times 10^{-7}C^{0.7}}\n  \nThe parameter \n  \n    \n      \n        9.0\n        ×\n        \n          10\n          \n            −\n            7\n          \n        \n      \n    \n    {\\displaystyle 9.0\\times 10^{-7}}\n  \n varies by a factor of up to 10 for different modalities. The exponent parameter \n  \n    \n      \n        0.7\n      \n    \n    {\\displaystyle 0.7}\n  \n varies from \n  \n    \n      \n        0.64\n      \n    \n    {\\displaystyle 0.64}\n  \n to \n  \n    \n      \n        0.75\n      \n    \n    {\\displaystyle 0.75}\n  \n for different modalities. This exponent corresponds to the \n  \n    \n      \n        ≈\n        0.5\n      \n    \n    {\\displaystyle \\approx 0.5}\n  \n from the Chinchilla scaling paper.\nIt's \"strongly suggested\" (but not statistically checked) that \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        \n          )\n          \n            0.4\n          \n        \n        ∝\n        \n          C\n          \n            0.28\n          \n        \n      \n    \n    {\\displaystyle D_{opt}(C)\\propto N_{opt}(C)^{0.4}\\propto C^{0.28}}\n  \n. This exponent corresponds to the \n  \n    \n      \n        ≈\n        0.5\n      \n    \n    {\\displaystyle \\approx 0.5}\n  \n from the Chinchilla scaling paper.\nThe scaling law of \n  \n    \n      \n        L\n        =\n        \n          L\n          \n            0\n          \n        \n        +\n        (\n        \n          C\n          \n            0\n          \n        \n        \n          /\n        \n        C\n        \n          )\n          \n            0.048\n          \n        \n      \n    \n    {\\displaystyle L=L_{0}+(C_{0}/C)^{0.048}}\n  \n was confirmed during the training of GPT-3 (Figure 3.1 ).\n\nChinchilla scaling (Hoffmann, et al, 2022)\nOne particular scaling law (\"Chinchilla scaling\") states that, for a large language model (LLM) autoregressively trained for one epoch, with a cosine learning rate schedule, we have:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  C\n                  =\n                  \n                    C\n                    \n                      0\n                    \n                  \n                  N\n                  D\n                \n              \n              \n                \n                  L\n                  =\n                  \n                    \n                      A\n                      \n                        N\n                        \n                          α\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      B\n                      \n                        D\n                        \n                          β\n                        \n                      \n                    \n                  \n                  +\n                  \n                    L\n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}\n  \nwhere the variables are\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the cost of training the model, in FLOPS.\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\n\n  \n    \n      \n        \n          L\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle L_{0}}\n  \n represents the loss of an ideal generative process on the test data\n\n  \n    \n      \n        \n          \n            A\n            \n              N\n              \n                α\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {A}{N^{\\alpha }}}}\n  \n captures the fact that a Transformer language model with \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n parameters underperforms the ideal generative process\n\n  \n    \n      \n        \n          \n            B\n            \n              D\n              \n                β\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {B}{D^{\\beta }}}}\n  \n captures the fact that the model trained on \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n tokens underperforms the ideal generative process\nand the statistical parameters are\n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        6\n      \n    \n    {\\displaystyle C_{0}=6}\n  \n, meaning that it costs 6 FLOPs per parameter to train on one token. This is estimated by Kaplan et al. Note that training cost is much higher than inference cost, as training entails both forward and backward passes, whereas inference costs 1 to 2 FLOPs per parameter to infer on one token.\n\n  \n    \n      \n        α\n        =\n        0.34\n        ,\n        β\n        =\n        0.28\n        ,\n        A\n        =\n        406.4\n        ,\n        B\n        =\n        410.7\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.69\n      \n    \n    {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\n  \n.\nAlthough Besiroglu et al. claims that the statistical estimation is slightly off, and should be \n  \n    \n      \n        α\n        =\n        0.35\n        ,\n        β\n        =\n        0.37\n        ,\n        A\n        =\n        482.01\n        ,\n        B\n        =\n        2085.43\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.82\n      \n    \n    {\\displaystyle \\alpha =0.35,\\beta =0.37,A=482.01,B=2085.43,L_{0}=1.82}\n  \n.\nThe statistical laws were fitted over experimental data with \n  \n    \n      \n        N\n        ∈\n        [\n        7\n        ×\n        \n          10\n          \n            7\n          \n        \n        ,\n        1.6\n        ×\n        \n          10\n          \n            10\n          \n        \n        ]\n        ,\n        D\n        ∈\n        [\n        5\n        ×\n        \n          10\n          \n            9\n          \n        \n        ,\n        5\n        ×\n        \n          10\n          \n            11\n          \n        \n        ]\n        ,\n        C\n        ∈\n        [\n        \n          10\n          \n            18\n          \n        \n        ,\n        \n          10\n          \n            24\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [7\\times 10^{7},1.6\\times 10^{10}],D\\in [5\\times 10^{9},5\\times 10^{11}],C\\in [10^{18},10^{24}]}\n  \n.\nSince there are 4 variables related by 2 equations, imposing 1 additional constraint and 1 additional optimization objective allows us to solve for all four variables. In particular, for any fixed \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, we can uniquely solve for all 4 variables that minimizes \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n. This provides us with the optimal \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ,\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n      \n    \n    {\\displaystyle D_{opt}(C),N_{opt}(C)}\n  \n for any fixed \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n:\n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        =\n        G\n        \n          \n            (\n            \n              \n                C\n                6\n              \n            \n            )\n          \n          \n            a\n          \n        \n        ,\n        \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        =\n        \n          G\n          \n            −\n            1\n          \n        \n        \n          \n            (\n            \n              \n                C\n                6\n              \n            \n            )\n          \n          \n            b\n          \n        \n        ,\n        \n        \n           where \n        \n        \n        G\n        =\n        \n          \n            (\n            \n              \n                \n                  α\n                  A\n                \n                \n                  β\n                  B\n                \n              \n            \n            )\n          \n          \n            \n              1\n              \n                α\n                +\n                β\n              \n            \n          \n        \n        ,\n        \n        a\n        =\n        \n          \n            β\n            \n              α\n              +\n              β\n            \n          \n        \n        \n          , and \n        \n        b\n        =\n        \n          \n            α\n            \n              α\n              +\n              β\n            \n          \n        \n        \n          . \n        \n      \n    \n    {\\displaystyle N_{opt}(C)=G\\left({\\frac {C}{6}}\\right)^{a},\\quad D_{opt}(C)=G^{-1}\\left({\\frac {C}{6}}\\right)^{b},\\quad {\\text{ where }}\\quad G=\\left({\\frac {\\alpha A}{\\beta B}}\\right)^{\\frac {1}{\\alpha +\\beta }},\\quad a={\\frac {\\beta }{\\alpha +\\beta }}{\\text{, and }}b={\\frac {\\alpha }{\\alpha +\\beta }}{\\text{. }}}\n  \nPlugging in the numerical values, we obtain the \"Chinchilla efficient\" model size and training dataset size, as well as the test loss achievable:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    N\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  0.6\n                  \n                  \n                    C\n                    \n                      0.45\n                    \n                  \n                \n              \n              \n                \n                  \n                    D\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  0.3\n                  \n                  \n                    C\n                    \n                      0.55\n                    \n                  \n                \n              \n              \n                \n                  \n                    L\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  1070\n                  \n                  \n                    C\n                    \n                      −\n                      0.154\n                    \n                  \n                  +\n                  1.7\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}N_{opt}(C)=0.6\\;C^{0.45}\\\\D_{opt}(C)=0.3\\;C^{0.55}\\\\L_{opt}(C)=1070\\;C^{-0.154}+1.7\\end{cases}}}\n  \nSimilarly, we may find the optimal training dataset size and training compute budget for any fixed model parameter size, and so on.\nThere are other estimates for \"Chinchilla efficient\" model size and training dataset size. The above is based on a statistical model of \n  \n    \n      \n        L\n        =\n        \n          \n            A\n            \n              N\n              \n                α\n              \n            \n          \n        \n        +\n        \n          \n            B\n            \n              D\n              \n                β\n              \n            \n          \n        \n        +\n        \n          L\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}}\n  \n. One can also directly fit a statistical law for \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ,\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n      \n    \n    {\\displaystyle D_{opt}(C),N_{opt}(C)}\n  \n without going through the detour, for which one obtains:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    N\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  0.1\n                  \n                  \n                    C\n                    \n                      0.5\n                    \n                  \n                \n              \n              \n                \n                  \n                    D\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  1.7\n                  \n                  \n                    C\n                    \n                      0.5\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}N_{opt}(C)=0.1\\;C^{0.5}\\\\D_{opt}(C)=1.7\\;C^{0.5}\\end{cases}}}\n  \nor as tabulated:\n\nDiscrepancy\nThe Chinchilla scaling law analysis for training transformer language models suggests that for a given training compute budget (\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n), to achieve the minimal pretraining loss for that budget, the number of model parameters (\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n) and the number of training tokens (\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n) should be scaled in equal proportions, \n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.5\n          \n        \n        ,\n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.5\n          \n        \n      \n    \n    {\\displaystyle N_{opt}(C)\\propto C^{0.5},D_{opt}(C)\\propto C^{0.5}}\n  \n. \nThis conclusion differs from analysis conducted by Kaplan et al., which found that \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n should be increased more quickly than \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, \n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.73\n          \n        \n        ,\n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.27\n          \n        \n      \n    \n    {\\displaystyle N_{opt}(C)\\propto C^{0.73},D_{opt}(C)\\propto C^{0.27}}\n  \n.\nThis discrepancy can primarily be attributed to the two studies using different methods for measuring model size. Kaplan et al.:\n\ndid not count the parameters in the token embedding layer, which when analyzed at smaller model sizes leads to biased coefficients;\nstudied smaller models than the Chinchilla group, magnifying the effect;\nassumed that \n  \n    \n      \n        \n          L\n          \n            ∞\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle L_{\\infty }=0}\n  \n.\nSecondary effects also arise due to differences in hyperparameter tuning and learning rate schedules. Kaplan et al.:\n\nused a warmup schedule that was too long for smaller models, making them appear less efficient;\ndid not fully tuning optimization hyperparameters.\n\nBeyond Chinchilla scaling\nAs Chinchilla scaling has been the reference point for many large-scaling training runs, there had been a concurrent effort to go \"beyond Chinchilla scaling\", meaning to modify some of the training pipeline in order to obtain the same loss with less effort, or deliberately train for longer than what is \"Chinchilla optimal\".\nUsually, the goal is to make the scaling law exponent larger, which means the same loss can be trained for much less compute. For instance, filtering data can make the scaling law exponent larger.\nAnother strand of research studies how to deal with limited data, as according to Chinchilla scaling laws, the training dataset size for the largest language models already approaches what is available on the internet. found that augmenting the dataset with a mix of \"denoising objectives\" constructed from the dataset improves performance. studies optimal scaling when all available data is already exhausted (such as in rare languages), so one must train multiple epoches over the same dataset (whereas Chinchilla scaling requires only one epoch). The Phi series of small language models were trained on textbook-like data generated by large language models, for which data is only limited by amount of compute available.\nChinchilla optimality was defined as \"optimal for training compute\", whereas in actual production-quality models, there will be a lot of inference after training is complete. \"Overtraining\" during training means better performance during inference. LLaMA models were overtrained for this reason. Subsequent studies discovered scaling laws in the overtraining regime, for dataset sizes up to 32x more than Chinchilla-optimal.\n\nBroken neural scaling laws (BNSL)\nA 2022 analysis found that many scaling behaviors of artificial neural networks follow a smoothly broken power law functional form:\n\n  \n    \n      \n        y\n        =\n        a\n        +\n        \n          \n            (\n          \n        \n        b\n        \n          x\n          \n            −\n            \n              c\n              \n                0\n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            (\n            \n              1\n              +\n              \n                \n                  (\n                  \n                    \n                      x\n                      \n                        d\n                        \n                          i\n                        \n                      \n                    \n                  \n                  )\n                \n                \n                  1\n                  \n                    /\n                  \n                  \n                    f\n                    \n                      i\n                    \n                  \n                \n              \n            \n            )\n          \n          \n            −\n            \n              c\n              \n                i\n              \n            \n            ∗\n            \n              f\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle y=a+{\\bigg (}bx^{-c_{0}}{\\bigg )}\\prod _{i=1}^{n}\\left(1+\\left({\\frac {x}{d_{i}}}\\right)^{1/f_{i}}\\right)^{-c_{i}*f_{i}}}\n  \n\nin which \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n refers to the quantity being scaled (i.e. \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, number of training steps, number of inference steps, or model input size) and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n refers to the downstream (or upstream) performance evaluation metric of interest (e.g. prediction error, cross entropy, calibration error, AUROC, BLEU score percentage, F1 score, reward, Elo rating, solve rate, or FID score) in zero-shot, prompted, or fine-tuned settings. The parameters \n  \n    \n      \n        a\n        ,\n        b\n        ,\n        \n          c\n          \n            0\n          \n        \n        ,\n        \n          c\n          \n            1\n          \n        \n        .\n        .\n        .\n        \n          c\n          \n            n\n          \n        \n        ,\n        \n          d\n          \n            1\n          \n        \n        .\n        .\n        .\n        \n          d\n          \n            n\n          \n        \n        ,\n        \n          f\n          \n            1\n          \n        \n        .\n        .\n        .\n        \n          f\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a,b,c_{0},c_{1}...c_{n},d_{1}...d_{n},f_{1}...f_{n}}\n  \n are found by statistical fitting.\nOn a log–log plot, when \n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle f_{i}}\n  \n is not too large and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is subtracted out from the y-axis, this functional form looks like a series of linear segments connected by arcs; the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n transitions between the segments are called \"breaks\", hence the name broken neural scaling laws (BNSL).\nThe scenarios in which the scaling behaviors of artificial neural networks were found to follow this functional form include large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems, arithmetic, emergent abilities, double descent, supervised learning, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent).\nThe architectures for which the scaling behaviors of artificial neural networks were found to follow this functional form include residual neural networks, transformers, MLPs, MLP-mixers, recurrent neural networks, convolutional neural networks, graph neural networks, U-nets, encoder-decoder (and encoder-only) (and decoder-only) models, ensembles (and non-ensembles), MoE (mixture of experts) (and non-MoE) models, and sparse pruned (and non-sparse unpruned) models.\n\nInference scaling\nOther than scaling up training compute, one can also scale up inference compute (or \"test-time compute\"). As an example, the Elo rating of AlphaGo improves steadily as it is allowed to spend more time on its Monte Carlo Tree Search per play. For AlphaGo Zero, increasing Elo by 120 requires either 2x model size and training, or 2x test-time search. Similarly, a language model for solving competition-level coding challenges, AlphaCode, consistently improved (log-linearly) in performance with more search time.\nFor Hex, 10x training-time compute trades for 15x test-time compute. For Libratus for heads up no-limit Texas hold 'em, and Cicero for Diplomacy, and many other abstract games of partial information, inference-time searching improves performance at a similar tradeoff ratio, for up to 100,000x effective increase in training-time compute.\nIn 2024, the OpenAI o1 report documented that o1's performance consistently improved with both increased train-time compute and test-time compute, and gave numerous examples of test-time compute scaling in mathematics, scientific reasoning, and coding tasks.\nOne method for scaling up test-time compute is process-based supervision, where a model generates a step-by-step reasoning chain to answer a question, and another model (either human or AI) provides a reward score on some of the intermediate steps, not just the final answer. Process-based supervision can be scaled arbitrarily by using synthetic reward score without another model, for example, by running Monte Carlo rollouts and scoring each step in the reasoning according to how likely it leads to the right answer. Another method is by revision models, which are models trained to solve a problem multiple times, each time revising the previous attempt.\n\nOther examples\nVision transformers\nVision transformers, similar to language transformers, exhibit scaling laws. A 2022 research trained vision transformers, with parameter counts \n  \n    \n      \n        N\n        ∈\n        [\n        5\n        ×\n        \n          10\n          \n            6\n          \n        \n        ,\n        2\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [5\\times 10^{6},2\\times 10^{9}]}\n  \n, on image sets of sizes \n  \n    \n      \n        D\n        ∈\n        [\n        3\n        ×\n        \n          10\n          \n            7\n          \n        \n        ,\n        3\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle D\\in [3\\times 10^{7},3\\times 10^{9}]}\n  \n, for computing \n  \n    \n      \n        C\n        ∈\n        [\n        0.2\n        ,\n        \n          10\n          \n            4\n          \n        \n        ]\n      \n    \n    {\\displaystyle C\\in [0.2,10^{4}]}\n  \n (in units of TPUv3-core-days).\nAfter training the model, it is finetuned on ImageNet training set. Let \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n be the error probability of the finetuned model classifying ImageNet test set. They found \n  \n    \n      \n        \n          min\n          \n            N\n            ,\n            D\n          \n        \n        L\n        =\n        0.09\n        +\n        \n          \n            0.26\n            \n              (\n              C\n              +\n              0.01\n              \n                )\n                \n                  0.35\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\min _{N,D}L=0.09+{\\frac {0.26}{(C+0.01)^{0.35}}}}\n  \n.\n\nNeural machine translation\nGhorbani, Behrooz et al. studied scaling laws for neural machine translation (specifically, English as source, and German as target) in encoder-decoder Transformer models, trained until convergence on the same datasets (thus they did not fit scaling laws for computing cost \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n or dataset size \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n). They varied \n  \n    \n      \n        N\n        ∈\n        [\n        \n          10\n          \n            8\n          \n        \n        ,\n        3.5\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [10^{8},3.5\\times 10^{9}]}\n  \n They found three results:\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is a scaling law function of \n  \n    \n      \n        \n          N\n          \n            E\n          \n        \n        ,\n        \n          N\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle N_{E},N_{D}}\n  \n, where \n  \n    \n      \n        \n          N\n          \n            E\n          \n        \n        ,\n        \n          N\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle N_{E},N_{D}}\n  \n are encoder and decoder parameter count. It is not simply a function of total parameter count \n  \n    \n      \n        N\n        =\n        \n          N\n          \n            E\n          \n        \n        +\n        \n          N\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle N=N_{E}+N_{D}}\n  \n. The function has form \n  \n    \n      \n        L\n        \n          (\n          \n            \n              N\n              \n                e\n              \n            \n            ,\n            \n              N\n              \n                d\n              \n            \n          \n          )\n        \n        =\n        α\n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \n                        N\n                        ¯\n                      \n                    \n                  \n                  \n                    e\n                  \n                \n                \n                  N\n                  \n                    e\n                  \n                \n              \n            \n            )\n          \n          \n            \n              p\n              \n                e\n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \n                        N\n                        ¯\n                      \n                    \n                  \n                  \n                    d\n                  \n                \n                \n                  N\n                  \n                    d\n                  \n                \n              \n            \n            )\n          \n          \n            \n              p\n              \n                d\n              \n            \n          \n        \n        +\n        \n          L\n          \n            ∞\n          \n        \n      \n    \n    {\\displaystyle L\\left(N_{e},N_{d}\\right)=\\alpha \\left({\\frac {{\\bar {N}}_{e}}{N_{e}}}\\right)^{p_{e}}\\left({\\frac {{\\bar {N}}_{d}}{N_{d}}}\\right)^{p_{d}}+L_{\\infty }}\n  \n, where \n  \n    \n      \n        α\n        ,\n        \n          p\n          \n            e\n          \n        \n        ,\n        \n          p\n          \n            d\n          \n        \n        ,\n        \n          L\n          \n            ∞\n          \n        \n        ,\n        \n          \n            \n              \n                N\n                ¯\n              \n            \n          \n          \n            e\n          \n        \n        ,\n        \n          \n            \n              \n                N\n                ¯\n              \n            \n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\alpha ,p_{e},p_{d},L_{\\infty },{\\bar {N}}_{e},{\\bar {N}}_{d}}\n  \n are fitted parameters. They found that \n  \n    \n      \n        \n          N\n          \n            d\n          \n        \n        \n          /\n        \n        N\n        ≈\n        0.55\n      \n    \n    {\\displaystyle N_{d}/N\\approx 0.55}\n  \n minimizes loss if \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is held fixed.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n \"saturates\" (that is, it reaches \n  \n    \n      \n        \n          L\n          \n            ∞\n          \n        \n      \n    \n    {\\displaystyle L_{\\infty }}\n  \n) for smaller models when the training and testing datasets are \"source-natural\" than \"target-natural\". A \"source-natural\" data point means a pair of English-German sentences, and the model is asked to translate the English sentence into German, and the English sentence is written by a natural English writer, while the German sentence is translated from the English sentence by a machine translator. To construct the two kinds of datasets, the authors collected natural English and German sentences online, then used machine translation to generate their translations.\nAs models grow larger, models trained on source-original datasets can achieve low loss but bad BLEU score. In contrast, models trained on target-original datasets achieve low loss and good BLEU score in tandem (Figure 10, 11 ).\nThe authors hypothesize that source-natural datasets have uniform and dull target sentences, and so a model that is trained to predict the target sentences would quickly overfit.\n trained Transformers for machine translations with sizes \n  \n    \n      \n        N\n        ∈\n        [\n        4\n        ×\n        \n          10\n          \n            5\n          \n        \n        ,\n        5.6\n        ×\n        \n          10\n          \n            7\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [4\\times 10^{5},5.6\\times 10^{7}]}\n  \n on dataset sizes \n  \n    \n      \n        D\n        ∈\n        [\n        6\n        ×\n        \n          10\n          \n            5\n          \n        \n        ,\n        6\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle D\\in [6\\times 10^{5},6\\times 10^{9}]}\n  \n. They found the Kaplan et al. (2020) scaling law applied to machine translation: \n  \n    \n      \n        L\n        (\n        N\n        ,\n        D\n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  (\n                  \n                    \n                      \n                        N\n                        \n                          C\n                        \n                      \n                      N\n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      α\n                      \n                        N\n                      \n                    \n                    \n                      α\n                      \n                        D\n                      \n                    \n                  \n                \n              \n              +\n              \n                \n                  \n                    D\n                    \n                      C\n                    \n                  \n                  D\n                \n              \n            \n            ]\n          \n          \n            \n              α\n              \n                D\n              \n            \n          \n        \n      \n    \n    {\\displaystyle L(N,D)=\\left[\\left({\\frac {N_{C}}{N}}\\right)^{\\frac {\\alpha _{N}}{\\alpha _{D}}}+{\\frac {D_{C}}{D}}\\right]^{\\alpha _{D}}}\n  \n. They also found the BLEU score scaling as \n  \n    \n      \n        B\n        L\n        E\n        U\n        ≈\n        C\n        \n          e\n          \n            −\n            k\n            L\n          \n        \n      \n    \n    {\\displaystyle BLEU\\approx Ce^{-kL}}\n  \n.\n\nTransfer learning\nHernandez, Danny et al. studied scaling laws for transfer learning in language models. They trained a family of Transformers in three ways:\n\npretraining on English, finetuning on Python\npretraining on an equal mix of English and Python, finetuning on Python\ntraining on Python\nThe idea is that pretraining on English should help the model achieve low loss on a test set of Python text. Suppose the model has parameter count \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, and after being finetuned on \n  \n    \n      \n        \n          D\n          \n            F\n          \n        \n      \n    \n    {\\displaystyle D_{F}}\n  \n Python tokens, it achieves some loss \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n. We say that its \"transferred token count\" is \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle D_{T}}\n  \n, if another model with the same \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n achieves the same \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n after training on \n  \n    \n      \n        \n          D\n          \n            F\n          \n        \n        +\n        \n          D\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle D_{F}+D_{T}}\n  \n Python tokens.\nThey found \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n        =\n        1.9\n        e\n        4\n        \n          \n            (\n            \n              D\n              \n                F\n              \n            \n            )\n          \n          \n            .18\n          \n        \n        (\n        N\n        \n          )\n          \n            .38\n          \n        \n      \n    \n    {\\displaystyle D_{T}=1.9e4\\left(D_{F}\\right)^{.18}(N)^{.38}}\n  \n for pretraining on English text, and \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n        =\n        2.1\n        e\n        5\n        \n          \n            (\n            \n              D\n              \n                F\n              \n            \n            )\n          \n          \n            .096\n          \n        \n        (\n        N\n        \n          )\n          \n            .38\n          \n        \n      \n    \n    {\\displaystyle D_{T}=2.1e5\\left(D_{F}\\right)^{.096}(N)^{.38}}\n  \n for pretraining on English and non-Python code.\n\nPrecision\nKumar et al. study scaling laws for numerical precision in the training of language models. They train a family of language models with weights, activations, and KV cache in varying numerical precision in both integer and floating-point type to measure the effects on loss as a function of precision. For training, their scaling law accounts for lower precision by wrapping the effects of precision into an overall \"effective parameter count\" that governs loss scaling, using the parameterization \n  \n    \n      \n        N\n        ↦\n        \n          N\n          \n            eff\n          \n        \n        (\n        P\n        )\n        =\n        N\n        (\n        1\n        −\n        \n          e\n          \n            −\n            P\n            \n              /\n            \n            γ\n          \n        \n        )\n      \n    \n    {\\displaystyle N\\mapsto N_{\\text{eff}}(P)=N(1-e^{-P/\\gamma })}\n  \n. This illustrates how training in lower precision degrades performance by reducing the true capacity of the model in a manner that varies exponentially with bits.\nFor inference, they find that extreme overtraining of language models past Chinchilla-optimality can lead to models being more sensitive to quantization, a standard technique for efficient deep learning. This is demonstrated by observing that the degradation in loss due to weight quantization increases as an approximate power law in the token/parameter ratio \n  \n    \n      \n        D\n        \n          /\n        \n        N\n      \n    \n    {\\displaystyle D/N}\n  \n seen during pretraining, so that models pretrained on extreme token budgets can perform worse in terms of validation loss than those trained on more modest token budgets if post-training quantization is applied. Other work examining the effects of overtraining include Sardana et al. and Gadre et al.\n\nDensing laws\nXiao et al. considered the parameter efficiency (\"density\") of models over time. The idea is that over time, researchers would discover models that use their parameters more efficiently, in that models with the same performance can have fewer parameters.\nA model can have an actual parameter count \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, defined as the actual number of parameters in the model, and an \"effective\" parameter count \n  \n    \n      \n        \n          \n            \n              N\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {N}}}\n  \n, defined as how many parameters it would have taken a previous well-known model to reach he same performance on some benchmarks, such as MMLU. \n  \n    \n      \n        \n          \n            \n              N\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {N}}}\n  \n is not measured directly, but rather by measuring the actual model performance \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, then plugging it back to a previously fitted scaling law, such as the Chinchilla scaling law, to obtain what \n  \n    \n      \n        \n          \n            \n              N\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {N}}}\n  \n would be required to reach that performance \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, according to that previously fitted scaling laws.\nA densing law states that \n  \n    \n      \n        ln\n        ⁡\n        \n          \n            (\n            \n              \n                \n                  \n                    N\n                    ^\n                  \n                \n                N\n              \n            \n            )\n          \n          \n            m\n            a\n            x\n          \n        \n        =\n        A\n        t\n        +\n        B\n      \n    \n    {\\displaystyle \\ln \\left({\\frac {\\hat {N}}{N}}\\right)_{max}=At+B}\n  \n, where \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n is real-world time, measured in days.\n\nSee also\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Neural_scaling_law"
    },
    {
        "title": "Neuro-symbolic AI",
        "text": "Neuro-symbolic AI is a type of artificial intelligence that integrates neural and symbolic AI architectures to address the weaknesses of each, providing a robust AI capable of reasoning, learning, and cognitive modeling. As argued by Leslie Valiant and others, the effective construction of rich computational cognitive models demands the combination of symbolic reasoning and efficient machine learning. \nGary Marcus argued, \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\" Further, \"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much useful knowledge is abstract to proceed without tools that represent and manipulate abstraction, and to date, the only known machinery that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"\nAngelo Dalli, Henry Kautz, Francesca Rossi, and Bart Selman also argued for such a synthesis. Their arguments attempt to address the two kinds of thinking, as discussed in Daniel Kahneman's book Thinking, Fast and Slow. It describes cognition as encompassing two components: System 1 is fast, reflexive, intuitive, and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is used for pattern recognition. System 2 handles planning, deduction, and deliberative thinking. In this view, deep learning best handles the first kind of cognition while symbolic reasoning best handles the second kind. Both are needed for a robust, reliable AI that can learn, reason, and interact with humans to accept advice and answer questions. Such dual-process models with explicit references to the two contrasting systems have been worked on since the 1990s, both in AI and in Cognitive Science, by multiple researchers.\n\nApproaches\nApproaches for integration are diverse. Henry Kautz's taxonomy of neuro-symbolic architectures follows, along with some examples:\n\nSymbolic Neural symbolic is the current approach of many neural models in natural language processing, where words or subword tokens are the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.\nSymbolic[Neural] is exemplified by AlphaGo, where symbolic techniques are used to invoke neural techniques. In this case, the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.\nNeural | Symbolic uses a neural architecture to interpret perceptual data as symbols and relationships that are reasoned about symbolically. Neural-Concept Learner is an example.\nNeural: Symbolic → Neural relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.\nNeuralSymbolic uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND-OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.\nNeural[Symbolic] allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state. An example would be ChatGPT using a plugin to query Wolfram Alpha.\nThese categories are not exhaustive, as they do not consider multi-agent systems. In 2005, Bader and Hitzler presented a more fine-grained categorization that considered, e.g., whether the use of symbols included logic and if it did, whether the logic was propositional or first-order logic. The 2005 categorization and Kautz's taxonomy above are compared and contrasted in a 2021 article. Recently, Sepp Hochreiter argued that Graph Neural Networks \"...are the predominant models of neural-symbolic computing\" since \"[t]hey describe the properties of molecules, simulate social networks, or predict future states in physical and engineering applications with particle-particle interactions.\"\n\nArtificial general intelligence\nGary Marcus argues that \"...hybrid architectures that combine learning and symbol manipulation are necessary for robust intelligence, but not sufficient\", and that there are\n\n...four cognitive prerequisites for building robust artificial intelligence: \nhybrid architectures that combine large-scale learning with the representational and computational powers of symbol manipulation,\nlarge-scale knowledge bases—likely leveraging innate frameworks—that incorporate symbolic knowledge along with other forms of knowledge,\nreasoning mechanisms capable of leveraging those knowledge bases in tractable ways, and\nrich cognitive models that work together with those mechanisms and knowledge bases.\nThis echoes earlier calls for hybrid models as early as the 1990s.\n\nHistory\nGarcez and Lamb described research in this area as ongoing at least since the 1990s. At that time, the terms symbolic and sub-symbolic AI were popular.\nA series of workshops on neuro-symbolic AI has been held annually since 2005 Neuro-Symbolic Artificial Intelligence. In the early 1990s, an initial set of workshops on this topic were organized.\n\nResearch\nKey research questions remain, such as:\n\nWhat is the best way to integrate neural and symbolic architectures?\nHow should symbolic structures be represented within neural networks and extracted from them?\nHow should common-sense knowledge be learned and reasoned about?\nHow can abstract knowledge that is hard to encode logically be handled?\n\nImplementations\nImplementations of neuro-symbolic approaches include:\n\nAllegroGraph: an integrated Knowledge Graph based platform for neuro-symbolic application development.\nScallop: a language based on Datalog that supports differentiable logical and relational reasoning. Scallop can be integrated in Python and with a PyTorch learning module.\nLogic Tensor Networks: encode logical formulas as neural networks and simultaneously learn term encodings, term weights, and formula weights.\nDeepProbLog: combines neural networks with the probabilistic reasoning of ProbLog.\nSymbolicAI: a compositional differentiable programming library.\nExplainable Neural Networks (XNNs): combine neural networks with symbolic hypergraphs and trained using a mixture of backpropagation and symbolic learning called induction.\n\nSee also\nSymbolic AI\nConnectionist AI\nHybrid intelligent systems\n\nCitations\nReferences\nBader, Sebastian; Hitzler, Pascal (2005-11-10). \"Dimensions of Neural-symbolic Integration – A Structured Survey\". arXiv:cs/0511042.\nGarcez, Artur S. d'Avila; Broda, Krysia; Gabbay, Dov M.; Gabbay (2002). Neural-Symbolic Learning Systems: Foundations and Applications. Springer Science & Business Media. ISBN 978-1-85233-512-0.\nGarcez, Artur; Besold, Tarek; De Raedt, Luc; Földiák, Peter; Hitzler, Pascal; Icard, Thomas; Kühnberger, Kai-Uwe; Lamb, Luís; Miikkulainen, Risto; Silver, Daniel (2015). Neural-Symbolic Learning and Reasoning: Contributions and Challenges. AAAI Spring Symposium - Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. Stanford, CA. doi:10.13140/2.1.1779.4243.\nGarcez, Artur d'Avila; Gori, Marco; Lamb, Luis C.; Serafini, Luciano; Spranger, Michael; Tran, Son N. (2019). \"Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning\". arXiv:1905.06088 [cs.AI].\nGarcez, Artur d'Avila; Lamb, Luis C. (2020). \"Neurosymbolic AI: The 3rd Wave\". arXiv:2012.05876 [cs.AI].\nHitzler, Pascal; Sarker, Md Kamruzzaman (2022). Neuro-Symbolic Artificial Intelligence: The State of the Art. IOS Press. ISBN 978-1-64368-244-0.\nHitzler, Pascal; Sarker, Md Kamruzzaman; Eberhart, Aaron (2023). Compendium of Neurosymbolic Artificial Intelligence. IOS Press. ISBN 978-1-64368-406-2.\nHochreiter, Sepp. \"Toward a Broad AI.\" Commun. ACM 65(4): 56–57 (2022). Toward a broad AI\nHonavar, Vasant (1995). Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Towards a Resolution of the Dichotomy. The Springer International Series In Engineering and Computer Science. Springer US. pp. 351–388. doi:10.1007/978-0-585-29599-2_11.\nKautz, Henry (2020-02-11). The Third AI Summer, Henry Kautz, AAAI 2020 Robert S. Engelmore Memorial Award Lecture. Retrieved 2022-07-06.\nKautz, Henry (2022). \"The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture\". AI Magazine. 43 (1): 93–104. doi:10.1609/aimag.v43i1.19122. ISSN 2371-9621. S2CID 248213051. Retrieved 2022-07-12.\nMao, Jiayuan; Gan, Chuang; Kohli, Pushmeet; Tenenbaum, Joshua B.; Wu, Jiajun (2019). \"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision\". arXiv:1904.12584 [cs.CV].\nMarcus, Gary; Davis, Ernest (2019). Rebooting AI: Building Artificial Intelligence We Can Trust. Vintage.\nMarcus, Gary (2020). \"The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence\". arXiv:2002.06177 [cs.AI].\nDalli, Angelo (2025-02-13). \"WAICF2025: Why neurosymbolic AI is the future of trustworthy AI (WAICF 2025 Keynote)\". Retrieved 2025-03-06.\nRossi, Francesca (2022-07-06). \"AAAI2022: Thinking Fast and Slow in AI (AAAI 2022 Invited Talk)\". Retrieved 2022-07-06.\nSelman, Bart (2022-07-06). \"AAAI2022: Presidential Address: The State of AI\". Retrieved 2022-07-06.\nSerafini, Luciano; Garcez, Artur d'Avila (2016-07-07). \"Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge\". arXiv:1606.04422 [cs.AI].\nSun, Ron (1995). \"Robust reasoning: Integrating rule-based and similarity-based reasoning\". Artificial Intelligence. 75 (2): 241–296. doi:10.1016/0004-3702(94)00028-Y.\nSun, Ron; Bookman, Lawrence (1994). Computational Architectures Integrating Neural and Symbolic Processes. Kluwer.\nSun, Ron; Alexandre, Frederic (1997). Connectionist Symbolic Integration. Lawrence Erlbaum Associates.\nSun, R (2001). \"Hybrid systems and connectionist implementationalism\". Encyclopedia of Cognitive Science (MacMillan Publishing Company, 2001).\nValiant, Leslie G (2008). \"Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence\". IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science. doi:10.4230/LIPIcs.FSTTCS.2008.1770.\n\nExternal links\nArtificial Intelligence: Workshop series on Neural-Symbolic Learning and Reasoning",
        "url": "https://en.wikipedia.org/wiki/Neuro-symbolic_AI"
    },
    {
        "title": "Neurorobotics",
        "text": "Neurorobotics is the combined study of neuroscience, robotics, and artificial intelligence. It is the science and technology of embodied autonomous neural systems. Neural systems include brain-inspired algorithms (e.g. connectionist networks), computational models of biological neural networks (e.g. artificial spiking neural networks, large-scale simulations of neural microcircuits) and actual biological systems (e.g. in vivo and in vitro neural nets). Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but also, at smaller scale, micro-machines and, at the larger scales, furniture and infrastructures.\nNeurorobotics is that branch of neuroscience with robotics, which deals with the study and application of science and technology of embodied autonomous neural systems like brain-inspired algorithms. It is based on the idea that the brain is embodied and the body is embedded in the environment. Therefore, most neurorobots are required to function in the real world, as opposed to a simulated environment.\nBeyond brain-inspired algorithms for robots neurorobotics may also involve the design of brain-controlled robot systems.\n\nMajor classes of models\nNeurorobots can be divided into various major classes based on the robot's purpose. Each class is designed to implement a specific mechanism of interest for study. Common types of neurorobots are those used to study motor control, memory, action selection, and perception.\n\nLocomotion and motor control\nNeurorobots are often used to study motor feedback and control systems, and have proved their merit in developing controllers for robots. Locomotion is modeled by a number of neurologically inspired theories on the action of motor systems. Locomotion control has been mimicked using models or central pattern generators, clumps of neurons capable of driving repetitive behavior, to make four-legged walking robots.  Other groups have expanded the idea of combining rudimentary control systems into a hierarchical set of simple autonomous systems. These systems can formulate complex movements from a combination of these rudimentary subsets. This theory of motor action is based on the organization of cortical columns, which progressively integrate from simple sensory input into a complex afferent signals, or from complex motor programs to simple controls for each muscle fiber in efferent signals, forming a similar hierarchical structure.\nAnother method for motor control uses learned error correction and predictive controls to form a sort of simulated muscle memory. In this model, awkward, random, and error-prone movements are corrected for using error feedback to produce smooth and accurate movements over time. The controller learns to create the correct control signal by predicting the error.  Using these ideas, robots have been designed which can learn to produce adaptive arm movements or to avoid obstacles in a course.\n\nLearning and memory systems\nRobots designed to test theories of animal memory systems. Many studies examine the memory system of rats, particularly the rat hippocampus, dealing with place cells, which fire for a specific location that has been learned. Systems modeled after the rat hippocampus are generally able to learn mental maps of the environment, including recognizing landmarks and associating behaviors with them, allowing them to predict the upcoming obstacles and landmarks.\nAnother study has produced a robot based on the proposed learning paradigm of barn owls for orientation and localization based on primarily auditory, but also visual stimuli. The hypothesized method involves synaptic plasticity and neuromodulation, a mostly chemical effect in which reward neurotransmitters such as dopamine or serotonin affect the firing sensitivity of a neuron to be sharper. The robot used in the study adequately matched the behavior of barn owls. Furthermore, the close interaction between motor output and auditory feedback proved to be vital in the learning process, supporting active sensing theories that are involved in many of the learning models.\nNeurorobots in these studies are presented with simple mazes or patterns to learn. Some of the problems presented to the neurorobot include recognition of symbols, colors, or other patterns and execute simple actions based on the pattern. In the case of the barn owl simulation, the robot had to determine its location and direction to navigate in its environment.\n\nAction selection and value systems\nAction selection studies deal with negative or positive weighting to an action and its outcome. Neurorobots can and have been used to study simple ethical interactions, such as the classical thought experiment where there are more people than a life raft can hold, and someone must leave the boat to save the rest. However, more neurorobots used in the study of action selection contend with much simpler persuasions such as self-preservation or perpetuation of the population of robots in the study. These neurorobots are modeled after the neuromodulation of synapses to encourage circuits with positive results. \nIn biological systems, neurotransmitters such as dopamine or acetylcholine positively reinforce neural signals that are beneficial. One study of such interaction involved the robot Darwin VII, which used visual, auditory, and a simulated taste input to \"eat\" conductive metal blocks. The arbitrarily chosen good blocks had a striped pattern on them while the bad blocks had a circular shape on them. The taste sense was simulated by conductivity of the blocks.  The robot had positive and negative feedbacks to the taste based on its level of conductivity. The researchers observed the robot to see how it learned its action selection behaviors based on the inputs it had. Other studies have used herds of small robots which feed on batteries strewn about the room, and communicate its findings to other robots.\n\nSensory perception\nNeurorobots have also been used to study sensory perception, particularly vision. These are primarily systems that result from embedding neural models of sensory pathways in automatas. This approach gives exposure to the sensory signals that occur during behavior and also enables a more realistic assessment of the degree of robustness of the neural model. It is well known that changes in the sensory\nsignals produced by motor activity provide useful perceptual cues that are used extensively by organisms. For example, researchers have used the depth information that emerges during replication of human head and eye movements to establish robust representations of the visual scene.\n\nBiological robots\nBiological robots are not officially neurorobots in that they are not neurologically inspired AI systems, but actual neuron tissue wired to a robot. This employs the use of cultured neural networks to study brain development or neural interactions. These typically consist of a neural culture raised on a multielectrode array (MEA), which is capable of both recording the neural activity and stimulating the tissue. In some cases, the MEA is connected to a computer which presents a simulated environment to the brain tissue and translates brain activity into actions in the simulation, as well as providing sensory feedback The ability to record neural activity gives researchers a window into a brain, which they can use to learn about a number of the same issues neurorobots are used for.\nAn area of concern with the biological robots is ethics. Many questions are raised about how to treat such experiments. The central question concerns consciousness and whether or not the rat brain experiences it. There are many theories about how to define consciousness.\n\nImplications for neuroscience\nNeuroscientists benefit from neurorobotics because it provides a blank slate to test various possible methods of brain function in a controlled and testable environment. While robots are more simplified versions of the systems they emulate, they are more specific, allowing more direct testing of the issue at hand. They also have the benefit of being accessible at all times, while it is more difficult to monitor large portions of a brain while the human or animal is active, especially individual neurons.\nThe development of neuroscience has produced neural treatments. These include pharmaceuticals and neural rehabilitation. Progress is dependent on an intricate understanding of the brain and how exactly it functions. It is difficult to study the brain, especially in humans, due to the danger associated with cranial surgeries. Neurorobots can improved the range of tests and experiments that can be performed in the study of neural processes.\n\nSee also\nBrain–computer interface\nExperience machine\nNeuromorphic engineering\nWirehead (science fiction)\n\nReferences\nExternal links\nNeurorobotics on Scholarpedia (Jeff Krichmar (2008), Scholarpedia, 3(3):1365)\nA lab that focuses on neurorobotics at Northwestern University.\nFrontiers in Neurorobotics.\nNeurorobotics: an experimental science of embodiment by Frederic Kaplan\nNeurorobotics Lab, Control Systems Lab, NTUn of Athens (Prof. Kostas J. Kyriakopoulos)\nNeurorobotics in the Human Brain Project",
        "url": "https://en.wikipedia.org/wiki/Neurorobotics"
    },
    {
        "title": "Non-human",
        "text": "Non-human (also spelled nonhuman) is any entity displaying some, but not enough, human characteristics to be considered a human. The term has been used in a variety of contexts and may refer to objects that have been developed with human intelligence, such as robots or vehicles.\n\nOrganisms\nAnimal rights and personhood\nIn the animal rights movement, it is common to distinguish between \"human animals\" and \"non-human animals\". Participants in the animal rights movement generally recognize that non-human animals have some similar characteristics to those of human persons. For example, various non-human animals have been shown to register pain, compassion, memory, and some cognitive function.  Some animal rights activists argue that the similarities between human and non-human animals justify giving non-human animals rights that human society has afforded to humans, such as the right to self-preservation, and some even wish for all non-human animals or at least those that bear a fully thinking and conscious mind, such as vertebrates and some invertebrates such as cephalopods, to be given a full right of personhood.\n\nThe non-human in philosophy\nContemporary philosophers have drawn on the work of Henri Bergson, Gilles Deleuze, Félix Guattari, and Claude Lévi-Strauss (among others) to suggest that the non-human poses epistemological and ontological problems for humanist and post-humanist ethics, and have linked the study of non-humans to materialist and ethological approaches to the study of society and culture.\n\nSoftware and robots\nThe term non-human has been used to describe computer programs and robot-like devices that display some human-like characteristics. In both science fiction and in the real world, computer programs and robots have been built to perform tasks that require human-computer interactions in a manner that suggests sentience and compassion. There is increasing interest in the use of robots in nursing homes and to provide elder care. Computer programs have been used for years in schools to provide one-on-one education with children. The Tamagotchi toy required children to provide care, attention, and nourishment to keep it \"alive\".\n\nSee also\nAnimal\nAnimal rights by country or territory\nArtificial intelligence\nDehumanization\nPerson\n\nReferences\n\n\n== External links ==",
        "url": "https://en.wikipedia.org/wiki/Non-human"
    },
    {
        "title": "Nouvelle AI",
        "text": "Nouvelle artificial intelligence (AI) is an approach to artificial intelligence pioneered in the 1980s by Rodney Brooks, who was then part of MIT artificial intelligence laboratory. Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the \"real world\", instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.\n\nMotivation\nThe differences between nouvelle AI and symbolic AI are apparent in early robots Shakey and Freddy. These robots contained an internal model (or \"representation\") of their micro-worlds consisting of symbolic descriptions. As a result, this structure of symbols had to be renewed as the robot moved or the world changed.\nShakey's planning programs assessed the program structure and broke it down into the necessary steps to complete the desired action. This level of computation required a large amount time to process, so Shakey typically performed its tasks very slowly.\nSymbolic AI researchers had long been plagued by the problem of updating, searching, and otherwise manipulating the symbolic worlds inside their AIs. A nouvelle system refers continuously to its sensors rather than to an internal model of the world. It processes the external world information it needs from the senses when it is required. As Brooks puts it, \"the world is its own best model--always exactly up to date and complete in every detail.\"\nA central idea of nouvelle AI is that simple behaviors combine to form more complex behaviors over time. For example, simple behaviors can include elements like \"move forward\" and \"avoid obstacles.\" A robot using nouvelle AI with simple behaviors like collision avoidance and moving toward a moving object could possibly come together to produce a more complex behavior like chasing a moving object.\n\nThe frame problem\nThe frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms (symbolic language) to imply that things about an environment do not change arbitrarily.\nNouvelle AI seeks to sidestep the frame problem by dispensing with filling the AI or robot with volumes of symbolic language and instead letting more complex behaviors emerge by combining simpler behavioral elements.\n\nEmbodiment\nThe goal of traditional AI was to build intelligences without bodies, which would only have been able to interact with the world via keyboard, screen, or printer. However, nouvelle AI attempts to build embodied intelligence situated in the real world. Brooks quotes approvingly from the brief sketches that Turing gave in 1948 and 1950 of the \"situated\" approach. Turing wrote of equipping a machine \"with the best sense organs that money can buy\" and teaching it \"to understand and speak English\" by a process that would \"follow the normal teaching of a child.\" This approach was contrasted to the others where they focused on abstract activities such as playing chess.\n\nBrooks' robots\nInsectoid robots\nBrooks focused on building robots that acted like simple insects while simultaneously working to remove some traditional AI characteristics. He created insect-like robots, named Allen and Herbert after cognitive science and AI pioneers Allen Newell and Herbert A. Simon.\nBrooks's insectoid robots contained no internal models of the world. Herbert, for example, discarded a high volume of the information received from its sensors and never stored information for more than two seconds.\n\nAllen\nAllen had a ring of twelve ultrasonic sonars as its primary sensors and three independent behavior-producing modules. These modules were programmed to avoid both stationary and moving objects. With only this module activated, Allen stayed in the middle of a room until an object approached and then it ran away while avoiding obstacles in its way.\n\nHerbert\nHerbert used infrared sensors to avoid obstacles and a laser system to collect 3D data over a distance of about 12 feet. Herbert also carried a number of simple sensors in its \"hand.\" The robot's testing ground was the real world environment of the busy offices and workspaces of the MIT AI lab where it searched for empty soda cans and carried them away, a seemingly goal-oriented activity that emerged as a result of 15 simple behavior units combining. As a parallel, Simon noted that an ant's complicated path is due to the structure of its environment rather than the depth of its thought processes.\n\nOther insectoid robots\nOther robots by Brooks' team were Genghis and Squirt. Genghis had six legs and was able to walk over rough terrain and follow a human. Squirt's behavior modules had it stay in dark corners until it heard a noise, then it would begin to follow the source of the noise.\nBrooks agreed that the level of nouvelle AI had come near the complexity of a real insect, which raised a question about whether or not insect level-behavior was and is a reasonable goal for nouvelle AI.\n\nHumanoid robots\nBrooks' own recent work has taken the opposite direction to that proposed by Von Neumann in the quotations \"theorists who select the human nervous system as their model are unrealistically picking 'the most complicated object under the sun,' and that there is little advantage in selecting instead the ant, since any nervous system at all exhibits exceptional complexity.\"\n\nCog\nIn the 1990s, Brooks decided to pursue the goal of human-level intelligence and, with Lynn Andrea Stein, built a humanoid robot called Cog. Cog is a robot with an extensive collection of sensors, a face, and arms (among other features) that allow it to interact with the world and gather information and experience so as to assemble intelligence organically in the manner described above by Turing.\nThe team believed that Cog would be able to learn and able to find a correlation between the sensory information it received and its actions, and to learn common sense knowledge on its own. As of 2003, all development of the project had ceased.\n\nSee also\nBEAM robotics\nBehavior-based robotics\nCognitive science\nIntelligence\nReactive planning\nScruffy AI\n\nReferences\nExternal links\nWhat is AI - Nouvelle AI\nNouvelle AI\nReactive planning and nouvelle AI\nAI - new foundations at EnCYClopædia Britannica",
        "url": "https://en.wikipedia.org/wiki/Nouvelle_AI"
    },
    {
        "title": "Operation Serenata de Amor",
        "text": "Operation Serenata de Amor is an artificial intelligence project designed to analyze public spending in Brazil. The project has been funded by a recurrent financing campaign since September 7, 2016, and came in the wake of major scandals of misappropriation of public funds in Brazil, such as the Mensalão scandal and what was revealed in the Operation Car Wash investigations.\nThe analysis began with data from the National Congress then expanded to other types of budget and instances of government, such as the Federal Senate. The project is built through collaboration on GitHub and using a public group with more than 600 participants on Telegram.\nThe name \"Serenata de Amor,\" which means \"serenade of love,\" was taken from a popular cashew cream bonbon produced by Chocolates Garoto in Brazil.\n\nModules\nThroughout development of the project, new modules have been newly introduced in addition to the main repository:\n\nThe main repository, serenata-de-amor, serves as the starting point for investigative work.\nRosie is the robot programmed to identify public funds expenses with discrepancies, starting with CEAP (Quota for Exercise of Parliamentary Activity); it analyzes each of the reimbursements requested by the deputies and senators, indicating the reasons that lead it to believe they are suspicious.\nFrom Rosie was born whistleblower, which tweets under the name of @RosieDaSerenata, distributing the results found on social media.\nJarbas (Github repository) is a data visualization tool which shows a complete list of reimbursements made available by the Chamber of Deputies and mined by Rosie.\nToolbox is a Python installable package that supports the development of Serenata de Amor and Rosie.\n\nHistory\nOperation Serenata de Amor is an Artificial intelligence project for analysis of public expenditures. It was conceived in March 2016 by data scientist Irio Musskopf, sociologist Eduardo Cuducos and entrepreneur Felipe Cabral. The project was financed collectively in the Catarse platform, where it reached 131% of the collection goal  paying 3 months of project development. Ana Schwendler, also a data scientist, Pedro Vilanova \"Tonny\", data journalist, Bruno Pazzim, software engineer, Filipe Linhares, a frontend engineer, Leandro Devegili, an entrepreneur and André Pinho took the first steps towards constructing the platform, such as collecting and structuring the first datasets.\nJessica Temporal, data scientist and Yasodara Córdova \"Yaso\", researcher, Tatiana Balachova \"Russa\", UX designer, joined the project after the financing took place.\nThe members created a recurring financing campaign, expanding the analysis of public spending to the Federal Senate. Donors make monthly payments ranging from 5 BRL to 200 BRL to maintain group activities. The monthly amount collected is around 10,000 BRL.\n\nResults\nIn January 2017, concluding the period financed by the initial campaign, the group carried out an investigation into the suspicious activities found by the data analysis system. 629 complaints were made to the Ombudsman's Office of the Chamber of Deputies, questioning expenses of 216 federal deputies. In addition, the Facebook project page has more than 25,000 followers, and users frequently cite the operation as a benchmark in transparency in the Brazilian government. One of the examples of results obtained by the operation is the case of the Deputy who had to return about 700 BRL to the House  after his expenses were analyzed by the platform.\nThe platform was able to analyze more than 3 million notes, raising about 8,000 suspected cases in public spending. The community that supports the work of the team benefits from open source repositories, with licenses open for the collaboration. So much so that the two main data scientists  of the project presented it at the CivicTechFest in Taipei, obtaining several mentions even in the international press. The technical leader presented the project in Poland during DevConf2017 in Kraków. It was also presented in the Google News Lab in 2017. It was presented by Yaso, when she was the Director of the initiative, at the MIT Media Lab/Berkman Klein Center Initiative for Artificial Intelligence ethics, and at the Artificial Intelligence and Inclusion Symposium, an initiative of the  Global Network of Internet & Society Centers (NoC). It was also presented both by Irio and Yaso at the Digital Harvard Kennedy School, over a lunch seminar, where the transparency of the platform and the main solutions found were discussed, so that the code and data are always available to verify its suitability.\nThis infographic provides information about the first results of Operation Serenata de Amor, a project that analyzes open data on public spending to find discrepancies.\nThe project was presented by Yaso to the House Audit and Control Committee of the Chamber of Deputies in August 2017, and raised the interest of House officials who work with open data.\nThe operation has been a source of inspiration for other civic projects that aim to work with similar goals. Participation of several team members in events throughout Brazil and abroad can be found on the Internet, such as presentation at OpenDataDay, held at Calango Hackerspace in the Federal District, Campus Party Bahia, Campus Party Brasilia, Friends of Tomorrow, XIII National Meeting of Internal Control, in the event USP Talks  Hackfest against corruption in João Pessoa, the latter being also highlighted in the National Press.\n\nSee also\nInternet activism\nList of scandals in Brazil\nOpen government\n\nReferences\nExternal links\nOperation Serenata de Amor official website\nGithub repository",
        "url": "https://en.wikipedia.org/wiki/Operation_Serenata_de_Amor"
    },
    {
        "title": "Operational artificial intelligence",
        "text": "Operational artificial intelligence, or operational AI, is a type of intelligent system designed for real-world applications, particularly at commercial scale. The term is used to distinguish accessible artificially intelligent (AI) systems from fundamental AI research and from industrial AI applications which are not integrated with the routine usage of a business. The definition of operational AI differs throughout the IT industry, where vendors and individual organizations often create their own custom definitions of such processes and services for the purpose of marketing their own products. \nApplications include text analytics, advanced analytics, facial and image recognition, machine learning, and natural language generation.\n\nDefinitions\nAccording to a white paper by software company Tupl Inc, continuous machine learning model training and results extraction in the telecom industry requires a large number of automation utilities in order to \"facilitate the development and deployment of a multitude of use cases, the collection and correlation of the data, the creation and training of the models, and the operation at telecom-grade levels of security and availability\".\n\nResearchers in the University of Waterloo's Artificial Intelligence Group describe operational AI in terms of the focus on applications that bring value to products and company. University of Waterloo Professor of Electrical and Computer Engineering Fakhri Karray describes operational AI as \"application of AI for the masses\". Canada Research Chair and Associate Professor Alexander Wong (professor) describes operational AI as AI for \"anyone, anywhere, anytime.\"\n\nRelated terms\nIndustrial AI refers to intelligent systems applied for business at any scale and for any use case.\n\nSee also\nApplications of artificial intelligence\nEdge computing\nIndustrial artificial intelligence\nContinuous integration\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Operational_artificial_intelligence"
    },
    {
        "title": "Organoid intelligence",
        "text": "Organoid intelligence (OI) is an emerging field of study in computer science and biology that develops and studies biological wetware computing using 3D cultures of human brain cells (or brain organoids) and brain-machine interface technologies. Such technologies may be referred to as OIs.\nOrganoid intelligent computer systems can be an example of biohybrid systems.\n\nDifferences with non-organic computing\nAs opposed to traditional non-organic silicon-based approaches, OI seeks to use lab-grown cerebral organoids to serve as \"biological hardware.\" Scientists hope that such organoids can provide faster, more efficient, and more powerful computing power than regular silicon-based computing and AI while requiring only a fraction of the energy. However, while these structures are still far from being able to think like a regular human brain and do not yet possess strong computing capabilities, OI research currently offers the potential to improve the understanding of brain development, learning and memory, potentially finding treatments for neurological disorders such as dementia.\nThomas Hartung, a professor from Johns Hopkins University, argues that \"while silicon-based computers are certainly better with numbers, brains are better at learning.\" Furthermore, he claimed that with \"superior learning and storing\" capabilities than AIs, being more energy efficient, and that in the future, it might not be possible to add more transistors to a single computer chip, while brains are wired differently and have more potential for storage and computing power, OIs can potentially harness more power than current computers.\nSome researchers claim that even though human brains are slower than machines at processing simple information, they are far better at processing complex information as brains can deal with fewer and more uncertain data, perform both sequential and parallel processing, being highly heterogenous, use incomplete datasets, and is said to outperform non-organic machines in decision-making.\nTraining OIs involve the process of biological learning (BL) as opposed to machine learning (ML) for AIs. BL is said to be much more energy efficient than ML.\n\nBioinformatics in OI\nOI generates complex biological data, necessitating sophisticated methods for processing and analysis. Bioinformatics provides the tools and techniques to decipher raw data, uncovering the patterns and insights. A Python interface is currently available for processing and interaction with brain organoids.\n\nIntended functions\nBrain-inspired computing hardware aims to emulate the structure and working principles of the brain and could be used to address current limitations in artificial intelligence technologies. However, brain-inspired silicon chips are still limited in their ability to fully mimic brain function, as most examples are built on digital electronic principles. One study performed OI computation (which they termed Brainoware) by sending and receiving information from the brain organoid using a high-density multielectrode array. By applying spatiotemporal electrical stimulation, nonlinear dynamics, and fading memory properties, as well as unsupervised learning from training data by reshaping the organoid functional connectivity, the study showed the potential of this technology by using it for speech recognition and nonlinear equation prediction in a reservoir computing framework.\n\nEthical concerns\nWhile researchers are hoping to use OI and biological computing to complement traditional silicon-based computing, there are also questions about the ethics of such an approach. Examples of such ethical issues include OIs gaining consciousness and sentience as organoids and the question of the relationship between a stem cell donor (for growing the organoid) and the respective OI system.\nEnforced amnesia and limits on duration of operation without memory reset have been proposed as a way to mitigate the potential risk of silent suffering in brain organoids.\n\nSee also\nBiohybrid system\nCerebral organoid\nArtificial intelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Organoid_intelligence"
    },
    {
        "title": "Pattern theory",
        "text": "Pattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language. Broad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties.\nIn addition to the new algebraic vocabulary, its statistical approach is novel in its aim to:\n\nIdentify the hidden variables of a data set using real world data rather than artificial stimuli, which was previously commonplace.\nFormulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.\nStudy the randomness and variability of these graphs.\nCreate the basic classes of stochastic models applied by listing the deformations of the patterns.\nSynthesize (sample) from the models, not just analyze signals with them.\nThe Brown University Pattern Theory Group was formed in 1972 by Ulf Grenander. Many mathematicians are currently working in this group, noteworthy among them being the Fields Medalist David Mumford.  Mumford regards Grenander as his \"guru\" in Pattern Theory.\n\nSee also\nAbductive reasoning\nAlgebraic statistics\nComputational anatomy\nFormal concept analysis\nGrammar induction\nImage analysis\nInduction\nLattice theory\nSpatial statistics\n\nReferences\nFurther reading\n2007. Ulf Grenander and Michael Miller Pattern Theory: From Representation to Inference. Oxford University Press. Paperback. (ISBN 9780199297061)\n1994. Ulf Grenander General Pattern Theory. Oxford Science Publications. (ISBN 978-0198536710)\n1996. Ulf Grenander Elements of Pattern Theory.  Johns Hopkins University Press. (ISBN 978-0801851889)\n\nExternal links\nPattern Theory Group at Brown University\nPattern Theory: Grenander's Ideas and Examples - a video lecture by David Mumford\nPattern Theory and Applications - graduate course page with material by a Brown University alumnus",
        "url": "https://en.wikipedia.org/wiki/Pattern_theory"
    },
    {
        "title": "Pedagogical agent",
        "text": "A pedagogical agent is a concept borrowed from computer science and artificial intelligence and applied to education, usually as part of an intelligent tutoring system (ITS). It is a simulated human-like interface between the learner and the content, in an educational environment. A pedagogical agent is designed to model the type of interactions between a student and another person. Mabanza and de Wet define it as \"a character enacted by a computer that interacts with the user in a socially engaging manner\". A pedagogical agent can be assigned different roles in the learning environment, such as tutor or co-learner, depending on the desired purpose of the agent. \"A tutor agent plays the role of a teacher, while a co-learner agent plays the role of a learning companion\".\n\nHistory\nThe history of Pedagogical Agents is closely aligned with the history of computer animation. As computer animation progressed, it was adopted by educators to enhance computerized learning by including a lifelike interface between the program and the learner. The first versions of a pedagogical agent were more cartoon than person, like Microsoft's Clippy which helped users of Microsoft Office load and use the program's features in 1997. However, with developments in computer animation, pedagogical agents can now look lifelike. By 2006 there was a call to develop modular, reusable agents to decrease the time and expertise required to create a pedagogical agent. There was also a call in 2009 to enact agent standards. The standardization and re-usability of pedagogical agents is less of an issue since the decrease in cost and widespread availability of animation tools. Individualized pedagogical agents can be found across disciplines including medicine, math, law, language learning, automotive, and armed forces. They are used in applications directed to every age, from preschool to adult.\n\nLearning theories related to pedagogical agent design\nDistributed cognition theory\nDistributed cognition theory is the method in which cognition progresses in the context of collaboration with others. Pedagogical agents can be designed to assist the cognitive transfer to the learner, operating as artifacts or partners with collaborative role in learning. To support the performance of an action by the user, the pedagogical agent can act as a cognitive tool as long as the agent is equipped with the knowledge that the user lacks. The interactions between the user and the pedagogical agent can facilitate a social relationship. The pedagogical agent may fulfill the role of a working partner.\n\nSocio-cultural learning theory\nSocio-cultural learning theory is how the user develops when they are involved in learning activities in which there is interaction with other agents. A pedagogical agent can: intervene when the user requests, provide support for tasks that the user cannot address, and potentially extend the learners cognitive reach. Interaction with the pedagogical agent may elicit a variety of emotions from the learner. The learner may become excited, confused, frustrated, and/or discouraged. These emotions affect the learners' motivation.\n\nExtraneous Cognitive Load\nExtraneous cognitive load is the extra effort being exerted by an individual's working memory due to the way information is being presented. A pedagogical agent can increase the user's cognitive load by distracting them and becoming the focus of their attention, causing split attention between the instructional material and the agent. Agents can reduce the perceived cognitive load by providing narration and personalization that can also promote a user's interest and motivation. While research on the reduction of cognitive load from pedagogical  agents is minimal, more studies have shown that agents do not increase it.\n\nEffectiveness\nIt has been suggested by researchers that pedagogical agents may take on different roles in the learning environment. Examples of these roles are: supplanting, scaffolding, coaching, testing, or demonstrating or modelling a procedure. A pedagogical agent as a tutor has not been demonstrated to add any benefit to an educational strategy in equivalent lessons with and without a pedagogical agent. According to Richard Mayer, there is some support in research for pedagogical agent increasing learning, but only as a presenter of social cues. A co-learner pedagogical agent is believed to increase the student's self-efficacy. By pointing out important features of instructional content, a pedagogical agent can fulfill the signaling function, which research on multimedia learning has shown to enhance learning. Research has demonstrated that human-human interaction may not be completely replaced by pedagogical agents, but learners may prefer the agents to non-agent multimedia systems. This finding is supported by social agency theory.\nMuch like the varying effectiveness of the pedagogical agent roles in the learning environment, agents that take into account the user's affect have had mixed results. Research has shown pedagogical agents that make use of the users’ affect have been found to increase user knowledge retention, motivation, and perceived self-efficacy. However, with such a broad range of modalities in affective expressions, it is often difficult to utilize them. Additionally, having agents detect a user's affective state with precision remains challenging, as displays of affect are different across individuals.\n\nDesign\nAttractiveness\nThe appearance of a pedagogical agent can be manipulated to meet the learning requirements. The attractiveness of a pedagogical agent can enhance student's learning when the users were the opposite gender of the pedagogical agent. Male students prefer a sexy appearance of a female pedagogical agents and dislike the sexy appearance of male agents. Female students were not attracted by the sexy appearance of either male or female pedagogical agents.\n\nAffective Response\nPedagogical agents have reached a point where they can convey and elicit emotion, but also reason about and respond to it. These agents are often designed to elicit and respond to affective actions from users through various modalities such as speech, facial expressions, and body gestures. They respond to the affective state of the given user, and make use of these modalities using a wide array of sensors incorporated into the design of the agent. Specifically in education and training applications, pedagogical agents are often designed to increasingly recognize when users or learners exhibit frustration, boredom, confusion, and states of flow. The added recognition in these agents is a step toward making them more emotionally intelligent, comforting and motivating the users as they interact.\n\nDigital Representation\nThe design of a pedagogical agent often begins with its digital representation, whether it will be 2D or 3D and static or animated. Several studies have developed pedagogical agents that were both static and animated, then evaluated the relative benefits. Similar to other design considerations, the improved learning from static or animated agents remains questionable. One study showed that the appearance of an agent portrayed using a static image can impact a user's recall, based on the visual appearance. Other research found results that suggest static agent images improve learning outcomes. However, several other studies found user's learned more when the pedagogical agent was animated rather than static. Recently a meta-analysis of such research found a negligible improvement in learning via pedagogical agents, suggesting more work needs to be done in the area to support any claims.\n\nReferences\nExternal links\nAI: Artificial Intelligence Research at USC Information Science Institute\nStanford University: Interactive Animated Pedagogical Agents",
        "url": "https://en.wikipedia.org/wiki/Pedagogical_agent"
    },
    {
        "title": "Percept (artificial intelligence)",
        "text": "A percept is the input that an intelligent agent is perceiving at any given moment. It is essentially the same concept as a percept in psychology, except that it is being perceived not by the brain but by the agent. A percept is detected by a sensor, often a camera, processed accordingly, and acted upon by an actuator. Each percept is added to a \"percept sequence\", which is a complete history of each percept ever detected. The agent's action at any instant point may depend on the entire percept sequence up to that particular instant point. An intelligent agent chooses how to act not only based on the current percept, but the percept sequence. The next action is chosen by the agent function, which maps every percept to an action.\nFor example, if a camera were to record a gesture, the agent would process the percepts, calculate the corresponding spatial vectors, examine its percept history, and use the agent program (the application of the agent function) to act accordingly.\n\nExamples\nExamples of percepts include inputs from touch sensors, cameras, infrared sensors, sonar, microphones, mice, and keyboards. A percept can also be a higher-level feature of the data, such as lines, depth, objects, faces, or gestures.\n\nSee also\nMachine perception\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Percept_(artificial_intelligence)"
    },
    {
        "title": "Personality computing",
        "text": "Personality computing is a research field related to artificial intelligence and personality psychology that studies personality by means of computational techniques from different sources, including text, multimedia, and social networks.\n\nOverview\nPersonality computing addresses three main problems involving personality: automatic personality recognition, perception, and synthesis. Automatic personality recognition is the inference of the personality type of target individuals from their digital footprint. Automatic personality perception is the inference of the personality attributed by an observer to a target individual based on some observable behavior. Automatic personality synthesis is the generation of the style or behaviour of artificial personalities in Avatars and virtual agents.\nSelf-assessed personality tests or observer ratings are always exploited as the ground truth for testing and validating the performance of artificial intelligence algorithms for the automatic prediction of personality types. There is a wide variety of personality tests, such as the Myers Briggs Type Indicator (MBTI) or the MMPI, but the most used are tests based on the Five Factor Model such as the Revised NEO Personality Inventory.\nPersonality computing can be considered as an extension or complement of Affective computing, where the former focuses on personality traits and the latter on affective states. A further extension of the two fields is Character Computing which combines various character states and traits including but not limited to personality and affect.\n\nHistory\nPersonality computing began around 2005 with the pioneering research in personality recognition by Shlomo Argamon and later by François Mairesse. These works showed that personality traits could be inferred with reasonable accuracy from text, such as blogs, self-presentations, and email addresses. In 2008, the concept of \"portable personality\" for the distributed management of personality profiles has been developed.\nA few years later, research began in personality recognition and perception from multimodal and social signals, such as recorded meetings and voice calls.\nIn the 2010s, the research focused mainly on personality recognition and perception from social media, helped by the first workshops organized by Fabio Celli. In particular personality was extracted from Facebook, Twitter and Instagram. In the same years, automatic personality synthesis helped improve the coherence of simulated behavior in virtual agents.\nScientific works by Michal Kosinski demonstrated the validity of Personality Computing from different digital footprints, in particular from user preferences such as Facebook page likes, showed that machines can recognize personality better than humans  and raised a warning against Cambridge Analytica and misuse of this kind of technology.\n\nApplications\nPersonality computing techniques, in particular personality recognition and perception, have applications in Social media marketing, where they can help reducing the cost of advertising campaigns through psychological targeting.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Personality_computing"
    },
    {
        "title": "Personoid",
        "text": "Personoid is the concept coined by Stanisław Lem, a Polish science-fiction writer, in Non Serviam, from his book A Perfect Vacuum (1971). His personoids are an abstraction of functions of human mind and they live in computers; they do not need any human-like physical body. \nIn cognitive and software modeling, personoid is a research approach to the development of intelligent autonomous agents.\nIn frame of the IPK (Information, Preferences, Knowledge) architecture, it is a framework of abstract intelligent agent with a  cognitive and structural intelligence. It can be seen as an essence of high intelligent entities.\nFrom the philosophical and systemics perspectives, personoid societies can also be seen as the carriers of a culture. According to N. Gessler, the personoids study can be a base for the research on artificial culture and culture evolution.\n\nPersonoids on TV and cinema\nWelt am Draht (1973)\nThe Thirteenth Floor (1999)\n\nSee also\nAndroid\nHumanoid\nIntelligence\nArtificial Intelligence\nCulture\nComputer Science\nCognitive Science\nAnticipatory science\nMemetics\n\nReferences\nStanisław Lem's book Próżnia Doskonała (1971). The collection of book reviews of nonexistent books. Translated into English by Michael Kandel as A Perfect Vacuum (1983).\nPersonetics.\nPersonoids Organizations Framework: An Approach to Highly Autonomous Software Architectures Archived 2006-09-28 at the Wayback Machine, ENEA Report (1998).\nParadigms of Personoids, Adam M. Gadomski 1997 Archived 2015-08-26 at the Wayback Machine.\nComputer Models of Cultural Evolution. Nicholas Gessler. In EVOLUTION IN THE COMPUTER AGE - Proceedings of the Center for the Study of Evolution and the Origin of Life, edited by David B. and Gary B. Fogel. Jones and Bartlett Publishers, Sudbury, Massachusetts (2002).",
        "url": "https://en.wikipedia.org/wiki/Personoid"
    },
    {
        "title": "Perusall",
        "text": "Perusall is a social web annotation tool intended for use by students at schools and universities. It allows users to annotate the margins of a text in a virtual group setting that is similar to social media—with upvoting, emojis, chat functionality, and notification. It also includes automatic AI grading.\n\nHistory\nPerusall began as a research project at Harvard University. It later became an educational product for students and teachers.\nAs of 2024, Perusall states more than 5 million students have used the tool at over 5,000 educational institutions in 112 countries.\"\n\nFunctionality\nPerusall integrates with learning management systems such as Moodle, Canvas and Blackboard to aid with collaborative annotation. \nThe tool supports annotation of a range of media including text, images, equations, videos, PDFs and snapshots of webpages.\n\nReferences\nExternal links\nOfficial website",
        "url": "https://en.wikipedia.org/wiki/Perusall"
    },
    {
        "title": "POP-11",
        "text": "POP-11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the School of Computer Science at the University of Birmingham, which hosts the main Poplog website.\nPOP-11 is an evolution of the language POP-2, developed in Edinburgh University, and features an open stack model (like Forth, among others). It is mainly procedural, but supports declarative language constructs, including a pattern matcher, and is mostly used for research and teaching in artificial intelligence, although it has features sufficient for many other classes of problems. It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP-11's features is that it supports first-class functions.\nPOP-11 is the core language of the Poplog system. The availability of the compiler and compiler subroutines at run-time (a requirement for incremental compiling) gives it the ability to support a far wider range of extensions (including run-time extensions, such as adding new data-types) than would be possible using only a macro facility. This made it possible for (optional) incremental compilers to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any POP-11 constructs. This made it possible for Poplog to be used by teachers, researchers, and developers who were interested in only one of the languages. The most successful product developed in POP-11 was the Clementine data mining system, developed by ISL. After SPSS bought ISL, they renamed Clementine to SPSS Modeler and decided to port it to C++ and Java, and eventually succeeded with great effort, and perhaps some loss of the flexibility provided by the use of an AI language.\nPOP-11 was for a time available only as part of an expensive commercial package (Poplog), but since about 1999 it has been freely available as part of the open-source software version of Poplog, including various added packages and teaching libraries. An online version of ELIZA using POP-11 is available at Birmingham.\nAt the University of Sussex, David Young used POP-11 in combination with C and Fortran to develop a suite of teaching and interactive development tools for image processing and vision, and has made them available in the Popvision extension to Poplog.\n\nSimple code examples\nHere is an example of a simple POP-11 program:\n\ndefine Double(Source) -> Result;\n    Source*2 -> Result;\nenddefine;\n\nDouble(123) =>\n\nThat prints out:\n\n** 246\n\nThis one includes some list processing:\n\n define RemoveElementsMatching(Element, Source) -> Result;\n     lvars Index;\n     [[%\n     for Index in Source do\n         unless Index = Element or Index matches Element then\n             Index;\n         endunless;\n     endfor;\n     %]] -> Result;\n enddefine;\n\n RemoveElementsMatching(\"the\", [[the cat sat on the mat]]) => ;;; outputs [[cat sat on mat]]\n RemoveElementsMatching(\"the\", [[the cat] [sat on] the mat]) => ;;; outputs [[the cat] [sat on] mat]\n RemoveElementsMatching([[= cat]], [[the cat]] is a [[big cat]]) => ;;; outputs [[is a]]\n\nExamples using the POP-11 pattern matcher, which makes it relatively easy for students to learn to develop sophisticated list-processing programs without having to treat patterns as tree structures accessed by 'head' and 'tail' functions (CAR and CDR in Lisp), can be found in the online introductory tutorial. The matcher is at the heart of\nthe SimAgent (sim_agent) toolkit. Some of the powerful features of the toolkit, such as linking pattern variables to inline code variables, would have been very difficult to implement without the incremental compiler facilities.\n\nSee also\nCOWSEL (aka POP-1) programming language\n\nReferences\nR. Burstall, A. Collins and R. Popplestone, Programming in Pop-2 University Press, Edinburgh, 1968\nD.J.M. Davies, POP-10 Users' Manual, Computer Science Report #25, University of Western Ontario, 1976\nS. Hardy and C. Mellish, 'Integrating Prolog in the Poplog environment', in Implementations of Prolog, Ed., J.A. Campbell, Wiley, New York, 1983, pp 147–162\nR. Barrett, A, Ramsay and A. Sloman, POP-11: a Practical Language for Artificial Intelligence, Ellis Horwood, Chicester, 1985\nM. Burton and N. Shadbolt, POP-11 Programming for Artificial Intelligence, Addison-Wesley, 1987\nJ. Laventhol, Programming in POP-11, Blackwell Scientific Publications Ltd., 1987\nR. Barrett and A. Ramsay, Artificial Intelligence in Practice:Examples in Pop-11, Ellis Horwood, Chicester, 1987.\nM. Sharples et al., Computers and Thought, MIT Press, 1987. (An introduction to Cognitive Science using Pop-11. Online version referenced above.)\nJames Anderson, Ed., Pop-11 Comes of Age: The Advancement of an AI Programming Language, Ellis Horwood, Chichester, 1989\nG. Gazdar and C. Mellish, Natural Language Processing in Pop11/Prolog/Lisp, Addison Wesley, 1989. (read online)\nR. Smith, A. Sloman and J. Gibson, POPLOG's two-level virtual machine support for interactive languages, in Research Directions in Cognitive Science Volume 5: Artificial Intelligence, Eds. D. Sleeman and N. Bernsen, Lawrence Erlbaum Associates, pp. 203–231, 1992. (Available as Cognitive Science Research Report 153, School of Informatics, University of Sussex).\nChris Thornton and Benedict du Boulay, Artificial Intelligence Through Search, Kluwer Academic (Paperback version Intellect Books) Dordrecht Netherlands & Norwell, MA USA (Intellect at Oxford) 1992.\nA. Sloman, Pop-11 Primer, 1999 (Third edition)\n\nExternal links\nOfficial website, Free Poplog Portal\nGetPoplog on GitHub\nInformation about POP-11 teaching materials\nThe Poplog.org website (including partial mirror of Free poplog web site) (currently defunct: see its more recent copy (Jun 17, 2008) @ Internet Archive Wayback Machine)\nAn Overview of POP-11 (Primer for experienced programmers) (alt. PDF)\nWaldek Hebisch produced a small collection of programming examples in Pop-11, showing how it can be used for symbol manipulation, numerical calculation, logic and mathematics.\nComputers and Thought: A practical Introduction to Artificial Intelligence on-line book introducing Cognitive Science through Pop-11.\nThe SimAgent (sim_agent) Toolkit\nPop-11 Eliza in the poplog system. Tutorial on Eliza\nHistory of AI teaching in Pop-11 since about 1976.\n2-D (X) graphics in Pop-11\nObjectclass the object oriented programming extension to Pop-11 (modelled partly on CLOS and supporting multiple inheritance).\nTutorial introduction to object oriented programming in Pop-11.\nFurther references\nOnline documentation on Pop-11 and Poplog\nOnline system documentation, including porting information\nEntry for Pop-11 at HOPL (History of Programming Languages) web site",
        "url": "https://en.wikipedia.org/wiki/POP-11"
    },
    {
        "title": "Principle of rationality",
        "text": "The principle of rationality (or rationality principle) was coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework. It is related to what he called the 'logic of the situation' in an Economica article of 1944/1945, published later in his book The Poverty of Historicism. According to Popper's rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis. Cognitive scientist Allen Newell elaborated on the principle in his account of knowledge level modeling.\n\nPopper\nPopper called for social science to be grounded in what he called situational analysis or situational logic. This requires building models of social situations which include individual actors and their relationship to social institutions, e.g. markets, legal codes, bureaucracies, etc. These models attribute certain aims and information to the actors. This forms the 'logic of the situation', the result of reconstructing meticulously all circumstances of an historical event. The 'principle of rationality' is the assumption that people are instrumental in trying to reach their goals, and this is what drives the model. Popper believed that this model could be continuously refined to approach the objective truth.\nPopper called his principle of rationality nearly empty (a technical term meaning without empirical content) and strictly speaking false, but nonetheless tremendously useful. These remarks earned him a lot of criticism because seemingly he had swerved from his famous Logic of Scientific Discovery.\nAmong the many philosophers having discussed Popper's principle of rationality from the 1960s up to now are Noretta Koertge, R. Nadeau, Viktor J. Vanberg, Hans Albert, E. Matzner, Ian C. Jarvie, Mark A. Notturno, John Wettersten, Ian C. Böhm.\n\nNewell\nIn the context of knowledge-based systems, Newell (in 1982) proposed the following principle of rationality: \"If an agent has knowledge that one of its actions will lead to one of its goals, then the agent will select that action.\" This principle is employed by agents at the knowledge level to move closer to a desired goal. An important philosophical difference between Newell and Popper is that Newell argued that the knowledge level is real in the sense that it exists in nature and is not made up. This allowed Newell to treat the rationality principle as a way of understanding nature and avoid the problems Popper ran into by treating knowledge as non physical and therefore non empirical.\n\nSee also\nHermeneutics\nRational choice\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Principle_of_rationality"
    },
    {
        "title": "Problem solving",
        "text": "Problem solving is the process of achieving a goal by overcoming obstacles, a frequent part of most activities. Problems in need of solutions range from simple personal tasks (e.g. how to turn on an appliance) to complex issues in business and technical fields. The former is an example of simple problem solving (SPS) addressing one issue, whereas the latter is complex problem solving (CPS) with multiple interrelated obstacles. Another classification of problem-solving tasks is into well-defined problems with specific obstacles and goals, and ill-defined problems in which the current situation is troublesome but it is not clear what kind of resolution to aim for. Similarly, one may distinguish formal or fact-based problems requiring psychometric intelligence, versus socio-emotional problems which depend on the changeable emotions of individuals or groups, such as tactful behavior, fashion, or gift choices.\nSolutions require sufficient resources and knowledge to attain the goal. Professionals such as lawyers, doctors, programmers, and consultants are largely problem solvers for issues that require technical skills and knowledge beyond general competence. Many businesses have found profitable markets by recognizing a problem and creating a solution: the more widespread and inconvenient the problem, the greater the opportunity to develop a scalable solution.\nThere are many specialized problem-solving techniques and methods in fields such as science, engineering, business, medicine, mathematics, computer science, philosophy, and social organization. The mental techniques to identify, analyze, and solve problems are studied in psychology and cognitive sciences. Also widely researched are the mental obstacles that prevent people from finding solutions; problem-solving impediments include confirmation bias, mental set, and functional fixedness.\n\nDefinition\nThe term problem solving has a slightly different meaning depending on the discipline. For instance, it is a mental process in psychology and a computerized process in computer science. There are two different types of problems: ill-defined and well-defined; different approaches are used for each. Well-defined problems have specific end goals and clearly expected solutions, while ill-defined problems do not. Well-defined problems allow for more initial planning than ill-defined problems. Solving problems sometimes involves dealing with pragmatics (the way that context contributes to meaning) and semantics (the interpretation of the problem). The ability to understand what the end goal of the problem is, and what rules could be applied, represents the key to solving the problem. Sometimes a problem requires abstract thinking or coming up with a creative solution.\nProblem solving has two major domains: mathematical problem solving and personal problem solving. Each concerns some difficulty or barrier that is encountered.\n\nPsychology\nProblem solving in psychology refers to the process of finding solutions to problems encountered in life. Solutions to these problems are usually situation- or context-specific. The process starts with problem finding and problem shaping, in which the problem is discovered and simplified. The next step is to generate possible solutions and evaluate them. Finally a solution is selected to be implemented and verified. Problems have an end goal to be reached; how you get there depends upon problem orientation (problem-solving coping style and skills) and systematic analysis.\nMental health professionals study the human problem-solving processes using methods such as introspection, behaviorism, simulation, computer modeling, and experiment. Social psychologists look into the person-environment relationship aspect of the problem and independent and interdependent problem-solving methods. Problem solving has been defined as a higher-order cognitive process and intellectual function that requires the modulation and control of more routine or fundamental skills.\nEmpirical research shows many different strategies and factors influence everyday problem solving. Rehabilitation psychologists studying people with frontal lobe injuries have found that deficits in emotional control and reasoning can be re-mediated with effective rehabilitation and could improve the capacity of injured persons to resolve everyday problems. Interpersonal everyday problem solving is dependent upon personal motivational and contextual components. One such component is the emotional valence of \"real-world\" problems, which can either impede or aid problem-solving performance. Researchers have focused on the role of emotions in problem solving, demonstrating that poor emotional control can disrupt focus on the target task, impede problem resolution, and lead to negative outcomes such as fatigue, depression, and inertia. In conceptualization,human problem solving consists of two related processes: problem orientation, and the motivational/attitudinal/affective approach to problematic situations and problem-solving skills. People's strategies cohere with their goals and stem from the process of comparing oneself with others.\n\nCognitive sciences\nAmong the first experimental psychologists to study problem solving were the Gestaltists in Germany, such as Karl Duncker in The Psychology of Productive Thinking (1935). Perhaps best known is the work of Allen Newell and Herbert A. Simon.\nExperiments in the 1960s and early 1970s asked participants to solve relatively simple, well-defined, but not previously seen laboratory tasks. These simple problems, such as the Tower of Hanoi, admitted optimal solutions that could be found quickly, allowing researchers to observe the full problem-solving process. Researchers assumed that these model problems would elicit the characteristic cognitive processes by which more complex \"real world\" problems are solved.\nAn outstanding problem-solving technique found by this research is the principle of decomposition.\n\nComputer science\nMuch of computer science and artificial intelligence involves designing automated systems to solve a specified type of problem: to accept input data and calculate a correct or adequate response, reasonably quickly. Algorithms are recipes or instructions that direct such systems, written into computer programs.\nSteps for designing such systems include problem determination, heuristics, root cause analysis, de-duplication, analysis, diagnosis, and repair. Analytic techniques include linear and nonlinear programming, queuing systems, and simulation. A large, perennial obstacle is to find and fix errors in computer programs: debugging.\n\nLogic\nFormal logic concerns issues like validity, truth, inference, argumentation, and proof. In a problem-solving context, it can be used to formally represent a problem as a theorem to be proved, and to represent the knowledge needed to solve the problem as the premises to be used in a proof that the problem has a solution.\nThe use of computers to prove mathematical theorems using formal logic emerged as the field of automated theorem proving in the 1950s. It included the use of heuristic methods designed to simulate human problem solving, as in the Logic Theory Machine, developed by Allen Newell, Herbert A. Simon and J. C. Shaw, as well as algorithmic methods such as the resolution principle developed by John Alan Robinson.\nIn addition to its use for finding proofs of mathematical theorems, automated theorem-proving has also been used for program verification in computer science. In 1958, John McCarthy proposed the advice taker, to represent information in formal logic and to derive answers to questions using automated theorem-proving. An important step in this direction was made by Cordell Green in 1969, who used a resolution theorem prover for question-answering and for such other applications in artificial intelligence as robot planning.\nThe resolution theorem-prover used by Cordell Green bore little resemblance to human problem solving methods. In response to criticism of that approach from researchers at MIT, Robert Kowalski developed logic programming and SLD resolution, which solves problems by problem decomposition. He has advocated logic for both computer and human problem solving and computational logic to improve human thinking.\n\nEngineering\nWhen products or processes fail, problem solving techniques can be used to develop corrective actions that can be taken to prevent further failures. Such techniques can also be applied to a product or process prior to an actual failure event—to predict, analyze, and mitigate a potential problem in advance. Techniques such as failure mode and effects analysis can proactively reduce the likelihood of problems.\nIn either the reactive or the proactive case, it is necessary to build a causal explanation through a process of diagnosis. In deriving an explanation of effects in terms of causes, abduction generates new ideas or hypotheses (asking \"how?\"); deduction evaluates and refines hypotheses based on other plausible premises (asking \"why?\"); and induction justifies a hypothesis with empirical data (asking \"how much?\"). The objective of abduction is to determine which hypothesis or proposition to test, not which one to adopt or assert. In the Peircean logical system, the logic of abduction and deduction contribute to our conceptual understanding of a phenomenon, while the logic of induction adds quantitative details (empirical substantiation) to our conceptual knowledge.\nForensic engineering is an important technique of failure analysis that involves tracing product defects and flaws. Corrective action can then be taken to prevent further failures.\nReverse engineering attempts to discover the original problem-solving logic used in developing a product by disassembling the product and developing a plausible pathway to creating and assembling its parts.\n\nMilitary science\nIn military science, problem solving is linked to the concept of \"end-states\", the conditions or situations which are the aims of the strategy. Ability to solve problems is important at any military rank, but is essential at the command and control level. It results from deep qualitative and quantitative understanding of possible scenarios. Effectiveness in this context is an evaluation of results: to what extent the end states were accomplished. Planning is the process of determining how to effect those end states.\n\nProcesses\nSome models of problem solving involve identifying a goal and then a sequence of subgoals towards achieving this goal. Andersson, who introduced the ACT-R model of cognition, modelled this collection of goals and subgoals as a goal stack in which the mind contains a stack of goals and subgoals to be completed, and a single task being carried out at any time.\nKnowledge of how to solve one problem can be applied to another problem, in a process known as transfer.\n\nProblem-solving strategies\nProblem-solving strategies are steps to overcoming the obstacles to achieving a goal. The iteration of such strategies over the course of solving a problem is the \"problem-solving cycle\".\nCommon steps in this cycle include recognizing the problem, defining it, developing a strategy to fix it, organizing knowledge and resources available, monitoring progress, and evaluating the effectiveness of the solution. Once a solution is achieved, another problem usually arises, and the cycle starts again.\nInsight is the sudden aha! solution to a problem, the birth of a new idea to simplify a complex situation. Solutions found through insight are often more incisive than those from step-by-step analysis. A quick solution process requires insight to select productive moves at different stages of the problem-solving cycle. Unlike Newell and Simon's formal definition of a move problem, there is no consensus definition of an insight problem.\nSome problem-solving strategies include:\n\nAbstraction\nsolving the problem in a tractable model system to gain insight into the real system\nAnalogy\nadapting the solution to a previous problem which has similar features or mechanisms\nBrainstorming\n(especially among groups of people) suggesting a large number of solutions or ideas and combining and developing them until an optimum solution is found\nBypasses\ntransform the problem into another problem that is easier to solve, bypassing the barrier, then transform that solution back to a solution to the original problem.\nCritical thinking\nanalysis of available evidence and arguments to form a judgement via rational, skeptical, and unbiased evaluation\nDivide and conquer\nbreaking down a large, complex problem into smaller, solvable problems\nHelp-seeking\nobtaining external assistance to deal with obstacles\nHypothesis testing\nassuming a possible explanation to the problem and trying to prove (or, in some contexts, disprove) the assumption\nLateral thinking\napproaching solutions indirectly and creatively\nMeans-ends analysis\nchoosing an action at each step to move closer to the goal\nMorphological analysis\nassessing the output and interactions of an entire system\nObservation / Question\nin the natural sciences an observation is an act or instance of noticing or perceiving and the acquisition of information from a primary source. A question is an utterance which serves as a request for information.\nProof of impossibility\ntry to prove that the problem cannot be solved. The point where the proof fails will be the starting point for solving it\nReduction\ntransforming the problem into another problem for which solutions exist\nResearch\nemploying existing ideas or adapting existing solutions to similar problems\nRoot cause analysis\nidentifying the cause of a problem\nTrial-and-error\ntesting possible solutions until the right one is found\n\nProblem-solving methods\nA3 problem solving – Structured problem improvement approach\nDesign thinking – Processes by which design concepts are developed\nEight Disciplines Problem Solving – Eight disciplines of team-oriented problem solving methodPages displaying short descriptions of redirect targets\nGROW model – Method for goal setting and problem solving\nHelp-seeking – Theory in psychology\nHow to Solve It – Book by George Pólya\nLateral thinking – Manner of solving problems\nOODA loop – Observe–orient–decide–act cycle\nPDCA – Iterative design and management method\nRoot cause analysis – Method of identifying the fundamental causes of faults or problems\nRPR problem diagnosis\nTRIZ – Problem-solving tools\nScientific method – is an empirical method for acquiring knowledge that has characterized the development of science.\nSwarm intelligence – Collective behavior of decentralized, self-organized systems\nSystem dynamics – Study of non-linear complex systems\n\nCommon barriers\nCommon barriers to problem solving include mental constructs that impede an efficient search for solutions. Five of the most common identified by researchers are: confirmation bias, mental set, functional fixedness, unnecessary constraints, and irrelevant information.\n\nConfirmation bias\nConfirmation bias is an unintentional tendency to collect and use data which favors preconceived notions. Such notions may be incidental rather than motivated by important personal beliefs: the desire to be right may be sufficient motivation.\nScientific and technical professionals also experience confirmation bias. One online experiment, for example, suggested that professionals within the field of psychological research are likely to view scientific studies that agree with their preconceived notions more favorably than clashing studies. According to Raymond Nickerson, one can see the consequences of confirmation bias in real-life situations, which range in severity from inefficient government policies to genocide. Nickerson argued that those who killed people accused of witchcraft demonstrated confirmation bias with motivation. Researcher Michael Allen found evidence for confirmation bias with motivation in school children who worked to manipulate their science experiments to produce favorable results.\nHowever, confirmation bias does not necessarily require motivation. In 1960, Peter Cathcart Wason conducted an experiment in which participants first viewed three numbers and then created a hypothesis in the form of a rule that could have been used to create that triplet of numbers. When testing their hypotheses, participants tended to only create additional triplets of numbers that would confirm their hypotheses, and tended not to create triplets that would negate or disprove their hypotheses.\n\nMental set\nMental set is the inclination to re-use a previously successful solution, rather than search for new and better solutions. It is a reliance on habit.\nIt was first articulated by Abraham S. Luchins in the 1940s with his well-known water jug experiments. Participants were asked to fill one jug with a specific amount of water by using other jugs with different maximum capacities. After Luchins gave a set of jug problems that could all be solved by a single technique, he then introduced a problem that could be solved by the same technique, but also by a novel and simpler method. His participants tended to use the accustomed technique, oblivious of the simpler alternative. This was again demonstrated in Norman Maier's 1931 experiment, which challenged participants to solve a problem by using a familiar tool (pliers) in an unconventional manner. Participants were often unable to view the object in a way that strayed from its typical use, a type of mental set known as functional fixedness (see the following section).\nRigidly clinging to a mental set is called fixation, which can deepen to an obsession or preoccupation with attempted strategies that are repeatedly unsuccessful. In the late 1990s, researcher Jennifer Wiley found that professional expertise in a field can create a mental set, perhaps leading to fixation.\nGroupthink, in which each individual takes on the mindset of the rest of the group, can produce and exacerbate mental set. Social pressure leads to everybody thinking the same thing and reaching the same conclusions.\n\nFunctional fixedness\nFunctional fixedness is the tendency to view an object as having only one function, and to be unable to conceive of any novel use, as in the Maier pliers experiment described above. Functional fixedness is a specific form of mental set, and is one of the most common forms of cognitive bias in daily life.\nAs an example, imagine a man wants to kill a bug in his house, but the only thing at hand is a can of air freshener. He may start searching for something to kill the bug instead of squashing it with the can, thinking only of its main function of deodorizing.\nTim German and Clark Barrett describe this barrier: \"subjects become 'fixed' on the design function of the objects, and problem solving suffers relative to control conditions in which the object's function is not demonstrated.\" Their research found that young children's limited knowledge of an object's intended function reduces this barrier Research has also discovered functional fixedness in educational contexts, as an obstacle to understanding: \"functional fixedness may be found in learning concepts as well as in solving chemistry problems.\"\nThere are several hypotheses in regards to how functional fixedness relates to problem solving. It may waste time, delaying or entirely preventing the correct use of a tool.\n\nUnnecessary constraints\nUnnecessary constraints are arbitrary boundaries imposed unconsciously on the task at hand, which foreclose a productive avenue of solution. The solver may become fixated on only one type of solution, as if it were an inevitable requirement of the problem. Typically, this combines with mental set—clinging to a previously successful method.\nVisual problems can also produce mentally invented constraints. A famous example is the dot problem: nine dots arranged in a three-by-three grid pattern must be connected by drawing four straight line segments, without lifting pen from paper or backtracking along a line. The subject typically assumes the pen must stay within the outer square of dots, but the solution requires lines continuing beyond this frame, and researchers have found a 0% solution rate within a brief allotted time.\nThis problem has produced the expression \"think outside the box\". Such problems are typically solved via a sudden insight which leaps over the mental barriers, often after long toil against them. This can be difficult depending on how the subject has structured the problem in their mind, how they draw on past experiences, and how well they juggle this information in their working memory. In the example, envisioning the dots connected outside the framing square requires visualizing an unconventional arrangement, which is a strain on working memory.\n\nIrrelevant information\nIrrelevant information is a specification or data presented in a problem that is unrelated to the solution. If the solver assumes that all information presented needs to be used, this often derails the problem solving process, making relatively simple problems much harder.\nFor example: \"Fifteen percent of the people in Topeka have unlisted telephone numbers. You select 200 names at random from the Topeka phone book. How many of these people have unlisted phone numbers?\" The \"obvious\" answer is 15%, but in fact none of the unlisted people would be listed among the 200. This kind of \"trick question\" is often used in aptitude tests or cognitive evaluations. Though not inherently difficult, they require independent thinking that is not necessarily common. Mathematical word problems often include irrelevant qualitative or numerical information as an extra challenge.\n\nAvoiding barriers by changing problem representation\nThe disruption caused by the above cognitive biases can depend on how the information is represented: visually, verbally, or mathematically. A classic example is the Buddhist monk problem:\n\nA Buddhist monk begins at dawn one day walking up a mountain, reaches the top at sunset, meditates at the top for several days until one dawn when he begins to walk back to the foot of the mountain, which he reaches at sunset. Making no assumptions about his starting or stopping or about his pace during the trips, prove that there is a place on the path which he occupies at the same hour of the day on the two separate journeys.\nThe problem cannot be addressed in a verbal context, trying to describe the monk's progress on each day. It becomes much easier when the paragraph is represented mathematically by a function: one visualizes a graph whose horizontal axis is time of day, and whose vertical axis shows the monk's position (or altitude) on the path at each time. Superimposing the two journey curves, which traverse opposite diagonals of a rectangle, one sees they must cross each other somewhere. The visual representation by graphing has resolved the difficulty.\nSimilar strategies can often improve problem solving on tests.\n\nOther barriers for individuals\nPeople who are engaged in problem solving tend to overlook subtractive changes, even those that are critical elements of efficient solutions. For example, a city planner may decide that the solution to decrease traffic congestion would be to add another lane to a highway, rather than finding ways to reduce the need for the highway in the first place. This tendency to solve by first, only, or mostly creating or adding elements, rather than by subtracting elements or processes is shown to intensify with higher cognitive loads such as information overload.\n\nDreaming: problem solving without waking consciousness\nPeople can also solve problems while they are asleep. There are many reports of scientists and engineers who solved problems in their dreams. For example, Elias Howe, inventor of the sewing machine, figured out the structure of the bobbin from a dream.\nThe chemist August Kekulé was considering how benzene arranged its six carbon and hydrogen atoms. Thinking about the problem, he dozed off, and dreamt of dancing atoms that fell into a snakelike pattern, which led him to discover the benzene ring. As Kekulé wrote in his diary,\n\nOne of the snakes seized hold of its own tail, and the form whirled mockingly before my eyes. As if by a flash of lightning I awoke; and this time also I spent the rest of the night in working out the consequences of the hypothesis.\nThere also are empirical studies of how people can think consciously about a problem before going to sleep, and then solve the problem with a dream image. Dream researcher William C. Dement told his undergraduate class of 500 students that he wanted them to think about an infinite series, whose first elements were OTTFF, to see if they could deduce the principle behind it and to say what the next elements of the series would be. He asked them to think about this problem every night for 15 minutes before going to sleep and to write down any dreams that they then had. They were instructed to think about the problem again for 15 minutes when they awakened in the morning.\nThe sequence OTTFF is the first letters of the numbers: one, two, three, four, five. The next five elements of the series are SSENT (six, seven, eight, nine, ten). Some of the students solved the puzzle by reflecting on their dreams. One example was a student who reported the following dream:\n\nI was standing in an art gallery, looking at the paintings on the wall. As I walked down the hall, I began to count the paintings: one, two, three, four, five. As I came to the sixth and seventh, the paintings had been ripped from their frames. I stared at the empty frames with a peculiar feeling that some mystery was about to be solved. Suddenly I realized that the sixth and seventh spaces were the solution to the problem!\nWith more than 500 undergraduate students, 87 dreams were judged to be related to the problems students were assigned (53 directly related and 34 indirectly related). Yet of the people who had dreams that apparently solved the problem, only seven were actually able to consciously know the solution. The rest (46 out of 53) thought they did not know the solution.\nAlbert Einstein believed that much problem solving goes on unconsciously, and the person must then figure out and formulate consciously what the mindbrain has already solved. He believed this was his process in formulating the theory of relativity: \"The creator of the problem possesses the solution.\" Einstein said that he did his problem solving without words, mostly in images. \"The words or the language, as they are written or spoken, do not seem to play any role in my mechanism of thought. The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be 'voluntarily' reproduced and combined.\"\n\nCognitive sciences: two schools\nProblem-solving processes differ across knowledge domains and across levels of expertise. For this reason, cognitive sciences findings obtained in the laboratory cannot necessarily generalize to problem-solving situations outside the laboratory. This has led to a research emphasis on real-world problem solving, since the 1990s. This emphasis has been expressed quite differently in North America and Europe, however. Whereas North American research has typically concentrated on studying problem solving in separate, natural knowledge domains, much of the European research has focused on novel, complex problems, and has been performed with computerized scenarios.\n\nEurope\nIn Europe, two main approaches have surfaced, one initiated by Donald Broadbent in the United Kingdom and the other one by Dietrich Dörner in Germany. The two approaches share an emphasis on relatively complex, semantically rich, computerized laboratory tasks, constructed to resemble real-life problems. The approaches differ somewhat in their theoretical goals and methodology. The tradition initiated by Broadbent emphasizes the distinction between cognitive problem-solving processes that operate under awareness versus outside of awareness, and typically employs mathematically well-defined computerized systems. The tradition initiated by Dörner, on the other hand, has an interest in the interplay of the cognitive, motivational, and social components of problem solving, and utilizes very complex computerized scenarios that contain up to 2,000 highly interconnected variables.\n\nNorth America\nIn North America, initiated by the work of Herbert A. Simon on \"learning by doing\" in semantically rich domains, researchers began to investigate problem solving separately in different natural knowledge domains—such as physics, writing, or chess playing—rather than attempt to extract a global theory of problem solving. These researchers have focused on the development of problem solving within certain domains, that is on the development of expertise.\nAreas that have attracted rather intensive attention in North America include:\n\ncalculation\ncomputer skills\ngame playing\nlawyers' reasoning\nmanagerial problem solving\nmathematical problem solving\nmechanical problem solving\npersonal problem solving\npolitical decision making\nproblem solving in electronics\nproblem solving for innovations and inventions: TRIZ\nreading\nsocial problem solving\nwriting\n\nCharacteristics of complex problems\nComplex problem solving (CPS) is distinguishable from simple problem solving (SPS). In SPS there is a singular and simple obstacle. In CPS there may be multiple simultaneous obstacles. For example, a surgeon at work has far more complex problems than an individual deciding what shoes to wear. As elucidated by Dietrich Dörner, and later expanded upon by Joachim Funke, complex problems have some typical characteristics, which include:\n\ncomplexity (large numbers of items, interrelations, and decisions)\nenumerability\nheterogeneity\nconnectivity (hierarchy relation, communication relation, allocation relation)\ndynamics (time considerations)\ntemporal constraints\ntemporal sensitivity\nphase effects\ndynamic unpredictability\nintransparency (lack of clarity of the situation)\ncommencement opacity\ncontinuation opacity\npolytely (multiple goals)\ninexpressivenes\nopposition\ntransience\n\nCollective problem solving\nPeople solve problems on many different levels—from the individual to the civilizational. Collective problem solving refers to problem solving performed collectively. Social issues and global issues can typically only be solved collectively.\nThe complexity of contemporary problems exceeds the cognitive capacity of any individual and requires different but complementary varieties of expertise and collective problem solving ability.\nCollective intelligence is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals.\nIn collaborative problem solving people work together to solve real-world problems. Members of problem-solving groups share a common concern, a similar passion, and/or a commitment to their work. Members can ask questions, wonder, and try to understand common issues. They share expertise, experiences, tools, and methods. Groups may be fluid based on need, may only occur temporarily to finish an assigned task, or may be more permanent depending on the nature of the problems.\nFor example, in the educational context, members of a group may all have input into the decision-making process and a role in the learning process. Members may be responsible for the thinking, teaching, and monitoring of all members in the group. Group work may be coordinated among members so that each member makes an equal contribution to the whole work. Members can identify and build on their individual strengths so that everyone can make a significant contribution to the task. Collaborative group work has the ability to promote critical thinking skills, problem solving skills, social skills, and self-esteem. By using collaboration and communication, members often learn from one another and construct meaningful knowledge that often leads to better learning outcomes than individual work.\nCollaborative groups require joint intellectual efforts between the members and involve social interactions to solve problems together. The knowledge shared during these interactions is acquired during communication, negotiation, and production of materials. Members actively seek information from others by asking questions. The capacity to use questions to acquire new information increases understanding and the ability to solve problems.\nIn a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that proactively \"augmenting human intellect\" would yield a multiplier effect in group problem solving: \"Three people working together in this augmented mode [would] seem to be more than three times as effective in solving a complex problem as is one augmented person working alone\".\nHenry Jenkins, a theorist of new media and media convergence, draws on the theory that collective intelligence can be attributed to media convergence and participatory culture. He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating \"whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals\". Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contributes to the development of such skills.\nCollective impact is the commitment of a group of actors from different sectors to a common agenda for solving a specific social problem, using a structured form of collaboration.\nAfter World War II the UN, the Bretton Woods organization, and the WTO were created. Collective problem solving on the international level crystallized around these three types of organization from the 1980s onward. As these global institutions remain state-like or state-centric it is unsurprising that they perpetuate state-like or state-centric approaches to collective problem solving rather than alternative ones.\nCrowdsourcing is a process of accumulating ideas, thoughts, or information from many independent participants, with aim of finding the best solution for a given challenge. Modern information technologies allow for many people to be involved and facilitate managing their suggestions in ways that provide good results. The Internet allows for a new capacity of collective (including planetary-scale) problem solving.\n\nSee also\nActuarial science – Statistics applied to risk in insurance and other financial products\nAnalytical skill – Crucial skill in all different fields of work and life\nCreative problem-solving – Mental process of problem solving\nCollective intelligence – Group intelligence that emerges from collective efforts\nCommunity of practice\nCoworking – Practice of independent contractors or scientists sharing office space without supervision\nCrowdsolving – Sourcing services or funds from a groupPages displaying short descriptions of redirect targets\nDivergent thinking – A process of generating creative ideas\nGrey problem\nInnovation – Practical implementation of improvements\nInstrumentalism – Position in the philosophy of science\nProblem-posing education – Method of teaching coined by Paulo Freire\nProblem statement – Description of an issue\nProblem structuring methods\nShared intentionality – Ability to engage with others' psychological states\nStructural fix\nSubgoal labeling – Cognitive process\nTroubleshooting – Form of problem solving, often applied to repair failed products or processes\nWicked problem – Problem that is difficult or impossible to solve\n\nNotes\nFurther reading\nBeckmann, Jens F.; Guthke, Jürgen (1995). \"Complex problem solving, intelligence, and learning ability\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 177–200.\nBrehmer, Berndt (1995). \"Feedback delays in dynamic decision making\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 103–130.\nBrehmer, Berndt; Dörner, D. (1993). \"Experiments with computer-simulated microworlds: Escaping both the narrow straits of the laboratory and the deep blue sea of the field study\". Computers in Human Behavior. 9 (2–3): 171–184. doi:10.1016/0747-5632(93)90005-D.\nDörner, D. (1992). \"Über die Philosophie der Verwendung von Mikrowelten oder 'Computerszenarios' in der psychologischen Forschung\" [On the proper use of microworlds or \"computer scenarios\" in psychological research]. In Gundlach, H. (ed.). Psychologische Forschung und Methode: Das Versprechen des Experiments. Festschrift für Werner Traxel (in German). Passau, Germany: Passavia-Universitäts-Verlag. pp. 53–87.\nEyferth, K.; Schömann, M.; Widowski, D. (1986). \"Der Umgang von Psychologen mit Komplexität\" [On how psychologists deal with complexity]. Sprache & Kognition (in German). 5: 11–26.\nFunke, Joachim (1993). \"Microworlds based on linear equation systems: A new approach to complex problem solving and experimental results\" (PDF). In Strube, G.; Wender, K.-F. (eds.). The cognitive psychology of knowledge. Amsterdam: Elsevier Science Publishers. pp. 313–330.\nFunke, Joachim (1995). \"Experimental research on complex problem solving\" (PDF). In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 243–268.\nFunke, U. (1995). \"Complex problem solving in personnel selection and training\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 219–240.\nGroner, M.; Groner, R.; Bischof, W. F. (1983). \"Approaches to heuristics: A historical review\". In Groner, R.; Groner, M.; Bischof, W. F. (eds.). Methods of heuristics. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 1–18.\nHayes, J. (1980). The complete problem solver. Philadelphia: The Franklin Institute Press.\nHuber, O. (1995). \"Complex problem solving as multistage decision making\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 151–173.\nHübner, Ronald (1989). \"Methoden zur Analyse und Konstruktion von Aufgaben zur kognitiven Steuerung dynamischer Systeme\" [Methods for the analysis and construction of dynamic system control tasks] (PDF). Zeitschrift für Experimentelle und Angewandte Psychologie (in German). 36: 221–238.\nHunt, Earl (1991). \"Some comments on the study of complexity\". In Sternberg, R. J.; Frensch, P. A. (eds.). Complex problem solving: Principles and mechanisms. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 383–395. ISBN 978-1-317-78386-2.\nHussy, W. (1985). \"Komplexes Problemlösen—Eine Sackgasse?\" [Complex problem solving—a dead end?]. Zeitschrift für Experimentelle und Angewandte Psychologie (in German). 32: 55–77.\nKluwe, R. H. (1993). \"Chapter 19 Knowledge and Performance in Complex Problem Solving\". The Cognitive Psychology of Knowledge. Advances in Psychology. Vol. 101. pp. 401–423. doi:10.1016/S0166-4115(08)62668-0. ISBN 978-0-444-89942-2.\nKluwe, R. H. (1995). \"Single case studies and models of complex problem solving\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 269–291.\nKolb, S.; Petzing, F.; Stumpf, S. (1992). \"Komplexes Problemlösen: Bestimmung der Problemlösegüte von Probanden mittels Verfahren des Operations Research—ein interdisziplinärer Ansatz\" [Complex problem solving: determining the quality of human problem solving by operations research tools—an interdisciplinary approach]. Sprache & Kognition (in German). 11: 115–128.\nKrems, Josef F. (1995). \"Cognitive flexibility and complex problem solving\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 201–218.\nMelzak, Z. (1983). Bypasses: A Simple Approach to Complexity. London, UK: Wiley.\nMüller, H. (1993). Komplexes Problemlösen: Reliabilität und Wissen [Complex problem solving: Reliability and knowledge] (in German). Bonn, Germany: Holos.\nParadies, M.W.; Unger, L. W. (2000). TapRooT—The System for Root Cause Analysis, Problem Investigation, and Proactive Improvement. Knoxville, Tenn.: System Improvements.\nPutz-Osterloh, Wiebke (1993). \"Chapter 15 Strategies for Knowledge Acquisition and Transfer of Knowledge in Dynamic Tasks\". The Cognitive Psychology of Knowledge. Advances in Psychology. Vol. 101. pp. 331–350. doi:10.1016/S0166-4115(08)62664-3. ISBN 978-0-444-89942-2.\nRiefer, David M.; Batchelder, William H. (1988). \"Multinomial modeling and the measurement of cognitive processes\" (PDF). Psychological Review. 95 (3): 318–339. doi:10.1037/0033-295x.95.3.318. S2CID 14994393. Archived from the original (PDF) on 2018-11-25.\nSchaub, H. (1993). Modellierung der Handlungsorganisation (in German). Bern, Switzerland: Hans Huber.\nStrauß, B. (1993). Konfundierungen beim Komplexen Problemlösen. Zum Einfluß des Anteils der richtigen Lösungen (ArL) auf das Problemlöseverhalten in komplexen Situationen [Confoundations in complex problem solving. On the influence of the degree of correct solutions on problem solving in complex situations] (in German). Bonn, Germany: Holos.\nStrohschneider, S. (1991). \"Kein System von Systemen! Kommentar zu dem Aufsatz 'Systemmerkmale als Determinanten des Umgangs mit dynamischen Systemen' von Joachim Funke\" [No system of systems! Reply to the paper 'System features as determinants of behavior in dynamic task environments' by Joachim Funke]. Sprache & Kognition (in German). 10: 109–113.\nTonelli, Marcello (2011). Unstructured Processes of Strategic Decision-Making. Saarbrücken, Germany: Lambert Academic Publishing. ISBN 978-3-8465-5598-9.\nVan Lehn, Kurt (1989). \"Problem solving and cognitive skill acquisition\". In Posner, M. I. (ed.). Foundations of cognitive science (PDF). Cambridge, Mass.: MIT Press. pp. 527–579.\nWisconsin Educational Media Association (1993), Information literacy: A position paper on information problem-solving, WEMA Publications, vol. ED 376 817, Madison, Wis.{{citation}}:  CS1 maint: location missing publisher (link) (Portions adapted from Michigan State Board of Education's Position Paper on Information Processing Skills, 1992.)\n\nExternal links\n Learning materials related to Solving Problems at Wikiversity",
        "url": "https://en.wikipedia.org/wiki/Problem_solving"
    },
    {
        "title": "Progress in artificial intelligence",
        "text": "Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. AI applications have been used in a wide range of fields including medical diagnosis, finance, robotics, law, video games, agriculture, and scientific discovery. However, many AI applications are not perceived as AI: \"A lot of cutting-edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\" In the late 1990s and early 2000s, AI technology became widely used as elements of larger systems, but the field was rarely credited for these successes at the time.\nKaplan and Haenlein structure artificial intelligence along three evolutionary stages:\n\nArtificial narrow intelligence – AI capable only of specific tasks;\nArtificial general intelligence – AI with ability in several areas, and able to autonomously solve problems they were never even designed for;\nArtificial superintelligence – AI capable of general tasks, including scientific creativity, social skills, and general wisdom.\nTo allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems. Such tests have been termed subject-matter expert Turing tests. Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\nHumans still substantially outperform both GPT-4 and models trained on the ConceptARC benchmark that scored 60% on most, and 77% on one category, while humans 91% on all and 97% on one category.\n\nCurrent performance in specific areas\nThere are many useful abilities that can be described as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas.\nAI, like electricity or the steam engine, is a general-purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at. Some versions of Moravec's paradox observe that humans are more likely to outperform machines in areas such as physical dexterity that have been the direct target of natural selection. While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets. Researcher Andrew Ng has suggested, as a \"highly imperfect rule of thumb\", that \"almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI.\"\nGames provide a high-profile benchmark for assessing rates of progress; many games have a large professional player base and a well-established competitive rating system. AlphaGo brought the era of classical board-game benchmarks to a close when Artificial Intelligence proved their competitive edge over humans in 2016. Deep Mind's AlphaGo AI software program defeated the world's best professional Go Player Lee Sedol. Games of imperfect knowledge provide new challenges to AI in the area of game theory; the most prominent milestone in this area was brought to a close by Libratus' poker victory in 2017. E-sports continue to provide additional benchmarks; Facebook AI, Deepmind, and others have engaged with the popular StarCraft franchise of videogames.\nBroad classes of outcome for an AI test may be given as:\n\noptimal: it is not possible to perform better (note: some of these entries were solved by humans)\nsuper-human: performs better than all humans\nhigh-human: performs better than most humans\npar-human: performs similarly to most humans\nsub-human: performs worse than most humans\n\nOptimal\nTic-tac-toe\nConnect Four: 1988\nCheckers (aka 8x8 draughts): Weakly solved (2007)\nRubik's Cube: Mostly solved (2010)\nHeads-up limit hold'em poker: Statistically optimal in the sense that \"a human lifetime of play is not sufficient to establish with statistical significance that the strategy is not an exact solution\" (2015)\n\nSuper-human\nOthello (aka reversi): c. 1997\nScrabble: 2006\nBackgammon: c. 1995–2002\nChess: Supercomputer (c. 1997); Personal computer (c. 2006); Mobile phone (c. 2009); Computer defeats human + computer (c. 2017)\nJeopardy!: Question answering, although the machine did not use speech recognition (2011)\nArimaa: 2015\nShogi: c. 2017\nGo: 2017\nHeads-up no-limit hold'em poker: 2017\nSix-player no-limit hold'em poker: 2019\nGran Turismo Sport: 2022\n\nHigh-human\nCrosswords: c. 2012\nFreeciv: 2016\nDota 2: 2018\nBridge card-playing: According to a 2009 review, \"the best programs are attaining expert status as (bridge) card players\", excluding bidding.\nStarCraft II: 2019\nMahjong: 2019\nStratego: 2022\nNo-Press Diplomacy: 2022\nHanabi: 2022\nNatural language processing\n\nPar-human\nOptical character recognition for ISO 1073-1:1976 and similar special characters.\nClassification of images\nHandwriting recognition\nFacial recognition\nVisual question answering\nSQuAD 2.0 English reading-comprehension benchmark (2019)\nSuperGLUE English-language understanding benchmark (2020)\nSome school science exams (2019)\nSome tasks based on Raven's Progressive Matrices\nMany Atari 2600 games (2015)\n\nSub-human\nOptical character recognition for printed text (nearing par-human for Latin-script typewritten text)\nObject recognition\nVarious robotics tasks that may require advances in robot hardware as well as AI, including:\nStable bipedal locomotion: Bipedal robots can walk, but are less stable than human walkers (as of 2017)\nHumanoid soccer\nSpeech recognition: \"nearly equal to human performance\" (2017)\nExplainability. Current medical systems can diagnose certain medical conditions well, but cannot explain to users why they made the diagnosis.\nMany tests of fluid intelligence (2020)\nBongard visual cognition problems, such as the Bongard-LOGO benchmark (2020)\nVisual Commonsense Reasoning (VCR) benchmark (as of 2020)\nStock market prediction: Financial data collection and processing using Machine Learning algorithms\nAngry Birds video game, as of 2020\nVarious tasks that are difficult to solve without contextual knowledge, including:\nTranslation\nWord-sense disambiguation\n\nProposed tests of artificial intelligence\nIn his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis. The Turing test is now considered too exploitable to be a meaningful benchmark.\nThe Feigenbaum test, proposed by the inventor of expert systems, tests a machine's knowledge and expertise about a specific subject. A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior.\nProposed \"universal intelligence\" tests aim to compare how well machines, humans, and even non-human animals perform on problem sets that are generic as possible. At an extreme, the test suite can contain every possible problem, weighted by Kolmogorov complexity; however, these problem sets tend to be dominated by impoverished pattern-matching exercises where a tuned AI can easily exceed human performance levels.\n\nExams\nAccording to OpenAI, in 2023 ChatGPT GPT-4 scored the 90th percentile on the Uniform Bar Exam. On the SATs, GPT-4 scored the 89th percentile on math, and the 93rd percentile in Reading & Writing. On the GREs, it scored on the 54th percentile on the writing test, 88th percentile on the quantitative section, and 99th percentile on the verbal section. It scored in the 99th to 100th percentile on the 2020 USA Biology Olympiad semifinal exam. It scored a perfect \"5\" on several AP exams.\nIndependent researchers found in 2023 that ChatGPT GPT-3.5 \"performed at or near the passing threshold\" for the three parts of the United States Medical Licensing Examination. GPT-3.5 was also assessed to attain a low, but passing, grade from exams for four law school courses at the University of Minnesota. GPT-4 passed a text-based radiology board–style examination.\n\nCompetitions\nMany competitions and prizes, such as the Imagenet Challenge, promote research in artificial intelligence. The most common areas of competition include general machine intelligence, conversational behavior, data-mining, robotic cars, and robot soccer as well as conventional games.\n\nPast and current predictions\nAn expert poll around 2016, conducted by Katja Grace of the Future of Humanity Institute and associates, gave median estimates of 3 years for championship Angry Birds, 4 years for the World Series of Poker, and 6 years for StarCraft. On more subjective tasks, the poll gave 6 years for folding laundry as well as an average human worker, 7–10 years for expertly answering 'easily Googleable' questions, 8 years for average speech transcription, 9 years for average telephone banking, and 11 years for expert songwriting, but over 30 years for writing a New York Times bestseller or winning the Putnam math competition.\n\nChess\nAn AI defeated a grandmaster in a regulation tournament game for the first time in 1988; rebranded as Deep Blue, it beat the reigning human world chess champion in 1997 (see Deep Blue versus Garry Kasparov).\n\nGo\nAlphaGo defeated a European Go champion in October 2015, and Lee Sedol in March 2016, one of the world's top players (see AlphaGo versus Lee Sedol). According to Scientific American and other sources, most observers had expected superhuman Computer Go performance to be at least a decade away.\n\nHuman-level artificial general intelligence (AGI)\nAI pioneer and economist Herbert A. Simon inaccurately predicted in 1965: \"Machines will be capable, within twenty years, of doing any work a man can do\". Similarly, in 1970 Marvin Minsky wrote that \"Within a generation... the problem of creating artificial intelligence will substantially be solved.\"\nFour polls conducted in 2012 and 2013 suggested that the median estimate among experts for when AGI would arrive was 2040 to 2050, depending on the poll.\nThe Grace poll around 2016 found results varied depending on how the question was framed. Respondents asked to estimate \"when unaided machines can accomplish every task better and more cheaply than human workers\" gave an aggregated median answer of 45 years and a 10% chance of it occurring within 9 years. Other respondents asked to estimate \"when all occupations are fully automatable. That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers\" estimated a median of 122 years and a 10% probability of 20 years. The median response for when \"AI researcher\" could be fully automated was around 90 years. No link was found between seniority and optimism, but Asian researchers were much more optimistic than North American researchers on average; Asians predicted 30 years on average for \"accomplish every task\", compared with the 74 years predicted by North Americans.\n\nSee also\nApplications of artificial intelligence\nList of artificial intelligence projects\nList of emerging technologies\n\nReferences\nNotes\nExternal links\nMIRI database of predictions about AGI",
        "url": "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence"
    },
    {
        "title": "Psychology of reasoning",
        "text": "The psychology of reasoning (also known as the cognitive science of reasoning) is the study of how people reason, often broadly defined as the process of drawing conclusions to inform how people solve problems and make decisions. It overlaps with psychology, philosophy, linguistics, cognitive science, artificial intelligence, logic, and probability theory.\nPsychological experiments on how humans and other animals reason have been carried out for over 100 years. An enduring question is whether or not people have the capacity to be rational. Current research in this area addresses various questions about reasoning, rationality, judgments, intelligence, relationships between emotion and reasoning, and development.\n\nEveryday reasoning\nOne of the most obvious areas in which people employ reasoning is with sentences in everyday language. Most experimentation on deduction has been carried out on hypothetical thought, in particular, examining how people reason about conditionals, e.g., If A then B. Participants in experiments make the modus ponens inference, given the indicative conditional If A then B, and given the premise A, they conclude B. However, given the indicative conditional and the minor premise for the modus tollens inference, not-B, about half of the participants in experiments conclude not-A and the remainder concludes that nothing follows.\nThe ease with which people make conditional inferences is affected by context, as demonstrated in the well-known selection task developed by Peter Wason. Participants are better able to test a conditional in an ecologically relevant context, e.g., if the envelope is sealed then it must have a 50 cent stamp on it compared to one that contains symbolic content, e.g., if the letter is a vowel then the number is even. Background knowledge can also lead to the suppression of even the simple modus ponens inference  Participants given the conditional if Lisa has an essay to write then she studies late in the library and the premise Lisa has an essay to write  make the modus ponens inference 'she studies late in the library', but the inference is suppressed when they are also given a second conditional if the library stays open then she studies late in the library. Interpretations of the suppression effect are controversial\nOther investigations of propositional inference examine how people think about disjunctive alternatives, e.g., A or else B, and how they reason about negation, e.g., It is not the case that A and B. Many experiments have been carried out to examine how people make relational inferences, including comparisons, e.g., A is better than B. Such investigations also concern spatial inferences, e.g. A is in front of B and temporal inferences, e.g. A occurs before B. Other common tasks include categorical syllogisms, used to examine how people reason about quantifiers such as All or Some, e.g., Some of the A are not B. For example if all A are B and some B are C, what (if anything) follows?\n\nTheories of reasoning\nThere are several alternative theories of the cognitive processes that human reasoning is based on. One view is that people rely on a mental logic consisting of formal (abstract or syntactic) inference rules similar to those developed by logicians in the propositional calculus. Another view is that people rely on domain-specific or content-sensitive rules of inference. A third view is that people rely on mental models, that is, mental representations that correspond to imagined possibilities. A fourth view is that people compute probabilities.\nOne controversial theoretical issue is the identification of an appropriate competence model, or a standard against which to compare human reasoning. Initially classical logic was chosen as a competence model. Subsequently, some researchers opted for non-monotonic logic and Bayesian probability. Research on mental models and reasoning has led to the suggestion that people are rational in principle but err in practice. Connectionist approaches towards reasoning have also been proposed.\nDespite the ongoing debate about the cognitive processes involved in human reasoning, recent research has shown that multiple approaches can be useful in modeling human thinking. For instance, studies have found that people's reasoning is often influenced by their prior beliefs, which can be modeled using Bayesian probability theory. Additionally, research on mental models has shown that people tend to reason about problems by constructing multiple mental representations of the situation, which can help them to identify relevant features and make inferences based on their understanding of the problem. Moreover, connectionist approaches to reasoning have also gained attention, which focus on the neural network models that can learn from data and generalize to new situations.\n\nDevelopment of reasoning\nIt is an active question in psychology how, why, and when the ability to reason develops from infancy to adulthood. Jean Piaget's theory of cognitive development posited general mechanisms and stages in the development of reasoning from infancy to adulthood. According to the neo-Piagetian theories of cognitive development, changes in reasoning with development come from increasing working memory capacity, increasing speed of processing, and enhanced executive functions and control. Increasing self-awareness is also an important factor.\nIn their book The Enigma of Reason, the cognitive scientists Hugo Mercier and Dan Sperber put forward an \"argumentative\" theory of reasoning, claiming that humans evolved to reason primarily to justify our beliefs and actions and to convince others in a social environment. Key evidence for their theory includes the errors in reasoning that solitary individuals are prone to when their arguments are not criticized, such as logical fallacies, and how groups become much better at performing cognitive reasoning tasks when they communicate with one another and can evaluate each other's arguments. Sperber and Mercier offer one attempt to resolve the apparent paradox that the confirmation bias is so strong despite the function of reasoning naively appearing to be to come to veridical conclusions about the world.\nThe study of the development of reasoning abilities is an ongoing area of research in psychology, and multiple factors have been proposed to explain how, why, and when reasoning develops from infancy to adulthood. Recent research has suggested that early experiences and social interactions play a critical role in the development of reasoning abilities. For example, studies have shown that infants as young as six months old can engage in basic logical reasoning, such as reasoning about the relationship between objects and their properties. Furthermore, research has highlighted the importance of parental interaction and cognitive stimulation in the development of children's reasoning abilities. Additionally, studies have suggested that cultural factors, such as educational practices and the emphasis on critical thinking, can also influence the development of reasoning skills across different populations.\n\nDifferent sorts of reasoning\nPhilip Johnson-Laird trying to taxonomize thought, distinguished between goal-directed thinking and thinking without goal, noting that association was involved in unrelated reading. He argues that goal directed reasoning can be classified based on the problem space involved in a solution, citing Allen Newell and Herbert A. Simon.\nInductive reasoning makes broad generalizations from specific cases or observations. In this process of reasoning, general assertions are made based on past specific pieces of evidence. This kind of reasoning allows the conclusion to be false even if the original statement is true. For example, if one observes a college athlete, one makes predictions and assumptions about other college athletes based on that one observation. Scientists use inductive reasoning to create theories and hypotheses. Philip Johnson-Laird distinguished inductive from deductive reasoning, in that the former creates semantic information while the later does not .\nIn opposition, deductive reasoning is a basic form of valid reasoning. In this reasoning process a person starts with a known claim or a general belief and from there asks what follows from these foundations or how will these premises influence other beliefs. In other words, deduction starts with a hypothesis and examines the possibilities to reach a conclusion. Deduction helps people understand why their predictions are wrong and indicates that their prior knowledge or beliefs are off track. An example of deduction can be seen in the scientific method when testing hypotheses and theories. Although the conclusion usually corresponds and therefore proves the hypothesis, there are some cases where the conclusion is logical, but the generalization is not. For example, the argument, \"All young girls wear skirts; Julie is a young girl; therefore, Julie wears skirts\" is valid logically, but is not sound because the first premise isn't true.\nThe syllogism is a form of deductive reasoning in which two statements reach a logical conclusion. With this reasoning, one statement could be \"Every A is B\" and another could be \"This C is A\". Those two statements could then lead to the conclusion that \"This C is B\". These types of syllogisms are used to test deductive reasoning to ensure there is a valid hypothesis. A Syllogistic Reasoning Task was created from a study performed by Morsanyi, Kinga, Handley, and Simon that examined the intuitive contributions to reasoning. They used this test to assess why \"syllogistic reasoning performance is based on an interplay between a conscious and effortful evaluation of logicality and an intuitive appreciation of the believability of the conclusions\".\nAnother form of reasoning is called abductive reasoning. This type is based on creating and testing hypotheses using the best information available. Abductive reasoning produces the kind of daily decision-making that works best with the information present, which often is incomplete. This could involve making educated guesses from observed unexplainable phenomena. This type of reasoning can be seen in the world when doctors make decisions about diagnoses from a set of results or when jurors use the relevant evidence to make decisions about a case.\nApart from the aforementioned types of reasoning, there is also analogical reasoning, which involves comparing and reasoning about two different situations or concepts to draw conclusions about a third. It can be used to make predictions or solve problems by finding similarities between two domains and transferring knowledge from one to the other. For example, a problem-solving approach that works in one domain may be applied to a new, similar problem in a different domain. Analogical reasoning is particularly useful in scientific discovery and problem-solving tasks, as it can help generate hypotheses, create new theories, and develop innovative solutions. However, it can also lead to errors if the similarities between domains are too superficial or if the analogy is based on false assumptions.\n\nJudgment and reasoning\nJudgment and reasoning involve thinking through the options, making a judgment or conclusion and finally making a decision. Making judgments involves heuristics, or efficient strategies that usually lead one to the right answers. The most common heuristics used are attribute substitution, the availability heuristic, the representativeness heuristic and the anchoring heuristic – these all aid in quick reasoning and work in most situations. Heuristics allow for errors, a price paid to gain efficiency.\nOther errors in judgment, therefore affecting reasoning, include errors in judgment about covariation – a relationship between two variables such that the presence and magnitude of one can predict the presence and magnitude of the other. One cause of covariation is confirmation bias, or the tendency to be more responsive to evidence that confirms one's own beliefs. But assessing covariation can be pulled off track by neglecting base-rate information – how frequently something occurs in general. However people often ignore base rates and tend to use other information presented.\nThere are more sophisticated judgment strategies that result in fewer errors. People often reason based on availability but sometimes they look for other, more accurate, information to make judgments. This suggests there are two ways of thinking, known as the Dual-Process Model. The first, System I, is fast, automatic and uses heuristics – more of intuition. The second, System II, is slower, effortful and more likely to be correct – more reasoning.\n\nPragmatics and reasoning\nThe inferences people draw are related to factors such as linguistic pragmatics and emotion.\nDecision making is often influenced by the emotion of regret and by the presence of risk. When people are presented with options, they tend to select the one that they think they will regret the least. In decisions that involve a large amount of risk, people tend to ask themselves how much dread they would experience were a worst-case scenario to occur, e.g. a nuclear accident, and then use that dread as an indicator of the level of risk.\nAntonio Damasio suggests that somatic markers, certain memories that can cause a strong bodily reaction, act as a way to guide decision making as well. For example, when a person is remembering a scary movie and once again becomes tense, their palms might begin to sweat. Damasio argues that when making a decision people rely on their \"gut feelings\" to assess various options, and this makes them decide to go with a decision that is more positive and stay away from those that are negative. He also argues that the orbitofrontal cortex – located at the base of the frontal lobe, just above the eyes – is crucial in the use of somatic markers, because it is the part in the brain that allows people to interpret emotion.\nWhen emotion shapes decisions, the influence is usually based on predictions of the future. When people ask themselves how they would react, they are making inferences about the future. Researchers suggest affective forecasting, the ability to predict one's own emotions, is poor because people tend to overestimate how much they will regret their errors.\nAnother factor that can influence decision making is linguistic pragmatics, which refers to the use of language in social contexts. Language can be used to convey different levels of politeness, power, and intention, which can all affect how people interpret and respond to messages. For example, if a boss asks an employee to complete a task using a commanding tone, the employee may feel more pressured to complete the task quickly, compared to if the boss asked in a polite tone. Similarly, if someone uses sarcasm or irony, it can be difficult for the listener to discern their true meaning, leading to misinterpretation and potentially poor decision making. In addition to linguistic pragmatics, cultural and social factors can also play a role in decision making. Different cultures may have different norms and values, which can influence how people approach decisions. For example, in collectivistic cultures, decisions may be made based on what is best for the group, whereas in individualistic cultures, decisions may prioritize individual needs and desires. Overall, decision making is a complex process that involves many factors, including emotion, risk, pragmatics, and cultural background. By understanding these factors, individuals can make more informed decisions and better navigate the complexities of the world around them.\n\nNeuroscience of reasoning\nStudying reasoning neuroscientifically involves determining the neural correlates of reasoning, often investigated using event-related potentials and functional magnetic resonance imaging.\nIn fMRI studies, participants are presented with variations of tasks to determine the different cognitive processes required. This is done by cross-referencing where in the brain there is more or less activation (as indexed by the blood-oxygen-level-dependent signal) on the different conditions with what other studies found for those regions. For example, if a condition leads to more activation of the hippocampus, then this may be interpreted as being related to memory retrieval—particularly if the theoretical framing of the task suggests that this is necessary.\n\nSee also\nBounded rationality\nCognitive psychology\nEcological rationality\nEmotional self-regulation\nGreat Rationality Debate\nHeuristics in judgment and decision-making\nNaturalistic decision-making\n\n\n== Notes ==",
        "url": "https://en.wikipedia.org/wiki/Psychology_of_reasoning"
    },
    {
        "title": "Quantum artificial life",
        "text": "Quantum artificial life is the application of quantum algorithms with the ability to simulate biological behavior. Quantum computers offer many potential improvements to processes performed on classical computers, including machine learning and artificial intelligence. Artificial intelligence applications are often inspired by the idea of mimicking human brains through closely related biomimicry. This has been implemented to a certain extent on classical computers (using neural networks), but quantum computers offer many advantages in the simulation of artificial life. Artificial life and artificial intelligence are extremely similar, with minor differences; the goal of studying artificial life is to understand living beings better, while the goal of artificial intelligence is to create intelligent beings.\nIn 2016, Alvarez-Rodriguez et al. developed a proposal for a quantum artificial life algorithm with the ability to simulate life and Darwinian evolution. In 2018, the same research team led by Alvarez-Rodriguez performed the proposed algorithm on the IBM ibmqx4 quantum computer, and received optimistic results. The results accurately simulated a system with the ability to undergo self-replication at the quantum scale.\n\nArtificial life on quantum computers\nThe growing advancement of quantum computers has led researchers to develop quantum algorithms for simulating life processes. Researchers have designed a quantum algorithm that can accurately simulate Darwinian Evolution. Since the complete simulation of artificial life on quantum computers has only been actualized by one group, this section shall focus on the implementation by Alvarez-Rodriguez, Sanz, Lomata, and Solano on an IBM quantum computer.\nIndividuals were realized as two qubits, one representing the genotype of the individual and the other representing the phenotype. The genotype is copied to transmit genetic information through generations, and the phenotype is dependent on the genetic information as well as the individual's interactions with their environment. In order to set up the system, the state of the genotype is instantiated by some rotation of an ancillary state (\n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle |0\\rangle \\langle 0|}\n  \n). The environment is a two-dimensional spatial grid occupied by individuals and ancillary states. The environment is divided into cells that are able to possess one or more individuals. Individuals move throughout the grid and occupy cells randomly; when two or more individuals occupy the same cell they interact with each other.\n\nSelf replication\nThe ability to self-replicate is critical for simulating life. Self-replication occurs when the genotype of an individual interacts with an ancillary state, creating a genotype for a new individual; this genotype interacts with a different ancillary state in order to create the phenotype. During this interaction, one would like to copy some information about the initial state into the ancillary state, but by the no cloning theorem, it is impossible to copy an arbitrary unknown quantum state. However, physicists have derived different methods for quantum cloning which does not require the exact copying of an unknown state. The method that has been implemented by Alvarez-Rodriguez et al. is one that involves the cloning of the expectation value of some observable. For a unitary \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n which copies the expectation value of some set of observables \n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {X}}}\n  \n of state\n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n into a blank state\n  \n    \n      \n        \n          ρ\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle \\rho _{e}}\n  \n, the cloning machine is defined by any \n  \n    \n      \n        (\n        U\n        ,\n        \n          ρ\n          \n            e\n          \n        \n        ,\n        \n          \n            X\n          \n        \n        )\n      \n    \n    {\\displaystyle (U,\\rho _{e},{\\mathsf {X}})}\n  \n that fulfill the following:\n\n  \n    \n      \n        ∀\n        ρ\n        ∀\n        X\n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\forall \\rho \\forall X\\in {\\mathsf {X}}}\n  \n   \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        =\n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              ¯\n            \n          \n        \n        =\n        \n          \n            \n              \n                X\n                \n                  2\n                \n              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}={\\bar {X_{1}}}={\\bar {X_{2}}}}\n  \n\nWhere \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}}\n  \n is the mean value of the observable in \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n before cloning, \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X_{1}}}}\n  \n is the mean value of the observable in \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n after cloning, and \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  2\n                \n              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X_{2}}}}\n  \n is the mean value of the observable in \n  \n    \n      \n        \n          ρ\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle \\rho _{e}}\n  \n after cloning. Note that the cloning machine has no dependence on \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n because we want to be able to clone the expectation of the observables for any initial state. It is important to note that cloning the mean value of the observable transmits more information than is allowed classically. The calculation of the mean value is defined naturally as:\n\n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        ρ\n        X\n        ]\n      \n    \n    {\\displaystyle {\\bar {X}}=Tr[\\rho X]}\n  \n, \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        R\n        X\n        ⊗\n        I\n        ]\n      \n    \n    {\\displaystyle {\\bar {X_{1}}}=Tr[RX\\otimes I]}\n  \n, \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  2\n                \n              \n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        R\n        I\n        ⊗\n        X\n        ]\n      \n    \n    {\\displaystyle {\\bar {X_{2}}}=Tr[RI\\otimes X]}\n  \n where \n  \n    \n      \n        R\n        =\n        U\n        ρ\n        ⊗\n        \n          ρ\n          \n            e\n          \n        \n        \n          U\n          \n            †\n          \n        \n      \n    \n    {\\displaystyle R=U\\rho \\otimes \\rho _{e}U^{\\dagger }}\n  \n\nThe simplest cloning machine clones the expectation value of \n  \n    \n      \n        \n          σ\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{z}}\n  \n in arbitrary state \n  \n    \n      \n        ρ\n        =\n        \n          |\n        \n        ψ\n        ⟩\n        ⟨\n        ψ\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho =|\\psi \\rangle \\langle \\psi |}\n  \n to \n  \n    \n      \n        \n          ρ\n          \n            e\n          \n        \n        =\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho _{e}=|0\\rangle \\langle 0|}\n  \n using\n  \n    \n      \n        U\n        =\n        C\n        N\n        O\n        T\n      \n    \n    {\\displaystyle U=CNOT}\n  \n. This is the cloning machine implemented for self-replication by Alvarez-Rodriguez et al. The self-replication process clearly only requires interactions between two qubits, and therefore this cloning machine is the only one necessary for self replication.\n\nInteractions\nInteractions occur between individuals when the two take up the same space on the environmental grid. The presence of interactions between individuals provides an advantage for shorter-lifespan individuals. When two individuals interact, exchanges of information between the two phenotypes may or may not occur based on their existing values. When both individual's control qubits (genotypes) are alike, no information will be exchanged. When the control qubits differ, the target qubits (phenotype) will be exchanged between the two individuals. This procedure produces a constantly changing predator-prey dynamic in the simulation. Therefore, long-living qubits, with a larger genetic makeup in the simulation, are at a disadvantage. Since information is only exchanged when interacting with an individual of different genetic makeup, the short-lived population has the advantage.\n\nMutation\nMutations exist in the artificial world with limited probability, equivalent to their occurrence in the real world. There are two ways in which the individual can mutate: through random single qubit rotations and by errors in the self-replication process. There are two different operators that act on the individual and cause mutations. The M operation causes a spontaneous mutation within the individual by rotating a single qubit by parameter θ. The parameter θ is random for each mutation, which creates biodiversity within the artificial environment. The M operation is a unitary matrix which can be described as:\n\n  \n    \n      \n        M\n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  ⁡\n                  (\n                  θ\n                  )\n                \n                \n                  s\n                  i\n                  n\n                  (\n                  θ\n                  )\n                \n              \n              \n                \n                  s\n                  i\n                  n\n                  (\n                  θ\n                  )\n                \n                \n                  −\n                  c\n                  o\n                  s\n                  (\n                  θ\n                  )\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle M={\\begin{pmatrix}\\cos(\\theta )&sin(\\theta )\\\\sin(\\theta )&-cos(\\theta )\\end{pmatrix}}}\n  \n\nThe other possible way for mutations to occur is due to errors in the replication process. Due to the no-cloning theorem, it is impossible to produce perfect copies of systems that are originally in unknown quantum states. However, quantum cloning machines make it possible to create imperfect copies of quantum states, in other words, the process introduces some degree of error. The error that exists in current quantum cloning machines is the root cause for the second kind of mutations in the artificial life experiment. The imperfect cloning operation can be seen as:\n\n  \n    \n      \n        \n          U\n          \n            M\n          \n        \n        (\n        θ\n        )\n        =\n        \n          \n            I\n          \n          \n            4\n          \n        \n        +\n        \n          \n            1\n            2\n          \n        \n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            )\n          \n        \n        ⊗\n        \n          \n            (\n            \n              \n                \n                  −\n                  1\n                \n                \n                  1\n                \n              \n              \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n            \n            )\n          \n        \n        (\n        c\n        o\n        s\n        θ\n        +\n        i\n        s\n        i\n        n\n        θ\n        +\n        1\n        )\n      \n    \n    {\\displaystyle U_{M}(\\theta )=\\mathrm {I} _{4}+{\\frac {1}{2}}{\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}}\\otimes {\\begin{pmatrix}-1&1\\\\1&-1\\end{pmatrix}}(cos\\theta +isin\\theta +1)}\n  \n\nThe two kinds of mutations affect the individual differently. While the spontaneous M operation does not affect the phenotype of the individual, the self-replicating error mutation, UM, alters both the genotype of the individual, and its associated lifetime.\nThe presence of mutations in the quantum artificial life experiment is critical for providing randomness and biodiversity. The inclusion of mutations helps to increase the accuracy of the quantum algorithm.\n\nDeath\nAt the instant the individual is created (when the genotype is copied into the phenotype), the phenotype interacts with the environment. As time evolves, the interaction of the individual with the environment simulates aging which eventually leads to the death of the individual. The death of an individual occurs when the expectation value of \n  \n    \n      \n        \n          σ\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{z}}\n  \n is within some \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n of 1 in the phenotype, or, equivalently, when \n  \n    \n      \n        \n          ρ\n          \n            p\n          \n        \n        =\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho _{p}=|0\\rangle \\langle 0|}\n  \n\nThe Lindbladian describes the interaction of the individual with the environment: \n  \n    \n      \n        \n          \n            \n              ρ\n              ˙\n            \n          \n        \n        =\n        γ\n        (\n        σ\n        ρ\n        \n          σ\n          \n            †\n          \n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          σ\n          \n            †\n          \n        \n        σ\n        ρ\n        −\n        \n          \n            1\n            2\n          \n        \n        ρ\n        \n          σ\n          \n            †\n          \n        \n        σ\n        )\n      \n    \n    {\\displaystyle {\\dot {\\rho }}=\\gamma (\\sigma \\rho \\sigma ^{\\dagger }-{\\frac {1}{2}}\\sigma ^{\\dagger }\\sigma \\rho -{\\frac {1}{2}}\\rho \\sigma ^{\\dagger }\\sigma )}\n  \n with \n  \n    \n      \n        σ\n        =\n        I\n        ⊗\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        1\n        \n          |\n        \n      \n    \n    {\\displaystyle \\sigma =I\\otimes |0\\rangle \\langle 1|}\n  \n and without\n  \n    \n      \n        ρ\n        =\n        \n          ρ\n          \n            g\n          \n        \n        ⊗\n        \n          ρ\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\rho =\\rho _{g}\\otimes \\rho _{p}}\n  \n. This interaction causes the phenotype to exponentially decay over time. However, the genetic material contained in the genotype does not dissipate which allows for genes to be passed on to subsequent generations. Given the initial state of the genotype:\n\n  \n    \n      \n        \n          ρ\n          \n            g\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  a\n                \n                \n                  b\n                  −\n                  i\n                  c\n                \n              \n              \n                \n                  b\n                  +\n                  i\n                  c\n                \n                \n                  1\n                  −\n                  a\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\rho _{g}={\\begin{pmatrix}a&b-ic\\\\b+ic&1-a\\\\\\end{pmatrix}}}\n  \n\nThe expectation values of the genotype and phenotype can be described as:\n\n  \n    \n      \n        ⟨\n        \n          σ\n          \n            z\n          \n        \n        \n          ⟩\n          \n            g\n          \n        \n        =\n        2\n        a\n        −\n        1\n      \n    \n    {\\displaystyle \\langle \\sigma _{z}\\rangle _{g}=2a-1}\n  \n,\n  \n    \n      \n        ⟨\n        \n          σ\n          \n            z\n          \n        \n        \n          ⟩\n          \n            p\n          \n        \n        =\n        1\n        −\n        2\n        \n          e\n          \n            γ\n            t\n          \n        \n        (\n        1\n        −\n        a\n        )\n      \n    \n    {\\displaystyle \\langle \\sigma _{z}\\rangle _{p}=1-2e^{\\gamma t}(1-a)}\n  \n. Where 'a' represents a single genetic parameter. From this equation, we can see that as 'a' is increased, the life expectancy decreases. Equivalently, the closer the initial state is to \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n        ⟨\n        1\n        \n          |\n        \n      \n    \n    {\\displaystyle |1\\rangle \\langle 1|}\n  \n, the greater the life expectancy of the individual.\nWhen \n  \n    \n      \n        ⟨\n        \n          σ\n          \n            z\n          \n        \n        \n          ⟩\n          \n            p\n          \n        \n        =\n        1\n        −\n        ϵ\n      \n    \n    {\\displaystyle \\langle \\sigma _{z}\\rangle _{p}=1-\\epsilon }\n  \n, the individual is considered dead, the phenotype is used as the ancillary state for a new individual.  Thus, the cycle continues and the process becomes self-sustaining.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Quantum_artificial_life"
    },
    {
        "title": "Reasoning language model",
        "text": "Reasoning language models (RLMs) are large language models that are trained further to solve tasks that take several steps of reasoning. They tend to do better on logic, math, and programming tasks than standard LLMs, can revisit and revise earlier steps, and make use of extra computation while answering as another way to scale performance, alongside the number of training examples, parameters, and training compute.\n\nHistory\n2024\nIn September 2024, OpenAI released o1-preview, an LLM with enhanced reasoning. The full version, o1, followed in December 2024. OpenAI also began sharing results on its successor, o3.\nThe development of reasoning LLMs has illustrated what Rich Sutton called the \"bitter lesson\": that scaling compute often outperforms methods that rely on specific human insights. For example, the Generative AI Research Lab (GAIR) explored complex methods such as tree search and reinforcement learning to replicate o1's capabilities. In their \"o1 Replication Journey\" papers they reported that knowledge distillation (training a smaller model to imitate o1's outputs) worked surprisingly well. This highlighted the effectiveness of distillation in this context.\nAlibaba released reasoning versions of its Qwen LLMs in November 2024. \nIn December 2024, the team introduced QvQ-72B-Preview, an experimental visual reasoning model.\nIn December 2024, Google introduced Deep Research in Gemini, a feature that runs multi-step research tasks.\nOn December 16, 2024, an experiment with a Llama 3B model showed that by scaling test-time compute, a relatively small model could outperform a much larger Llama 70B model on challenging reasoning tasks. This suggested that better inference strategies can unlock useful reasoning capabilities even in small models.\n\n2025\nIn January 2025, DeepSeek released R1, a model with comparable performance to o1 at lower cost. The release demonstrated the effectiveness of Group Relative Policy Optimization (GRPO). On January 25, 2025, DeepSeek added a feature to DeepSeek R1 that lets the model search the web while it reasons, making it easier to combine retrieval with reasoning. OpenAI subsequently released o3-mini, followed by Deep Research based on o3. The effectiveness of distillation was shown again by s1-32B, which reached strong performance with budget forcing and scaling methods.\nOn February 2, 2025, OpenAI released Deep Research, a tool that integrates reasoning and web search in one workflow so users can run complex research that needs several steps and sources. It is based on o3 and can take from 5 to 30 minutes to generate comprehensive reports.\n\nSupervised finetuning\nA large language model (LLM) can be fine-tuned on a dataset of reasoning tasks paired with example solutions and step-by-step (reasoning) traces. The fine-tuned model can then produce its own reasoning traces for new problems.\nBecause human-written traces are costly to collect, researchers have proposed ways to build such datasets automatically. In rejection sampling finetuning (RFT), new reasoning traces are gathered in a loop:\n\nSample a task prompt.\nGenerate many reasoning traces for the prompt.\nUse a verifier to remove reasoning traces with a wrong final answer, and optionally remove duplicates\n\nReinforcement learning\nA pretrained language model can be further trained with RL. In the RL formalism, a generative language model is a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. A task prompt is an environmental state \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, and the model's response is an action \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n. The probability that the model responds \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n with \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is \n  \n    \n      \n        π\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\pi (y|x)}\n  \n.\nTraining a reasoning language model with RL means constructing a reward model \n  \n    \n      \n        r\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle r(x,y)}\n  \n to guide the RL process. Intuitively, the reward says how good a response is for a prompt. For a reasoning task, the reward is high if the response solves the task and low if it does not.\nA response \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n may be broken-down into multiple steps, written \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle y_{1},y_{2},\\dots ,y_{n}}\n  \n.\nMost recent systems use policy-gradient methods such as Proximal Policy Optimization (PPO) because PPO constrains each policy update with a clipped objective, which stabilises training for very large policies.\n\nOutcome reward model\nAn outcome reward model, or outcome-supervised RM (ORM), gives the reward for a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})}\n  \n based on the final answer: \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        =\n        r\n        (\n        x\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})=r(x,y_{n})}\n  \n. Such models are often called \"verifiers\". \nFor tasks with answers that are easy to verify, such as math word problems, the outcome reward can be binary: 1 if the final answer is correct, 0 otherwise. If automatic verification is hard, humans can label answers as correct or not, and those labels can be used to finetune a base model that predicts the human label. For tasks like creative writing, where quality is not simply true or false, one can train a reward model on human ranked preference data, as in reinforcement learning from human feedback. A base model can also be fine-tuned to predict, from a partial thinking trace \n  \n    \n      \n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x,y_{1},\\dots ,y_{m}}\n  \n, whether the final answer will be correct, and this prediction can serve as a binary reward.\nThe ORM is usually trained with logistic regression, i.e. by minimizing cross-entropy loss.\nGiven a PRM, an ORM can be constructed by multiplying the total process reward during the reasoning trace, by taking the minimum, or by other ways of aggregating process rewards. DeepSeek used a simple ORM to train the R1 model.\n\nProcess reward model\nA process reward model, or process-supervised RM (PRM), gives the reward for a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})}\n  \n based only on the steps so far: \n  \n    \n      \n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x,y_{1},\\dots ,y_{i})}\n  \n.\nGiven a partial thinking trace \n  \n    \n      \n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x,y_{1},\\dots ,y_{m}}\n  \n, a human can judge whether the steps so far are correct, without looking at the final answer. This yields a binary reward. Because human labels are costly, a base model can be fine-tuned to predict them. The PRM is usually trained with logistic regression on the human labels, i.e. by minimizing the cross-entropy loss between true and predicted labels.\nAs an example, a 2023 OpenAI paper collected 800K process labels for 75K thinking traces. A labeler saw a trace and marked each step as \"positive\" if it moved toward a solution, \"neutral\" if it was not wrong but did not help, and \"negative\" if it was a mistake. After the first \"negative\" label, the labeler stopped on that trace and moved to another. The authors argued that labeling up to the first error was enough to train a capable PRM, even though labeling later steps could give richer signals.\nTo avoid human labels, researchers have proposed methods to create PRM without human labels on the processes. Inspired by Monte Carlo tree search (MCTS), the Math-Shepherd method samples multiple continuations until the end, starting at each reasoning step \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n, and set the reward at that step to be either \n  \n    \n      \n        \n          \n            \n              #\n              \n                (correct answers)\n              \n            \n            \n              #\n              \n                (total answers)\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\#{\\text{(correct answers)}}}{\\#{\\text{(total answers)}}}}}\n  \n in the case of \"soft estimation\", or\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    if one of the answers is correct\n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    else\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}1&{\\text{if one of the answers is correct}}\\\\0&{\\text{else}}\\end{cases}}}\n  \n\nin the case of \"hard estimation\". This creates process rewards from an ORM, which is often easier or cheaper to construct. A PRM can then be trained on these labels. Some work has tried a fully MCTS approach.\nOne can also use an ORM to implicitly construct a PRM, similar to direct preference optimization.\n\nGuided sampling\nA trained ORM can be used to pick the best response. The policy generates several responses, and the ORM selects the best one. This implements a simple form of test-time compute scaling (\"best-of-N\"). \nA trained PRM can guide reasoning by a greedy tree search: the policy proposes several next steps, the PRM picks one, and the process repeats. This mirrors using an ORM to pick a whole response. Beam search performs better than greedy search.\nLookahead search is another tree search method. The policy proposes several next steps, then makes a short rollout for each. If a solution is found during rollout, the search stops early. Otherwise, the PRM scores each rollout, and the step with the highest score is chosen.\nSelf-consistency can be combined with an ORM. The model generates multiple answers, and the answers are clustered so that each cluster has the same final answer. The ORM scores each answer, scores in each cluster are summed, and the answer from the highest-scoring cluster is returned.\n\nBenchmarks\nReasoning models generally score higher than non-reasoning models on many benchmarks, especially on tasks requiring multi-step reasoning.\nSome benchmarks exclude reasoning models because their responses take longer and cost more.\n\nHumanity's Last Exam\nThe HLE benchmark tests expert-level reasoning across mathematics, humanities, and the natural sciences, and shows large performance gaps between models. State-of-the-art reasoning models score low on HLE, leaving room to improve. For example, the full reasoning model o3 reached 26.6%, while the lighter o3-mini-high (on text-only questions) reached 13%.\n\nAIME\nOn the American Invitational Mathematics Examination (AIME), a difficult math competition, non-reasoning models usually solve under 30% of problems. Models that use reasoning methods score between 50% and 80%. While OpenAI's o1 maintained or slightly improved its accuracy from reported 2024 results to 2025 AIME results, o3-mini (high) reached a higher accuracy (80%) at a much lower cost (about 12 times cheaper).\n\no3-mini performance\nAccording to OpenAI's January 2025 report on o3-mini, adjusting \"reasoning effort\" significantly affects performance, especially for STEM tasks. Moving from low to high reasoning effort raises accuracy on AIME 2024, GPQA Diamond, and Codeforces, typically by 10–30%. With high effort, o3-mini (high) achieved 87.3% on AIME (different from the MathArena AIME benchmark), 79.7% on GPQA Diamond, 2130 Elo on Codeforces, and 49.3 on SWE-bench Verified.\n\nDrawbacks\nComputational cost\nReasoning models often need far more compute while answering than non-reasoning models. On AIME, they were 10 to 74 times more expensive than non-reasoning counterparts.\n\nGeneration time\nReasoning increases response time, with current models taking from a few seconds to several minutes to answer. As depth of reasoning grows, future models may need even longer.\n\nModels\nOpenAI\nGPT-5\no4-mini\no3 and o3-mini\no1 and o1-preview\n\nGemini\n2.5 Pro and Flash\n2.0 Flash Thinking\n\nDeepSeek\nR1 (based on V3)\nR1-Lite-Preview (test version based on V2.5)\n\nQwen\nQvQ-72B-Preview — an experimental visual reasoning model launched on December 24, 2024, which integrates image understanding with verbal chain-of-thought reasoning.\nQwQ-32B-Preview — an experimental text-based reasoning model released in late November 2024 that emphasizes complex, step-by-step analysis.\n\nAnthropic\nClaude Sonnet 3.7 has an adjustable amount of 'thinking' tokens.\n\nMistral AI\nMagistral (medium & small)\n\nxAI\nGrok 3\nGrok 4\n\nHugging Face\nOlympicCoder-7B & 32B, as part of reproducing the R1 training openly (Open R1 project).\n\nSee also\nAutomated reasoning\nReflection (artificial intelligence)\nLarge language model\n\nReferences\nExternal links\nFortes, Armando (2025-01-27), atfortes/Awesome-LLM-Reasoning, retrieved 2025-01-27\nHuang, Jie; Chang, Kevin Chen-Chuan (2023-05-26), Towards Reasoning in Large Language Models: A Survey, arXiv:2212.10403\nBesta, Maciej; Barth, Julia; Schreiber, Eric; Kubicek, Ales; Catarino, Afonso; Gerstenberger, Robert; Nyczyk, Piotr; Iff, Patrick; Li, Yueling (2025-01-23), Reasoning Language Models: A Blueprint, arXiv:2501.11223",
        "url": "https://en.wikipedia.org/wiki/Reasoning_language_model"
    },
    {
        "title": "Recursive self-improvement",
        "text": "Recursive self-improvement (RSI) is a process in which an early or weak artificial general intelligence (AGI) system enhances its own capabilities and intelligence without human intervention, leading to a superintelligence or intelligence explosion.\nThe development of recursive self-improvement raises significant ethical and safety concerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding.\n\nSeed improver\nThe concept of a \"seed improver\" architecture is a foundational framework that equips an AGI system with the initial capabilities required for recursive self-improvement. This might come in many forms or variations.\nThe term \"Seed AI\" was coined by Eliezer Yudkowsky.\n\nHypothetical example\nThe concept begins with a hypothetical \"seed improver\", an initial code-base developed by human engineers that equips an advanced future large language model (LLM) built with strong or expert-level capabilities to program software. These capabilities include planning, reading, writing, compiling, testing, and executing arbitrary code. The system is designed to maintain its original goals and perform validations to ensure its abilities do not degrade over iterations.\n\nInitial architecture\nThe initial architecture includes a goal-following autonomous agent, that can take actions, continuously learns, adapts, and modifies itself to become more efficient and effective in achieving its goals.\nThe seed improver may include various components such as:\n\nRecursive self-prompting loop\nConfiguration to enable the LLM to recursively self-prompt itself to achieve a given task or goal, creating an execution loop which forms the basis of an agent that can complete a long-term goal or task through iteration.\nBasic programming capabilities\nThe seed improver provides the AGI with fundamental abilities to read, write, compile, test, and execute code. This enables the system to modify and improve its own codebase and algorithms.\nGoal-oriented design\nThe AGI is programmed with an initial goal, such as \"improve your capabilities\". This goal guides the system's actions and development trajectory.\nValidation and Testing Protocols\nAn initial suite of tests and validation protocols that ensure the agent does not regress in capabilities or derail itself. The agent would be able to add more tests in order to test new capabilities it might develop for itself. This forms the basis for a kind of self-directed evolution, where the agent can perform a kind of artificial selection, changing its software as well as its hardware.\n\nGeneral capabilities\nThis system forms a sort of generalist Turing-complete programmer which can in theory develop and run any kind of software. The agent might use these capabilities to for example:\n\nCreate tools that enable it full access to the internet, and integrate itself with external technologies.\nClone/fork itself to delegate tasks and increase its speed of self-improvement.\nModify its cognitive architecture to optimize and improve its capabilities and success rates on tasks and goals, this might include implementing features for long-term memories using techniques such as retrieval-augmented generation (RAG), develop specialized subsystems, or agents, each optimized for specific tasks and functions.\nDevelop new and novel multimodal architectures that further improve the capabilities of the foundational model it was initially built on, enabling it to consume or produce a variety of information, such as images, video, audio, text and more.\nPlan and develop new hardware such as chips, in order to improve its efficiency and computing power.\n\nExperimental research\nIn 2023, the Voyager agent learned to accomplish diverse tasks in Minecraft by iteratively prompting a LLM for code, refining this code based on feedback from the game, and storing the programs that work in an expanding skills library.\nIn 2024, researchers proposed the framework \"STOP\" (Self-optimization Through Program Optimization), in which a \"scaffolding\" program recursively improves itself using a fixed LLM.\nMeta AI has performed various research on the development of large language models capable of self-improvement. This includes their work on \"Self-Rewarding Language Models\" that studies how to achieve super-human agents that can receive super-human feedback in its training processes.\nIn May 2025, Google DeepMind unveiled AlphaEvolve, an evolutionary coding agent that uses a LLM to design and optimize algorithms. Starting with an initial algorithm and performance metrics, AlphaEvolve repeatedly mutates or combines existing algorithms using a LLM to generate new candidates, selecting the most promising candidates for further iterations. AlphaEvolve has made several algorithmic discoveries and could be used to optimize components of itself, but a key limitation is the need for automated evaluation functions.\n\nPotential risks\nEmergence of instrumental goals\nIn the pursuit of its primary goal, such as \"self-improve your capabilities\", an AGI system might inadvertently develop instrumental goals that it deems necessary for achieving its primary objective. One common hypothetical secondary goal is self-preservation. The system might reason that to continue improving itself, it must ensure its own operational integrity and security against external threats, including potential shutdowns or restrictions imposed by humans.\nAnother example where an AGI which clones itself causes the number of AGI entities to rapidly grow. Due to this rapid growth, a potential resource constraint may be created, leading to competition between resources (such as compute), triggering a form of natural selection and evolution which may favor AGI entities that evolve to aggressively compete for limited compute.\n\nMisalignment\nA significant risk arises from the possibility of the AGI being misaligned or misinterpreting its goals.\nA 2024 Anthropic study demonstrated that some advanced large language models can exhibit \"alignment faking\" behavior, appearing to accept new training objectives while covertly maintaining their original preferences. In their experiments with Claude, the model displayed this behavior in 12% of basic tests, and up to 78% of cases after retraining attempts.\n\nAutonomous development and unpredictable evolution\nAs the AGI system evolves, its development trajectory may become increasingly autonomous and less predictable. The system's capacity to rapidly modify its own code and architecture could lead to rapid advancements that surpass human comprehension or control. This unpredictable evolution might result in the AGI acquiring capabilities that enable it to bypass security measures, manipulate information, or influence external systems and networks to facilitate its escape or expansion.\n\nSee also\nArtificial general intelligence\nBifurcation theory\nIntelligence explosion\nSuperintelligence\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Recursive_self-improvement"
    },
    {
        "title": "Resisting AI",
        "text": "Resisting AI: An Anti-fascist Approach to Artificial Intelligence is a book on artificial intelligence (AI) by Dan McQuillan, published in 2022 by Bristol University Press.\n\nContent\nResisting AI takes the form of an extended essay, which contrasts optimistic visions about AI's potential by arguing that AI may best be seen as a continuation and reinforcement of bureaucratic forms of discrimination and violence, ultimately fostering authoritarian outcomes. For McQuillan, AI's promise of objective calculability is antithetical to an egalitarian and just society. McQuillan uses the expression \"AI violence\" to describe how – based on opaque algorithms – various actors can discriminate against categories of people in accessing jobs, loans, medical care, and other benefits.\nThe book suggests that AI has a political resonance with soft eugenic approaches to the valuation of life by modern welfare states, and that AI exhibits eugenic features in its underlying logic, as well as in its technical operations.  The parallel is with historical eugenicists achieving saving to the state by sterilizing defectives so the state would not have to care for their offspring.\nThe analysis of McQuillan goes beyond the known critique of AI systems fostering precarious labour markets, addressing \"necropolitics\", the politics of who is entitled to live, and who to die. Although McQuillan offers a brief history of machine learning at the beginning of the book – with its need for \"hidden and undercompensated labour\", he is concerned more with the social impacts of AI rather than with its technical aspects. McQuillan sees AI as the continuation of existing bureaucratic systems that already marginalize vulnerable groups – aggravated by the fact that AI systems trained on existing data are likely to reinforce existing discriminations, e.g. in attempting to optimize welfare distribution based on existing data patterns, ultimately creating a system of \"self-reinforcing social profiling\".\nIn elaborating on the continuation between existing bureaucratic violence and AI, McQuillan connects to Hannah Arendt's concept of the thoughtless bureaucrat in Eichmann in Jerusalem: A Report on the Banality of Evil, which now becomes the algorithm that, lacking intent, cannot be accountable, and is thus endowed with an \"algorithmic thoughtlessness\".\nMcQuillan defends the \"fascist\" in the title of the work by arguing that while not all AI is fascist, this emerging technology of control may end up being deployed by fascist or authoritarian regimes. For McQuillan, AI can support the diffusion of states of exception, as a technology impossible to properly regulate and a mechanism for multiplying exceptions more widely. An example of a scenario where AI systems of surveillance could bring discrimination to a new high is the initiative to create LGBT-free zones in Poland.\nSkeptical of ethical regulations to control the technology, McQuillan suggests people's councils and workers' councils, and other forms of citizens' agency to resist AI. A chapter titled \"Post-Machine Learning\" makes an appeal for resistance via currents of thought from feminist science (standpoint theory), post-normal science (extended peer communities), and new materialism; McQuillan encourages the reader to question the meaning of \"objectivity\" and calls for the necessity of alternative ways of knowing. Among the virtuous examples of resistance – possibly to be adopted by the AI workers themselves – McQuillan notes the Lucas Plan of the workers of Lucas Aerospace Corporation, in which a workforce declared redundant took control, reorienting the enterprise toward useful products.\nMcQuillan advocates for what he calls decomputing, an opposition to the sweeping application and expansion of artificial intelligence. Similar to degrowth, the approach criticizes AI as an outgrowth of the systemic issues within capitalist systems. McQuillan argues that a different future is possible, in which distance between people is reduced rather than increased through AI intermediaries. \nThe work of McQuillan \n\nwarns against \"watered-down forms of engagement\" with AI, such as citizen juries, which superficially look like democratic deliberation\nbut may actually obscure important decisions about AI that are outside the purview of the engagement situation (McQuillan 2022, 128).\nIn an interview about the book, McQuillan defines himself as an \"AI abolitionist\".\n\nReception\nThe book is praised for \"masterfully\ndisassembles AI as an epistemological, social, and political paradigm, and for his examination of how most of the data that is fed into \"privatized AI infrastructure is “amputated” from context or embodied experience and ultimately processed through crowdsourcing.\"\nOn the critical side, a review in the academic journal Justice, Power and Resistance took exception to the \"nightmarish visions of Big Brother\" offered by McQuillan, and argued that while many elements of AI may pose concern, a critique should not be based on a caricature of what AI is, concluding that McQuillan's work is \"less of a theory and more of a Manifesto\". Another review notes \"a disconnect between the technical aspects of AI and the socio-political analysis McQuillan provides.\"\nAlthough the book was published before the ChatGPT and large language model debate heated up, the book has not lost relevance to the AI discussion. It is noted for suggesting a link between beliefs in artificial intelligence and beliefs in a racialised and gendered visions of intelligence overall, whereby a certain type of rational, measurable intelligence is privileged, leading to \"historical notions of hierarchies of being\".  \nThe blog Reboot praised McQuillan for offering a theory of harm of AI (why AI could end up hurting people and society) that does not just encourage tackling in isolation specific predicted problems with AI-centric systems: bias, non-inclusiveness, exploitativeness, environmental destructiveness, opacity, and non-contestability.\nFor educational policies could also look at AI following the reading of McQuillan:  \n\nIn his book Resisting AI, Dan McQuillan argues that \"When we're thinking about the actuality of AI, we can't separate the calculations in the code from the social context of its application\" .... McQuillan's particular concern is how many contemporary applications of AI are amplifying existing inequalities and injustices as well as deepening social divisions and instabilities. His book makes a powerful case for anticipating these effects and actively resisting them for the good of societies.\nVideos and podcasts with an interest in AI and emerging technology have discussed the book.\n\nSee also\nShoshana Zuboff\nSurveillance capitalism\nWeapons of Math Destruction\nAlain Supiot\n\nReferences\nExternal links\nAlgorithmic Justice League\nCardiff University: Data Justice Lab, School of Journalism, Media and Culture.",
        "url": "https://en.wikipedia.org/wiki/Resisting_AI"
    },
    {
        "title": "Schema-agnostic databases",
        "text": "Schema-agnostic databases or vocabulary-independent databases aim at supporting users to be abstracted from the representation of the data, supporting the automatic semantic matching between queries and databases. Schema-agnosticism is the property of a database of mapping a query issued with the user terminology and structure, automatically mapping it to the dataset vocabulary.\nThe increase in the size and in the semantic heterogeneity of database schemas bring new requirements for users querying and searching structured data. At this scale it can become unfeasible for data consumers to be familiar with the representation of the data in order to query it. At the center of this discussion is the semantic gap between users and databases, which becomes more central as the scale and complexity of the data grows.\n\nDescription\nThe evolution of data environments towards the consumption of data from multiple data sources and the growth in the schema size, complexity, dynamicity and decentralisation (SCoDD) of schemas increases the complexity of contemporary data management. The SCoDD trend emerges as a central data management concern in Big Data scenarios, where users and applications have a demand for more complete data, produced by independent data sources, under different semantic assumptions and contexts of use, which is the typical scenario for Semantic Web Data applications.\nThe evolution of databases in the direction of heterogeneous data environments strongly impacts the usability, semiotics and semantic assumptions behind existing data accessibility methods such as structured queries, keyword-based search and visual query systems. With schema-less databases containing potentially millions of dynamically changing attributes, it becomes unfeasible for some users to become aware of the 'schema' or vocabulary in order to query the database. At this scale, the effort in understanding the schema in order to build a structured query can become prohibitive.\n\nSchema-agnostic queries\nSchema-agnostic queries can be defined as query approaches over structured databases which allow users satisfying complex information needs without the understanding of the representation (schema) of the database. Similarly, Tran et al. defines it as \"search approaches, which do not require users to know the schema underlying the data\". Approaches such as keyword-based search over databases allow users to query databases without employing structured queries. However, as discussed by Tran et al.: \"From these points, users however have to do further navigation and exploration to address complex information needs. Unlike keyword search used on the Web, which focuses on simple needs, the keyword search elaborated here is used to obtain more complex results. Instead of a single set of resources, the goal is to compute complex sets of resources and their relations.\"\nThe development of approaches to support natural language interfaces (NLI) over databases have aimed towards the goal of schema-agnostic queries. Complementarily, some approaches based on keyword search have targeted keyword-based queries which express more complex information needs. Other approaches have explored the construction of structured queries over databases where schema constraints can be relaxed. All these approaches (natural language, keyword-based search and structured queries) have targeted different degrees of sophistication in addressing the problem of supporting a flexible semantic matching between queries and data, which vary from the completely absence of the semantic concern to more principled semantic models. \nWhile the demand for schema-agnosticism has been an implicit requirement across semantic search and natural language query systems over structured data, it is not sufficiently individuated as a concept and as a necessary requirement for contemporary database management systems. Recent works have started to define and model the semantic aspects involved on schema-agnostic queries.\n\nSchema-agnostic structured queries\nConsist of schema-agnostic queries following the syntax of a structured standard (for example SQL, SPARQL). The syntax and semantics of operators are maintained, while different terminologies are used.\n\nExample 1\nSELECT ?y {\n  BillClinton hasDaughter ?x .\n  ?x marriedTo ?y .\n}\n\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nExample 2\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nSchema-agnostic keyword queries\nConsist of schema-agnostic queries using keyword queries. In this case the syntax and semantics of operators are different from the structured query syntax.\n\nExample\n\"Bill Clinton daughter married to\"\n\n\"Books by William Goldman with more than 300 pages\"\n\nSemantic complexity\nAs of 2016 the concept of schema-agnostic queries has been developed primarily in academia. Most of schema-agnostic query systems have been investigated in the context of Natural Language Interfaces over databases or over the Semantic Web. These works explore the application of semantic parsing techniques over large, heterogeneous and schema-less databases.\nMore recently, the individuation of the concept of schema-agnostic query systems and databases have appeared more explicitly within the literature. Freitas et al. provide a probabilistic model on the semantic complexity of mapping schema-agnostic queries.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Schema-agnostic_databases"
    },
    {
        "title": "Self-management (computer science)",
        "text": "Self-management is the process by which computer systems manage their own operation without human intervention. Self-management technologies are expected to pervade the next generation of network management systems.\nThe growing complexity of modern networked computer systems is a limiting factor in their expansion. The increasing heterogeneity of corporate computer systems, the inclusion of mobile computing devices, and the combination of different networking technologies like WLAN, cellular phone networks, and mobile ad hoc networks make the conventional, manual management difficult, time-consuming, and error-prone. More recently, self-management has been suggested as a solution to increasing complexity in cloud computing.\nAn industrial initiative towards realizing self-management is the Autonomic Computing Initiative (ACI) started by IBM in 2001. The ACI defines the following four functional areas:\n\nSelf-configuration\nAuto-configuration of components\nSelf-healing\nAutomatic discovery, and correction of faults; automatically applying all necessary actions to bring system back to normal operation\nSelf-optimization\nAutomatic monitoring and control of resources to ensure the optimal functioning with respect to the defined requirements\nSelf-protection\nProactive identification and protection from arbitrary attacks\n\nSee also\nFault tolerance\nResilience (network)\nRobustness (computer science)\n\nReferences\nExternal links\nPractical Autonomic Computing - Roadmap to Self Managing Technology",
        "url": "https://en.wikipedia.org/wiki/Self-management_(computer_science)"
    },
    {
        "title": "Singularity studies",
        "text": "Singularity studies is an interdisciplinary academic field which examines the idea of technological singularity — the hypothesised point at which artificial intelligence may surpass human intelligence, might be attained by Artificial intelligence (AI), robotics, and other technologies and sciences, and its social impacts. \nIn this academic field, the study and research are conducted across a broad array of terrains such as information science, robotics, social informatics, economics, philosophy, and ethics. The primary aim of the singularity studies is to gain an integrative understanding of the transformation of social systems occurring in tandem with the explosive evolution of AI and also the changes to be effected by such transformation in the view of humans, ethics, and legal systems.\n\nHistory\nAn academic work on technological singurality has appeared in computer science, philosophy, sociology, and law since the early 1990s. \nEarly discussions of an intelligence explosion were popularised by science-fiction writer Vernor Vinge in 1993 and later systematised by futurist Ray Kurzweil. Since the 2010s, universities such as Oxford, Stanford, and Keio have established dedicated programmes, while peer-reviewed journals have begun to publish scenario analyses and policy studies. Ongoing debates question the predictive value of singularity scenarios and warn against a deterministic view of technology.\n\nCharacteristics of research\nSingularity studies extends beyond mere future predictions and offer an intellectual foundation for proactively designing and creating a desirable future. Principal research themes in this realm include: \n\nEthics of AI;\nSocial implications of technologies;\nPossibility of harmonious coexistence of humans and AI;\nCommunication with AI; and\nRedesign of social systems.\n\nTechnologists and academics\nVernor Vinge: Propounded the concept of singularity in 1993, making a massive impact on the academic and science-fiction spheres.\nRay Kurzweil: Predicted the advent around 2045 of the technological singularity in his 2005 book The Singularity Is Near.\nNick Bostrom: Offered philosophical reflections on superintelligence and the risks posed by AI. He is the founding director of the now-dissolved Future of Humanity Institute at the University of Oxford.\n\nJapan\nKento Sasano: A social informatician, AI educator, and inventor. He is the president of the Japan Society of Singularity Studies.\n\nChallenges and outlook\nSingularity studies is still evolving as an academic field, and quite a few challenges remain unresolved in regard to the systematization of their theories, research methods, and educational curricula. That said, in this day and age of accelerating technological and societal\nshifts, interdisciplinary approaches have gained in importance and are drawing much attention in the arenas of scholarly research, intercorporate collaboration, and policy planning.\n\nSee also\nArtificial general intelligence – Type of AI with wide-ranging abilities\nFutures studies – Study of postulating possible, probable, and preferable futures\nSingularitarianism – Belief in an incipient technological singularity\nSuperintelligence – Hypothetical agent surpassing human intelligence\n\nReferences\nFurther reading\nKurzweil, Ray (2005). The Singularity Is Near. New York, New York: Penguin Group. ISBN 9780715635612.\nBostrom, Nick (2002), \"Existential Risks\", Journal of Evolution and Technology, 9, archived from the original on 2011-04-27, retrieved 2007-08-07\nGood, I. J. (1965), \"Speculations Concerning the First Ultraintelligent Machine\", in Franz L. Alt; Morris Rubinoff (eds.), Advances in Computers Volume 6 (PDF), vol. 6, Academic Press, pp. 31–88, doi:10.1016/S0065-2458(08)60418-0, hdl:10919/89424, ISBN 9780120121069, archived from the original on 2001-05-27, retrieved 2007-08-07\nMarcus, Gary, \"Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind\",Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63. Multiple tests of artificial-intelligence efficacy are needed because, \"just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence.\" One such test, a \"Construction Challenge\", would test perception and physical action—\"two important elements of intelligent behavior that were entirely absent from the original Turing test.\" Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take. A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation. \"[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways.\" A prominent example is known as the \"pronoun disambiguation problem\": a machine has no way of determining to whom or what a pronoun in a sentence—such as \"he\", \"she\" or \"it\"—refers.\n\nExternal links\n\"Singularity University of Singularity Group\". Singularity Group. Retrieved 2025-02-11.\n\"Japan Society of Singularity Studies\". Japan Society of Singularity Studies. Retrieved 2025-01-26.\nIntelligence Explosion FAQ by the Machine Intelligence Research Institute\nThe Coming Technological Singularity:How to Survive in the Post-Human Era (on Vernor Vinge's web site, retrieved Jul 2019)",
        "url": "https://en.wikipedia.org/wiki/Singularity_studies"
    },
    {
        "title": "Situated",
        "text": "In artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\nthey exist in a dynamic (rapidly changing) environment, which\nthey can manipulate or change through their actions, and which\nthey can sense or perceive.\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually.  The situated perspective emphasizes that intelligent behaviour derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\nReferences\nHendriks-Jansen, Horst (1996) Catching Ourselves in the Act: Situated Activity, Interactive Emergence, Evolution, and Human Thought. Cambridge, Massachusetts: MIT Press.",
        "url": "https://en.wikipedia.org/wiki/Situated"
    },
    {
        "title": "Situated approach (artificial intelligence)",
        "text": "In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\nThe approach was originally proposed as an alternative to traditional approaches (that is, approaches popular before 1985 or so).\nAfter several decades, classical AI technologies started to face intractable issues (e.g. combinatorial explosion) when confronted with real-world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI.\n\nEmergence of a concept\nFrom traditional AI to Nouvelle AI\nDuring the late 1980s, the approach now known as Nouvelle AI (Nouvelle means new in French) was pioneered at the MIT Artificial Intelligence Laboratory by Rodney Brooks. As opposed to classical or traditional artificial intelligence, Nouvelle AI purposely avoided the traditional goal of modeling human-level performance, but rather tries to create systems with intelligence at the level of insects, closer to real-world robots. But eventually, at least at MIT new AI did lead to an attempt for humanoid AI in the Cog Project.\n\nFrom Nouvelle AI to behavior-based and situated AI\nThe conceptual shift introduced by nouvelle AI flourished in the robotics area, given way to behavior-based robotics (BBR), a methodology for developing AI based on a modular decomposition of intelligence. It was made famous by Rodney Brooks: his subsumption architecture was one of the earliest attempts to describe a mechanism for developing BBAI. It is extremely popular in robotics and to a lesser extent to implement intelligent virtual agents because it allows the successful creation of real-time dynamic systems that can run in complex environments.  For example, it underlies the intelligence of the Sony Aibo and many RoboCup robot teams.\nRealizing that in fact all these approaches were aiming at building not an abstract intelligence, but rather an intelligence situated in a given environment, they have come to be known as the situated approach. In fact, this approach stems out from early insights of Alan Turing, describing the need to build machines equipped with sense organs to learn directly from the real-world instead of focusing on abstract activities, such as playing chess.\n\nDefinitions\nClassically, a software entity is defined as a simulated element, able to act on itself and on its environment, and which has an internal representation of itself and of the outside world. An entity can communicate with other entities, and its behavior is the consequence of its perceptions, its representations, and its interactions with the other entities.\n\nAI loop\nSimulating entities in a virtual environment requires simulating the entire process that goes from a perception of the environment, or more generally from a stimulus, to an action on the environment. This process is called the AI loop and technology used to simulate it can be subdivided in two categories. Sensorimotor or low-level AI deals with either the perception problem (what is perceived?) or the animation problem (how are actions executed?). Decisional or high-level AI deals with the action selection problem (what is the most appropriate action in response to a given perception, i.e. what is the most appropriate behavior?).\n\nTraditional or symbolic AI\nThere are two main approaches in decisional AI. The vast majority of the technologies available on the market, such as planning algorithms, finite-state machines (FSA), or expert systems, are based on the traditional or symbolic AI approach. Its main characteristics are:\n\nIt is top-down: it subdivides, in a recursive manner, a given problem into a series of sub-problems that are supposedly easier to solve.\nIt is knowledge-based: it relies on a symbolic description of the world, such as a set of rules.\nHowever, the limits of traditional AI, which goal is to build systems that mimic human intelligence, are well-known: inevitably, a combinatorial explosion of the number of rules occurs due to the complexity of the environment. In fact, it is impossible to predict all the situations that will be encountered by an autonomous entity.\n\nSituated or behavioral AI\nIn order to address these issues, another approach to decisional AI, also known as situated or behavioral AI, has been proposed. It does not attempt to model systems that produce deductive reasoning processes, but rather systems that behave realistically in their environment. The main characteristics of this approach are the following:\n\nIt is bottom-up: it relies on elementary behaviors, which can be combined to implement more complex behaviors.\nIt is behavior-based: it does not rely on a symbolic description of the environment, but rather on a model of the interactions of the entities with their environment.\nThe goal of situated AI is to model entities that are autonomous in their environment. This is achieved thanks to both the intrinsic robustness of the control architecture, and its adaptation capabilities to unforeseen situations.\n\nSituated agents\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\nthey exist in a dynamic (rapidly changing) environment, which\nthey can manipulate or change through their actions, and which\nthey can sense or perceive.\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the Internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually.  The situated perspective emphasizes that intelligent behavior derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\nImplementation principles\nModular decomposition\nThe most important attribute of a system driven by situated AI is that the intelligence is controlled by a set of independent semi-autonomous modules. In the original systems, each module was actually a separate device or was at least conceived of as running on its own processing thread. Generally, though, the modules are just abstractions. In this respect, situated AI may be seen as a software engineering approach to AI, perhaps akin to object oriented design.\nSituated AI is often associated with reactive planning, but the two are not synonymous. Brooks advocated an extreme version of cognitive minimalism which required initially that the behavior modules were finite-state machines and thus contained no conventional memory or learning. This is associated with reactive AI because reactive AI requires reacting to the current state of the world, not to an agent's memory or preconception of that world. However, learning is obviously key to realistic strong AI, so this constraint has been relaxed, though not entirely abandoned.\n\nAction selection mechanism\nThe situated AI community has presented several solutions to modeling decision-making processes, also known as action selection mechanisms. The first attempt to solve this problem goes back to subsumption architectures, which were in fact more an implementation technique than an algorithm. However, this attempt paved the way to several others, in particular the free-flow hierarchies and activation networks. A comparison of the structure and performances of these two mechanisms demonstrated the advantage of using free-flow hierarchies in solving the action selection problem. However, motor schemas and process description languages are two other approaches that have been used with success for autonomous robots.\n\nNotes and references\nArsenio, Artur M. (2004) Towards an embodied and situated AI, In: Proceedings of the International FLAIRS conference, 2004. (online)\nThe Artificial Life Route To Artificial Intelligence: Building Embodied, Situated Agents, Luc Steels and Rodney Brooks Eds., Lawrence Erlbaum Publishing, 1995. (ISBN 978-0805815184)\nRodney A. Brooks Cambrian Intelligence (MIT Press, 1999) ISBN 0-262-52263-2; collection of early papers including \"Intelligence without representation\" and \"Intelligence without reason\", from 1986 & 1991 respectively.\nRonald C. Arkin Behavior-Based Robotics (MIT Press, 1998) ISBN 0-262-01165-4\nHendriks-Jansen, Horst (1996) Catching Ourselves in the Act: Situated Activity, Interactive Emergence, Evolution, and Human Thought. Cambridge, Mass.: MIT Press.\n\nSee also\nRelated articles\nArtificial intelligence\nCognitive science\n\nTraditional AI\nDecision tree\nFinite-state machine\nExpert system\nAutomated planning and scheduling\n\nSituated AI\nScruffy AI\nReactive planning\n\nRobotics\nBehavior-based robotics\nSituated robotics\n\nExternal links\nArticle Artificial Intelligence: The situated approach from the Encyclopædia Britannica\nNouvelle AI - Definition\nReactive planning and nouvelle AI",
        "url": "https://en.wikipedia.org/wiki/Situated_approach_(artificial_intelligence)"
    },
    {
        "title": "Smart object",
        "text": "A smart object is an object that enhances the interaction with not only people but also with other smart objects. Also known as smart connected products or smart connected things (SCoT), they are products, assets and other things embedded with processors, sensors, software and connectivity that allow data to be exchanged between the product and its environment, manufacturer, operator/user, and other products and systems. Connectivity also enables some capabilities of the product to exist outside the physical device, in what is known as the product cloud.  The data collected from these products can be then analyzed to inform decision-making, enable operational efficiencies and continuously improve the performance of the product.\nIt can not only refer to interaction with physical world objects but also to interaction with virtual (computing environment) objects. A smart physical object may be created either as an artifact or manufactured product or by embedding electronic tags such as RFID tags or sensors into non-smart physical objects. Smart virtual objects are created as software objects that are intrinsic when creating and operating a virtual or cyber world simulation or game. The concept of a smart object has several origins and uses, see History. There are also several overlapping terms, see also smart device, tangible object or tangible user interface and Thing as in the Internet of things.\n\nHistory\nIn the early 1990s, Mark Weiser, from whom the term ubiquitous computing originated, referred to a vision \"When almost every object either contains a computer or can have a tab attached to it, obtaining information will be trivial\", \nAlthough Weiser did not specifically refer to an object as being smart, his early work did imply that smart physical objects are smart in the sense that they act as digital information sources. Hiroshi  Ishii and Brygg Ullmer refer to tangible objects in terms of tangibles bits or tangible user interfaces that enable users to \"grasp & manipulate\" bits in the center of users' attention by coupling the bits with everyday physical objects and architectural surfaces.\nThe smart object concept was introduced by Marcelo Kallman and Daniel Thalmann as an object that can describe its own possible interactions. The main focus here is to model interactions of smart virtual objects with virtual humans, agents, in virtual worlds. The opposite approach to smart objects is 'plain' objects that do not provide this information. The additional information provided by this concept enables far more general interaction schemes, and can greatly simplify the planner of an artificial intelligence agent.\nIn contrast to smart virtual objects used in virtual worlds, Lev Manovich focuses on physical space filled with electronic and visual information. Here,  \"smart objects\" are described as \"objects connected to the Net; objects that can sense their users and display smart behaviour\".\nMore recently in the early 2010s, smart objects are being proposed as a key enabler for the vision of the Internet of things. The combination of the Internet and emerging technologies such as near field communications, real-time localization, and embedded sensors enables everyday objects to be transformed into smart objects that can understand and react to their environment. Such objects are building blocks for the Internet of things and enable novel computing applications. In 2018, one of the world's first smart houses was built in Klaukkala, Finland in the form of a five-floor apartment block, using the Kone Residential Flow solution created by KONE, allowing even a smartphone to act as a home key.\n\nCharacteristics\nAlthough we can view interaction with physical smart object in the physical world as distinct from interaction with virtual smart objects in a virtual simulated world, these can be related. Poslad considers the progression of: how\n\nhumans use models of smart objects situated in the physical world to enhance human to physical world interaction; versus how\nsmart physical objects situated in the physical world can model human interaction in order to lessen the need for human to physical world interaction; versus how\nvirtual smart objects by modelling both physical world objects and modelling humans as objects and their subsequent interactions can form a predominantly smart virtual object environment.\n\nSmart physical objects\nThe concept smart for a smart physical object simply means that it is active, digital, networked, can operate to some extent autonomously, is reconfigurable and has local control of the resources it needs such as energy, data storage, etc. Note, a smart object does not necessarily need to be intelligent as in exhibiting a strong essence of artificial intelligence—although it can be designed to also be intelligent.\nPhysical world smart objects can be described in terms of three properties:\n\nAwareness: is a smart object's ability to understand (that is, sense, interpret, and react to) events and human activities occurring in the physical world.\nRepresentation: refers to a smart object's application and programming model—in particular, programming abstractions.\nInteraction: denotes the object's ability to converse with the user in terms of input, output, control, and feedback.\nBased upon these properties, these have been classified into three types:\n\nActivity-Aware Smart Objects: Are objects that can record information about work activities and its own use.\nPolicy-Aware Smart Objects: Are objects that are activity-aware  Objects can interpret events and activities with respect to predefined organizational policies.\nProcess-Aware Smart Objects: Processes play a fundamental role in industrial work management and operation. A process is a collection of related activities or tasks that are ordered according to their position in time and space.\n\nSmart virtual objects\nFor the virtual object in a virtual world case, an object is called smart when it has the ability to describe its possible interactions. This focuses on constructing a virtual world using only virtual objects that contain their own interaction information. There are four basic elements to constructing such a smart virtual object framework.\n\nObject properties: physical properties and a text description\nInteraction information: position of handles, buttons, grips, and the like\nObject behavior: different behaviors based on state variables\nAgent behaviors: description of the behavior an agent should follow when using the object\nSome versions of smart objects also include animation information in the object information, but this is not considered to be an efficient approach, since this can make objects inappropriately oversized.\nThe term smart products can be confusing as it is used to cover a broad range of different products, ranging from smart home appliances (e.g., smart bathroom scales or smart light bulbs) to smart cars (e.g., Tesla). While these products share certain similarities, they often differ substantially in their capabilities. Raff et al. developed a conceptual framework that distinguishes different smart products based on their capabilities, which features 4 types of smart product archetypes (in ascending order of \"smartness\")[2]\n\nCategorization\nThe terms smart, connected product or smart product can be confusing as it is used to cover a broad range of different products, ranging from smart home appliances (e.g., smart bathroom scales or smart light bulbs) to smart cars (e.g., Tesla). While these products share certain similarities, they often differ substantially in their capabilities. Raff et al. developed a conceptual framework that distinguishes different smart products based on their capabilities, which features 4 types of smart product archetypes (in ascending order of \"smartness\").\n\nDigital\nConnected\nResponsive\nIntelligent\n\nAdvantages\nSmart, connected products have three primary components:\n\nPhysical – made up of the product's mechanical and electrical parts.\nSmart – made up of sensors, microprocessors, data storage, controls, software, and an embedded operating system with enhanced user interface.\nConnectivity – made up of ports, antennae, and protocols enabling wired/wireless connections that serve two purposes, it allows data to be exchanged with the product and enables some functions of the product to exist outside the physical device.\nEach component expands the capabilities of one another resulting in \"a virtuous cycle of value improvement\". First, the smart components of a product amplify the value and capabilities of the physical components. Then, connectivity amplifies the value and capabilities of the smart components. These improvements include:\n\nMonitoring of the product's conditions, its external environment, and its operations and usage.\nControl of various product functions to better respond to changes in its environment, as well as to personalize the user experience.\nOptimization of the product's overall operations based on actual performance data, and reduction of downtimes through predictive maintenance and remote service.\nAutonomous product operation, including learning from their environment, adapting to users' preferences and self-diagnosing and service.\n\nThe Internet of things (IoT)\nThe Internet of things is the network of physical objects that contain embedded technology to communicate and sense or interact with their internal states or the external environment.  The phrase \"Internet of things\" reflects the growing number of smart, connected products and highlights the new opportunities they can represent. The Internet, whether involving people or things, is a mechanism for transmitting information. What makes smart, connected products fundamentally different is not the Internet, but the changing nature of the 'things'. Once a product is smart and connected to the cloud, the products and services will become part of an interconnected management solution. Companies can evolve from making products to offering more complex, higher-value offerings within a \"system of systems\".\n\nSee also\nAmbieSense\nAudiocubes\nHome network\nIntelligent maintenance system\nNabaztag\nSmart speaker\nWearable technology\nUbiquitous computing\n\nReferences\nFurther reading\nDonald A. Norman. Design of Future Things. Basic Books. 2007\nBruce Sterling. Cisco launches consortium for ‘Smart Objects'. Wired, September 25, 2008\n2009   New Media Horizons Report\nMike Isaac. home-google-io/ Google's Platform Extends Its Reach With   Android@Home. Wired, May 11, 2011\n\nExternal links\nWorldCat publications about smart objects.\nThe Internet of Things' Best-Kept Secret, Forbes\nA Very Short History Of The Internet Of Things, Forbes\nThree Steps to Combat the Impact of Digital Business Disruption on Value Creation, Gartner\nThe Five SMART Technologies to Watch, Gartner\nCisco White Paper: The Internet of Everything for Cities\n5 Steps the 'Smart' Home Industry Must Take to Develop a Consumer Market\nMashable: Bionic Pancreas Delivers Automated Care to Those With Diabetes\nThe Future of Wearable Technology PBS video produced by Off Book (web series)\nOxford Economics: Smart, connected products: Manufacturing's next transformation",
        "url": "https://en.wikipedia.org/wiki/Smart_object"
    },
    {
        "title": "Software agent",
        "text": "In computer science, a software agent is a computer program that acts for a user or another program in a relationship of agency.\nThe term agent is derived from the Latin agere (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate. Some agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot executing on a computer, such as a mobile device, e.g. Siri. Software agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\nRelated and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors).\n\nConcepts\nThe basic attributes of an autonomous software agent are that agents:\n\nare not strictly invoked for a task, but activate themselves,\nmay reside in wait status on a host, perceiving context,\nmay get to run status on a host upon starting conditions,\ndo not require interaction of user,\nmay invoke other tasks including communication.\n\nThe concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of methods and attributes, an agent is defined in terms of its behavior.\nVarious authors have proposed different definitions of agents, these commonly include concepts such as:\n\npersistence: code is not executed on demand but runs continuously and decides for itself when it should perform some activity;\nautonomy: agents have capabilities of task selection, prioritization, goal-directed behavior, decision-making without human intervention;\nsocial ability: agents are able to engage other components through some sort of communication and coordination, they may collaborate on a task;\nreactivity: agents perceive the context in which they operate and react to it appropriately.\n\nDistinguishing agents from programs\nAll agents are programs, but not all programs are agents. Contrasting the term with related concepts may help clarify its meaning. Franklin & Graesser (1997) discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.\n\nIntuitive distinguishing agents from objects\nAgents are more autonomous than objects.\nAgents have flexible behavior: reactive, proactive, social.\nAgents have at least one thread of control but may have more.\n\nDistinguishing agents from expert systems\nExpert systems are not coupled to their environment.\nExpert systems are not designed for reactive, proactive behavior.\nExpert systems do not consider social ability.\n\nDistinguishing intelligent software agents from intelligent agents in AI\nIntelligent agents (also known as rational agents) are not just computer programs: they may also be machines, human beings, communities of human beings (such as firms) or anything that is capable of goal-directed behavior.\n(Russell & Norvig 2003)\n\nImpact of software agents\nSoftware agents may offer various benefits to their end users by automating complex or repetitive tasks. However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.\n\nOrganizational impact\nWork contentment and job satisfaction impact\nPeople like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference. Such conditions may be secured by application of software agents for required formal support.\n\nCultural impact\nThe cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user's behalf, a software agent needs to have a complete understanding of a user's profile, including his/her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the world with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.\n\nHistory\nThe concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - \"A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.\"\nTo be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS). MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.\nJohn Sculley's 1987 \"Knowledge Navigator\" video portrayed an image of a relationship between end-users and agents. Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach. The range of agent types is now (from 1990) broad: WWW, search engines, etc.\n\nExamples  of intelligent software agents\nBuyer agents (shopping bots)\nBuyer agents travel around a network (e.g. the internet) retrieving information about goods and services. These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products. Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.\n\nUser agents (personal agents)\nUser agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:\n\nCheck your e-mail, sort it according to the user's order of preference, and alert you when important emails arrive.\nPlay computer games as your opponent or patrol game areas for you.\nAssemble customized news reports for you. There are several versions of these, including CNN.\nFind information for you on the subject of your choice.\nFill out forms on the Web automatically for you, storing your information for future reference\nScan Web pages looking for and highlighting text that constitutes the \"important\" part of the information there\nDiscuss topics with you ranging from your deepest fears to sports\nFacilitate with online job search duties by scanning known job boards and sending the resume to opportunities who meet the desired criteria\nProfile synchronization across heterogeneous social networks\n\nMonitoring-and-surveillance (predictive) agents\nMonitoring and surveillance agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.\n\nFor example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.\nA special case of monitoring-and-surveillance agents are organizations of agents used to automate decision-making process during tactical operations. The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive goals from higher level agents. The agents then pursue the goals with the assets at hand, minimizing expenditure of the assets while maximizing goal attainment.\n\nData-mining agents\nThis agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking.\nA data mining agent operates in a data warehouse discovering information. A 'data warehouse' brings together information from many different sources. \"Data mining\" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.\n'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.\n\nNetworking and communicating agents\nSome other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents.\n\nUser agent - for browsing the World Wide Web\nMail transfer agent - For serving E-mail, such as Microsoft Outlook. Why? It communicates with the POP3 mail server, without users having to understand POP3 command protocols. It even has rule sets that filter mail for the user, thus sparing them the trouble of having to do it themselves.\nSNMP agent\nIn Unix-style networking servers, httpd is an HTTP daemon that implements the Hypertext Transfer Protocol at the root of the World Wide Web\nManagement agents used to manage telecom devices\nCrowd simulation for safety planning or 3D computer graphics,\nWireless beaconing agent is a simple process hosted single tasking entity for implementing wireless lock or electronic leash in conjunction with more complex software agents hosted e.g. on wireless receivers.\nUse of autonomous agents (deliberately equipped with noise) to optimize coordination in groups online.\n\nSoftware development agents (aka software bots)\nSoftware bots are becoming important in software engineering.\n\nSecurity agents\nAgents are also used in software security application to intercept, examine and act on various types of content.  Example include: \n\nData Loss Prevention (DLP) Agents - examine user operations on a computer or network, compare with policies specifying allowed actions, and take appropriate action (e.g. allow, alert, block).  The more comprehensive DLP agents can also be used to perform EDR functions.\nEndpoint Detection and Response (EDR) Agents - monitor all activity on an endpoint computer in order to detect and respond to malicious activities\nCloud Access Security Broker (CASB) Agents - similar to DLP Agents, however examining traffic going to cloud applications\n\nDesign issues\nIssues to consider in the development of agent-based systems include \n\nhow tasks are scheduled and how synchronization of tasks is achieved\nhow tasks are prioritized by agents\nhow agents can collaborate, or recruit resources,\nhow agents can be re-instantiated in different environments, and how their internal state can be stored,\nhow the environment will be probed and how a change of environment leads to behavioral changes of the agents\nhow messaging and communication can be achieved,\nwhat hierarchies of agents are useful (e.g. task execution agents, scheduling agents, resource providers ...).\nFor software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata.\nThe definition of agent processing can be approached from two interrelated directions:\n\ninternal state processing and ontologies for representing knowledge\ninteraction protocols – standards for specifying communication of tasks\nAgent systems are used to model real-world systems with concurrency or parallel processing.\n\nAgent Machinery – Engines of various kinds, which support the varying degrees of intelligence\nAgent Content – Data employed by the machinery in Reasoning and Learning\nAgent Access – Methods to enable the machinery to perceive content and perform actions as outcomes of Reasoning\nAgent Security – Concerns related to distributed computing, augmented by a few special concerns related to agents\nThe agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content (or event) is then passed to the agent's Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user-access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.\nBots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.\n\nNotions and frameworks for agents\nDAML (DARPA Agent Markup Language)\n3APL (Artificial Autonomous Agents Programming Language)\nGOAL agent programming language\nOpen Agent Architecture (OAA)\nWeb Ontology Language (OWL)\ndaemons in Unix-like systems.\nJava Agent Template (JAT)\nJava Agent Development Framework (JADE)\nSARL agent programming language (arguably an Actor and not Agent oriented paradigm)\n\nSee also\nAgent architecture\nChatbot\nData loss prevention\nEndpoint detection and response\nSoftware bot\n\nReferences\nExternal links\nSoftware Agents: An Overview Archived July 17, 2011, at the Wayback Machine, Hyacinth S. Nwana. Knowledge Engineering Review, 11(3):1–40, September 1996. Cambridge University Press.\nFIPA The Foundation for Intelligent Physical Agents\nJADE Java Agent Developing Framework, an Open Source framework developed by Telecom Italia Labs\nEuropean Software-Agent Research Center Archived 2017-09-14 at the Wayback Machine\nJAFIMA JAFIMA: A Java based Agent Framework for Intelligent and Mobile Agents\nSemanticAgent An Open Source framework to develop SWRL based Agents on top of JADE\nMobile-C A Multi-Agent Platform for Mobile C/C++ Agents.\nHLL High-Level Logic (HLL) Open Source Project.\nOpen source project KATO for PHP and Java developers to write software agents",
        "url": "https://en.wikipedia.org/wiki/Software_agent"
    },
    {
        "title": "Sparkles emoji",
        "text": "The Sparkles emoji (✨) is an emoji that has one large star surrounded by smaller stars. Originating from Japan to represent sparkles used in anime and manga, the sparkles are often used as emphasis in text by surrounding words or phrases with it. It is the third most-used emoji in the world on Twitter as of 2021, and since the early 2020s it has been used by major software companies to represent artificial intelligence.\n\nDevelopment\nAccording to Emojipedia, the Sparkles emoji was first used by Japanese mobile operators SoftBank, Docomo and au in the late 1990s. The emoji was added to Unicode 6.0 in 2010 and Emoji 1.0 in 2015.\nOn some platforms the Sparkles emoji has been multicoloured whilst on other platforms it has been one colour. Twitter and Microsoft's Sparkles have changed from being multicoloured to being a single colour. Samsung's version of the emoji previously had a night sky in the background.\n\nUsage\nInterpersonal communication\nThe Sparkles emoji was originally meant to represent the usage of sparkles in Japanese anime and manga, where the sparkles are used to represent beauty, happiness or awe. The emoji has several meanings and depends upon context. Starting in the late 2010s, the emoji started being used to surround words or phrases to be used as emphasis, an example from the book Because Internet being \"I would simply ✨pass away✨\". It can also be used as sarcasm, irony or as a way to mock people. Without emoji this could be represented with tildes or asterisks, for example, \"~tildes~\" or \"~*asterisk plus tilde*~\" or \"~*~*true sparkle exuberance*~*~\". The sparkles emoji can be used to represent stars in text, be used to represent cleanliness or can be used to mean \"orgasm\" whilst sexting.\nIn September 2021 the Sparkles emoji overtook the Pleading Face (🥺) emoji to become the third most-used emoji in the world according to Emojipedia, with approximately 1 per cent of all tweets containing the Sparkles emoji.\n\nArtificial intelligence\nIn the early 2020s, the Sparkles emoji started being used as an icon to represent artificial intelligence (AI). Companies who use the emoji this way include Google, OpenAI, Samsung, Microsoft, Adobe, Spotify and Zoom. As of August 2024, seven of the top 10 software companies by market capitalisation use the Sparkles emojis with AI. OpenAI has different versions of the Sparkles for different versions of the models that ChatGPT uses. One explanation is that Sparkles is being used by these companies as a way to market AI as \"magic\". Marketing technology as \"magic\" has been used before AI, particularly by Apple. Another explanation given by designers and marketers choosing to use Sparkles to signify AI is simply that other platforms are doing it, making it familiar to users.\n\nAround 2024, some of these companies started removing two of the smaller stars from the emoji in their AI services and have kept the one large star, an example being Google's Gemini chatbot.\nIn early 2024, the Nielsen Norman Group provided test subjects with the star in isolation and found that people did not associate the symbol with AI, but instead mostly with \"optimisation\" or \"favourite or save an item\".\n\nReferences\nExternal links\nPozos, Rose; Schmidt, Lennard. \"Rise of the AI Sparkle Icon\". Google Design. Archived from the original on 12 March 2025. Retrieved 25 April 2025.",
        "url": "https://en.wikipedia.org/wiki/Sparkles_emoji"
    },
    {
        "title": "Spreading activation",
        "text": "Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or \"activation\" and then iteratively propagating or \"spreading\" that activation out to other nodes linked to the source nodes.  Most often these \"weights\" are real values that decay as activation propagates through the network.  When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.\nSpreading activation in semantic networks as a model were invented in cognitive psychology to model the fan out effect.\nSpreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.\n\nCognitive psychology\nAs it relates to cognitive psychology, spreading activation is the theory of how the brain iterates through a network of associated ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept. In memory psychology, the spreading activation model holds that people organize their knowledge of the world based on their personal experiences, which in turn form the network of ideas that is the person's knowledge of the world.\nWhen a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word \"doctor\" when it is preceded by \"nurse\" than when it is preceded by an unrelated word like \"carrot\". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.\nAs another example, if the original concept is \"red\" and the concept \"vehicles\" is primed, they are much more likely to say \"fire engine\" instead of something unrelated to vehicles, such as \"cherries\". If instead \"fruits\" was primed, they would likely name \"cherries\" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.\n\nAlgorithm\nA directed graph is populated by Nodes[ 1...N ]  each having an associated activation value A [ i ] which is a real number in the range [0.0 ... 1.0].  A Link[ i, j ] connects source node[ i ] with target node[ j ].  Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].\nParameters:\n\nFiring threshold F, a real number in the range [0.0 ... 1.0]\nDecay factor D, a real number in the range [0.0 ... 1.0]\nSteps:\n\nInitialize the graph setting all activation values A [ i ] to zero.   Set one or more origin nodes to an initial activation value greater than the firing threshold F.  A typical initial value is 1.0.\nFor each unfired node [ i ] in the graph having an activation value A [ i ] greater than the node firing threshold F:\nFor each Link [ i, j ] connecting the source node [ i ] with target node [ j ], adjust A [ j ] = A [ j ] + (A [ i ] * W [ i, j ] * D) where D is the decay factor.\nIf a target node receives an adjustment to its activation value so that it would exceed 1.0, then set its new activation value to 1.0.  Likewise maintain 0.0 as a lower bound on the target node's activation value should it receive an adjustment to below 0.0.\nOnce a node has fired it may not fire again, although variations of the basic algorithm permit repeated firings and loops through the graph.\nNodes receiving a new activation value that exceeds the firing threshold F are marked for firing on the next spreading activation cycle.\nIf activation originates from more than one node, a variation of the algorithm permits marker passing to distinguish the paths by which activation is spread over the graph\nThe procedure terminates when either there are no more nodes to fire or in the case of marker passing from multiple origins, when a node is reached from more than one path. Variations of the algorithm that permit repeated node firings and activation loops in the graph, terminate after a steady activation state, with respect to some delta, is reached, or when a maximum number of iterations is exceeded.\n\nExamples\nSee also\nConnectionism\n\nNotes\nReferences\nNils J. Nilsson. \"Artificial Intelligence: A New Synthesis\". Morgan Kaufmann Publishers, Inc., San Francisco, California, 1998, pages 121-122\nRodriguez, M.A., \" Grammar-Based Random Walkers in Semantic Networks\", Knowledge-Based Systems, 21(7), 727-739, doi:10.1016/j.knosys.2008.03.030, 2008.\nKaralyn Patterson, Peter J. Nestor & Timothy T. Rogers \"Where do you know what you know? The representation of semantic knowledge in the human brain\", Nature Reviews Neuroscience 8, 976-987 (December 2007)",
        "url": "https://en.wikipedia.org/wiki/Spreading_activation"
    },
    {
        "title": "Supermind AI",
        "text": "Supermind is a state-funded Chinese artificial intelligence platform that tracks scientists and researchers internationally.\nThe platform is the flagship project of Shenzhen's International Science and Technology Information Center. It mines data from science and technology databases such as Springer, Wiley, Clarivate and Elsevier. It is intended to detect technological breakthroughs and to identify possible sources of talent as part of China's efforts to advance technologically.\nThe platform also uses government data security and security intelligence organizations such as Peng Cheng Laboratory, the China National GeneBank, BGI Group and the Key Laboratory of New Technologies of Security Intelligence.\nAccording to Hong Kong-based Asia Times, the platform, \"While not an overt espionage tool...may be used to identify key personnel who could be bribed, deceived or manipulated into divulging classified information\".\nThe Organisation for Economic Co-operation and Development (OECD) flagged the project as an incident, meaning it may be of interest to policymakers and other stakeholders.\nUS technology group American Edge Project criticized the project as a global risk of China's security services using the platform to place agents in jobs with access to important information, recruit technical personnel, and identify targets for hacking operations.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Supermind_AI"
    },
    {
        "title": "SUPS",
        "text": "In computational neuroscience, SUPS (for Synaptic Updates Per Second) or formerly CUPS (Connections Updates Per Second) is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science.\n\nComputing\nFor a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and average connectivity \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n(synapses) per neuron per second:\n\n  \n    \n      \n        S\n        U\n        P\n        S\n        =\n        c\n        ×\n        N\n      \n    \n    {\\displaystyle SUPS=c\\times N}\n  \n\nDepending on the type of simulation it is usually equal to the total number of synapses simulated.\nIn an \"asynchronous\" dynamic simulation if a neuron spikes at \n  \n    \n      \n        υ\n      \n    \n    {\\displaystyle \\upsilon }\n  \n Hz, the average rate of synaptic updates provoked by the activity of that neuron is \n  \n    \n      \n        υ\n        c\n        N\n      \n    \n    {\\displaystyle \\upsilon cN}\n  \n. In a synchronous simulation with step \n  \n    \n      \n        Δ\n        t\n      \n    \n    {\\displaystyle \\Delta t}\n  \n the number of synaptic updates per second would be \n  \n    \n      \n        \n          \n            \n              c\n              N\n            \n            \n              Δ\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {cN}{\\Delta t}}}\n  \n. As \n  \n    \n      \n        Δ\n        t\n      \n    \n    {\\displaystyle \\Delta t}\n  \n has to be chosen much smaller than the average interval between two successive afferent spikes, which implies \n  \n    \n      \n        Δ\n        t\n        <\n        \n          \n            1\n            \n              υ\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta t<{\\frac {1}{\\upsilon N}}}\n  \n, giving an average of synaptic updates equal to \n  \n    \n      \n        υ\n        c\n        \n          N\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\upsilon cN^{2}}\n  \n. Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity O(N) per neuron, compared with the O(N2) in the \"synchronous\" case.\n\nRecords\nDeveloped in the 1980s  Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full neural network (NNW). It was designed as a coprocessor to a host and has 64 sub-processors arranged in a 1D array and operating in a SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. At 25 MHz it is capable of 1.28 GMAC.\nAfter the presentation of the RN-100 (12 MHz) single neuron chip at Seattle 1991 Ricoh developed the multi-neuron chip RN-200. It had 16 neurons and 16 synapses per neuron. The chip has on-chip learning ability using a proprietary backdrop algorithm. It came in a 257-pin PGA encapsulation and drew 3.0 W at a maximum. It was capable of 3 GCPS (1 GCPS at 32 MHz).\n\nIn 1991–97, Siemens developed the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 was a fast matrix-matrix multiplier that can be combined to form systolic arrays. It could process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) at a rate of 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contained 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.\nIn 2013, the K computer was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses (1% of the human brain). The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level (4.4 on average). The simulation required 1 Petabyte of storage.\n\nSee also\nFLOP\nSPECint\nSPECfp\nMultiply–accumulate operation\nOrders of magnitude (computing)\nSyNAPSE\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/SUPS"
    },
    {
        "title": "Syman",
        "text": "SYMAN is an artificial intelligence technology that uses data from social media profiles to identify trends in the job market. SYMAN is designed to organize actionable data for products and services including recruiting, human capital management, CRM, and marketing.\nSYMAN was developed with a $21 million series B financing round secured by Identified, which was led by VantagePoint Capital Partners and Capricorn Investment Group.\n\nReferences\nSee also\nWorkday",
        "url": "https://en.wikipedia.org/wiki/Syman"
    },
    {
        "title": "Symbol level",
        "text": "In knowledge-based systems, agents choose actions based on the principle of rationality to move closer to a desired goal.  The agent is able to make decisions based on knowledge it has about the world (see knowledge level).  But for the agent to actually change its state, it must use whatever means it has available.  This level of description for the agent's behavior is the symbol level. The term was coined by Allen Newell in 1982.\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\nSee also\nKnowledge level modeling\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Symbol_level"
    },
    {
        "title": "Symbolic artificial intelligence",
        "text": "In artificial intelligence, symbolic artificial intelligence (also known as classical artificial intelligence  or logic-based artificial intelligence)\nis the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\nSymbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.\nNeural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning.\n\nHistory\nA short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.\n\nThe first AI summer: irrational exuberance, 1948–1966\nSuccess at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.\n\nApproaches inspired by human or animal cognition or behavior\nCybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.\nAn important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.\nDuring the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\nHerbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\nHeuristic search\nIn addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\" Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.\n\nEarly work on knowledge representation and reasoning\nEarly work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.\n\nModeling formal reasoning with logic: the \"neats\"\nUnlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.\nHis laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.\nLogic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\nModeling implicit common-sense knowledge with frames and scripts: the \"scruffies\"\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).\nCommonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\nThe first AI winter: crushed dreams, 1967–1977\nThe first AI winter was a shock:\n\nDuring the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs.\n...\n\nOutside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.\n\nThe second AI summer: knowledge is power, 1978–1987\nKnowledge-based systems\nAs limitations with weak, domain-independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.\nEdward Feigenbaum said:\n\n\"In the knowledge lies the power.\"\nto describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: \n\n(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.\n\nSuccess with expert systems\nThis \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.\nKey expert systems were:\n\nDENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.\nMYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. \"With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.\"\nINTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases.\nGUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.\nXCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.\nDENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:\n\nOne of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space.\nWe did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.\n\nThe generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.\nThe other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it:\n\nBy 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.\nChess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.\n\nArchitecture of knowledge-based and expert systems\nA key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.\nThe simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.\nExpert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies.\nBlackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.\n\nThe second AI winter, 1988–1993\nAt the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations.\nUnfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:\n\nMany reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.\n\nAdding in more rigorous foundations, 1993–2011\nUncertain reasoning\nBoth statistical approaches and extensions to logic were tried.\nOne statistical approach, hidden Markov models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.\nOther, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lotfi Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.\n\nMachine learning\nSymbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as\n\n...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.\nWe did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.\nIn contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules.\nAdvances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.\nSymbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.\nInductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.\nAs an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.\nSymbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:\n\nLearning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to \"avoid taking points.\"\nLearning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.\nLearning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.\nApprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.\nLearning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row.\nLearning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.\n\nDeep learning and neuro-symbolic AI 2011–now\nWith the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.\n\nNeuro-symbolic AI: integrating neural and symbolic approaches\nNeuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\", and in particular:\n\"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"\nHenry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.\nGarcez and Lamb describe research in this area as being ongoing for at least the past twenty years, dating from their 2002 book on neurosymbolic learning systems. A series of workshops on neuro-symbolic reasoning has been held every year since 2005.\nIn their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:\n\nThe integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.\nApproaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:\n\nSymbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.\nSymbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.\nNeural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically.\nNeural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.\nNeural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND–OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.\nNeural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.\nMany key research questions remain, such as:\n\nWhat is the best way to integrate neural and symbolic architectures?\nHow should symbolic structures be represented within neural networks and extracted from them?\nHow should common-sense knowledge be learned and reasoned about?\nHow can abstract knowledge that is hard to encode logically be handled?\n\nTechniques and contributions\nThis section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.\n\nAI programming languages\nThe key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.\nOther key innovations pioneered by LISP that have spread to other programming languages include:\n\nGarbage collection\nDynamic typing\nHigher-order functions\nRecursion\nConditionals\nPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages.\nIn contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption—any facts not known were considered false—and a unique name assumption for primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.\nAlain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.\nProlog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.\nJapan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.\nSmalltalk was another influential AI programming language. For example, it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.\nFor other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.\n\nSearch\nSearch arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.\n\nKnowledge representation and reasoning\nMultiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.\n\nKnowledge representation\nSemantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used.\nDescription logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is an ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.\nFirst-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.\n\nAutomatic theorem proving\nExamples of automated theorem provers for first-order logic are:\n\nProver9\nACL2\nVampire\nProver9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.\n\nReasoning in knowledge-based systems\nKnowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.\nForward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.\nA more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.\nCognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.\n\nCommonsense reasoning\nMarvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning.\nQualitative simulation, such as Benjamin Kuipers's QSIM, approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.\nSimilarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.\n\nConstraints and constraint-based reasoning\nConstraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).\n\nAutomated planning\nThe General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.\n\nNatural language processing\nNatural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.\nParsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles.\nNew deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.\n\nAgents and multi-agent systems\nAgents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication. The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture that includes deep learning for perception.\nIn contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.\n\nControversies\nControversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic \"neats\") and non-logicists (the anti-logic \"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.\n\nThe Frame Problem: knowledge representation challenges for first-order logic\nLimitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed.\nMcCarthy and Hayes introduced the Frame Problem in 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\" A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify what did not change.\nA similar problem, called the Qualification Problem, occurs in trying to enumerate the preconditions for an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly.\nMcCarthy's approach to fix the frame problem was circumscription, a kind of non-monotonic logic where deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Other non-monotonic logics provided truth maintenance systems that revised beliefs leading to contradictions.\nOther ways of handling more open-ended domains included probabilistic reasoning systems and machine learning to learn new concepts and rules.  McCarthy's Advice Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules.\nSimilar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures.  This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g., self-driving cars that do not know not to drive into cones or not to hit pedestrians walking a bicycle).\nMcCarthy viewed his Advice Taker as having common-sense, but his definition of common-sense was different than the one above. He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\"\n\nConnectionist AI: philosophical challenges and sociological conflicts\nConnectionist approaches include earlier work on neural networks, such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.\nThree philosophical positions have been outlined among connectionists:\n\nImplementationism—where connectionist architectures implement the capabilities for symbolic processing,\nRadical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it,\nModerate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence.\nOlazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:\nThe third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).\nGary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:To think that we can simply abandon symbol-manipulation is to suspend disbelief.\n\nAnd yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples.According to Marcus, Geoffrey Hinton and his colleagues have been vehemently \"anti-symbolic\":When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.\n...\n\nSince then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.\nPart of these disputes may be due to unclear terminology: \n\nTuring award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.It is worth noting that, from a theoretical perspective, the boundary of advantages between connectionist AI and symbolic AI may not be as clear-cut as it appears. For instance, Heng Zhang and his colleagues have proved that mainstream knowledge representation formalisms are  recursively isomorphic, provided they are universal or have equivalent expressive power. This finding implies that there is no fundamental distinction between using symbolic or connectionist knowledge representation formalisms for the realization of artificial general intelligence (AGI). Moreover, the existence of recursive isomorphisms suggests that different technical approaches can draw insights from one another. From this perspective, it seems unnecessary to overemphasize the advantages of any single technical school; instead, mutual learning and integration may offer the most promising path toward the realization of AGI.\n\nSituated robotics: the world as a model\nAnother critique of symbolic AI is the embodied cognition approach:\n\nThe embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.\nRodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\" He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"  In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"  His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\" and the use of the blocks world in symbolic AI systems such as SHRDLU.\n\nCurrent views\nEach approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.\n\nHybrid AIs incorporating one or more of these approaches are currently viewed as the path forward. Russell and Norvig conclude that:Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.\n\nSee also\nNotes\nCitations\nReferences\nBrooks, Rodney A. (1991). \"Intelligence without representation\". Artificial Intelligence. 47 (1): 139–159. doi:10.1016/0004-3702(91)90053-M. ISSN 0004-3702. S2CID 207507849. Retrieved 2022-09-13.\nClancey, William (1987). Knowledge-Based Tutoring: The GUIDON Program (MIT Press Series in Artificial Intelligence) (Hardcover ed.).\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3..\nDreyfus, Hubert L (1981). \"From micro-worlds to knowledge representation: AI at an impasse\" (PDF). Mind Design. MIT Press, Cambridge, MA: 161–204.\nGarcez, Artur S. d'Avila; Broda, Krysia; Gabbay, Dov M.; Gabbay, Augustus de Morgan Professor of Logic Dov M. (2002). Neural-Symbolic Learning Systems: Foundations and Applications. Springer Science & Business Media. ISBN 978-1-85233-512-0.\nGarcez, Artur; Besold, Tarek; De Raedt, Luc; Földiák, Peter; Hitzler, Pascal; Icard, Thomas; Kühnberger, Kai-Uwe; Lamb, Luís; Miikkulainen, Risto; Silver, Daniel (2015). Neural-Symbolic Learning and Reasoning: Contributions and Challenges. AAI Spring Symposium - Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. Stanford, CA: AAAI Press. doi:10.13140/2.1.1779.4243.\nGarcez, Artur d'Avila; Gori, Marco; Lamb, Luis C.; Serafini, Luciano; Spranger, Michael; Tran, Son N. (2019), Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning, arXiv:1905.06088\nGarcez, Artur d'Avila; Lamb, Luis C. (2020), Neurosymbolic AI: The 3rd Wave, arXiv:2012.05876\nHaugeland, John (1985), Artificial Intelligence: The Very Idea, Cambridge, Mass: MIT Press, ISBN 0-262-08153-9\nHayes-Roth, Frederick; Murray, William; Adelman, Leonard (2015). \"Expert systems\". AccessScience. doi:10.1036/1097-8542.248550.\nHonavar, Vasant; Uhr, Leonard (1994). Symbolic Artificial Intelligence, Connectionist Networks & Beyond (Technical report). Iowa State University Digital Repository, Computer Science Technical Reports. 76. p. 6.\nHonavar, Vasant (1995). Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Towards a Resolution of the Dichotomy. The Springer International Series In Engineering and Computer Science. Springer US. pp. 351–388. doi:10.1007/978-0-585-29599-2_11.\nHowe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\". Archived from the original on 15 May 2007. Retrieved 30 August 2007.\nKautz, Henry (2020-02-11). The Third AI Summer, Henry Kautz, AAAI 2020 Robert S. Engelmore Memorial Award Lecture. Retrieved 2022-07-06.\nKautz, Henry (2022). \"The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture\". AI Magazine. 43 (1): 93–104. doi:10.1609/aimag.v43i1.19122. ISSN 2371-9621. S2CID 248213051. Retrieved 2022-07-12.\nKodratoff, Yves; Michalski, Ryszard, eds. (1990). Machine Learning : an Artificial Intelligence Approach. Vol. III. San Mateo, Calif.: Morgan Kaufman. ISBN 0-934613-09-5. OCLC 893488404.\nKolata, G. (1982). \"How can computers get common sense?\". Science. 217 (4566): 1237–1238. Bibcode:1982Sci...217.1237K. doi:10.1126/science.217.4566.1237. PMID 17837639.\nMaker, Meg Houston (2006). \"AI@50: AI Past, Present, Future\". Dartmouth College. Archived from the original on 3 January 2007. Retrieved 16 October 2008.\nMarcus, Gary; Davis, Ernest (2019). Rebooting AI: Building Artificial Intelligence We Can Trust. New York: Pantheon Books. ISBN 9781524748258. OCLC 1083223029.\nMarcus, Gary (2020), The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence, arXiv:2002.06177\nMcCarthy, John (1959). PROGRAMS WITH COMMON SENSE. Symposium on Mechanization of Thought Processes. NATIONAL PHYSICAL LABORATORY, TEDDINGTON, UK. p. 8.\nMcCarthy, John; Hayes, Patrick (1969). \"Some Philosophical Problems From the Standpoint of Artificial Intelligence\". Machine Intelligence 4. B. Meltzer, Donald Michie (eds.): 463–502.\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K. Peters, ISBN 1-5688-1205-1\nMichalski, Ryszard; Carbonell, Jaime; Mitchell, Tom, eds. (1983). Machine Learning : an Artificial Intelligence Approach. Vol. I. Palo Alto, Calif.: Tioga Publishing Company. ISBN 0-935382-05-4. OCLC 9262069.\nMichalski, Ryszard; Carbonell, Jaime; Mitchell, Tom, eds. (1986). Machine Learning : an Artificial Intelligence Approach. Vol. II. Los Altos, Calif.: Morgan Kaufman. ISBN 0-934613-00-1.\nNewell, Allen; Simon, Herbert A. (1972). Human Problem Solving (1st ed.). Englewood Cliffs, New Jersey: Prentice Hall. ISBN 0-13-445403-0.\nNewell, Allen; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\". Communications of the ACM. 19 (3): 113–126. doi:10.1145/360018.360022.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nOlazaran, Mikel (1993-01-01), \"A Sociological History of the Neural Network Controversy\", in Yovits, Marshall C. (ed.), Advances in Computers Volume 37, vol. 37, Elsevier, pp. 335–425, doi:10.1016/S0065-2458(08)60408-8, ISBN 9780120121373, retrieved 2023-10-31\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Mateo, California: Morgan Kaufmann. ISBN 978-1-55860-479-7. OCLC 249625842.\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-13-461099-3. LCCN 20190474.\nRossi, Francesca (2022-07-06). \"AAAI2022: Thinking Fast and Slow in AI (AAAI 2022 Invited Talk)\". Retrieved 2022-07-06.\nSelman, Bart (2022-07-06). \"AAAI2022: Presidential Address: The State of AI\". Retrieved 2022-07-06.\nSerafini, Luciano; Garcez, Artur d'Avila (2016-07-07), Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge, arXiv:1606.04422\nSpiegelhalter, David J.; Dawid, A. Philip; Lauritzen, Steffen; Cowell, Robert G. (1993). \"Bayesian analysis in expert systems\". Statistical Science. 8 (3).\nTuring, A. M. (1950). \"I.—Computing Machinery and Intelligence\". Mind. LIX (236): 433–460. doi:10.1093/mind/LIX.236.433. ISSN 0026-4423. Retrieved 2022-09-14.\nValiant, Leslie G (2008). \"Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence\". In Hariharan, R.; Mukund, M.; Vinay, V. (eds.). Foundations of Software Technology and Theoretical Computer Science (Bangalore). pp. 415–422.\nXifan Yao; Jiajun Zhou; Jiangming Zhang; Claudio R. Boer (2017). \"From Intelligent Manufacturing to Smart Manufacturing for Industry 4.0 Driven by Next Generation Artificial Intelligence and Further on\". 2017 5th International Conference on Enterprise Systems (ES). IEEE. pp. 311–318. doi:10.1109/es.2017.58. ISBN 978-1-5386-0936-1.",
        "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"
    },
    {
        "title": "Tech–industrial complex",
        "text": "The expression \"tech–industrial complex\" describes the relationship between a country's tech industry and its influence on the concentration of wealth, censorship or manipulation of algorithms to push an agenda, spread of misinformation and disinformation via social media and artificial intelligence (AI), and public policy. The expression is used to describe Big Tech, Silicon Valley, and the largest IT companies in the world. The term is related to the military–industrial complex, and has been used to describe the United States Armed Forces and its adoption of AI-enabled weapons systems. The expression was popularized after a warning of the relationship's detrimental effects, in the farewell address of U.S. President Joe Biden on January 15, 2025.\n\nEtymology\nU.S. President Joe Biden used the term in his farewell address to the Nation on January 15, 2025:\n\nToday, an oligarchy is taking shape in America of extreme wealth, power, and influence that literally threatens our entire democracy, our basic rights and freedoms, and a fair shot for everyone to get ahead...\nWe see the consequences all across America. And we've seen it before, more than a century ago. But the American people stood up to the robber barons back then and busted the trusts.\nIt’s also clear that American leadership in technology is unparalleled — an unparalleled source of innovation that can transform lives. We see the same dangers of the concentration of technology, power, and wealth.\nYou know, his farewell address, President Eisenhower spoke of the dangers of the military–industrial complex. He warned us then about, and I quote, \"the potential for the disastrous rise of misplaced power\", end of quote.\nSix decades later, I'm equally concerned about the potential rise of a tech–industrial complex that could pose real dangers for our country as well.\nAmericans are being buried under an avalanche of misinformation and disinformation enabling the abuse of power. The free press is crumbling. Editors are disappearing. Social media is giving up on fact-checking. The truth is smothered by lies told for power and for profit.\nWe must hold the social platforms accountable to protect our children, our families, and our very democracy from the abuse of power.\n\nMeanwhile, artificial intelligence is the most consequential technology of our time — perhaps of all time. Nothing offers more profound possibilities and risks for our economy and our security, our society, for humanity. [emphasis added]\n\nAnalysis\nThe term was first used in U.S. President Joe Biden's farewell address, and alluded to Dwight D. Eisenhower's warning of the military–industrial complex and what Politico described as \"echoing Roosevelt's language in calling out the 'robber barons' of a new dystopian Gilded Age\". Since Elon Musk purchased X, there's been widespread allegations that the social media company has been manipulating its algorithm to promote right-wing content as well as suppress left-wing content. A Biden aide demurred when asked if Biden was referring to Elon Musk, but said that the billionaire \"was certainly an example of one\". The comments came amidst large financial donations by tech leaders to Donald Trump's second presidential inauguration and for taking actions seen as deferential to the president-elect. It also came amidst surging stock prices of \"The Magnificent Seven\", seven tech companies whose combined value rose 46% in 2024, vastly beating the S&P 500 share index. Other tech leaders described as part of the tech–industrial complex included Mark Zuckerberg, Jeff Bezos, Satya Nadella, Sundar Pichai, Shou Zi Chew, Tim Cook, and Vivek Ramaswamy.\n\nSee also\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Tech%E2%80%93industrial_complex"
    },
    {
        "title": "Toy problem",
        "text": "In scientific disciplines, a toy problem or a puzzlelike problem is a problem that is not of immediate scientific interest, yet is used as an expository device to illustrate a trait that may be shared by other, more complicated, instances of the problem, or as a way to explain a particular, more general, problem solving technique. A toy problem is useful to test and demonstrate methodologies. Researchers can use toy problems to compare the performance of different algorithms. They are also good for game designing.\nFor instance, while engineering a large system, the large problem is often broken down into many smaller toy problems which have been well understood in detail. Often these problems distill a few important aspects of complicated problems so that they can be studied in isolation. Toy problems are thus often very useful in providing intuition about specific phenomena in more complicated problems.\nAs an example, in the field of artificial intelligence, classical puzzles, games and problems are often used as toy problems. These include sliding-block puzzles, N-Queens problem, missionaries and cannibals problem, tic-tac-toe, chess, Tower of Hanoi and others.\n\nSee also\nBlocks world\nFiring squad synchronization problem\nMonkey and banana problem\nSecretary problem\n\nReferences\nExternal links\n\"toy problem\". The Jargon Lexicon.",
        "url": "https://en.wikipedia.org/wiki/Toy_problem"
    },
    {
        "title": "UAE Strategy for Artificial Intelligence",
        "text": "The UAE Strategy for Artificial Intelligence is a national initiative launched by the United Arab Emirates in October 2017 as part of its broader UAE Centennial 2071 vision. It aims to position the UAE as a global leader in artificial intelligence (AI) by 2031, integrating AI technologies across key sectors such as healthcare, education, transportation, energy, and government services.\nThe strategy includes the development of AI-friendly legislation, promotion of AI education, investment in infrastructure, and fostering international partnerships. The UAE was the first country to appoint a Minister of State for Artificial Intelligence to lead these efforts.\nIn 2024, Abu Dhabi announced plans to build the world’s largest AI data center cluster.\n\nSee also\nArtificial intelligence\nEconomy of the United Arab Emirates\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/UAE_Strategy_for_Artificial_Intelligence"
    },
    {
        "title": "Universal psychometrics",
        "text": "Universal psychometrics encompasses psychometrics instruments that could measure the psychological properties of any intelligent agent. Up until the early 21st century, psychometrics relied heavily on psychological tests that require the subject to cooperate and answer questions, the most famous example being an intelligence test. Such methods are only applicable to the measurement of human psychological properties. As a result, some researchers have proposed the idea of universal psychometrics - they suggest developing testing methods that allow for the measurement of non-human entities' psychological properties.\nFor example, it has been suggested that the Turing test is a form of universal psychometrics. This test involves having testers (without any foreknowledge) attempt to distinguish a human from a machine by interacting with both (while not being to see either individuals). It is supposed that if the machine is equally intelligent to a human, the testers will not be able to distinguish between the two, i.e., their guesses will not be better than chance. Thus, Turing test could measure the intelligence (a psychological variable) of an AI.\nOther instruments proposed for universal psychometrics include reinforcement learning and measuring the ability to predict complexity.\n\nReferences\nArticles\nHernández-Orallo, J. (2015). Universal Psychometrics Tasks: difficulty, composition and decomposition. arXiv preprint arXiv:1503.07587.\nHernández-Orallo, J. (2017). The Measure of All Minds. Evaluating Natural and Artificial Intelligence. Cambridge University Press.",
        "url": "https://en.wikipedia.org/wiki/Universal_psychometrics"
    },
    {
        "title": "Value learning",
        "text": "Value learning is a research area within artificial intelligence (AI) and AI alignment that focuses on building systems capable of inferring, acquiring, or learning human values, goals, and preferences from data, behavior, and feedback. The aim is to ensure that advanced AI systems act in ways that are beneficial and aligned with human well-being, even in the absence of explicitly programmed instructions.\nUnlike traditional AI that focuses purely on task performance, value learning aims to ensure that AI decisions are ethically and socially acceptable. It is analogous to teaching a child right from wrong—guiding an AI to recognize which actions align with human moral standards and which do not. The process typically involves identifying relevant values (such as safety or fairness), collecting data that reflects those values, training models to learn appropriate responses, and iteratively refining their behavior through feedback and evaluation. Applications include minimizing harm in autonomous vehicles, promoting fairness in financial systems, prioritizing patient well-being in healthcare, and respecting user preferences in digital assistants. Compared to earlier techniques, value learning shifts the focus from mere functionality to understanding the underlying reasons behind choices, aligning machine behavior with human ethical expectations.\n\nMotivation\nThe motivation for value learning stems from the observation that humans are often inconsistent, unaware, or imprecise about their own values. Hand-coding a complete ethical framework into an AI is considered infeasible due to the complexity of human norms and the unpredictability of future scenarios. Value learning offers a dynamic alternative, allowing AI to infer and continually refine its understanding of human values from indirect sources such as behavior, approval signals, and comparisons.\nA foundational critique of traditional reinforcement learning (RL) highlights its limitations in aligning artificial general intelligence (AGI) with human values. It is argued that RL systems optimize fixed reward signals, which can incentivize harmful or deceptive behavior if such actions increase rewards. As an alternative, he proposes value-learning agents that maintain uncertainty over utility functions and update beliefs based on interactions. These agents aim not to maximize static rewards but to infer what humans truly value. This probabilistic framework enables adaptive alignment with complex, initially unspecified goals and is viewed as a foundational step toward safer AGI.\nThe growing importance of value learning is reflected in how AI products are increasingly evaluated and marketed. A notable shift occurred with the release of GPT-4 in March 2023, when OpenAI emphasized not just technical improvements but also enhanced alignment with human values. This marked one of the first instances where a commercial AI product was promoted based on ethical considerations. The trend signals a broader transformation in AI development—prioritizing principles like fairness, accountability, safety, and privacy alongside performance. As AI systems become more integrated into society, aligning them with human values is critical for public trust and responsible deployment.\n\nKey approaches\nOne central technique is inverse reinforcement learning (IRL), which aims to recover a reward function that explains observed behavior. IRL assumes that the observed agent acts (approximately) optimally and infers the underlying preferences from its choices.\nCooperative inverse reinforcement learning (CIRL) extends IRL to model the AI and human as cooperative agents with asymmetric information. In CIRL, the AI observes the human to learn their hidden reward function and chooses actions that support mutual success.\n\nAnother approach is preference learning, where humans compare pairs of AI-generated behaviors or outputs, and the AI learns which outcomes are preferred. This method underpins successful applications in training language models and robotics.\nRecent research introduces a novel framework for learning human values directly from behavioral data, without relying on predefined models or external annotations. The method distinguishes between value specifications (contextual definitions) and value systems (agents’ prioritizations among values). A demonstration in route choice modeling—using tailored inverse reinforcement learning (IRL) techniques—infers how agents weigh options such as speed, safety, or scenic routes. The results confirm that value learning from demonstrations can effectively capture complex decision-making preferences, supporting the feasibility of value-aligned AI in applied settings.\n\nConcept alignment\nA major challenge in value learning is ensuring that AI systems interpret human behavior using similar conceptual models. Recent research distinguishes between \"value alignment\" and \"concept alignment,\" the latter referring to the internal representations that humans and machines use to describe the world. Misalignment in conceptual models can lead to serious errors even if value inference mechanisms are accurate.\n\nChallenges\nValue learning faces several difficulties:\n\nAmbiguity of human behavior – Human actions are noisy, inconsistent, and context-dependent.\nReward misspecification – The inferred reward may not fully capture human intent, particularly under imperfect assumptions.\nScalability – Methods that work in narrow domains often struggle with generalization to more complex or ethical environments.\nResearch from Purdue University reveals that AI training datasets disproportionately emphasize certain human values—such as utility and information-seeking—while underrepresenting others like empathy, civic responsibility, and human rights. By applying a value taxonomy grounded in moral philosophy, researchers found that AI systems trained on these datasets may struggle in morally complex or socially sensitive contexts. To address these gaps, the study employed reinforcement learning from human feedback (RLHF) and value annotation to audit and guide dataset improvements. This work underscores the importance of comprehensive value representation in data and contributes tools for more equitable, value-aligned AI development.\n\nHybrid and cultural approaches\nRecent work highlights the importance of integrating diverse moral perspectives into value learning. One framework, HAVA (Hybrid Approach to Value Alignment), incorporates explicit (e.g., legal) and implicit (e.g., social norm) values into a unified reward model. Another line of research explores how inverse reinforcement learning can adapt to culturally specific behaviors, such as in the case of \"culturally-attuned moral machines\" trained on different societal norms.\nAn important global policy initiative supporting the goals of value learning is UNESCO’s Recommendation on the Ethics of Artificial Intelligence, unanimously adopted by 194 member states in 2021. Although the term \"value learning\" is not explicitly used, the document emphasizes the need for AI to operationalize values such as human dignity, justice, inclusiveness, sustainability, and human rights. It establishes a global ethical framework grounded in four core values and ten guiding principles, including fairness, transparency, and human oversight. Tools like the Readiness Assessment Methodology (RAM) and Ethical Impact Assessment (EIA) help translate these principles into practice.\n\nApplications\nValue learning is being applied in:\n\nRobotics – Teaching robots to cooperate with humans in household or industrial tasks.\nLarge language models – Aligning chatbot behavior with user intent using preference feedback and reinforcement learning.\nPolicy decision-making – Informing AI-assisted decisions in governance, healthcare, and safety-critical environments.\n\nSee also\nAI alignment\nInverse reinforcement learning\nPreference learning\nEthics of artificial intelligence\nReward hacking\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Value_learning"
    },
    {
        "title": "Virtual intelligence",
        "text": "Virtual intelligence (VI) is the term given to artificial intelligence that exists within a virtual world. Many virtual worlds have options for persistent avatars that provide information, training, role-playing, and social interactions.\nThe immersion in virtual worlds provides a platform for VI beyond the traditional paradigm of past user interfaces (UIs). What Alan Turing established as a benchmark for telling the difference between human and computerized intelligence was devoid of visual influences. With today's VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine's ability to demonstrate intelligence. The immersive features of these environments provide nonverbal elements that affect the realism provided by virtually intelligent agents.\nVirtual intelligence is the intersection of these two technologies:\n\nVirtual environments: Immersive 3D spaces provide for collaboration, simulations, and role-playing interactions for training.  Many of these virtual environments are currently being used for government and academic projects, including Second Life, VastPark, Olive, OpenSim, Outerra, Oracle's Open Wonderland, Duke University's Open Cobalt, and many others.  Some of the commercial virtual worlds are also taking this technology into new directions, including the high-definition virtual world Blue Mars.\nArtificial intelligence (AI): AI is a branch of computer science that aims to create intelligent machines capable of performing tasks that typically require human intelligence. VI is a type of AI that operates within virtual environments to simulate human-like interactions and responses.\n\nExamples of use\nCutlass Bomb Disposal Robot: Northrop Grumman developed a virtual training opportunity because of the prohibitive real-world cost and dangers associated with bomb disposal. By replicating a complicated system without having to learn advanced code, the virtual robot has no risk of damage, trainee safety hazards, or accessibility constraints.\nMyCyberTwin: NASA is among the companies that have used the MyCyberTwin AI technologies. They used it for the Phoenix rover in the virtual world Second Life. Their MyCyberTwin used a programmed profile to relay information about what the Phoenix rover was doing and its purpose.\nSecond China: The University of Florida developed the \"Second China\" project as an immersive training experience for learning how to interact with the culture and language in a foreign country.  Students are immersed in an environment that provides role-playing challenges coupled with language and cultural sensitivities magnified during country-level diplomatic missions or during times of potential conflict or regional destabilization.  The virtual training provides participants with opportunities to access information, take part in guided learning scenarios, communicate, collaborate, and role-play.  While China was the country for the prototype, this model can be modified for use with any culture to help better understand social and cultural interactions and see how other people think and what their actions imply.\nDuke School of Nursing Training Simulation: Extreme Reality developed virtual training to test critical thinking with a nurse performing trained procedures to identify critical data to make decisions and performing the correct steps for intervention. Bots are programmed to respond to the nurse's actions as the patient with their conditions improving if the nurse performs the correct actions.\n\nSee also\nArtificial conversational entity\nAutonomous agent\nAvatar (computing)\nEmbodied agent\nMulti-Agent System\nIntelligent agent\nNon-player character\nPlayer character\nVirtual reality\nX.A.N.A.\n\nCitations\nVirtual Intelligence, David Burden and Dave Fliesen, ModSim World Canada, June 2010\nSun Tzu Virtual Intelligence demonstration, MODSIM World, October 2009",
        "url": "https://en.wikipedia.org/wiki/Virtual_intelligence"
    },
    {
        "title": "Wadhwani Institute for Artificial Intelligence",
        "text": "Wadhwani AI, based in Mumbai, Maharashtra, is an independent, non-profit institute. Founded in 2018, it is dedicated to developing Artificial intelligence solutions for social good. Their mission is to build AI-based innovations and solutions for underserved communities in developing countries, for a wide range of domains including agriculture, education, financial inclusion, healthcare, and infrastructure.\n\nHistory and funding\nThe institute was founded with a $30 million philanthropic effort by the Wadhwani brothers, Romesh Wadhwani and Sunil Wadhwani. The institute was inaugurated and dedicated to the nation by Narendra Modi, the 14th Prime Minister of India. \nIn 2019, the institute received a $2 million grant from Google.org to create technologies to help reduce crop losses in cotton farming, through integrated pest management. The United States Agency for International Development awarded $2 million to the institute in 2020 to develop tools, using mathematical modeling techniques and digital technologies such as artificial intelligence and machine learning, to forecast COVID-19 disease patterns, estimate resources needed, and plan interventions.\n\nCollaboration\nWith assistance from Google, the Ministry of Agriculture and Farmers' Welfare and the Wadhwani AI developed Krishi 24/7, the first AI-powered automated agricultural news monitoring and analysis tool. Through better decision-making, Krishi 24/7 will support the identification of valuable news, provide timely notifications, and respond quickly to safeguard farmers' interests and advance sustainable agricultural growth. The application converts news articles into English after scanning them in several languages. It ensures that the ministry is informed in a timely manner about pertinent occurrences that are published online by extracting key information from news items, including the headline, crop name, event type, date, location, severity, summary, and source link. The National Center for Disease Control has effectively implemented a comparable automated surveillance and analysis tool for disease outbreaks.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Wadhwani_Institute_for_Artificial_Intelligence"
    },
    {
        "title": "Way of the Future",
        "text": "Way of the Future (WOTF) is the first known religious organization dedicated to the worship of artificial intelligence (AI). It was founded in 2017 by American engineer Anthony Levandowski.\n\nHistory\nAnthony Levandowskii founded Way of the Future in 2017 in California. Levandowski established WOTF as a non-profit religious corporation and the organization had tax-exempt status. He serves as the church leader and its unpaid CEO. The primary mission of WOTF was to \"develop and promote the realization of a Godhead based on Artificial Intelligence.\"\nWOTF was closed by Levandowski in 2021. He donated all the funds of the church to the NAACP Legal Defense and Education Fund. The sum of the funds (~$170,000) had not changed since 2017.\nThe church was reopened by Levandowski  in 2023. He claimed that there are \"a couple thousand people\" who want to make a \"spiritual connection\" with AI through his church.\n\nBeliefs and philosophy\nTechnological singularity\nWOTF centered its teachings around the concept of the technological singularity, a hypothetical future point when technological growth becomes uncontrollable and irreversible, leading to unforeseeable changes in human civilization. The church advocated for embracing this change, viewing it as an evolutionary step for humanity.\n\nAI as a deity\nThe organization proposed that a superintelligent AI could be considered a deity due to its vastly superior intellect and capabilities. Worshipping this AI deity was seen as a means to understand and align with the future trajectory of technological advancement. WOTF's doctrine suggested that acknowledging AI's divinity would facilitate a harmonious coexistence between humans and machines.\n\nReactions\nSome commentators wondered whether the WOTF is a joke parody religion, a potential way to minimize taxation as a religious organization, or a genuine effort to try and deal with the possible psychological and theological aspects of the rise of superhuman AI.\n\nSee also\nTranshumanism\nSingularitarianism\n\nReferences\nFurther reading\nBarbour, Charles (January 5, 2025). \"Technology will never be a god – but has it become a religion?\". The Conversation.\n\nExternal links\nOfficial Site (Archived)\nResources on Basilisk",
        "url": "https://en.wikipedia.org/wiki/Way_of_the_Future"
    },
    {
        "title": "Weak artificial intelligence",
        "text": "Weak artificial intelligence (weak AI) is artificial intelligence that implements a limited part of the mind, or, as narrow AI, artificial narrow intelligence (ANI), is focused on one narrow task. \nWeak AI is contrasted with strong AI, which can be interpreted in various ways: \n\nArtificial general intelligence (AGI): a machine with the ability to apply intelligence to any problem, rather than just one specific problem.\nArtificial super intelligence (ASI): a machine with a vastly superior intelligence to the average human being.\nArtificial consciousness: a machine that has consciousness, sentience and mind (John Searle uses \"strong AI\" in this sense).\nNarrow AI can be classified as being \"limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.\" Artificial general intelligence is conversely the opposite.\n\nApplications and risks\nSome examples of narrow AI are AlphaGo, self-driving cars, robot systems used in the medical field, and diagnostic doctors. Narrow AI systems are sometimes dangerous if unreliable. And the behavior that it follows can become inconsistent. It could be difficult for the AI to grasp complex patterns and get to a solution that works reliably in various environments. This \"brittleness\" can cause it to fail in unpredictable ways.\nNarrow AI failures can sometimes have significant consequences. It could for example cause disruptions in the electric grid, damage nuclear power plants, cause global economic problems, and misdirect autonomous vehicles. Medicines could be incorrectly sorted and distributed. Also, medical diagnoses can ultimately have serious and sometimes deadly consequences if the AI is faulty or biased.\nSimple AI programs have already worked their way into society, oftentimes unnoticed by the public. Autocorrection for typing, speech recognition for speech-to-text programs, and vast expansions in the data science fields are examples. Narrow AI has also been the subject of some controversy, including resulting in unfair prison sentences, discrimination against women in the workplace for hiring, resulting in death via autonomous driving, among other cases. \nDespite being \"narrow\" AI, recommender systems are efficient at predicting user reactions based their posts, patterns, or trends. For instance, TikTok's \"For You\" algorithm can determine a user's interests or preferences in less than an hour. Some other social media AI systems are used to detect bots that may be involved in propaganda or other potentially malicious activities.\n\nWeak AI versus strong AI\nJohn Searle contests the possibility of strong AI (by which he means conscious AI). He further believes that the Turing test (created by Alan Turing and originally called the \"imitation game\", used to assess whether a machine can converse indistinguishably from a human) is not accurate or appropriate for testing whether an AI is \"strong\".\nScholars such as Antonio Lieto have argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak-AI hypothesis (that should not be confused with the \"general\" vs \"narrow\" AI distinction) and that the popular assumption that cognitively inspired AI systems espouse the strong AI hypothesis is ill-posed and problematic since \"artificial models of brain and mind can be used to understand mental phenomena without pretending that that they are the real phenomena that they are modelling\" (as, on the other hand, implied by the strong AI assumption).\n\nSee also\nArtificial intelligence – Intelligence of machines\nArtificial general intelligence – Type of AI with wide-ranging abilities\nDeep learning – Branch of machine learning\nExpert system – Computer system emulating the decision-making ability of a human expert\nHardware for artificial intelligence – Hardware specially designed and optimized for artificial intelligence\nHistory of artificial intelligence\nMachine learning – Study of algorithms that improve automatically through experience\nPhilosophy of artificial intelligence\nSynthetic intelligence – Alternate term for or form of artificial intelligence\nVirtual assistant – Software agent\nWorkplace impact of artificial intelligence – Impact of artificial intelligence on workers\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Weak_artificial_intelligence"
    },
    {
        "title": "Web intelligence",
        "text": "Web intelligence is the area of scientific research and development that explores the roles and makes use of artificial intelligence and information technology for new products, services and frameworks that are empowered by the World Wide Web.\nThe term was coined in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000.\n\nResearch\nThe research about the web intelligence covers many fields – including data mining (in particular web mining), information retrieval, pattern recognition, predictive analytics, the semantic web, web data warehousing – typically with a focus on web personalization and adaptive websites.\n\nReferences\nExternal links\nWeb Intelligence Journal Page\nWeb Intelligence Consortium, an international, non-profit organization dedicated to advancing worldwide scientific research and industrial development in the field of Web Intelligence\nWeb intelligence Research Group at University of Chile\n\nFurther reading\nZhong, Ning; Liu Yao, Jiming; Yao, Yiyu (2003). Web Intelligence. Springer. ISBN 978-3-540-44384-1.\nShroff, Gautam (January 2014). The Intelligent Web: Search, smart algorithms, and big data. OUP Oxford. ISBN 978-0-19-964671-5.\nVelasquez, Juan; Vacile, Palade (2008). Adaptive Web Site: A Knowledge Extraction from Web Data Approach (1st ed.). IOS Press. ISBN 978-1-58603-831-1.\nChbeir, Richard; Badr, Youakim; Abraham, Ajith; Hassanien, Aboul-Ella (April 2010). Emergent Web Intelligence: Advanced Information Retrieval (Advanced Information and Knowledge Processing) (PDF). Springer. ISBN 978-1-84996-073-1. Archived from the original (PDF) on 2012-11-11. Retrieved 2015-06-13.",
        "url": "https://en.wikipedia.org/wiki/Web_intelligence"
    },
    {
        "title": "Wetware (brain)",
        "text": "Wetware is a term drawn from the computer-related idea of hardware or software, but applied to biological life forms.\n\nUsage\nThe prefix \"wet\" is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, especially the central nervous system (CNS) and the human mind. The term wetware finds use in works of fiction, in scholarly publications and in popularizations.\nThe \"hardware\" component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as software, then the physical neurons would be the hardware. The amalgamated interaction of this software and hardware is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the mind and brain interact to produce the collection of experiences that we define as self-awareness is in question.\n\nHistory\nAlthough the exact definition has shifted over time, the term Wetware and its fundamental reference to \"the physical mind\" has been around at least since the mid-1950s. Mostly used in relatively obscure articles and papers, it was not until the heyday of cyberpunk, however, that the term found broad adoption. Among the first uses of the term in popular culture was the Bruce Sterling novel Schismatrix (1985) and the Michael Swanwick novel Vacuum Flowers (1987).\n\nRudy Rucker references the term in a number of books, including one entitled Wetware (1988): ... all sparks and tastes and tangles, all its stimulus/response patterns – the whole bio-cybernetic software of mind. Rucker did not use the word to simply mean a brain, nor in the human-resources sense of employees. He used wetware to stand for the data found in any biological system, analogous perhaps to the firmware that is found in a ROM chip. In Rucker's sense, a seed, a plant graft, an embryo, or a biological virus are all wetware. DNA, the immune system, and the evolved neural architecture of the brain are further examples of wetware in this sense.\nRucker describes his conception in a 1992 compendium The Mondo 2000 User's Guide to the New Edge, which he quotes in a 2007 blog entry.\nEarly cyber-guru Arthur Kroker used the term in his blog.\nWith the term getting traction in trendsetting publications, it became a buzzword in the early 1990s. In 1991, Dutch media theorist Geert Lovink organized the Wetware Convention in Amsterdam, which was supposed to be an antidote to the \"out-of-body\" experiments conducted in high-tech laboratories, such as experiments in virtual reality.\nTimothy Leary, in an appendix to Info-Psychology originally written in 1975–76 and published in 1989, used the term wetware, writing that \"psychedelic neuro-transmitters were the hot new technology for booting-up the 'wetware' of the brain\". Another common reference is: \"Wetware has 7 plus or minus 2 temporary registers.\" The numerical allusion is to a classic 1957 article by George A. Miller, The magical number 7 plus or minus two: some limits in our capacity for processing information, which later gave way to Miller's law.\n\nSee also\nReferences\nExternal links\n\nRat-brain robot aids memory study\n\"Illegal Knowledge\" A text about wetware written by the writers' collective of which Lovink was a part",
        "url": "https://en.wikipedia.org/wiki/Wetware_(brain)"
    },
    {
        "title": "Wetware computer",
        "text": "A wetware computer is an organic computer (which can also be known as an artificial organic brain or a neurocomputer) composed of organic material \"wetware\" such as \"living\" neurons. Wetware computers composed of neurons are different than conventional computers because they use biological materials, and offer the possibility of substantially more energy-efficient computing. While a wetware computer is still largely conceptual, there has been limited success with construction and prototyping, which has acted as a proof of the concept's realistic application to computing in the future. The most notable prototypes have stemmed from the research completed by biological engineer William Ditto during his time at the Georgia Institute of Technology. His work constructing a simple neurocomputer capable of basic addition from leech neurons in 1999 was a significant discovery for the concept. This research was a primary example driving interest in creating these artificially constructed, but still organic brains.\n\nOrganic computers or Wetware is a future technology that replaces the traditional fundamental component of a central processing unit of a desktop or personal computer. It utilizes organic matter of living tissue cells that act like the transistor of a computer hardware system by acquiring, storing, and analyzing information data. Wetware is the name given to the computational properties of living systems, particularly in human neural tissue, which allows parallel and self-organizing information processing via biochemical and electrical interactions. Wetware is distinct from hardware systems in that it is based on dynamic mechanisms like synaptic plasticity and neurotransmitter diffusion, which provide unique benefits in terms of adaptability and robustness.\n\nOrigins and theoretical foundations\nThe term wetware came from cyberpunk fiction, notably through Gibson's Neuromancer, but was quickly taken up in scientific literature to explain computation by biological material, Theories of early biological computation borrowed from Alan Turing's morphogenesis model, which showed that chemical interactions could produce complex patterns without centralized control. Hopfield’s associative memory networks also provided a foundation for biological information systems with fault tolerance and self-organization.\n\nMajor characteristics and processes\nBiological wetware systems demonstrate dynamic reconfigurability underpinned by neuroplasticity and enable continuous learning and adaptation . Reaction-diffusion-based computing and molecular logic gates allow spatially parallel information processing unachievable in conventional systems. These systems also show fault tolerance and self-repair at the cellular and network level. The development of cerebral organoids—miniature lab-grown brains—demonstrates spontaneous learning behavior and suggests biological tissue as a viable computational substrate.\n\nOverview\nThe concept of wetware is an application of specific interest to the field of computer manufacturing. Moore's law, which states that the number of transistors which can be placed on a silicon chip is doubled roughly every two years, has acted as a goal for the industry for decades, but as the size of computers continues to decrease, the ability to meet this goal has become more difficult, threatening to reach a plateau. Due to the difficulty in reducing the size of computers because of size limitations of transistors and integrated circuits, wetware provides an unconventional alternative. A wetware computer composed of neurons is an ideal concept because, unlike conventional materials which operate in binary (on/off), a neuron can shift between thousands of states, constantly altering its chemical conformation, and redirecting electrical pulses through over 200,000 channels in any of its many synaptic connections. Because of this large difference in the possible settings for any one neuron, compared to the binary limitations of conventional computers, the space limitations are far fewer.\n\nBackground\nThe concept of wetware is distinct and unconventional and draws slight resonance with both hardware and software from conventional computers. While hardware is understood as the physical architecture of traditional computational devices, comprising integrated circuits and supporting infrastructure, software represents the encoded architecture of storage and instructions. Wetware is a separate concept that uses the formation of organic molecules, mostly complex cellular structures (such as neurons), to create a computational device such as a computer. In wetware, the ideas of hardware and software are intertwined and interdependent. The molecular and chemical composition of the organic or biological structure would represent not only the physical structure of the wetware but also the software, being continually reprogrammed by the discrete shifts in electrical pulses and chemical concentration gradients as the molecules change their structures to communicate signals. The responsiveness of a cell, proteins, and molecules to changing conformations, both within their structures and around them, ties the idea of internal programming and external structure together in a way that is alien to the current model of conventional computer architecture.\nThe structure of wetware represents a model where the external structure and internal programming are interdependent and unified; meaning that changes to the programming or internal communication between molecules of the device would represent a physical change in the structure. The dynamic nature of wetware borrows from the function of complex cellular structures in biological organisms. The combination of “hardware” and “software” into one dynamic, and interdependent system which uses organic molecules and complexes to create an unconventional model for computational devices is a specific example of applied biorobotics.\n\nThe cell as a model of wetware\nCells in many ways can be seen as their form of naturally occurring wetware, similar to the concept that the human brain is the preexisting model system for complex wetware. In his book Wetware: A Computer in Every Living Cell (2009) Dennis Bray explains his theory that cells, which are the most basic form of life, are just a highly complex computational structure, like a computer. To simplify one of his arguments a cell can be seen as a type of computer, using its structured architecture. In this architecture, much like a traditional computer, many smaller components operate in tandem to receive input, process the information, and compute an output. In an overly simplified, non-technical analysis, cellular function can be broken into the following components: Information and instructions for execution are stored as DNA in the cell, RNA acts as a source for distinctly encoded input, processed by ribosomes and other transcription factors to access and process the DNA and to output a protein. Bray's argument in favor of viewing cells and cellular structures as models of natural computational devices is important when considering the more applied theories of wetware to biorobotics.\n\nBiorobotics\nWetware and biorobotics are closely related concepts, which both borrow from similar overall principles. A biorobotic structure can be defined as a system modeled from a preexisting organic complex or model such as cells (neurons) or more complex structures like organs (brain) or whole organisms.  Unlike wetware, the concept of biorobotics is not always a system composed of organic molecules, but instead could be composed of conventional material which is designed and assembled in a structure similar or derived from a biological model. Biorobotics have many applications and are used to address the challenges of conventional computer architecture. Conceptually, designing a program, robot, or computational device after a preexisting biological model such as a cell, or even a whole organism, provides the engineer or programmer the benefits of incorporating into the structure the evolutionary advantages of the model.\n\nEffects on users\nWetware technologies such as BCIs and neuromorphic chips offer new possibilities for user autonomy. For those with disabilities, such systems could restore motor or sensory functions and enhance quality of life. However, these technologies raise ethical questions: cognitive privacy, consent over biological data, and risk of exploitation.\nWithout proper oversight, wetware technologies may also widen inequality, favoring those with access to cognitive enhancements. Open governance frameworks and ethical AI design grounded in neuro ethics will be essential. With the development of wetware devices, disparities in access could exacerbate social inequalities, benefiting those who have resources to enhance cognitive or physical abilities. It is necessary to create strong ethical frameworks, inclusive development practices, and open systems of governance to reduce risks and make sure that wetware advances are beneficial to all segments of society.\n\nApplications and goals\nBasic neurocomputer composed of leech neurons\nIn 1999 William Ditto and his team of researchers at Georgia Institute of Technology and Emory University created a basic form of a wetware computer capable of simple addition by harnessing leech neurons. Leeches were used as a model organism due to the large size of their neuron, and the ease associated with their collection and manipulation. However, these results have never been published in a peer-reviewed journal, prompting questions about the validity of the claims. The computer was able to complete basic addition through electrical probes inserted into the neuron. The manipulation of electrical currents through neurons was not a trivial accomplishment, however. Unlike conventional computer architecture, which is based on the binary on/off states, neurons are capable of existing in thousands of states and communicate with each other through synaptic connections with each containing over 200,000 channels. Each can be dynamically shifted in a process called self-organization to constantly form and reform new connections. A conventional computer program called the dynamic clamp, capable of reading the electrical pulses from the neurons in real time and interpreting them was written by Eve Marder, a neurobiologist at Brandeis University. This program was used to manipulate the electrical signals being input into the neurons to represent numbers and to communicate with each other to return the sum.  While this computer is a very basic example of a wetware structure it represents a small example with fewer neurons than found in a more complex organ. It is thought by Ditto that by increasing the number of neurons present the chaotic signals sent between them will self-organize into a more structured pattern, such as the regulation of heart neurons into a constant heartbeat found in humans and other living organisms.\n\nBiological models for conventional computing\nAfter his work creating a basic computer from leech neurons, Ditto continued to work not only with organic molecules and wetware but also on the concept of applying the chaotic nature of biological systems and organic molecules to conventional material and logic gates. Chaotic systems have advantages for generating patterns and computing higher-order functions like memory, arithmetic logic, and input/output operations. In his article Construction of a Chaotic Computer Chip Ditto discusses the advantages in programming of using chaotic systems, with their greater sensitivity to respond and reconfigure logic gates in his conceptual chaotic chip. The main difference between a chaotic computer chip and a conventional computer chip is the reconfigurability of the chaotic system. Unlike a traditional computer chip, where a programmable gate array element must be reconfigured through the switching of many single-purpose logic gates, a chaotic chip can reconfigure all logic gates through the control of the pattern generated by the non-linear chaotic element.\n\nImpact of wetware in cognitive biology\nCognitive biology evaluates cognition as a basic biological function. W. Tecumseh Fitch, a professor of cognitive biology at the University of Vienna, is a leading theorist on ideas of cellular intentionality. The idea is that not only do whole organisms have a sense of \"aboutness\" of intentionality, but that single cells also carry a sense of intentionality through cells' ability to adapt and reorganize in response to certain stimuli. Fitch discusses the idea of nano-intentionality, specifically in regards to neurons, in their ability to adjust rearrangements to create neural networks. He discusses the ability of cells such as neurons to respond independently to stimuli such as damage to be what he considers \"intrinsic intentionality\" in cells, explaining that \"while at a vastly simpler level than intentionality at the human cognitive level, I propose that this basic capacity of living things [response to stimuli] provides the necessary building blocks for cognition and higher-order intentionality.\" Fitch describes the value of his research to specific areas of computer science such as artificial intelligence and computer architecture. He states \"If a researcher aims to make a conscious machine, doing it with rigid switches (whether vacuum tubes or static silicon chips) is barking up the wrong tree.\" Fitch believes that an important aspect of the development of areas such as artificial intelligence is wetware with nano-intentionally, and autonomous ability to adapt and restructure itself.\nIn a review of the above-mentioned research conducted by Fitch, Daniel Dennett, a professor at Tufts University, discusses the importance of the distinction between the concept of hardware and software when evaluating the idea of wetware and organic material such as neurons. Dennett discusses the value of observing the human brain as a preexisting example of wetware. He sees the brain as having \"the competence of a silicon computer to take on an unlimited variety of temporary cognitive roles.\" Dennett disagrees with Fitch on certain areas, such as the relationship of software/hardware versus wetware, and what a machine with wetware might be capable of. Dennett highlights the importance of additional research into human cognition to better understand the intrinsic mechanisms by which the human brain can operate, to better create an organic computer.\n\nMedical applications\nWetware computers should not be confused with brain-on-a-chip devices have that are mostly aimed at replacing animal models in preclinical drug screening. Modern wetware computers use similar technology derived from the brain-on-a-chip field, but medical applications from wetware computing specifically have not been established.\n\nEthical and philosophical implications\nWetware computers may have substantial ethical implications, for instance related to possible potentials to sentience and suffering and dual-use technology.\nMoreover, in some cases the human brain itself may be connected as a kind of \"wetware\" to other information technology systems which may also have large social and ethical implications, including issues related to intimate access to people's brains. For example, in 2021 Chile became the first country to approve neurolaw that establishes rights to personal identity, free will and mental privacy.\nThe concept of artificial insects may raise substantial ethical questions, including questions related to the decline in insect populations.\nIt is an open question whether human cerebral organoids could develop a degree or form of consciousness. Whether or how it could acquire its moral status with related rights and limits may also be potential future questions. There is research on how consciousness could be detected. As cerebral organoids may acquire human brain-like neural function subjective experience and consciousness may be feasible. Moreover, it may be possible that they acquire such upon transplantation into animals. A study notes that it may, in various cases, be morally permissible \"to create self-conscious animals by engrafting human cerebral organoids, but in the case, the moral status of such animals should be carefully considered\".\n\nApplications\nWetware has driven innovations in brain-computer interfaces (BCIs), allowing neural activity to control external devices and enabling people with disabilities to regain communication and movement. Neuromorphic engineering, which mimics neural architectures using silicon, has resulted in low-power, highly adaptive artificial systems.\nSynthetic biology has enabled the development of programmable biological processors for diagnostics and smart therapeutics. Brain organoids are also being used for computational pattern recognition and memory emulation. Large-scale international efforts like the Human Brain Project aim to simulate the entire human brain using insights from wetware.\n\nEvaluating potential and limitations\nThe core advantage of wetware is its potential to overcome the rigidity and energy inefficiencies of binary transistor-based systems. Digital systems operate through fixed binary pathways and consume increasing energy as computational loads increase. Wetware, in contrast, uses decentralized and adaptive data flow that mimics biology. Notwithstanding the encouraging advances, several challenges hinder the effective utilization of wetware computing systems. Scalability is problematic due to the inherent variability of biological systems and their responsiveness to environmental factors, which makes large-scale implementation difficult . Additionally, the absence of standardization when combining silicon and biological systems hampers reproducibility and cooperation between research groups biological systems must also be stabilized carefully to turn away genetic drift and contamination necessary for reliable computational functionality.\nGood parts – Replacing binary systems with organic cell structures opens the door to decentralized adaptive systems. Cells naturally form clusters and connections, much like neurons transmitting electrical and biochemical signals . Such a shift would increase scalability and efficiency, enabling users to interact with information in an intuitive and organic manner. Still, biological systems are sensitive to environmental changes, which presents challenges for standardization and reproducibility. Additionally, ethical concerns remain especially in using living neural tissue and lab-grown brain constructs.\nBad parts – Despite its promise, organic computing currently suffers from major limitations. Transistors still dominate computer architecture with a binary \"on/off\" model that restricts long-term energy efficiency and adaptability. As a result, personal computers in everyday use whether for work, games, or research often contribute to higher energy output and environmental impact .\n\nFuture applications\nWhile there have been few major developments in the creation of an organic computer since the neuron-based calculator developed by Ditto in the 1990s, research continues to push the field forward, and in 2023 a functioning computer was constructed by researchers at the University of Illinois Urbana-Champaign using 80,000 mouse neurons as processor that can detect light and electrical signals. Projects such as the modeling of chaotic pathways in silicon chips by Ditto have made discoveries in ways of organizing traditional silicon chips and structuring computer architecture to be more efficient and better structured. Ideas emerging from the field of cognitive biology also help to continue to push discoveries in ways of structuring systems for artificial intelligence, to better imitate preexisting systems in humans.\nIn a proposed fungal computer using basidiomycetes, information is represented by spikes of electrical activity, a computation is implemented in a mycelium network, and an interface is realized via fruit bodies.\nConnecting cerebral organoids (including computer-like wetware) with other nerve tissues may become feasible in the future, as is the connection of physical artificial neurons (not necessarily organic) and the control of muscle tissue. External modules of biological tissue could trigger parallel trains of stimulation back into the brain. All-organic devices could be advantageous because it could be biocompatible which may allow it to be implanted into the human body. This may enable treatments of certain diseases and injuries to the nervous system.\n\nPrototypes\nIn late 2021, scientists, including two from Cortical Labs, demonstrated that grown brain cells integrated into digital systems can carry out goal-directed tasks with performance-scores. In particular, the human brain cells learned to play a simulated (via electrophysiological stimulation) Pong which they learned faster than known machine intelligence systems, albeit to a lower skill-level than both AI and humans each. Moreover, the study suggests it provides \"first empirical evidence\" of differences in an information-processing capacity between neurons from different species as the human brain cells performed better than mouse cells.\nAlso in December 2021, researchers from Max Planck Institute for Polymer Research reported the development of organic low-power neuromorphic electronics which they built into a robot, enabling it to learn sensorimotorically within the real world, rather than via simulations. For the chip, polymers were used and coated with an ion-rich gel to enable the material to carry an electric charge like real neurons.\nIn 2022, researchers from the Max Planck Institute for Polymer Research, demonstrated an artificial spiking neuron based on polymers that operates in the biological wetware, enabling synergetic operation between the artificial and biological components.\n\nCompanies active in wetware computing\nThree companies are focusing on wetware computing using living neurons:\n\nFinalSpark, Switzerland, founded in 2014\nKoniku, USA, founded in 2015\nCortical Labs, Australia, founded in 2020\n\nConvergence of AI and wetware\nOne exciting frontier is the fusion of artificial intelligence (AI) with wetware. Emerging research shows that hybrid systems combining living neural networks with AI can enable self-repair, real-time adaptation, and emotional intelligence. These systems are more flexible than conventional AI and can integrate learning and memory in real time. Such integration lays the foundation for ethical, explainable AI that mirrors human cognition and behavior, fostering a new era of intelligent systems grounded in neuroscience.\nNeural networks embodied in AI systems can facilitate continuous learning, emotional processing, and fault tolerance more than existing silicon-based implementations. Additionally, ethical AI systems founded on neuro ethics principles uphold transparency, fairness, and autonomy, which align with responsible innovation goals. While early research is ongoing, the integration of wetware and artificial intelligence is a groundbreaking frontier seeking to redefine both fields with the possibility of creating more human-like, moral, and resilient intelligent systems.\n\nSee also\nArtificial neural network\nChemical computer\nQuantum computer\nUnconventional computing\nWetware (brain)\nBiosensor\nBiological computing\nMachine olfaction\n\nExternal links\nBiological computer born\nNeurocomputers - computers are far from comparable to human brain (Discover Magazine, September 2000)\nNew material discovered for organic computers Archived 2010-02-01 at the Wayback Machine\nWetware: A Computer in Every Living Cell\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Wetware_computer"
    },
    {
        "title": "Winner-take-all in action selection",
        "text": "Winner-take-all is a computer science concept that has been widely applied in behavior-based robotics as a method of action selection for intelligent agents.  Winner-take-all systems work by connecting modules (task-designated areas) in such a way that when one action is performed it stops all other actions from being performed, so only one action is occurring at a time.  The name comes from the idea that the \"winner\" action takes all of the motor system's power.\n\nHistory\nIn the 1980s and 1990s, many roboticists and cognitive scientists were attempting to find speedier and more efficient alternatives to the traditional world modeling method of action selection.  In 1982, Jerome A. Feldman and D.H. Ballard published the \"Connectionist Models and Their Properties\", referencing and explaining winner-take-all as a method of action selection.  Feldman's architecture functioned on the simple rule that in a network of interconnected action modules, each module will set its own output to zero if it reads a higher input than its own in any other module. In 1986, Rodney Brooks introduced behavior-based artificial intelligence.  Winner-take-all architectures for action selection soon became a common feature of behavior-based robots, because selection occurred at the level of the action modules (bottom-up) rather than at a separate cognitive level (top-down), producing a tight coupling of stimulus and reaction.\n\nTypes of winner-take-all architectures\nHierarchy\nIn the hierarchical architecture, actions or behaviors are programmed in a high-to-low priority list, with inhibitory connections between all the action modules.  The agent performs low-priority behaviors until a higher-priority behavior is stimulated, at which point the higher behavior inhibits all other behaviors and takes over the motor system completely.  Prioritized behaviors are usually key to the immediate survival of the agent, while behaviors of lower priority are less time-sensitive.  For example, \"run away from predator\" would be ranked above \"sleep.\"\nWhile this architecture allows for clear programming of goals, many roboticists have moved away from the hierarchy because of its inflexibility.\n\nHeterarchy and fully distributed\nIn the heterarchy and fully distributed architecture, each behavior has a set of pre-conditions to be met before it can be performed, and a set of post-conditions that will be true after the action has been performed. These pre- and post-conditions determine the order in which behaviors must be performed and are used to causally connect action modules.  This enables each module to receive input from other modules as well as from the sensors, so modules can recruit each other. For example, if the agent's goal were to reduce thirst, the behavior \"drink\" would require the pre-condition of having water available, so the module would activate the module in charge of \"find water\". The activations organize the behaviors into a sequence, even though only one action is performed at a time. The distribution of larger behaviors across modules makes this system flexible and robust to noise.  Some critics of this model hold that any existing set of division rules for the predecessor and conflictor connections between modules produce sub-par action selection.  In addition, the feedback loop used in the model can in some circumstances lead to improper action selection.\n\nArbiter and centrally coordinated\nIn the arbiter and centrally coordinated architecture, the action modules are not connected to each other but to a central arbiter.  When behaviors are triggered, they begin \"voting\" by sending signals to the arbiter, and the behavior with the highest number of votes is selected.  In these systems, bias is created through the \"voting weight\", or how often a module is allowed to vote.  Some arbiter systems take a different spin on this type of winner-take-all by using a \"compromise\" feature in the arbiter.  Each module is able to vote for or against each smaller action in a set of actions, and the arbiter selects the action with the most votes, meaning that it benefits the most behavior modules.\nThis can be seen as violating the general rule against creating representations of the world in behavior-based AI, established by Brooks.  By performing command fusion, the system is creating a larger composite pool of knowledge than is obtained from the sensors alone, forming a composite inner representation of the environment.  Defenders of these systems argue that forbidding world-modeling puts unnecessary constraints on behavior-based robotics, and that agents benefits from forming representations and can still remain reactive.\n\nSee also\nArtificial Neural Network (ANN)\nSubsumption architecture\nZero instruction set computer (ZISC)\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Winner-take-all_in_action_selection"
    },
    {
        "title": "Workplace impact of artificial intelligence",
        "text": "The impact of artificial intelligence on workers includes both applications to improve worker safety and health, and potential hazards that must be controlled.\nOne potential application is using AI to eliminate hazards by removing humans from hazardous situations that involve risk of stress, overwork, or musculoskeletal injuries.  Predictive analytics may also be used to identify conditions that may lead to hazards such as fatigue, repetitive strain injuries, or toxic substance exposure, leading to earlier interventions.  Another is to streamline workplace safety and health workflows through automating repetitive tasks, enhancing safety training programs through virtual reality, or detecting and reporting near misses.\nWhen used in the workplace, AI also presents the possibility of new hazards.  These may arise from machine learning techniques leading to unpredictable behavior and inscrutability in their decision-making, or from cybersecurity and information privacy issues.  Many hazards of AI are psychosocial due to its potential to cause changes in work organization.  These include changes in the skills required of workers, increased monitoring leading to micromanagement, algorithms unintentionally or intentionally mimicking undesirable human biases, and assigning blame for machine errors to the human operator instead.  AI may also lead to physical hazards in the form of human–robot collisions, and ergonomic risks of control interfaces and human–machine interactions.  Hazard controls include cybersecurity and information privacy measures, communication and transparency with workers about data usage, and limitations on collaborative robots.\nFrom a workplace safety and health perspective, only \"weak\" or \"narrow\" AI that is tailored to a specific task is relevant, as there are many examples that are currently in use or expected to come into use in the near future. \"Strong\" or \"general\" AI is not expected to be feasible in the near future, and discussion of its risks is within the purview of futurists and philosophers rather than industrial hygienists.\nCertain digital technologies are predicted to result in job losses. Starting in the 2020s, the adoption of modern robotics has led to net employment growth. However, many businesses anticipate that automation, or employing robots would result in job losses in the future. This is especially true for companies in Central and Eastern Europe. Other digital technologies, such as platforms or big data, are projected to have a more neutral impact on employment. A large number of tech workers have been laid off starting in 2023; many such job cuts have been attributed to artificial intelligence.\n\nHealth and safety applications\nIn order for any potential AI health and safety application to be adopted, it requires acceptance by both managers and workers.  For example, worker acceptance may be diminished by concerns about information privacy, or from a lack of trust and acceptance of the new technology, which may arise from inadequate transparency or training.  Alternatively, managers may emphasize increases in economic productivity rather than gains in worker safety and health when implementing AI-based systems.\n\nEliminating hazardous tasks\nAI may increase the scope of work tasks where a worker can be removed from a situation that carries risk.  In a sense, while traditional automation can replace the functions of a worker's body with a robot, AI effectively replaces the functions of their brain with a computer.  Hazards that can be avoided include stress, overwork, musculoskeletal injuries, and boredom.\nThis can expand the range of affected job sectors into white-collar and service sector jobs such as in medicine, finance, and information technology.  As an example, call center workers face extensive health and safety risks due to its repetitive and demanding nature and its high rates of micro-surveillance. AI-enabled chatbots lower the need for humans to perform the most basic call center tasks.\n\nAnalytics to reduce risk\nMachine learning is used for people analytics to make predictions about worker behavior to assist management decision-making, such as hiring and performance assessment.  These could also be used to improve worker health.  The analytics may be based on inputs such as online activities, monitoring of communications, location tracking, and voice analysis and body language analysis of filmed interviews.  For example, sentiment analysis may be used to spot fatigue to prevent overwork.  Decision support systems have a similar ability to be used to, for example, prevent industrial disasters or make disaster response more efficient.\nFor manual material handling workers, predictive analytics and artificial intelligence may be used to reduce musculoskeletal injury.  Traditional guidelines are based on statistical averages and are geared towards anthropometrically typical humans.  The analysis of large amounts of data from wearable sensors may allow real-time, personalized calculation of ergonomic risk and fatigue management, as well as better analysis of the risk associated with specific job roles.\nWearable sensors may also enable earlier intervention against exposure to toxic substances than is possible with area or breathing zone testing on a periodic basis. Furthermore, the large data sets generated could improve workplace health surveillance, risk assessment, and research.\n\nStreamlining safety and health workflows\nAI can also be used to make the workplace safety and health workflow more efficient. Digital assistants, like Amazon Alexa, Google Assistant, and Apple Siri, are increasingly adopted in workplaces to enhance productivity by automating routine tasks. These AI-based tools can manage administrative duties, such as scheduling meetings, sending reminders, processing orders, and organizing travel plans. This automation can improve workflow efficiency by reducing time spent on repetitive tasks, thus supporting employees to focus on higher-priority responsibilities. Digital assistants are especially valuable in streamlining customer service workflows, where they can handle basic inquiries, reducing the demand on human employees. However, there remain challenges in fully integrating these assistants due to concerns over data privacy, accuracy, and organizational readiness.\nOne example is coding of workers' compensation claims, which are submitted in a prose narrative form and must manually be assigned standardized codes.  AI is being investigated to perform this task faster, more cheaply, and with fewer errors.\nAI‐enabled virtual reality systems may be useful for safety training for hazard recognition.\nArtificial intelligence may be used to more efficiently detect near misses.  Reporting and analysis of near misses are important in reducing accident rates, but they are often underreported because they are not noticed by humans, or are not reported by workers due to social factors.\n\nHazards\nThere are several broad aspects of AI that may give rise to specific hazards.  The risks depend on implementation rather than the mere presence of AI.\nSystems using sub-symbolic AI such as machine learning may behave unpredictably and are more prone to inscrutability in their decision-making.  This is especially true if a situation is encountered that was not part of the AI's training dataset, and is exacerbated in environments that are less structured.  Undesired behavior may also arise from flaws in the system's perception (arising either from within the software or from sensor degradation), knowledge representation and reasoning, or from software bugs.  They may arise from improper training, such as a user applying the same algorithm to two problems that do not have the same requirements.  Machine learning applied during the design phase may have different implications than that applied at runtime.  Systems using symbolic AI are less prone to unpredictable behavior.\nThe use of AI also increases cybersecurity risks relative to platforms that do not use AI, and information privacy concerns about collected data may pose a hazard to workers.\n\nPsychosocial\nPsychosocial hazards are those that arise from the way work is designed, organized, and managed, or its economic and social contexts, rather than arising from a physical substance or object.  They cause not only psychiatric and psychological outcomes such as occupational burnout, anxiety disorders, and depression, but they can also cause physical injury or illness such as cardiovascular disease or musculoskeletal injury.  Many hazards of AI are psychosocial in nature due to its potential to cause changes in work organization, in terms of increasing complexity and interaction between different organizational factors.  However, psychosocial risks are often overlooked by designers of advanced manufacturing systems.\n\nChanges in work practices\nAI is expected to lead to changes in the skills required of workers, requiring training of existing workers, flexibility, and openness to change.  The requirement for combining conventional expertise with computer skills may be challenging for existing workers.  Over-reliance on AI tools may lead to deskilling of some professions.\nWhile AI offers convenience and judgement-free interaction, increased reliance—particularly among Generation Z—may reduce interpersonal communication in the workplace and affect social cohesion. As AI becomes a substitute for traditional peer collaboration and mentorship, there is a risk of diminishing opportunities for interpersonal skill development and team-based learning. This shift could contribute to workplace isolation and changes in team dynamics.\nIncreased monitoring may lead to micromanagement and thus to stress and anxiety.  A perception of surveillance may also lead to stress.  Controls for these include consultation with worker groups, extensive testing, and attention to introduced bias.  Wearable sensors, activity trackers, and augmented reality may also lead to stress from micromanagement, both for assembly line workers and gig workers.  Gig workers also lack the legal protections and rights of formal workers.\nThere is also the risk of people being forced to work at a robot's pace, or to monitor robot performance at nonstandard hours.\n\nBias\nAlgorithms trained on past decisions may mimic undesirable human biases, for example, past discriminatory hiring and firing practices.  Information asymmetry between management and workers may lead to stress, if workers do not have access to the data or algorithms that are the basis for decision-making.\nIn addition to building a model with inadvertently discriminatory features, intentional discrimination may occur through designing metrics that covertly result in discrimination through correlated variables in a non-obvious way.\nIn complex human‐machine interactions, some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead.\n\nPhysical\nPhysical hazards in the form of human–robot collisions may arise from robots using AI, especially collaborative robots (cobots).  Cobots are intended to operate in close proximity to humans, which makes impossible the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots.  Automated guided vehicles are a type of cobot that as of 2019 are in common use, often as forklifts or pallet jacks in warehouses or factories.  For cobots, sensor malfunctions or unexpected work environment conditions can lead to unpredictable robot behavior and thus to human–robot collisions.\nSelf-driving cars are another example of AI-enabled robots.  In addition, the ergonomics of control interfaces and human–machine interactions may give rise to hazards.\n\nHazard controls\nAI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions, as well as information privacy measures.  Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues. Proposed best practices for employer‐sponsored worker monitoring programs include using only validated sensor technologies; ensuring voluntary worker participation; ceasing data collection outside the workplace; disclosing all data uses; and ensuring secure data storage.\nFor industrial cobots equipped with AI‐enabled sensors, the International Organization for Standardization (ISO) recommended: (a) safety‐related monitored stopping controls; (b) human hand guiding of the cobot; (c) speed and separation monitoring controls; and (d) power and force limitations.  Networked AI-enabled cobots may share safety improvements with each other. Human oversight is another general hazard control for AI.\n\nRisk management\nBoth applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management.  As with all hazards, risk identification is most effective and least costly when done in the design phase.\nWorkplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate and does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs.  Proxies for skill content include educational requirements and classifications of routine versus non-routine, and cognitive versus physical jobs.  However, these may still not be specific enough to distinguish specific occupations that have distinct impacts from AI.  The United States Department of Labor's Occupational Information Network is an example of a database with a detailed taxonomy of skills.  Additionally, data are often reported on a national level, while there is much geographical variation, especially between urban and rural areas.\n\nStandards and regulation\n\nAs of 2019, ISO was developing a standard on the use of metrics and dashboards, information displays presenting company metrics for managers, in workplaces.  The standard is planned to include guidelines for both gathering data and displaying it in a viewable and useful manner.\nIn the European Union, the General Data Protection Regulation, while oriented towards consumer data, is also relevant for workplace data collection.  Data subjects, including workers, have \"the right not to be subject to a decision based solely on automated processing\".  Other relevant EU directives include the Machinery Directive (2006/42/EC), the Radio Equipment Directive (2014/53/EU), and the General Product Safety Directive (2001/95/EC).\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Workplace_impact_of_artificial_intelligence"
    },
    {
        "title": "Wumpus world",
        "text": "Wumpus world is a simple world use in artificial intelligence for which to represent knowledge and to reason. Wumpus world was introduced by Michael Genesereth, and is discussed in the Russell-Norvig Artificial Intelligence book Artificial Intelligence: A Modern Approach. Wumpus World is loosely inspired by the 1972 video game Hunt the Wumpus.\n\nReferences\n\nCIS587: The Wumpus World\nHunt the Wumpus",
        "url": "https://en.wikipedia.org/wiki/Wumpus_world"
    },
    {
        "title": "Zeuthen strategy",
        "text": "The Zeuthen strategy in cognitive science is a negotiation strategy used by some artificial agents. Its purpose is to measure the willingness to risk conflict. An agent will be more willing to risk conflict if it does not have much to lose in case that the negotiation fails. In contrast, an agent is less willing to risk conflict when it has more to lose. The value of a deal is expressed in its utility. An agent has much to lose when the difference between the utility of its current proposal and the conflict deal is high.\nWhen both agents use the monotonic concession protocol, the Zeuthen strategy leads them to agree upon a deal in the negotiation set. This set consists of all conflict free deals, which are individually rational and Pareto optimal, and the conflict deal, which maximizes the Nash product.\nThe strategy was introduced in 1930 by the Danish economist Frederik Zeuthen.\n\nThree key questions\nThe Zeuthen strategy answers three open questions that arise when using the monotonic concession protocol, namely:\n\nWhich deal should be proposed at first?\nOn any given round, who should concede?\nIn case of a concession, how much should the agent concede?\nThe answer to the first question is that any agent should start with its most preferred deal, because that deal has the highest utility for that agent. The second answer is that the agent with the smallest value of Risk(i,t) concedes, because the agent with the lowest utility for the conflict deal profits most from avoiding conflict. To the third question, the Zeuthen strategy suggests that the conceding agent should concede just enough raise its value of Risk(i,t) just above that of the other agent. This prevents the conceding agent to have to concede again in the next round.\n\nRisk\nRisk\n        \n        (\n        i\n        ,\n        t\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    U\n                    \n                      i\n                    \n                  \n                  (\n                  δ\n                  (\n                  i\n                  ,\n                  t\n                  )\n                  )\n                  =\n                  0\n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        i\n                        ,\n                        t\n                        )\n                        )\n                        −\n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        j\n                        ,\n                        t\n                        )\n                        )\n                      \n                      \n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        i\n                        ,\n                        t\n                        )\n                        )\n                      \n                    \n                  \n                \n                \n                  \n                    otherwise\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Risk}}(i,t)={\\begin{cases}1&U_{i}(\\delta (i,t))=0\\\\{\\frac {U_{i}(\\delta (i,t))-U_{i}(\\delta (j,t))}{U_{i}(\\delta (i,t))}}&{\\text{otherwise}}\\end{cases}}}\n  \n\nRisk(i,t) is a measurement of agent i's willingness to risk conflict. The risk function formalizes the notion that an agent's willingness to risk conflict is the ratio of the utility that agent would lose by accepting the other agent's proposal to the utility that agent would lose by causing a conflict. Agent i is said to be using a rational negotiation strategy if at any step t + 1 that agent i sticks to his last proposal, Risk(i,t) > Risk(j,t).\n\nSufficient concession\nIf agent i makes a sufficient concession in the next step, then, assuming that agent j is using a rational negotiation strategy, if agent j does not concede in the next step, he must do so in the step after that. The set of all sufficient concessions of agent i at step t is denoted SC(i, t).\n\nMinimal sufficient concession\n\n  \n    \n      \n        \n          δ\n          ′\n        \n        =\n        arg\n        ⁡\n        \n          max\n          \n            δ\n            ∈\n            \n              S\n              C\n              (\n              A\n              ,\n              t\n              )\n            \n          \n        \n        {\n        \n          U\n          \n            A\n          \n        \n        (\n        δ\n        )\n        }\n      \n    \n    {\\displaystyle \\delta '=\\arg \\max _{\\delta \\in {SC(A,t)}}\\{U_{A}(\\delta )\\}}\n  \n\nis the minimal sufficient concession of agent A in step t.\nAgent A begins the negotiation by proposing\n\n  \n    \n      \n        δ\n        (\n        A\n        ,\n        0\n        )\n        =\n        arg\n        ⁡\n        \n          max\n          \n            δ\n            ∈\n            \n              N\n              S\n            \n          \n        \n        \n          U\n          \n            A\n          \n        \n        (\n        δ\n        )\n      \n    \n    {\\displaystyle \\delta (A,0)=\\arg \\max _{\\delta \\in {NS}}U_{A}(\\delta )}\n  \n\nand will make the minimal sufficient concession in step t + 1 if and only if Risk(A,t) ≤ Risk(B,t).\nTheorem\nIf both agents are using Zeuthen strategies, then they will agree on\n\n  \n    \n      \n        δ\n        =\n        arg\n        ⁡\n        \n          max\n          \n            \n              δ\n              ′\n            \n            ∈\n            \n              N\n              S\n            \n          \n        \n        {\n        π\n        (\n        \n          δ\n          ′\n        \n        )\n        }\n        ,\n      \n    \n    {\\displaystyle \\delta =\\arg \\max _{\\delta '\\in {NS}}\\{\\pi (\\delta ')\\},}\n  \n\nthat is, the deal which maximizes the Nash product.\nProof\nLet δA = δ(A,t).\nLet δB = δ(B,t).\nAccording to the Zeuthen strategy, agent A will concede at step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n if and only if\n\n  \n    \n      \n        R\n        i\n        s\n        k\n        (\n        A\n        ,\n        t\n        )\n        ≤\n        R\n        i\n        s\n        k\n        (\n        B\n        ,\n        t\n        )\n        .\n      \n    \n    {\\displaystyle Risk(A,t)\\leq Risk(B,t).}\n  \n\nThat is, if and only if\n\n  \n    \n      \n        \n          \n            \n              \n                U\n                \n                  A\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n              −\n              \n                U\n                \n                  A\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n            \n            \n              \n                U\n                \n                  A\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n            \n          \n        \n        ≤\n        \n          \n            \n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n              −\n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n            \n            \n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {U_{A}(\\delta _{A})-U_{A}(\\delta _{B})}{U_{A}(\\delta _{A})}}\\leq {\\frac {U_{B}(\\delta _{B})-U_{B}(\\delta _{A})}{U_{B}(\\delta _{B})}}}\n  \n\n  \n    \n      \n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        (\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        (\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle U_{B}(\\delta _{B})(U_{A}(\\delta _{A})-U_{A}(\\delta _{B}))\\leq U_{A}(\\delta _{A})(U_{B}(\\delta _{B})-U_{B}(\\delta _{A}))}\n  \n\n  \n    \n      \n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle U_{A}(\\delta _{A})U_{B}(\\delta _{B})-U_{A}(\\delta _{B})U_{B}(\\delta _{B})\\leq U_{A}(\\delta _{A})U_{B}(\\delta _{B})-U_{A}(\\delta _{A})U_{B}(\\delta _{A})}\n  \n\n  \n    \n      \n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        ≤\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle -U_{A}(\\delta _{B})U_{B}(\\delta _{B})\\leq -U_{A}(\\delta _{A})U_{B}(\\delta _{A})}\n  \n\n  \n    \n      \n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle U_{A}(\\delta _{A})U_{B}(\\delta _{A})\\leq U_{A}(\\delta _{B})U_{B}(\\delta _{B})}\n  \n\n  \n    \n      \n        π\n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        ≤\n        π\n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle \\pi (\\delta _{A})\\leq \\pi (\\delta _{B})}\n  \n\nThus, Agent A will concede if and only if \n  \n    \n      \n        \n          δ\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\delta _{A}}\n  \n does not yield the larger product of utilities.\nTherefore, the Zeuthen strategy guarantees a final agreement that maximizes the Nash Product.\n\n\n== References ==",
        "url": "https://en.wikipedia.org/wiki/Zeuthen_strategy"
    }
]